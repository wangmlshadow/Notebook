{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 深度学习 基于Python的理论与实践"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一章 Python入门 略过\n",
    "\n",
    "# Python介绍\n",
    "# 环境配置\n",
    "# 基本数据类型\n",
    "# 流程控制\n",
    "# Numpy\n",
    "# Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第二章 感知机\n",
    "# perception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1\n",
    "# 感知机：接收多个输入信号 输出一个信号\n",
    "# 输入信号x1\n",
    "# 输出信号\n",
    "# 权重w\n",
    "# 阈值theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2\n",
    "# 通过感知机实现简单的逻辑电路\n",
    "# 与门AND  全为1则输出1\n",
    "# 与非门NAND   存在0就输出1   和AND相反\n",
    "# 或门    存在1就输出1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3\n",
    "# 感知机的实现\n",
    "# AND\n",
    "def AND(x1, x2):\n",
    "    w1, w2, theta = 0.5, 0.5, 0.7\n",
    "    tmp = x1*w1 + x2*w2\n",
    "    if tmp <= theta:\n",
    "        return 0.5\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 偏置b\n",
    "# 使用权重和偏置实现AND\n",
    "import numpy as np\n",
    "def AND(x1, x2):\n",
    "    x = np.array([x1, x2])\n",
    "    w = np.array([0.5, 0.5])\n",
    "    b = -0.7\n",
    "    tmp = np.sum(w*x) + b# 矩阵相乘（对应元素相乘，和线性代数的矩阵相乘不同）对矩阵元素求和\n",
    "    if tmp <= 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "# 与非门\n",
    "def NAND(x1, x2):\n",
    "    x = np.array([x1, x2])\n",
    "    w = np.array([-0.5, -0.5])\n",
    "    b = 0.7\n",
    "    tmp = np.sum(x*w) + b\n",
    "    if tmp <= 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "# 或门\n",
    "def OR(x1, x2):\n",
    "    x = np.array([x1, x2])\n",
    "    w = np.array([0.5, 0.5])\n",
    "    b = -0.2\n",
    "    tmp = np.sum(x*w) + b\n",
    "    if tmp <= 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4\n",
    "# 感知机的局限性\n",
    "# 异或门XOR   输入x1 x2相同输出0 不同输出1\n",
    "\n",
    "# 线性和非线性\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.5\n",
    "# 多层感知机\n",
    "\n",
    "# 通过组合AND NAND OR实现XOR\n",
    "def XOR(x1, x2):\n",
    "    s1 = NAND(x1, x2)\n",
    "    s2 = OR(x1, x2)\n",
    "    return AND(s1, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.6\n",
    "# 从XOR到计算机"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第三章 神经网络\n",
    "# 权重的设置 从人工到自动设置   -> 从感知机到神经网络\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1\n",
    "# 输入层\n",
    "# 输出层\n",
    "# 中间层（隐藏层）\n",
    "# 本书从输入层开始 从0开始编号\n",
    "\n",
    "# 激活函数\n",
    "# 神经元 （节点）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2\n",
    "# 激活函数\n",
    "\n",
    "# sigmod函数  h(x) = 1 / 1 + exp(-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 阶跃函数的实现 \n",
    "\n",
    "def step_function(x):\n",
    "    if x > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#画出该函数图像\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "def step_function(x):\n",
    "    return np.array(x > 0, dtype=np.int)\n",
    "\n",
    "x = np.arange(-5.0, 5.0, 0.1)# np.arange(start, end, step) 默认起点为0，默认步长step=1  返回一个从start->end 步长=step的序列\n",
    "y = step_function(x)\n",
    "plt.plot(x, y)\n",
    "plt.ylim(-0.1, 1.1)# 指定y轴范围\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sigmod()函数实现 以及其图形\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "x = np.arange(-5.0, 5.0, 0.1)\n",
    "y = sigmoid(x)\n",
    "plt.plot(x, y)\n",
    "plt.ylim(-0.1, 1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 阶跃函数和sigmod函数比较\n",
    "# 平滑性\n",
    "# 输出值\n",
    "# 宏观相似性\n",
    "# 两者都是非线性函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ReLU函数\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "x = np.arange(-5.0, 5.0, 0.1)\n",
    "y = ReLU(x)\n",
    "plt.plot(x, y)\n",
    "plt.ylim(-1, 6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3\n",
    "# NumPy 多维数组运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 神经网络的內积\n",
    "# 通过矩阵乘法进行神经网络计算\n",
    "# X   W   =   Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.31682708 0.69627909]\n"
     ]
    }
   ],
   "source": [
    "# 3.4\n",
    "# 3层神经网络的实现\n",
    "import numpy as np\n",
    "\n",
    "def identity_function(x):\n",
    "    return x\n",
    "\n",
    "def init_network():\n",
    "    network = {}\n",
    "    network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])\n",
    "    network['b1'] = np.array([0.1, 0.2, 0.3])\n",
    "    network['W2'] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])\n",
    "    network['b2'] = np.array([0.1, 0.2])\n",
    "    network['W3'] = np.array([[0.1, 0.3], [0.2, 0.4]])\n",
    "    network['b3'] = np.array([0.1, 0.2])\n",
    "    \n",
    "    return network\n",
    "\n",
    "def forward(network, x):\n",
    "    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
    "    b1, b2, b3 = network['b1'], network['b2'], network['b3']\n",
    "    \n",
    "    a1 = np.dot(x, W1) + b1\n",
    "    z1 = sigmiod(a1)\n",
    "    a2 = np.dot(z1, W2) + b2\n",
    "    z2 = sigmoid(a2)\n",
    "    a3 = np.dot(z2, W3) + b3\n",
    "    y = identity_function(a3)\n",
    "    \n",
    "    return y\n",
    "\n",
    "network = init_network()\n",
    "x = np.array([1.0, 0.5])\n",
    "y = forward(network, x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.5 输出层设计\n",
    "# 分类问题 回归问题\n",
    "# softmax函数\n",
    "import numpy as np\n",
    "\n",
    "def softmax(a):\n",
    "    exp_a = np.exp(a)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "    return y\n",
    "\n",
    "# 防止溢出\n",
    "def softmax(a):\n",
    "    c = np.max(a)\n",
    "    exp_a = np.exp(a - c)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000,)\n",
      "(10000, 784)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "# 3.6\n",
    "# 手写数字识别\n",
    "# 通过本书提供的mnist.py脚本下载实验数据 需要文件notebookDL的父目录名称为ch01...8\n",
    "# 后面看看怎么修改mnist.py弄掉这个限制\n",
    "# 暂时先把模型跑起来再说\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from dataset.mnist import load_mnist\n",
    "# 第一次调用会下载实验数据\n",
    "# (训练图像，训练标签), (测试图像，测试标签)\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, normalize=False)\n",
    "# 输出各个数据的shape\n",
    "print(x_train.shape)#(60000, 784)\n",
    "print(t_train.shape)#(60000,)\n",
    "print(x_test.shape)#(10000, 784)\n",
    "print(t_test.shape)#(10000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "(784,)\n",
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from PIL import Image\n",
    "\n",
    "def img_show(img):\n",
    "    pil_img = Image.fromarray(np.uint8(img))\n",
    "    pil_img.show()\n",
    "\n",
    "# flatten=True表示读入的图像数据以一列（一维）NumPy数组的形式保存\n",
    "# normalize=False表示不将输入图像归一化到0.0 - 1.0  输入图像的像素会保持原来的0-255\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, normalize=False)\n",
    "img = x_train[0]\n",
    "label = t_train[0]\n",
    "\n",
    "print(label)# 5\n",
    "print(img.shape)# (784, )\n",
    "img = img.reshape(28, 28)# 为了show image，需要将数据变为二维数组的形状\n",
    "print(img.shape)# (28, 28)\n",
    "\n",
    "img_show(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.9352\n"
     ]
    }
   ],
   "source": [
    "# 神经网络的推理处理\n",
    "# 输入层：输入一张图片 24*24 = 728 个神经元\n",
    "# 输出层：输出的结果0-9 共有10个类别\n",
    "# 两个隐藏层：第一个隐藏层50个神经元 第二个隐藏层100个神经元\n",
    "\n",
    "import pickle\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(a):\n",
    "    c = np.max(a)\n",
    "    exp_a = np.exp(a - c)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "    return y\n",
    "\n",
    "def get_data():\n",
    "    (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, flatten=True, one_hot_label=False)\n",
    "    return x_test, t_test\n",
    "\n",
    "def init_network():\n",
    "    with open(\"sample_weight.pkl\", 'rb') as f:# 假设学习已经完成 直接读取直接已经学习到的模型\n",
    "        network = pickle.load(f)\n",
    "    return network\n",
    "\n",
    "def predict(network, x):\n",
    "    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
    "    b1, b2, b3 = network['b1'], network['b2'], network['b3']\n",
    "    \n",
    "    #print(W1.shape)  (784, 50)\n",
    "    #print(W2.shape)  (50, 100)\n",
    "    #print(W3.shape)  (100, 10)\n",
    "    \n",
    "    a1 = np.dot(x, W1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    a2 = np.dot(z1, W2) + b2\n",
    "    z2 = sigmoid(a2)\n",
    "    a3 = np.dot(z2, W3) + b3\n",
    "    y = softmax(a3)\n",
    "    return y\n",
    "\n",
    "x, t = get_data()\n",
    "\n",
    "#print(x.shape) (10000, 784)\n",
    "#print(x[0].shape) (784, )\n",
    "\n",
    "network = init_network()\n",
    "accuracy_cnt = 0\n",
    "for i in range(len(x)):\n",
    "    y = predict(network, x[i])\n",
    "    p = np.argmax(y)# 获取概率最高元素索引\n",
    "    if p == t[i]:\n",
    "        accuracy_cnt += 1\n",
    "\n",
    "print(\"Accuracy:\" + str(float(accuracy_cnt) / len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.9352\n"
     ]
    }
   ],
   "source": [
    "# 批处理\n",
    "x, t = get_data()\n",
    "network = init_network()\n",
    "\n",
    "batch_size = 100# 批处理数量\n",
    "accuracy_cnt = 0\n",
    "for i in range(0, len(x), batch_size):\n",
    "    x_batch = x[i:i+batch_size]\n",
    "    y_batch = predict(network, x_batch)\n",
    "    p = np.argmax(y_batch, axis=1)\n",
    "    accuracy_cnt += np.sum(p == t[i:i+batch_size])\n",
    "\n",
    "print(\"Accuracy:\" + str(float(accuracy_cnt) / len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第四章\n",
    "# 从训练数据中自动获取最优权重参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1\n",
    "# 从数据中学习\n",
    "# 求解一个问题的答案 使用算法、机器学习、深度学习得到答案的过程的区别 显示了这三种方式的区别\n",
    "# 泛化能力\n",
    "# 拟合问题 过拟合 欠拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2\n",
    "# 损失函数\n",
    "# 损失函数表示神经网络性能的恶劣程度 对其加上负号 就可以来表示 性能有多好\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 均方误差\n",
    "def mean_squared_error(y, t):\n",
    "    return 0.5 * np.sum((y - t)**2)\n",
    "\n",
    "# 交叉熵误差\n",
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7 # 注意此处为0.0000001 用于防止出现np.log(0)时出现的无穷小 防止出现-inf后无法继续计算\n",
    "    return -np.sum(t * np.log(y + delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "# mini-batch 小批量 从大量的训练数据中选出一批数据\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "print(x_train.shape)\n",
    "print(t_train.shape)\n",
    "\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size)# 随机选取\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]\n",
    "\n",
    "#print(batch_mask)\n",
    "\n",
    "# mini-batch版交叉熵误差\n",
    "def cross_entropy_error(y, t): # one-hot-label=True\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y + 1e-7)) / batch_size\n",
    "\n",
    "def cross_entropy_error(y, t): # one-hot-label=False\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为什么要设定损失函数作为指标 而不是识别精度\n",
    "# 为什么使用sigmod() 而不是阶跃函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# 4.3\n",
    "# 数值微分\n",
    "# 通过数值方法近似求解函数导数\n",
    "def numerical_diff(f, x):\n",
    "    h = 10e-50  # 模拟求导过程中的无穷小变量\n",
    "    return (f(x + h) - f(x)) / h\n",
    "\n",
    "def x_2(x):  # f(x) = x^2   导数为 2x\n",
    "    return x**2\n",
    "\n",
    "#res = numerical_diff(x_2, 1)# y = x^2 在 x=1出导数应该为2\n",
    "#print(res)# 输出结果为0\n",
    "\"\"\"\n",
    "上述通过数值微分求导的函数存在的问题：\n",
    "     h = 10e-50这个极小值来模拟无穷小 但是在Python中使用float存储这个值时 可能这个值因为舍入误差 变成了0\n",
    "     改进：使 h = 1e-4 即0.0001\n",
    "     \n",
    "     导数的意思使在点x出的斜率 而数值微分模拟的是 x+h 和 x 之一段的斜率 而h不能无穷小\n",
    "     改进：使用中心差分 f(x+h) - f(x-h)\n",
    "\"\"\"\n",
    "#print(np.float32(10e-50))# 输出0.0\n",
    "\n",
    "# 改进后的函数\n",
    "def numerical_diff(f, x):\n",
    "    h = 1e-4\n",
    "    return (f(x+h) - f(x-h)) / (2*h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "def numerical_diff(f, x):\n",
    "    h = 1e-4\n",
    "    return (f(x+h) - f(x-h)) / (2*h)\n",
    "\n",
    "def function_1(x):  # y = 0.01*x^2 + 0.1*x\n",
    "    return 0.01*x**2 + 0.1*x\n",
    "\n",
    "x = np.arange(0.0, 20.0, 0.1)\n",
    "y = function_1(x)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.plot(x, y)\n",
    "\n",
    "derivative_5 = numerical_diff(function_1, 5)# 在 (5, function_1(5)) 的导数\n",
    "y_2 = [derivative_5 * (i - 5) + function_1(5) for i in x]\n",
    "plt.plot(x, y_2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPcAAADuCAYAAADlVZEAAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAACqq0lEQVR4nOz9d5DcaXrfCX7e9/259FlZDoWC9+hudKPRAHqGMzQaShRvjjIh6nQXknirOJ60p9DtaWW4S23sxiq0Uoi6uNCZ1d6eqNWd7NIvRXFkyOFoSGp6ugfobviCRxXK2/TmZ9/3/sjM6gIapgpmGtNT34iKzMrMn3+/7/O8jxXGGLaxjW18/iA/6xPYxja28XKwTe5tbONzim1yb2Mbn1Nsk3sb2/icYpvc29jG5xTb5N7GNj6nsJ7y/bafbBvbePkQL2On25J7G9v4nGKb3NvYxucU2+TexjY+p9gm9za28TnFNrm3sY3PKbbJvY1tfE6xTe5tbONzim1yb2Mbn1Nsk3sb2/icYpvc29jG5xTb5N7GNj6n2Cb3NrbxOcU2ubexjc8ptsm9jW18TrFN7s8IWmu2K89u42Xiafnc23jBMMYQxzHtdhsA27axLAvLshBCIMRLSe3dxvchxFOkx7ZoeYHQWhNFEUmSUKvV8DwPIQTGmHVi27aNbdsopbbJ/v2Dl/KQt8n9XYAxhiRJiKKIKIq4fv06URQRhiFKKQYGBigWi+Tz+Qe2k1JiWdY62aXcXkV9TrFN7u9FGGMekNYTExMcOHCAUqmEEIIwDKlWq1QqFer1OpZlrZM9l8s9sK9tsn9usU3u7zVorQnDEK01U1NTrK6ucuLECVKpFGEYPlLlDoKASqVCpVKh0WjgOM4DZN/4vMIwxHVd0un0Ntm/t7FN7u8V9I1mcRwThiFXr14ll8tx+PBhpJQYYx5L7ofh+/462ZvNJq7rrpN9dXWVdDrN8PAw0JXsfQPdNtm/p7BN7u8F9ImrtWZtbY1bt25x9OhRhoaGPvWbZzGWdTqddbJXKhU8z2N0dJRisUg6nX7gt0qpdRW+b43fxiuJbXK/6kiShNnZWYaGhrhz5w7NZpMTJ07guu6nfhuG4bqV/FkxOTmJZVlIKalUKrRaLTKZDMVikWKxSCqVAlg/Tp/sfcm+TfZXBtvkflWxUQ3/9re/jWVZjI6Osm/fvscS6EWQe2pqilQqxejo6Pp5tNvtdanebrfJZrPrZPc874Htt8n+yuCl3PjtIJbnRN93rbVmcXGRdrvN2bNnKRaLm96H/T/9PNH/8S9s+dgPk1EIQSaTIZPJsGvXLowxtFotKpUKd+/exfd9stns+prddV1831/ffpvsny9sS+5nxEbfdRzH3Lx5kyRJaLVafOlLX3rq9uuSu9kge/Qw7d/6HfSJE1s6h/v376+vuTd7zo1GY931FgQBuVxuneyO4/Dhhx/yzjvvANtk/y5iW3K/KtiohjebTa5evcqePXsYHx/n/fff39K+1IffAc/D/Tv/HZ1f/OVnOpfNQghBPp8nn8+zZ88etNY0Gg0qlQo3btwgDEOCIGB1dZVisYgQgk6ns07qfpjsNtm/N7BN7i1io+96dnaW+fl53nzzTbLZ7DPtz/r9b5C8cxLrd76O+uB9ki98cdPbPi+5pJQUCgUKhQLQvbbvfOc7tNtt5ufnieOYfD7PwMAAhUIBIQRRFD1A9r41fpvsrx62yb1JPBxCOjExgeu6nD17FqXUM+/X+vbvI9bW0END2L/2P2+J3C8a/Qi4/fv3A13rf71ep1KpMDs7S5IkFAqFB8gex/H69n2y9y3422T/bLFN7k1go++6Wq1y/fp1Dh06tOm17uMg791Gzs8CkLz1DtZ7v4P1u79N/CM/9iJO+7nRj3sfGBgAWA+hrVQqTE9PY4x5gOxJkqyTXQjxgBq/TfbvPrbJ/RRsVMMnJycpl8ucOnVq3Yf8PJCXP1p/L0SIKZVw/sH/lfjLXwHr1Xs0SilKpRKlUgmAOI7XyT41NYUQYt3tVigUiOOYKIq4d+8eBw4ceCC9dZvsLx+v3gh6RWCModlsAl2JdfXqVYrFIqdPn35qWOdm/dfO+d8hOfYa6sYEplTADOWwPvgO1td+lfiP/+82fZ6fFSzLYnBwkMHBQaBL9mq1SrlcZnJyEiEEAwMDrK6usn///vUlDXwi2TcmwWyT/cVim9yPQF8Nv3v3Lo7jsLi4yLFjx9YH8ZOwMT/7iWg3URMfkwzt6W7XKiPX5kj2H8R+/+vEf/iPQCrz1GO9SrAsi6GhofVQ2yiKqFarzM3NceHChQfU/Fwu9ymyPxwX/6pd3/catsn9EPqqpNaaSqWCEIIzZ87gOM6mtu8nhjwN1sffRsQx1uI94i99GWv2CgBmqICauorza/+Y8M/+X57rWj5r2LbN8PAwU1NTnDlzZj29dWlpidu3b6+ntw4MDJDNZtddcRsLV2xXqXl2bJO7h42+63a7zZUrV7Btmz179mya2PCJ5H4SlpeXSd77Ovt6/4cFGzUvEVojPAu9/zDO1/4l0Y/9JGZk/Nkv6hWD4ziMjIwwMjICfJLeOj8//0B668Nkh08y3rar1Gwe2+TmwRDShYUF7t+/z+uvv87KysqW9yWEQGv92OPcvn2bZqPOu9V76EIRWatiZEht32GK927Sdh1c00ImMc7X/gXB/+G/fN7Le2Xhui47duxgx44dwCfprbOzszSbTTzPWzfQ9ckehiHQVfktyyKbzW6ntz4G39fkfjiE9MaNGwCcPXsWy7JYW1t7LFEfh8dJ7k6nw+XLlxkeHuadgoMqLxK/cQJx7jzp9grGy2BsF8+0sBsrVPYcwr79Ecv/8d/hnPgi+Xz+kQP481RB1fM8xsbGGBsbAz5Jb52ZmaHZbJJOp9fJXq1WEUKs/3a7Ss2n8X1L7o3ljxqNBteuXWPv3r2Mj3+iBm92/bwRjyL3ysoKt27d4rXXXmNgYAD1y/89AGr2BvGb72CvXEdEAdGZd7FnLwNQsAOQhl2//wtcHtzLrVu31gs19NXWz7tamkqlSKVS7Ny5E2PMOtnv379PtVpdz3Lr57JvlOzbZP8+JfdG3/XMzAwLCwu89dZbZDIPWqefpGI/DhvJ3VfDG43GA0Y5sTbdfY0j2FGEnvYvshYmk0e06uihUchlcK9/zIm1G0Q/+CfWB/f09DTNZhMhxHpKZyqV+lyTXQhBOp0mnU4zPj7O5OTkemTg5OTkenrrxoy3h8n+/Val5vuK3BuNZlEUcfXqVdLpNO++++4jH7aUcsvk7kt73/e5fPkyg4ODvPPOO+vEE8v3kfVpjGUj4gjVWSDZuQc1Pw22JjlwBOvKh5BJo1ZukYzsxrp9nvjUV0hlig9Isnv37tHpdLhz5w6+769neA0MDDyyQMTnDalUiuHh4U+lt/bvx8NkD4Jg3UD3/VCl5vuG3BtDSPtZUIcPH1633D4Kzyq519bWmJ6e5vjx4+vRXH3I6+8h6qvEb7yNWFhAtcsko4cwywuoxjzoCD08juysIjCYQhbRWcX95v8P/yf+ygPHcV13newbM7wmJiaI43g9NHRgYADrFYx4ex5orR+YkPtaTDabZffu3Q+kt966detT6a1CiM99Lvvn64k/BnEcMzU1RaFQYHFxkVqtxjvvvPOpyiQPQ0q5HmSxGfQHVKfT4fTp04+UnvLat7qvzQWSHTtR5ZuopTtEJ7+AvdRdb8d7xrEXusY9GTXQI+PY175J9OYfJNnzYM53fwmwMcNr3759D8SB379/fz00tB8H/jzJLq8CHib3w9hMems/420j2WdmZhgaGiKbzX7Pk/1zTe6Nani1WmV6epodO3Zw+vTpTT2srajlvu9z5coVhBAcP3780WpxfQXjdW+5bNdI9h6Fcu+7godZlgijEVmXZNcR1OytrhSPyxgnhf3xb5KMHwfV3ceTruHhOPB+tNjq6ip3795dDyAplUrkcrmXNnhfljXfGLOldfOj0lv7GW8b01ubzSbFYhGt9fe8ZP/cknuj73p5eZmVlRX279+/ns64GWwmIAVgbW2NGzducOzYMRYWFh77O3X3fYQMMUJisgNYnWl0Jo9s1VH+AsmhE1i3LyFkjHAjjLLAEshOg3jXYUTcwLn8G4Rv/+Smr6GPfrRYvwxyP4Bko095YGCAJEmeu7bbw3gZRNBaP9d+pZTrbjX4JL21XC5z586dT2W8PVy44nuB7J87cm/0XSdJwu3btwmCgJ07d37KGv40PE1yG2O4e/culUplXQ1fXFx87IQgb7+HrC2gD59Aa4HVmSTZfwSzsIgKayBBZ4rI1jzCaOIDJ1Cdbkqoqk+iB3fjXP4N4v1fQBefL3JtYwDJRjdTGIacO3du3RhVKpWeunx5El70RNHH09TyraIf955KpXj99deRUn4qvXVjxtv3Atk/V+Te6LtutVpcvXqV8fFxjh8/zr17915YQAp0Jd/ly5fXM8X6D/OxvvFWGbF4q7vfuIqwu5Vb1Npd4rGjqMoNpF8jOvIm9kJ37S28BJOkELGPHhiHtIKqwfn4l/H/wF/e0rU87Tr7bqb5+XlOnz5Ns9n81Pq0VCqt11rbLJ5Xwj5pvy/DnZUkCVLKLae3aq0fIPuv/Mqv8BM/8RPrQTafBT435N7ou56fn2dmZoY33nhjvbnes7q1HrVNXw1/uNkAPN7CrmbPoXcfQ01PQBzC+CA0ZxEmgYE0VHrbOwadG0Q21jCZHCabRk5WMNkCVnCfePwYQnewJ78J7rGXsqYVQpDL5cjlcuvGqL7KOjs7i9aaQqFAqVSiUCg80RL/siT3d1sj2Gx6a7/t02/+5m/yla985ZnOQQgxBTSABIiNMaeFECXgl4B9wBTwp4wxlSft53ue3BuNZnEcMzExgWVZ6yGkfTyPz3rjse7du8fa2tpjre2Pk/Zy+jtgRd319sheZLiMsVMY28Nu3iYZPYBcnux+PrITGmsI00QFayQjBxF0AFDBHElmB+6NX8c59p8SOANbuqZnwcPr074U6w/s/velUulTYbLfK2r5RmzmfB+X3rq6uspf+kt/iWvXrvEP/+E/5I/8kT/CF7/4xWdxRf4BY8zqhv9/FviGMebnhBA/2/v/iYkH39Pk3ui7rtfrTExMsH///keqQkqpZ1LL+9uEYcjly5fJ5/NPLNjwSHI3V5BrdwDQ+14HZRB+i3jvUfBjZDIDKYku7UYlFWhMEe85geVPdbfPKKS/1r1mNwtZB5oRw3O/zey+P7Wla3oReFiK9VM5FxcXuXXrFo7jUCqVGBgYwLbt7zlyPws2Gix/4Rd+ga985SucOHGCX/iFX+Ctt976VHvmZ8AfA36k9/6fAr/L55XcfaOZ1pr79++zvLzMyZMnP9Uvq4+t+qz722itKZfLXL9+nSNHjqxbmx+HR5FbrlzBiK6bCxVAWANAtWZIMjugBaq9RDTyOmqtq2mZXAodWkgdYzJ54vwA9uJ1dG4EGc6RjBxBhhED9Q+B3Vu6rheNh1M5+9ld09PTNBoN4jhmbm5u3WD1Isj+qpH7YcRxzJ/+03+an/qpn3qWzQ3w20IIA/xDY8zPA6PGmL4rZhF4agG/7zlyP66D5tmzZ5/4sJ9FLQdoNBrcvn17U0Ev8Ghyq7lvYcaOIeYnMJkiZmgUOXsVoxz0QAFaXYu4dn20lULGHYwVEY0dw527irYlIpwjye7AKCABkSxhnAGGqu/ht97FZHZu+dpeFjZmd7XbbW7evIkx5oGw0L5kf9Yw2Ved3MDznN+XjTFzQogR4OtCiBsbvzTGmB7xn4jvKXJrrQmCgIsXL7Jnzx5u3ry5KWkKWyd3GIZcv36dJEn44he/uOkH9fA6XTRmkc05jFvASBtsA8EikXRoqDy59n389AiW0YhwmWjsIM78bUy0AlKRZIYQ0QoCQ5JLo6LeMkwqwpRFqqFxJn8d/7W/gJD2pq/vuwnbttm1a9d6DHij0aBcLjMxMUEURevGuWKxiG1v/hpetLr/ooyTxpjn2pcxZq73uiyE+HXgLLAkhBgzxiwIIcaA5aft53uC3A/7rqvVKlrrx4Z4PgpbIXc/Pnvv3r0sLS1taQb+lLV85VL386BGsut18O8hBUQ7DpHVISJukmRTdNqaDAF0pmkO7MNlHrQmHD6IXe+t112HOLMXr3yLJLMDKZZppHfjSoFZ/HeInX900+f5KLwMy/vDBrWNYaH9MNm+JX6jP7lvif9uhsk+0fiXJLDFc3mWyUcIkQGkMabRe/9jwN8C/jXwnwA/13v9jaft65Un90Y1fGOI58ZMq81gM+Q2xjA1NcXy8jKnTp1CSsni4uKWznejWm6MJq5fxrLSyLhNPWqTlg62CVFJhchykDG48Rpm5BDUqwgMrbSH2+7ur65D7MJe8rX7RG6GtlnC9gaJbBtiMNYakbMbU/4OJnsIkX9tS+f7svE0a/nDtdH7LqZ+mGz/+36Y7MtUxR+n6ovp+8jZGZIf+PKm9vOcHoJR4Nd721vA/2yM+fdCiPPALwshfhq4DzzVkvpKk7vvuzbGsLS0xL1793j99deZmJjY8s17GrnDMOTKlStkMhnOnDmzboB7nsAXU7sJ/jKdwT1klm5ARhCnD2EvTRBnR2mlLIqLZbRTpOm2KUq76/f2anS8Q6TKd9BuSJsEVzg0wjLK1tQyOWzdNcoZoah7CdnARi//B0jvRFnFLZ1zH98Nyf00POxiCsOQcrm8XmfNdV1KpdJLCZNNkuTTmkK7hfdf/Qydf/Dzm95Pp9N55rr2xph7wFuP+HwN+NGt7OuVJPdGNVxrzY0bN4jjmLNnz25pTbYRTyJ3tVrl2rVrn0oBfd5KLHr1YwASf44gO46QNaKgjeMW8W1DEs4T5HaCncEkS7SHDpFqNzDU6MgGVmYniAYKaO88iupMAdBBUzdpSjTpiAKxqRHm96A0+Cu/Tm7H/x4hnk2dfRnr2OeRto7jPFBnrdPpUC6X18NkM5nMunHueRtFfEpya433N38GPbYDttCSudlsbjnU+WXglTM39n3XURTRbDY5d+4chUKBt95665mJDY8md18Nv3HjBm+//fancrufpxJLHDUpd7o2DyUSwpGeNdtE+KVxgqRbfqWddmmrbrWQIFmgne76Q41u0xzYsb7fyJLEmV0A2LkhdDEglEVC1b0nbZZYSxKScJ7W2m9t6ZxfJl60dE2lUoyPj5NKpTh79iz79u0jjmNu3brFuXPnuH79OktLS+sVWLaChyW3+//6u4i1FczRw1vaT6vVeubGkC8Sr5Tk3hhCOjc3x9zcHCdOnCCXy33qt1sdNA+TO4oirly5sj5IHrnW2mRW2MPH6XQ63L3ymwy7q8SZMazWAnWrQcobQvqrRLbCqBGEv4w2IWF2CKtRAZNQz3l4vaDCpu3jpsZQnQVCFRPaMfmOQyhCwNDMpRBJ75qETZAPsJpZ/Nodplb/DWnn+COjxh6HlxFN9rJiy+HBAg0bc7bL5TJzc3MkSbKew14sFp8aJbZRcltf/1dYv/8N9NgY8Rd/aEvn1Wq1XgnJ/UqQ++EQ0mvXruE4zmM7aG66q8cGbCR3rVbj2rVrHDx48InN/J5lULZare6ktHMJYghTKRQ7iUwdkR0j7a/SVhGJm8LzQXtDtGWTnHSRdpG2WcTK7sXqrOFTIU4VyYUZOqYC2uAP7CcOpkFAohShlScTt5HOMEZWCHNZ0jpDnjs4Zv961Fh/rVoqlUin09+1rKWXMWE8NutuQ872/v371z0rG5M9+sa7QqHwqQmvL7nVh7+P9Vv/ClIG0WiiX3tzS+e3Te4eNoaQ1mo1JiYmOHjw4Poa61FQSq1n72wWUkqSJOH+/fssLCw8MZrtWdBX8ZeWlhjf5UKqCLUV4mCRdu4wJPcJowWc4mF8usS3M3vwrRBt2oT53VjaAG1adkBOjIBYIdY1guJ+iCYBiJTApHchO7MkMksnXSfrj5P01PNQNMEbAj9BW9/h4JE/hS0z62vVe/fu0W63yeVy62TvZ3m9LCJ+VvtUSj0QJhtFEZVKheXlZW7fvo1t2+vr9Vwuh9aazOxdvH/yc+jRPSAlyYmTsMXzbzab22p5v1Bhnxirq6ub6qDZJ/dW1uB9lS2bzT41mm2riOOYK1eu4HkeBw8eJAi/SYcyaekgjKGa1tgNgcDQSWcx/hICCBzwTRMBdPQywhoEDYlp0UyPQtBdl7ccg2IQojVCS9KSPgNhilB1NZGy28HTgAEpPNbkAoPuHoSOmfH/LftSf3x9rTo+Pv5AIMnVq1fX1ddCofDCLeYvS9V/ludn2/anOp70M90ajQa5xgqHv/krxPv3Y1+7Tnzg+DP1S+9XYv2s8ZkY1Pp515cuXSIMQz7++GOSJOHMmTObsnhuNdqsVqvx0UcfYds2r7322gsldqPR4Ny5c+zYsYPjx48jZIQQ02jdQRf2ITK78PUqIrMPgIpqrL+P7QxkunHhyirRTnXDW6XwWFNLCHsIUDRElZbngbBpihoJHTrZcUKr24VUqjTNlAtIlFUCIVhTK0R2Gl+vMOf/zoNRc71Akn379nHq1ClOnTpFqVSiUqnQbDa5cOECU1NT1Ov15yb7q0Tuh+G6LmNjY7z22mu8e2AXp775PyEtQdhq4DsefthibueBB8otbQavirX8uy65N5Y/Wltbo1arbbqDZh99yf00GGOYmZlhbm6OkydPcunSpec59U9hfn6eqampB4x+gbxHLArYVOgkywi3m+zflk2y3jihqKMFZIVN0/IJREhBZUnsNB3WcL3dSG0wokzbtckxghY1Amp42YNoMwNAoiSxHsMyCwiVoyVX8bzdxKZ3X4SgbPtkoiJtU2Fav89e9QOPvI6++prP52m327z22mvrJZgajQbpdHpdhd+qu+lVJncfor6C94t/D390N8UbF4nH3kDYCmHl8S2XhV7Bio3VZJ+kNX7fWcsf9l3fuXOHKIp49913t1zGZzOSO45jrl69im3bjzXMPSv6vvcwDB/IGzdG0xE3MbbCjkCqLJ10HtprxLpJJ30QkjqxaZHkDuKLeQCi1Ai+qQPQUC082bUFBKaG5e2BXsBK2xbIZBCdrBFLRSPVIN/MEooYgDWrTE4PgwZH5KnINonjktcFlvV1bFLsVG8/9foeLsHUbrcpl8vrJYL7VVmeNsi79+TVtsCLxhqpX/6bGNtD+G2MVCAjcFPwzg+xd+9e9u7du24T6rc36ofJ9i3xG8dXq9XaVL7Dy8Z3hdwbyx+1222uXr3K6OgomUzmmXzXT5Pc9Xqdq1evsm/fPnbufLHZUv2eX6Ojo101fMMgq0eTaFpggbR2kUibul4ir/IY7bNq1UjpFNp06NgSmWTQpkUsNUYVIV4CDC0vB0EZEKw4TXJhkURXqVktpGWTadvUZQMjNC03gxJd9dwTRRbcFqPBAEqkgTVCEdC2bWRss6ivYWSacXH0kdf2KCIKIchkMmQyGXbv3v2pQQ48EAv+sER9WQa1FyG5RWuN1C/+1+jsIPbdCwRylM6xt0hPfEx07F2S05+4wKSUjwyT3Viwov/d86rlohuB9CEwZ4z5CSHEfuAXgUHgI+CnjDFPdeS/dHJvDCFdXFxkamqK119/nUKhwOrq6qND/p6CvuX7YWxUw998880Xrhr1yyv1e349jPnoIogcmAaB1NRVDYMmcQdwE00kKqS9HYjONBXVwFMDELSILY+mFeHFElsNsqZWGbJGUNpQlW18N08mkFRFAASkU3tJuolDGGOhvXFEcB9kCi19Ko5FNumulQUWZblGzirhaskdcR6JYoxDz3QPHh7k/QokfQt0v59ZqVQik8m8MCJuxItQy0VjGe93/yEmW0QYjREKnUlhmYRk5x6E8dE79j52+0eFyVYqFf75P//n/MIv/ALf/OY3mZyc5M/+2T/7RHfrY/CXgetAv8LD3wP+78aYXxRC/H+Anwb+x6ft5KWRe6PvOkkSbty4gdb6ATV2s2vnh/Goqip9/7hS6qWo4ZOTk6ytrT02E60RL9BMFvCsQWTUwNhZbKtIEM7Q1EvEzjgYqJklBr19RGKZiDVK1g5WrSaJiMk4u/FFd0Ju2pBNPKBNS9RxnJ1AV433lcDRI4R6mVhJGqrMqL2Ltum2yomkpqocnFjhiAFaokZN1ZFyHGEq3OY8ibDYZfZ96jq2KmUfLpncr6I6NTVFq9VCKUU6nSYIghfW4uh5yS2qs6R/6++S5HZhtSYxfppk/wly9y/RSe3FjAyhh7Y2+TmOw+joKH/9r/91ZmZm+Imf+Amq1eqWjXFCiF3A/xr4O8BfFd0H8hXgT/d+8k+Bv8lnRe6Nvut+B809e/YwPj7+wOB5VnI/LLkbjQZXrlx5KWo4wIULF8hms08sr7SSdNMyfbmGm+SoqZBQ+LjYODJLzRGIAASGtuN1S98BvvJIRDcFrKHaQHdS8kUb4ZRAd7sWrDgB6aRIaKrUVJvYMhTaKZpudzKoWRGW7i5xUqLIgt1klFE2GruX7SbFeBRpYi6rc+gkYY85uP79i3CDPdyZ8+7du3Q6nfUWR1uJGHscnofcsnwX9/Kvo4tjyFYF4w2gvTwoSISFZUdgPOI3v/RM+4euK+zQoUO8+ebWgl96+H8A/wXQD8scBKrGmLj3/yywqbrWL5zcfWm9sYPm41Tk55Xcxhjm5uaYmZl5KWp4vV6n1Wo9ddJo6Qpz+hZZkSU2TQKZpi1bAOTcnWgMTSoM2+MkcZkZuUKqkcVON1mLOpggg51pIJI0vmshkgZpBpi3KoxEJaSJWZQdDBnycZG67EqDQA6QqAoCcMhSdgJygYeW3ce6ZDfYEXclqkuepghYsgNG41GgzlX1MaExHEo+kVIvuhmB67pkMhnGxsYeiBjrd+nsq/D5fH7Tx37WdbxavEjqo39EnD+IlSxDZZV49DjWyg10O0N91zEGpq4QHfkiyYFPJWZtGq1W65Eh00+DEOIngGVjzEdCiB955hPo4YWR++EOmteuXcPzvCeqyM8jufux4UKITavh/USQzcz6s7OzzMzMkMvlPlW++GHMxBfRJEh7CMImbUeSkiN09DIN6oSqGwFWkx3sIA2pkKRokw4zlHMJQoAbeNQJ6ag2uWaejiUwrqFpKYqxB1RpyRYZuYO+eh4phQgHwV0jEAZf+LhOEV93ye+ZDJNujX3BDrSQ9HsXrVk+WT0KxueKfYWO6PBG/MZLT/l8OGJsYzrnzZs3SaVSD2R4PY7AzyK57dnfwbr/IfHIYazl2yTuDhjJIwkxbh4zOIJcrhDuOoJIp0E++7LuOcJPvwT8USHEVwGP7pr7/wkUhRBWT3rvAuY2s7MXRu4+carVKtevX+fQoUNPNSQopUji+Im/eRSiKGJ6eprDhw8zPr75zht9F9qTBkaSJFy/fh2tNWfOnOHSpUtPHPQtU6MuO5BA1SxTFDtYzTYBF6MFjir1Ciu0CU2H2BkCuio71g6MXMaQIO1BOqqbMeLnBDruTnot2SYMstBzKizbPlk9RJtV6k5EoGJ2RaOsyq67TANGDYBZwCUDVJmym4zr7gTlGJe6atOUMBaPII3PbXWHlkg44W8t+2kzeJKU3ZjOudHl1q+19jiX21bIbXSIc+9fYS99B+2OgwkROsI4GaxoBVbqxDuOYc9dRiRFTCFPcujsc11zs9l8JsltjPkbwN8A6Enuv26M+TNCiF8B/iRdi/mmqrDAC1bL7927x+rq6qaLCSql0JUKPKGN7sPoq+HDw8NbIjY83T/ebre5fPkyO3fuZPfu3Qghnpr2eUdfoswqeZElMk18Ows0adNgSI2xZoU0qJP1XVJ2iqVcQDHJENBixY7xTA5fNIilRdYM0RSrpESWjueAWSajs8wXY4ZaGYwJqGR92kYxEg3QSHUldFvZWCZNSAsLjxmrxq5oDF921+OWcLhrN9kbDQKCOt2JYM2KcEyJrE6YtGep5OuM2lsflE/CZlXoR7nc+uWX+n7lvpU+juPNZbkFq+iZX8KqrBAMHcNbnUBbu4gHDqA682hnCLN7DNlqkmRGcUUdGSeEB955rmuOomhLXVk2gf8S+EUhxN8GLgD/eDMbvVByl0ol9u3bt+lZVSlF7Hlw9y4cPPjE3yZJwsTEBMYYjh49Sr1e3/L5PYncKysr3Lp1i9dff329+P7TtqlTo2xWMWikVcCJDTNulUwrDV6bSEmaRCBApgtoqUhEHaEKZBKLGatDMclhdIM1GZBISMU2sbBZUXXGo2EkAkSdVtpmOMpSoUZsJdSaLsYFIaETJfiuh6sDGrJbvnnRalNMskCDjMlTlw2mbJ+9cbfLpWVsaqqFEbAzKpHSMVXVoPqGz06zxg69+YjBJ+FZ18ePaoRQqVRYXV1ldXV1fYwNDAyQzWY/dYyweQ01/b8g7EFaGQ8n6d4XrRyECpG1BlFpN1b1LqYekAwfQDWWiccPgvN8RR9ehPvPGPO7dGuT96uzbFmdeOHk3krMd3/NLZIEUynDQOmRv2s2m1y5coXdu3czPj5OuVx+5rX6owo23Llzh1qtxpkzZz414z4pp/saFxEqDUmdCisUrXGMXCO0FDawEkWoJE2caRCqiFh0B82qqDAidwCrVFWDXXoX91V3PVwUQyyr7sS1YnVIJd3z6ciAlipCT+q2ihbpYIDQrlBxQ2JlMPUUQaGDALI6x327zZ6o1FtvA0Jwxw7YE5cAQV00AKjLmI6wGA3TLHo1ftuc51R4nNfjPQiez8D2ooJYLMtad7llMhmiKMKyLKanp9ezsLoVVDPo1u9CZxGRGsVICxXMYYWaYOAoVvU22LsJR49iVedI3BHELpDLy1SK+8jseDb//8brfVXwmVZi6ZPbHDmCPHfukb+Zn5/n8uXLvPHGG+zatQshxDN1D4FPkzsMQz766CMA3nnnnUeqUo+T3KussMAsZdZIU8QhRdXu3s4wFUG9QD0T087FuKTwRIGKipBGYRmLRRUjTff3TaWwTffYkZCkTFe6pkwKX3U9AFmd5q7TIK9HyOkMdRWymPJxW3li1R1QqVyBVNiVuLVWBy0MMyqkrbvf53WOQCbcsQK69hqwjaIi23RkTF06pOoeCsUH7m1+M/UhddHZ8n3eiJcVfuq6Ljt37uT111/n7Nmz7NmzhzC+z9Lir+I3r9EIFXVrDRlUQWYIMiViSyAEaNfDiACCOiadQfg1zMAgSkfEe0+/kHN8Fbp8vlByb/WCNlrL9cFDiG//x/XvkiTh6tWrrKyscPbs2QcMFM9jZd9YsOH8+fPs2bOHw4cPP/bcH1VHzWD4SFwiz2CvaIKDJUusyirpqFcmqZBHGkUiEmxRYEG2aQufnBgmL0pUVYecGcIyFjOqiUsRgKYUlJVBGQtpPJZUk0Iygmu68eazVgfL9Fx+AlrKJqW733WEYT7lU0iG8QvdR+vFLjMqQbYs/HZXNTUC7lo+Q/Eg+SSH6dW3b6mYhaIknwxjIVlSVX4zdYmL9gyaZ5NI343Y8ti0CeTHCO/bFHJZ/NwIltvBJA46qLKqi7RYRLabdAYOgn8PYVJEu15H1btdX5Q/R2BnwX0+d+qr1CzhlZDcABw6BJN3oFah1Wqt10578803PxXs8KzdQ/rBL9PT00xMTDyybtrDeJRBbYppyqKML7pDPhAB1d4DbegQ6VvMuS2KomuhToTCpTtoFmWNRq944YKqU9SDxNIwb9Up6WEWZJuWDEmZQRZV12A2Z7Vp9FTrWGgq0kIZBQZWPUNTuKS1x5LqBsPUhCKddI+XURliC+rpDMbpRoi5LUVHJtyyO8ShjTACT9vUVDfCrSkFIWlG4gEqqs23vNv8e2+COVXd8j1/mbHliYlZDS9xp/0vicJ5pLObuqpghEaYBilniPbgTrI5G2MUNWlR7XQQRtMWAh210KkCIumQZHZSHtj/3OfWbrdfaBGQ58GrQ27AfPWP0fpH/z2XLl3i9ddfX7dYP227reDOnTvU63XOnj27qYfw8EQSEPKxuoVlbBqiyQAjIApUdYgxEKUSIj+NkYJFWcc1LqtS05ICjCBPllavM4gWhppw1qPIAuFgC6v3HlKmr5J7VIRCGkFBp5ix2mSSEoUwhW8bqjLA0iVMb30ssZmRMfkkR61nYLOxmHck2SRD0Sv0bjjcdUJMw0E2JP3ldU0G1GRMUziMxCVcbTFplfnV9CW+6dxhtRegsxm8jNjyxMQ03ftciX+FSnQFWxZoOJq2FaJNB89kkM4oHUcRmAYiWIHcAUSqyqDr0Bw4Siqeod2BjozRrTVCqVhxnr/n2quSyw2vkFqeJAnX5hcouym+6K8+sSvis0juVqvF8vIy2WyWN954Y9Ox5w8b1C7IG9REk5zoGv98Y5gWTXwvpsQwKZNitSCxjCIWCWkxxJryqcg2JYZJ8FiUTQZ1iUKS4Y7dZCgpgYEFGZJPusRLjMWKiHG1jWtcysqnmJRI6e5aecZqYuJPJqe6MAzEQxgDKyIgEpq6cUh66/qM9uiIhCUhCXqfDegMoW1YLVhob4BUoLBb0Oi50DpxxF2rjZsUGexpArfsNf5p+mO+4dxlXjaeev9epOQOCbjPVRZ2fUQ5da277LHz+LaLLxqkTAoQJFaKFbeDjlZwZInOwF4MEaDw01mU1GgnRy5rcN0SZmiESCsaoeDDDz/kzp07z2y0fVWqsMArIrn7angul2PXf/qXURe+g5i9+9TtNoulpSUuXrzI8PDwlopCwIMTyYJYY15WAVgWNSxtsxJElJIiAGXlY4kCoQsF0yX/qjQM6K69oCYilkRXkq7IEGG61vN5FTCSDFCTMdNWk+G4yKxq05Yxrs6x1CPbfdWko7uSXRi4bxsybRvHKOalz12rzWg0QrMnrT3jsopFRnu06V6DZRS3pGYgLuD2jHjCCGackCUrRcr31j8r2101vdFsc0sF5Dp5PGODgCmryr/MXOJr7k0u2ov4PDoY6Xlzrw2GFbHMRfUBH6jfYUpcREYWygxirAIVVcbV3WEsdELs7aImlkiLIhqIvAKdZB6CVcgeJIimseKETnEc/DVwbCQW1uEfIpPJcPLkSYrFIqurq3z88cdcuHCB+/fv02g0NmUJf5Uk92daQ00pRbvdXlfDC4Wu1DJf/sOI3/gnmJ/+WfA+faM2K7m11ty+fZtms8nZs2eZnp7essTvG9QiYr6trrEqa4wkRSqiSlJ3qQ9BYAIcY2GhaIsuYeZknd16kOuqw5BOgwHHpEmh6LBGgqFFN+srEAmRTgHdNXZsbJQJSURCLCCdZGnIKkM6zV0ZM5x4pFDcsRIibA7FLhWrl3yCYiQusGzVaKJpihhXe1g98hV0ihXV4Q6ao0n33g4maWZVCAJWbJfBKIcjImatrvqd5BwgpB0mrHia4ZqDdjUoWFJtrjqrXLdWsZAcjQbZleQp9SauZ5HcPgHLao2qqDKjJsnrNA1WGI6HMMKjI9p0Ug2Go+54iXUDT+1kVVVImyyGGMt4NAqjZFprOKKIX8jiBSEGh6ZnyIQJrdHj5BevE+cPEg4cRS1PfiqVs19nre9y6zdBKJVKjwzU+tySeysPsV+NpdPp8EM/9EMPhBead38Ice0D1Nd+nuQn//NPVZ/czHGCIODy5cuUSiVOnTqFEOKZ1Pm+Qe1bagLViwFthD7CEtRLGbLG0BRthvUAPoJZ2cT1BbGnCUkDPquyzYFkiDvCJxCafTqPYywmVItdcYa2DLiqOuyPC6xYNdaAXFxgzSljtM19ETCaeNjaIZA+gXbJIEAl+JahFblY2ieWmlUi6sQcigpMqq4bK6UdloVkIOlPH5A2FhdFwNFwEEckQIitJZWUYU0GHIqzjCBoy5BKr2mCnelORpaXYdrxKdYd/NjHzgsaiU/ZDQhJ+LepO+yJ89go1M42rXSZQRWSMTa2USgEGkOCpiNDAkJqskVMyJy1TE47rKo1dsYFYmJs7eDKYVoW1GSVgmUQGnyquCZP6NgIJEkS4SSSyN5BzfioRJPoFsrdRzucxAstTH4/un6TyNmHCjt0hg5ja5dEpR5pG+jXWRsbG8MYQ6vVolwur1fi6XcoHRgYwLKsZy6xJITwgN8HXLq8/FVjzH/7rIUa4DOS3P0wzx07dlCv1x9ZjUX/2J9E/eufR77/K+gfeGrPswfQ79J59OjRB5I+noXcUkqmvTLX5RwSsH1JJxUzluzkpqqxU+eBNh0SykIQC03Gt8k7DldlnT06x4psEGIhUICmDsQmwUhoCsmgzrIoA+aI2RVnuSVDIORoVGRSBETC4GuHRi/rb02EeHEelI+dwE3RYTzOg/SZ66nkncShBKxZHSIEdREjE5dMb14c0ClWZYcbwudQnMWVPsXIYcbujpslEVPG8FZYILTrBCJhWXYni769IpPKMaPaeInACg25tqQum1CAOEmYduuUCpK7+WlKiUdVdXC1Qouol8kGkUgoJoqODBlIbFqyg6cFpWSAUEiaMo2xm8QmwjMJnnZI3DrpKIetPGIhqbHIaFQgRBDaHm2aWGGdjBnGz+1G+lVcUaKZk2T9Niazj7AxSdraB1GTePD0poqGPKoJQq1Wo1wuc//+fa5fv843v/lNisUiURRttcpQAHzFGNMUQtjAt4QQ/w74qzxDoQb4DNbcS0tLXLhwgePHj7Nv377H/3DXAfT+1xE3LyInvrGpffdLJN+6dYtTp059KpvrWazsTSfk/MgspShLIjQ5maNgsszICGkE87LOgM7TEDYeXVW0VjDonu+5gcE2ikkiir21t2ccUqarulVFiJ90XVQdkSCST1Q6X1u4vRxtx1i4ve/GdJrr0me0nabYUUQCpqRPJv4kFqBtBAtaMBSnmBddeZ3FYU5LhuIMfs9vXdQ212WHTpxGJt3Bndc25d46f1UIFhOX8ajUDYU1sNJLOZU98/ogKRYygpV8iumsiwnSNFsRubLEdDSZwCKTOAgDQzqFEVBK0kgEI3GWtPEYjQexTYa0HmJGBawqzaxdJ41LIEKKJksiEvI6jWhlia0cM04diEBDTIBy9rIml0hrDymztG1ITIdYdzBOgSipEisLkBhvlMSfRLt5ktIbz+Sf7lelOXjwIKdPn+arX/0qIyMjXLlyhTNnzvBLv/RLm96X6aLZ+9fu/Rm6hRp+tff5PwX++KbPb9NH3wSepC5rrbl+/Tpzc3OcPXt2fX39JJgf/hMYlUJe/T3kzPkn/jaOYy5dukS73X5sieStSm6fiAsjy0RCsypaWEZSsdsEJkNVBuzQ3VJDFmnmhc+MbDCks6SbkhUhwUBF+Iwmg9RFwqRoMqyzNLTktmnhtQz5hmBC+AxEDrYRTBAw3iNp1YDRLkKD1orb0mdnnMeYLgnv2Qlx9InyNWlixqMCnlbMCJ+20CRRipzuTh5GSzpCM58ITNzdrtj7rmpi7iIpVB0Gep8pI1gQPqEwlI2iHqW74ay945V7VWNUbxiNaA8jQCvFXFGyULBZLrosOBZzgU8Nl3InoaEdtEmxKixiUty1IjpCMmW3u/uSgkLvHLIbJreCGaYtHVZKAl/5KC1pmypZMcaKK5Cm31rJou1m8c0aKZPDz5RI4jKOtYOmWEXoCGGliVM70FYKoVy01s9dvWdwcJD9+/fz5/7cn+PixYv85E/+5Ja2F0IoIcRFYBn4OnCXZyzUAN8lyd1utzl//jypVIq333578+pKOoN+84cxRiCv/HvE8pVH/qzfMHBkZOSJdcm3Qu4Yze/I6yxl2mTrFrFjGGSAoikR9oJQFmWLjHG5IyJ26iIAHQQNY7MkfMZ1EWkEUyYhp22MAJPYzGofLcF2cuCm0QLqoSFbMXSE4Z4JGIvTTIuIORGyMy5wj57F3ETUe5fgGsGkbTGQOAwlLisi5hoBO6M8ujfPRkgWI8lQ4jHXI+OgcbiqY8bDAp0eVUdxqSvNZEbRiT2KicsO7RH1otdWRUhTGGpasRp67AoGsHrDZ623374kH+oRc1B7hA44WtDMCDCGlmfQQlAOu/Hz7air6pv1KaMXsSi6r8JAXo+wbGnuW22qqoEVCDqyRd4USOxRYiWJRYTRHVw1zpJdJq1tlMziqwSZaLSdwQiDEjk6VgepDdpOIQsnukfdYgebx2FjoYatVpoxxiTGmJN0c7bPAsee51xeOrmXl5e5cOECR48eZd++fVt3i5z8IUxmBKMV6tq/QSx/Untca83CwgKXL1/mxIkTTy2xtGkrO5rfEBe4Fy+DgSAtkUYSYZiWIfOywYjO4ouYfFKiIRJWRIhtJK5x0FF38poTAeN6gCURke1Zj1caHcairnodYaD3ed0TeKle2xsJ9bUQ0Sty2IoNw7r7u1HtsRgr0tpiKLLpKEE9VmTiT+LiF7RgV5RBacGMCWkJTRR65JN+2yDZDUFNIvzIxdGStOmes53AhPG5H0ncOIWlBYPaodorn1wnwReGplHcThSDQZFCkmJAO6yKrutMi+49zvUMkMOJixaGIe3RsRLS2qKdBksLam6I0IYVGghtKIsm2cRFGAc3GeSm47NixTRUh5LOEIsYt61ImR3EyqGiasS0sbRDx8kQCzBCI7HoOGl8s4Yt89RUmySpo+wBjNZ0ZBNhpTHZw+tj6UWQ+0X4uY0xVeCbwBfpFWrofbXpQg3wEtXyfm3v2dlZzpw580Aa5cN4mv9Qn/0JiAwIibr7WySL7yGl5Pr16ywuLn4q9vxx2FS9czS/rD9ipVUlSBmGgzS+nTCmB5lEM6C7Dy7EMKSzXKJFQbs0RMSoLjKrDeW0JKUtQjSd3np6UrQZXIPFjM2UHVPSLl7ictuEFBObUe1ywXSt4soIFnIZdvUCR1bCmPtBRCqEWhhTFQlWnKLeE88VkVCJbWwtSGnJFCHXTcyBOIvfI5owiskEdkdpZnvG1jFcbhISR2mCnimi1IFYGGIBN3VMEGYoRWnQkDOKpZ6U7vvNtbG4aGKaUYpmlGIsyiO1xUiSIu4d2+pJ9FxvAhk2LggYMWk8bMaTIkMMUOoUaSQpqm3FLbtNFEQYYSj2JqWUUWTNCC3LYt6p0xIN0tolERaeHGZNVVAmIcUIq6pCJrGx7DE6okGGPO10FhHVUN4YRtpodxDZq7jyLFV4H4VnrcIihBgWQhR771PAH6JbAfWbdAs1wBYKNcBLktydTofz58/jui5vv/32ExPXNyVNR/aid72GiQVGpRBzH1AwF3BsxcmTJzet/jy1WIMJ+RfhB8xadaKChTKChh1hx4IVBBqYFg0K2qMmAnTSlRRp0yVwaBSxkYS2YMhkGNd5Lps2Rb87aOzsIA6KGIOlXW7piEgYhHawEgsjoJII9iRp6kJznYhDcZYlV+HbknSSZq6nJpdDn3azS5odict1E5KPMuzQHgld5bYc2+yO0wgNs0SEGOqxYrQX2Rb3SKeN4VJkGGunMKb72YixKYuEikiYNmBH2a7GYSBlBPM9Kd3uEThvLNZEQi1RfEzMVKK4HBsqHY/lxMYNC7QTm1RUwI894jBPJ0kxbRxaOEyokMS1aTtQSncnNVt2z6XZrGHVM8yZhBnVJsjGFJMUIRFpSizaEYkIEFoSS49EKLSIQbq0ZUxsfLR0kImmncog4jbCKuJkX19/9i9Kcj9HE8Ax4JtCiMvAeeDrxpiv0S3U8FeFEHfousM2VagBXgK5V1ZW+Pjjjzly5Aj79+9/qhq+6dZAb30VYxyiVot6vc2IKDOYukgcr2363J5E7jld4Rfb77OQ67BDFmjLiDFdxJcxXjPLjOwwYnJoYXCNw0AywDRdVfy+aLEvKTCRBORMN7BhFp+1qJuBFRjFkHa4pH126u6sLrXFrp5FvUZCGHUnwLKIiaKeAUxAJ7bJ6d7kIBx2Jl0NZYdMM+nZlNYMYa1rvb5rIpLARhhQBu7piIlYcyTO0ViPULO4mMTsCbLMmq7bbNA4xMA1Y6iHLgOJTaGnUqeNYJaQRWKWYokJsuyO8rhGkTKCObokb/VInu4luOwwDlpAJoQ5FbFGwm3pM0/EfeVTkcn6Wr0u+wkrvYg46SOMQTuKUjTMWiFLO2PTcEOyDTAKRBN8CrREgDKCDm2K7GTJWsM2CSkxzqpaIqM9Ym8IX69gqTxad2g7BilTWM4nnWRflOR+VrXcGHPZGPO2MeZNY8wbxpi/1fv8njHmrDHmkDHmf2NMr371JvBCyW2MoVwuc+bMmUcW7X8ULMsi3kQdNWM7zGYP0ag0KWayICxUa5GV5V9lufltEh09dR+PIndMzH/Q1/nNzgVWShFDOs2CbJAxDkuySSEoMJM1pIxiRjTIm27DvaqRNETMrl5yh5/YxKarfg/UDUNRitV2iNBQdgy5OIdGcN102KPT3EwibpuIkrYYSTyu6ogR7TBqbD6II/bHaRwjmIgSnMhDaZiNDVd1xIE4w0JvPpxKebipruchHRu+E0cUyjDSEbQxJEAlUhyIMggDM7p7r1takgpS5LSi0cv33qkVU65iOrQwoYWtBeO4aMAygmm6JF2NJfO+ww4/z844w4C2mO+RvNILr+1P6Zmwe793GhctYMx4+EIzrB3qMmJA21RkSFHbVGXAYJzCSbIMxMPckwmhMPgypkR3wiulcyT1DM2MJBARNVHHrinqOIS6g9CCwHK7mWHYtK0ERwuEs4MoWcORA4BApvY8MA5elOR+VfqEwUtYcx87dmxL9aM2o5ZHUcSFCxeoDxykuOsYUtpYGLTKkoodQn+S+81/y2J4hcg8vgj8xmOFxFyRc/y8+hbXkjnCosQyEotuIErBpMnqIh0cIgUjJkssDAM6za1E4xuDMDBJi4NJkYvGZ3+v1HSkBdejkEpKcYAsI8bh4zigZCwMYCUuoYFQGKzE4U6UEAPt2CLVM4zdShIOJmmaGCZNzME4z1LP1VONuymaAKMBXIo0e7TLLpHGCMFUysUJXdAGtOFeFPBxFHMkSFOnvwaX3NMxie9ikl7udy9GO20k345iIj+FFVlgYDcOAQZpYIaIGKgZwaU4wQ7SZIIc+4McTuyQj22Wetb9wO5nqnWR7r0rGgtLS4a0y46owECUIw6LGJ3hhup74dfrQqJJKMZDTKsI34WWHTBChjyj2NkMoYppJQ2opVhRZZIgQIohWqKKEA7GhBinRKKbKJkl7T3YUulFrrlfFXK/8Ai1J5UlehSeppb3Gw4cOHCgWyWzMYi48ssYZSNinyRbQgR1EtFmiTKT8UXSzh6KaicZUSRFBguLBE3VarFQarJoXaJJyH1RRUWQT2VYlm12J0VmVJXxpEADizqaqh2SDgT3nSYjOs39BAaEx4LocNjkWBU+ldgCGTCJTyoWdCLJHjfLXdrc1QHjxsMnxDMuntBcCWL2qwy3VBNbK0a1Q011aKDJRB7IhABDNbCwZUIkDJVYcAiPO9LH0ZJbkWbYlphEEyu4F8LB3hrVMYKLxuJo7CKsmBuqe39nqz4lAc284J7pxpIXsLgcxLxtZ1jtSd8xrG4Yq9acD6EUZUhbEiEjduNwj26E2SxdKZ3QddFljMuEjthLigURsgOLatSmEKSoCcjHBeqAHSnWJCwblyySWeGzTzhUZEShN3RaMkAYQ0vEDEeDTKoGJS3pyIhBX6PsNKFwWJJVBkRMKcnTTkcMOopOlFB1EtKNOonJUUstko9LaHx82yatSyj5YBzE51Fyf6aJI9ArkvgYtXxubo779+8/2HAgN4wZPo6aP09UyGLCJkkmjye6a00LmyCpMyPq1FRAG4Et81jCYyXbJEhrho1gnjrSKEbsArOyxo4kz5JsMKyztIzLHdFmt8mQyIBUJAkdjZVkWRItxnpBJAsEZKIUE6bDQe0xI31GG5KrWZdlQoaNTVYoWpENKuSeCXgnyfMePleSgAM43I8MZR2xV9gUhMX5OOYNyyWUCR/HMW9aHssq4FoYIxEcdm1uRAkdoBBY1HouqpxRTHQEY55NHsklEi5FMV/QLpZskwBrqRRVY3ijbVi0Y7AF5XYH49gsh5p2YjMiAzq9fIj90uamjpnXCfVAYJEm70hyMqEkFPeJUMB0j+SdnrzNCcls7/V2yiID3Cdk0CimZUTWSCoixjWCBenjGsGi9EkbybLsUNAW0kjG9TB3rQb7gFBo8ih8I4kSxZqSZKgzkqSxjMRI8GkSC4+0GqAmFvEKg8g4QEcOHbGMaNg4wtBMimRU54FApxdF7jiOX1jbpOfFZ14PxrKsT0lurTXXrl1jdXWVs2fPfmomNIf+ANp4WJEGK42dJLRkgJYOcVwlSCokukNGpximgGM0q5SxUWQaimVTRQrBTplnXtQoao9YJJR0gUDnuC3b7DdZZmSL0dhjLaUZiQa5YNrsNikWRMB+sgzpNKaXNrni+6QSwX0nzVArIcKQMjaVSHFDBxw0aVJIrvqaIgoNeIlDNYEY8BPJ/aBLjqlYk+r5yi/HEftjjwhBAKjIwennaCcS3bHwjGDQWDSNYdmXmLj7vTLwka8ZCtMcxqHa06gimaIZpDmUuMw73eNYLZ8lYCayCJqCnJbrQSp7hEXFaFaMZiLUzHdsMqHHgTjNIePiY0gB0z1VvNxP/+wtvHO9/Yz2ZMm4cDACduGQCBg3LtoYxuMUI9EA+TjHbaEJhCERBtELaDHGQiQFVkswmrgYBMqkmVYd6rLOkB5gyQrQxqdkxlhRK1jSxbEljjtGKmeTThVQ0SC3bt3i3Llz3Lx5k9XVVaIoeiFq+ee6QOJWg1Qe7vvV6XQ4d+4c2Wz2kSWWegehWjyDaIdYicZIFzfSxCJCu0OkrFFyJoPUIXWzhm86DJscqUDQyUe4wmKMPEuiSt64FEyONWxa2NwXDUrGYUUE2EYQC3AaDsu9tWobjTCQaMFEFHNDdyi0E+quZHdSYA1NG4UyAstYpHtr49kkZl+SZkknFLWDBCZ92N+zrueNte7PtRAs+Qplug9oog17esRYDsGOHFwjKCeSeWUxEHrMRn2/M0y0JGNYHJQODWO4ESXguwwYiW3gTpxQNQY/sDkYp5AGGr2qNKOR5ooRLDcVtbUOjoZMz5++SyiWjSYBbscJHwaaduCQ7WQ4FGXYG6c4qF0qJkFhmOmp+X2y933uoCkkNqnEYjzMQ+zQDtPUkVwXPq2eUa4uA2wDsZGMRCUmrDY5ulLaMzbaZFmyGoxoj5QuEQuDrSWRdDFEFPUoy3IJKWyk1hjLo+geZPfu3bz11lucPn2a4eFhqtUqq6ur3Lp1i6mpqU3nbj+MV4nY8Iqo5X1y92uHP65F7kaY4k7i8g6sqI2yA0wqg0oStGzTtAUd5ZAWRYpIDFDpNGi4EW7LoZQp4GMo6SECFBdkkx06S1n43XBHYzMlWxxO8szHhiSJWCDgMBnu0OINnefjOGJnqJh0Y9JempKRnAtjSkpRduGkdrkSJoAmpyTSCGqhAiJuJCFfJMs3k5C5JOINx2E2gvk45qR0sITgg1jzjvTQVsJHiWEgEBz1bD7ukfi08Lig426BRi3wEgepfPZIm3NaE3cUJVcAGsvABV/jCIc30/B+L1S5nQgm/IS3nRRNq/uZ7gV17LZtLqPINA2NsIOXlaR1DK7FTqGY7RkmZ3RC2WjyicWE1rypHBYSeEMpqkKTa7Xo5DxKwJLUCGNxTcWESMoqJhAJeQEJmnkRkzaSReUzlFi42OR1hlt2nWOmP1RjvDWbyVKbIe3QEgbLpJhXVQZ0xKAusSyX2JUUsEjIs4MqSxSFh4dLQX1SH19KuZ6b7fs+u3btwvf9T5VLLpVKm1a1+40sXgW8Emp5HMfcuXOHqakpTp8+vSk3mpSSxsAZnI5GGIGKI0IFLhmy2iZnLDoiYFk1uSXrrDo2KWsnTelxRbVZFOALwaSoM6w9EhKahOw1WWZkk8NxnntJt0r4mivwjGCJkCHjcC8Ekxju2TEjxqYuNGHo4WMo9tT0RqRIoWgYzahxGdQuH4cRh4WLAm62YajX0VPEimZPk70fGhY73cHxUZhgh13JX9EGO7DpD7F2JDkUd7dXieRKkLBPe6yEPdVbw4W65ICwOaRsGsawpg2rHYvjOOQR3Iz68duSew3F4Y5kvhc3n+n5q4tKccHOUA5SkKQY8TVus1vEYWdiKBuNhWGq52Jr9iz6lhQsmQRlYErHOEKwYBIGpaSDYa9waAvNHnqvwiEUht2Jw1hYIBNnuEFEKLvn2JYBO+Is8wi0BaFIyGgLo9OsWA1GkwyRyNERLUbNEPNWFYFCmoSMGMYIC48Clng0SbXWeJ7Hjh07HiyXHIZMTExw/vz5p5ZfelHx6S8Kn/mZGGOYmZlBa/3Y3tePglIKLST2zi/hdgJAkNIWiUzQKgPGIOMIUY8YidKU7AwN0cBPJezUGdIoZkWdYZPCQbAiO+wxOVZkh91xiRktWCFiyNj4tmAP3Woq6SDDjIkZCSRaCFIorCDFnTgiYwQ3dcj+WsJHYYKnu26khtY0wu6tno00r4sU9yKNpy2kgblAUtIOGNglLDqRwjEwLiXvNwy7hCIjBO81NLsSl5QR3PA1FyLBoVbMRNgdbKshWJGDMnBIKSracLUBqV4qpwdc9RM+aBoOG3ddbZuLDCHQjBVJ0+Kwtrnf61U20pPkJSn4IJbcCFNEqsShwGEwAqkNo35IG0O2R2SBYb5H9o7VK4HUE2Zu7zXTs+xn6MaeZxKHXJClbhRXCajKCGUMS7LDjtgjTDy0UdRljINmJBpgWvnkUHjaJZSSbsxdmpiY4bjEolwllhLLgEQxrB50f23Ew64wIQS5XI69e/fy9ttvc+rUqfXySx999BEXL15kZmaGVqu1ro4/R+jpbiHEN4UQE0KIa0KIv9z7vCSE+LoQ4nbvdXPBIz28FFfYZlGr1bh37x6FQoEjR45s6Tj9tbosHcOr3sSO2viOQWpNbJrUkHTamlw2S8VqERCRMWlE3SccCFkjZKfJkQiYFS32JwW0sajqbgrjgvA5bDLcFm1Kbc1qKkb4Lhe0zyCCKdewCweZ2NhaUTc+b0qXaUJmE4e0JbiTRLxteywHktUIMpakow3Njg0E3IkTvmyn+EbYDRh9N+Vwtw2LieYdz0EauGk0tUBy1JP8noGLvuZHUi7/sadat0KLI5bDFUIGUHzQ1rydcmlEGjA4An63Au9kHYSl+bAXVHK/I/C0w2tpw4dhd1+xESwjKQQSW1sc8BKWk+7vx5RiPk4YlYLLvclkp0rRbmsG3TRuO8LTPqvCkBYJqxmLnBAsuxZ2j+wFrQgQHCBFknRjy+9LzSoxFTskRlMlYdAolmTAgSRNrAWWgEnV5AgW2diiCUgMHRmTitLMqwglmuyJ8yyrKuNxCiEihs0obapo4TMidpB+AjeeZi1XSj1QfqnT6VAul7l37x7tdhvXdfnWt771rGWNY+CvGWM+FkLkgI+EEF8H/hzwDWPMzwkhfhb4WbrhqJvCZyK5+9J6YmKCQ4cObapp4MPY2HXE3vNVMs0OlpZoIGwarFZEvpAnclzSaoSiKZEhRaIMtrHYp0t4xkEZh13RCDNaclEHjJLiLi3GcFnCx0HghoaFtqTZDkikIK+6UVvFxOXDTsysjsgguJIE7ElSzFsOe3vhF0miKEdQ1prdwuWw9Ph2O+aoEQhjuFQJ2dOb+f1Irgd53As1QU89X0kM1Za1HtBxqyV4s1ceeSlRfKtueFPY3PC792MxNPi+RQo4bFn4Bt5rGCzfwgMGpeBmoJmJDO2O4g3jUEBwO+keL4/kRqBZbkmCts0xHGpx9xzHra5026Uk84kmMHAl1HwcS+pWljsmjaXyLLU8vIqh1nLJ1xQzbQsV2VwIDOVI8GESEQLLJByQNk00+4RNZAw7jctgmCM2ijsioC0jPCMQiU0m9qhkBSkMI2GJSavJiHbI6Txl1WQ0ybOkWmhhYUxCzqRJUWJIPBiR9jC26gpLpVKMj49z4sQJzpw5w+DgIDdu3ODixYt86Utf4p/9s3+26X0ZYxaMMR/33jfoJoyMA3+MboEG2GKhBvgMyN3vJFKpVDh79iyZTOaZu4f0t5OWTWrkB0lXyrQrTaRIyOQHMCYg1E2q1FlQDRZEhG9cYlxuipDrJqStbeZEQIWQQ6S5T5sCFhbQIuFwkmfCTlHoxCymbY5Ijzsm4KTJ8nutmNeUR9Vo9iuX12SKez4obbiSxJyQHh/WDMWegjQXJdR7hL0VwElpMYtDORSkkoTb1Yj5liEP7BUWHzQFB6TiNVvxfstwRNocVIo7geHbDcMpLZnrZVrVfcW+XmbgTqm43DYUIodW1D3egBR8vWLwfIcjquuKw8C9AL5Vh/HIZY/uTjh3e9J9zJbcDAz3WoLphs3rxqX/qEZVd+gctrrqf0Z0LegAyz2jsZXJoIUg43TPS7W7a/V01LWiF3tkykgY0jZe7KDCDAtGM0XAgggYMgqpFbviHNeljxKadBvWECA0trZ6deE0KZ2jLXxGkzxl0aQpA4wQOMJjcBM1Dp7VECalZPfu3fz0T/80P/7jP87XvvY13n333WfalxBiH/A28B1g1Biz0PtqEXhyT+yHz+uZzuAJeNINarfbnDt3jmKxyIkTJ1BKPXODgYf7hXUYoboaUUCTTefRxscIlwwFhnWaEe2RBsJUwopoUjQWu0mxLDp0iDlEhjnRxkEwTLdRwJ6gyLdCn2yYUM04ZBCs6YjjpLjsaxwEt+OIIpJGollpS2aThH1hggHCUBEbwbUg5k3LoahtJpoJeZ2gpaTc6areqyheVx6LOKwZSb4d81E5IjIw0wLf797T8y3NUNIligFWWoqDcS+AJBa8V4W3lcVdv7cGjGCqKtmlJAcsSQJMBYbbFYs3pc0RR7IYdX9bjeGjhs3Rjibdywzrf7fXlizHhjsdwzfXJNm2iw4Uh4VNofe8DzuKCNijJAtakxVwJ4mRRjPbG2XNbDdopGIJ3FhTKzfYUYWZlmHGF1zQIWCYExH7hENa24zEKW7qmI6MsI1Aa4UdKVZkgGskKkmxKFt4xqEpIhQpfOEzpPOkjEdATMlsiRPPjH7l04GBAY4effz6/nEQQmSBXwP+c2PMA21sTXdhvyVf23dNcveLNrz22msPdBJ5nr5f/e3m5uaYmJhg97E/w3CkseIYgYNnoCMCVqVBGAcBpDuG3SaDEgkzsk4ayV5SzIgWaRT7TYZaLKn5NjfCNpaBtIYGmn3SYxiXdmizojXHLJeWMewSLtMtRUODZeC2ZfO2tvlWI+G1XpCIH8G9WkBTSEaVx3Flca5leMuxsYArdcmpnsqbddIccXv+b50wtRrjas2A0fz7FcPbtsWAEFxoSyY6HmcdiyttgwGqHcmunttopyWZC2GmJtE9Cb7PFky0Db+/BqXIoiQFAxKutbvjph0rrpYt3jYOpqeml5NPSA7dZdU3aoZzFcGlmiLdcbECi+PaZbexOW4c3pA2B4zDwYbPeOJwCo9i6HE8SlH3U6SiDBNultBxmJewww8IMAzFETtCGzeyuR3BsgzJGknLGPbFWW5YEZYy7I5y3JAd8gKGkgFmVIOidumIkFA4aGGwgIxJs9M8WSV/UXie0NNeUcRfA/6lMeZ/6X28JIQY630/Rrf80qbx0sltjOHWrVtMT09z5syZT9VOex7JnSQJ169fZ2VlhTNnzpDN5ciO/Dj5tQU8I4iFJGMUKZ3QkBEtFBXboqklKZ3iQFJiSGcItWI4LuBEWb4VBzQjTTPs4FuCI1aKxbTNa9IjjCXzvuRKFDIuLa7EAYeVzVRbMCItZuOEE7bDUKKZbHeDUD7sRByXgpuVmJzsSur5WON3ukT+TlPzruMwGwo+bMEbtuJyQ/DtBrxlS1LSYcZ47DE2O2NDZATvlQ37Oj6REUQIwrbiRI94IhH83hq8LS1ut7qkHFGCby5LztgWIz1V2Aa+VYG1uuItyyIxoIzhbmRjAD8RXFpTvJ7YuHHX4lzur7ttyRt3Jvgr3/hVlmNDLdG819K83zRcaxver8OCLznXgKa2+aBlaESCc52EGMGyNgxbveWC070PA9kMByKXZtwl9fW4w1CUdENzY4/pGFoyJh13C0ciwDGSGqDQDMVFllWTlHFwDTSFT5OAETOG/C7JsGdN9xRdSfePgevGmL+/4at/TbdAA2yxUAO8ZLU8DEM+/PBDpJSPbZH7rOSO45i1tTU8z+Ott95aj2RLpXeQTh8mU17E0gkhAhePlOnmJmcjgURQx3ALn2umQ8cIWmhu0mYsEsRRQM2TvGaluGk67GqE1APFZb+rbmvAQeIhsAKH+4FmMdakEdyJYkRLcTuCk57TVc+rHdrC5nYseNu12Wks3q/DAUviCbhWFgxKQWTASxSqF15aDiUNv/v+Zkfg9KLZJII7rSzHTICtNZcqMVfLgndtwYVeh59OINidWKQFDFsCA7y3BrqtGFFwIiOoxlBP4G5Nsje2OaMS6r1j90NhjZZ8a1UyFNi4ocVxSxFUKvz83/7P+Av/6P+GHYUccxWBgb22YCYyZKXhZtj1fy9avSYNurce770umZiMEZhEciBMc7ktmI8F9yw4YjmESjKOi++7zIYtnDih7UcM+Rbz6W7X8KEkw5II0EiESEgnWQJiEJqcdimQZbd+ev+vFxVZ9qyuMOBLwE8BXxFCXOz9fRX4OeAPCSFuA3+w9/+m8dKmtGq1yvnz59m3bx+HDh167Fr8WchdrVa5evUqqVTqkQUhhgd/mAyKjN/GM5JIRtRFtxle7CaURYuWCBnGZq/wqImQRXx2+YbIxFRSkuMyTZ2Yg0mGtcjhahRzzHK4n8S8aTus6IT9cZr32jFvuw4rieaobVOKHWqJQhr4sBVxrONzPs5z1O5OPvUYVtqS0EAjFLxhW9xqwwCSooTvrEBRd9sfFIxktaXYaQve9hTvlSVf8iSnPMFSJLnfSXHG+DSMRWgE/qrP63G38whhwoUa7E8s1nottk9mBOcqoJsWwz0S73EEN1pwqwWtps3roeZLKclMryTAYi8oZtDurunLdc1//ff+Gu3BYUYX5/jPfvdrOHSXI2O99M6jXnf9fcQRNKXioC2pJ4Y3LEVBW7wjXMKOSyHy+HbbkGBoY9hlC6SBglFkwhTTSpBSioZncUhkuG8p6jJkaC3kpvGJow67oizzskOEoT/1JUhiKdiZbE5qv6h+Zs9ahcUY8y1jjOgVajjZ+/u3xpg1Y8yPGmMOG2P+oDGmvJX9vnByG2O4f/8+N27c4NSpUwwPDz/x91u9qbOzs1y/fp233nrriYH+owN/lGyzjKUDPG2TM4YIgwhcBuMcO3UKKTQzNPEMjDZj2i54jsNJnaMRSmbakraWLHguJyyHq3HAHqWoaUM2cLngJ5Sk4EoQsUNJwkjSCSWzWByNfdLGUBdFbATnWppTrsV8XVIJBBkJsYFGq/sIrrQMb9s21VhwrWk461qcX6PrH48Vy63ufXp/TVDqGdUCI2i1s5z0ugELyzrL3U6WH7FjrrW698Zpt2mtSE71qpBC96F/MCP5QWlx2PnEYn2jYzHRcaAjOWUsfjgludebGPok/9u//Hf5yvVvs796n3NH3+bP/Juf573ZmHLVZqZqUeg4JB3FaODghRZ2VZAOLZaaNiZWfNA0dDRMxppsL8pCKEMWgUwUudDjwzgmJQyrJOwWNjr0WFEJoygc0nieR1YrVqXAb9fJlDW10GdNtHCMwDWCjHbYozdXBfhFVmHZTC2/7xZeOLmTJCEIAs6ePfvI2uHPin6m2Nra2roL7UlFHlw7Qyn7AwyUpxBJgMBBojFWQlmGLJAQJA5DYZZqLWLO8QjjNHac5nwUMRtr9iiHiSRglx9wP4nJINiBw62q6NZX04ZddtePvF84/H7FUI4Njkm4JTz24XGpZXgr1R04MpJ0IsFsYDhsWwyj+HYVvphRjNqC/zgPP5DtPpKgI/lyprvdDiQDkcQV8HZa8t4snNAt3nANd9sWM6uKH8tKVkKBQeBGLl92JAqDkinqiWR12aBWWyhjOOIkRAYur0F5WfIHPMnbGUFgBNIYbjbgSg1ER7E/sPjDKYVOBD91/l/xJ3+r67+NvRSN/aPsK8/x5771SxxOwY2OoZYY3m8a7gWaS52uJ+Bez+o+F2sEhpkkwcIwm8QcFYogUKRDh28HMcOWIMAwIhVjUYopnZCW3eVBybhMWwmJgBFSaEvi59KU8jmyxsZpChq1Bq16k9yKSxQ8vTpPf2y9qPppr0qfMHgJ5LZtmyNHjrzQGFvf9zl//jyZTIY333wTpdR6g74nYSR9BNfbz0B9DtdoQgxOpMgZg5IxNdNiljYi7bHbSpFVghumw6BU7LYUV5OAg6obfloSkoMmzW9XE457NlfCmJOuzeUg4su2x79ZNbzjCeZjwwETc8QIJpsSF3i/ofnBtOK3lwwDsqt6xgk4vZDU71QNJyxFJxFcq8AP5yQXVwznV+ALWclSDSaqcNpRmGaMQTDVybFf9iS4hkZZ8cWsxAam6pKPVyQ/bEsW2t1jvJazuNks8HoEnWZ30B+0faZagg8XJemm4qyKOe5GlKNuvfDJFiz4EAWCEx99wM//h59jcXCcjpuiNTbAyfYUTS/DV6e+zd6k68M+nhHEwIm0pK5hvwhZTgxHXcFqbDjpWIwam3eky0LTAm1xMdAM9cwxLnAwSnEpTNBoKiZhzNhMRTCNz8FAcd8WREIzpj0sI5hSPq5jU8xkGMznGXQLDNQyD8SEVyqVxwqDz7p+2svCS8kK22o1lieh3/fr2LFjW26/C3Ao/SNcbP0q2dYcUXYfTRWitY3TSsjHCcO5LDUks0kbg8Vx5VHXhtva5zUr1Q0tbYOTcvimH/Gaa3M1iBhTktk44SQO36oYCsJwpW3Y4QgsPBbLMVMYTjkRk8JiuqIoWporLcOPFCVTFUE5gON5gZBwZ0Ew4hmWI/A6khHHsBxCzhdoJZjHUGtE5MMYYWx2e4JzU4p3xzSVGK6vgkDw47sVX1/s3ns7shgJDCNZzWJP/beFYq6c5cujmo5woANDMuSjZQeDx9vZFj+YlTSV4XKtm5KdmrzBP/vtn8HRMZM7dlNQO3mtdguA33vnh/lDM+9R/t1/Rucr/2fSieGMLSgAb0lQvsZTNoUYrjUlkRR86Ce8mwPfQAeNxBBrwWva4/12yBuuoYlhWFi0I5cbTsQhqdCxRV212O1LJtMxx6VFDsVQkiYRAYGMcQ28wX5G9wywd89e4jimUqmwvLzM7du38TyPwcFBBgcH1yMjP49VWOAVSPns42GjRj9EdX5+nlOnTj2zii+l5Gjqx7nS+de0/WVUkqEZNGnZDn4qjUkkA7gcMSkSI+gYyGnJCe1xL9DMRZrxRHIxjNhvW1R61t4xS7HUFNSNohwnHNU+t60UBy3Fby1CybYpAhdDh9Pa5/2OwzE7pGJsjC/ZoSSLxhAEglEEE4HgiCvYlTN8OCs4lBcESjNflvgx7HQDMgHcbGX50pjBaMGSgasLkh/ZCdN0/dyrK5IfzBoudDS31npGO0fhOTCHYcCW3NOCO6uSMVtwIm3IOjYfrgiyMuZaPUVck5zItPgB12bMLPGnP/gfyEVdQ108kqdY7tbFv777BO9EN1hLl9h77wLx8QV+2xsjrSC0DLERZFWaTmwYSnVjMG6HumtNjzS7lcBLJG8JxQfNiC9kDEkCaQWHY4+PTchhW2K0RSQMgYwpRJLE1uw3DndpcQCHNAYHhWsMg9pl1HwSQ25ZFsPDwwwPD2OMod1ur3fpjKKIYrGI53mfS3J/5llh8GmLeT9EtVarPbbv11aQsfKMeV8k4y+TCSoIqdjl5iip7kBaJmRC+1yNIyqxoJXAR2FIyxjecG0mPZdxpVAC1pKEL7guH5UFaaH4sJ1wjICbMsWPpBT/bkFwNmexHMEBT/LFtMU9P8eQDTcij69YIRcWJTPVmGGZsEcZHF+hgLsNGA+6j+ROHX7IU6y2DbUQRhNo9bqOzFUEmaA7EQ67cO6u4EsDgiNZuF2GC3OCH0xJZE95co3g0rTgS2nBYqO73esFwWRNMrMgSbUl+z3D/lREbCQpZbjTzrAw3+K/++2/xtvxfSJpc3HfaX4wusHC0Ci3i3vZK5fIEjK17xjHnRV+7tr/GwO8noOOhjcz3X7hb2UF5QTeTEsGhOS0YzEcOQwai/cahrBXi903hpPC4XxLI6QBA56RrMaGeRMxjMWSFEgtiUTCYZMhwVCWIYGIcIzhzfjxAStCCDKZDLt37+bkyZOcOnWKUqlEtVplZWWFS5cuMTs7S7vdfqZx9n1B7q1awDeSu9/QoFAo8MYbb7yQtRBArjFAs5xlh6owJkJ8NK52yRrFoJAcUDa7LUVVREzpgP2WxZgtuRwF7IgjUsqQQ/AWaX5zVbPTFlxtRYxIzZJy+YKn+L1lwaAlOFfXHPIESoPpSNYi2GNLRmyYrmU5nhU0tMVeqZmcF0xU4C3H51Qm4f37ki8PCHIKJmYEx2UvHps0XigYcuGgK/h4RvClguCgJ4g1fDwjOGJJ+ndrblUwEAreGYCJle5nthZQFnyhAKvd3XJ8UPDxvGRlSZJuxhxJG44XwAmb/OK1v8Lu9hzDSZXzR77Mm8kdEIJ9Zo1k9w7SJqRtpRjNtFjOjXJs9Qb/SfUcrhC87giGMLytfQaF5JCxiALJ5bJkNoSJjmEu6UrxWmL4gmVztQmJ6KafppEMJjYfJSEHrG6X0EkTMhZrqkoTaIlGkzGCIWOTM4oD8RBZs/kkJKUUg4OD7Ny5k507d3L4cLe10O3btzl37hy3bt1idXV1067ajX3CXgW8Emp5n9zlcpnr169vqhJLH5vxUU5PT7OwsMCPvvW/4hvhbzEYzRFZWVakoIyklUgGjMQxgj04CFtQSQxhbHjX8pgLG0yFkgOOzYfNmD2WYM0PMNJmp6sQoWStJanEhreygqXIsNOSXF4AjWGfBxMNwx/OWvyHDkgDg47Bij3eKErOlQ1zvseBtg84fDgn+GKhxfkgRy3I84f2wLemutdyvCRY6Hk7p5YFR/NgCcOQBx/cEry1A4xnuDbbvSf7s4IzA/CdNcNKHeqBIGoIigZURqN6NqadqYDrlSIAP7yjxd9f+W9Qveixm7lD7C2s4fseQkBlZBRPCuKaZHrkIMeSae6mx1nIjPF3Jv8RZ+UJtJdmWglsJFcSSFuGTmTY5cIN3/B6SnTDfS3F79YSCsVu77TICN7C4T92Qr6YEgwYl+tJwFFL0Ugc1uyIUS1JI1gzEVmZYAtNUdscT8a2MOo+Qd+glk6nSafT7Nq1iyRJqNVqrK2tMTk5iW3bDA4OUiqVSKfTjxxz7Xb7820tfxZIKZmZmeH27du88847myb202qe991n1WqV06dPk/JS7F89Qkd5jET3yCQho1JRtKAjNMuJ4WYU83Erod6xEKHL+03NlPE46Cou+hGjyqBDn4q0+ELaYr6uiCPFR03DD+QFl5qGrxYll2YFb+UUnaRL5i+nBe/NCE4UBMs+nE4r7i5LPlqCs4OCI57iTjXLa0XYYcdcn0+zz+mK16WZiNMDXekxYASZSDLowbG84NJ9yVtpyf6UIDGC6wuCUiAYz4Cr4N6i4PKk5EdKgqinymcU3FkW6IpErfkMWRFj+a7EO5xp83+a+G842rqB48FcZg+7i03GRZWJ4Te4NXqEQ3KRfSzwH/b+MMeYBqBdLFLNlijoJv9F49c4UoDICPZ7AYGB17OCooQ3UpLjWLiJ5NtVmIsNGWmoJJov2DbnG4AyjAvJaixJBAxjMa8TilJgItktxy4MQ8JiEIu8lrwT7ULwbIEojzKoKaUolUocPnyYM2fOcOzYMaSU3Lt3j/Pnz3Pz5k1WVlYeqNwbx/HmO9hugBDi/yuEWBZCXN3w2XMVaoBXQC3vz5CtVoszZ85sKbf74eKKGxEEAR9++CGZTGY9Aw0gLVIcqh7HVxa74tuYpE1G22QQlJTggG1xwFM0ZMztOGSfLRkVMZfCmN0iQSQhruvyJdvma/OQF4IPG5qDKbjYNPxoTnJuRrA7LfhO2XCmIBi3Jaqj0AaWG/DOAHw8Cae7ef/oEHS7q17PrRl2JB1CbVHpZPjBYc1M3eXqrM2bbpWr0zBfFYwYKNe62y9WIFmTlDzD3oLho3uSoCz48ijUehllnYbEVCRfGhHcWOxuN6ia3FvN4jZc0h3B0XSbv+H/j5yOu2NMSMPC0C7SvaTKzEia/U4FgCuFNzk9tEhTpvhO/k1OWNOccu5zcfgN/lL8dc4u/i6vmQ7ZIOaHhSRoSapVxUdVmGgaLvuagx4kGs56FtcbkoYBTxhIJCWhmIoSPCOQWpAzknliSklCIgSRMcQkSCJeT0oUzTMVSQA25wrzPG89f/v06dOMjIxQq9W4cOECFy5c4B/8g3+AbdvP6iX6J8CPP/TZz9It1HAY+Ebv/y3hM5Xc/RTQVCrF3r17t2yxfDjts496vc6HH37I/v37P9U2WCnFQJjhgDxB23LZZe6jaFLRhmbcDU4RBnZZkjdSippIqCF5I/LJhREOaTKxw2+tCY6kJHORIa0MCsFZV7JakURaECSQlt2meeUVwUfLcGYIEgO5lgQj+GgBvjQCq4tdVXuHG7LH8mm2igylIEkEjRWL3fnugCmaEicGu5OZ22nQWY0ZS0cczGnuLgqyHcmeVI/MAcxPS84OdQl/cx7agUC14ZAU7HFarLa6KuTBIZicavFzjb/FDybvMekdZMUZJT0sGS91SJCcz77NcW+SW94BbqaP87o7RUk2OVc8xZnUJAA3M8coZWKaIsX/NnmPnUGDS80iK82Ayw047AUshvB2Hk5Yip1CcrkimYkNRQVCC950bN7vJFgS3rRszoURWSGwEezQNnULhDBkFRSEYK9OcywZ2frg24CtusKklAwMDHDo0CHOnDmz3hN+YWGBkydP8jM/8zNbOr4x5veBh0NLn6tQA3yG5F5bW+PChQscP36cYrH43GmffSwsLHD16lVOnjz5yNDXvir/tjjIgNhNKA17ucW4amMpzZpOWIkMt9ow0RCkAhuvHXMhTnPPZMlIyfmW5lCqW0e8lRjeyUq8jkIGittNOD0gmOnAD5UEV6ckrpC4Em6swckUXJwXvFXqktA0BMMuNEOBp0FGWVabgjyCM4MwtSKIWoJTo4aJacGN+w5fHDMs1go0AgfVFrTWupLVxBG3rwtOjhpe2wELZcHVe5IDluBgCWxpuL8IMyuSETwOuYo9eUPSqvLfZv8H3lBd33XdyaOKDoNWkzFrlW+VvsyZwj0AcjmNyUqkhHtiF++Oz3Pf2cPH6ggns/c54i1zdfgkJ905/iv1y2QIGB5wGLMMY7bgVBwysxbw0armfENzNGVIGcFxR/HthmHNJHzBUXzU0gipeVs5XIsjIgGONHgReEJgGygCX4p3bXncPIznDWJxXZe/+Bf/IsPDw3z88cf8+T//55/7nHjOQg3wGajlxhgmJye5e/cup0+fplgsvpCCDf3U0vn5+fXw1Edh4zr9x8VJLDVGU6YZllMMiyZDUrLDkRzyJGO2ZjbyWbY9jrmCIQc+aie8mRF0hMbB8AfSivdmBIMKPigb3i7C+2XDV0uS925L3hkWTNbhRElwIiOYXZYUXbiwAD82KpiYliyWNSOpmF2WhxUJUrYhjsAvSzzbUGkJUi3JaL53DS3BG715a1/BYnUly1tjmrFst1TTzduKVL2JqzQ51zBxR7AwJfjCSEgQdK89jmxu3Jcc14v8/eLf5thAN1V4Ru9g/94286LbAfOCOcFbo/N0SDEvxtk7WCc7YLMshxkYCMlYEauZQQ4M1EAIPhTHOFu4y0V5hEOZVX514F9QK4Pdknyn4hJbLjMmxZsFw2EiUp0W58qGiU7I246m2lF0BBy2Jdd9sKThmHKwESybmNjS2MJQFIYfjcaxeX5vyosIYuk3NbAsa8v1AJ+GZynUAN9lyR3HMZcuXaLT6TxQ6fR5CzbEccyFCxcAOHXq1BP7dT9shPsT5i2wckRKste5hS3LJLGAIEI3m7yWstjvGSbjmKof8AUnJiMEo4liIFH81oLg1IDko5phfwYWOvCjWcGlGcmOtODSKhwqgBsJ3Eiy0hKMpeG1Enw8IdibbtKOLY5kLGaWBNOrgv1ZwZgU3F0Q7M8ITu2Aa/ckuvX/b+/No+s663vvz7P3PvvMOpoHS5Yty5Ytj7Idx9CQBFLAIUljJ6SBloS0QMjblruguYXAzQotlwYKt4VmMbxAue9NGdpeYmcgzgQkhJChZLTlSZY8aB6OztGZxz087x/inDqKbEv2OXbi6LOW10pka+/nSPu7n9/zGwVrF0kOHxfs71PYukhydEBgWoJwUMFv66hCsqLB5kh/JfW2zRJ3lJwhkFIwPCAJ5DT+YIlkMAjrAn18vu5r1GmTLHKH2CMuoqpJUOFIU1WZo0dfx8amASpdWV5jNd6AgUs1cKtZRivbqdIyhGQ1S2sTjLmW8qqxnE2Vg+SlhqhSieLBpeX5i8DPaa+eTmpp8Qje7RQksir9WReHhI8tPkGVoZDKGSSzOZLxLIplslQVvJa3UAR4BTQoKhWmjQeDP7BqqWD+vfdmoxTpp2dR7nkyzqpRA5xDcafTaV566SXq6uqKZ5QCZ7NzF87tTU1NdHR0nNaZN1PcOjrX2RvJKRVEFT+tzlGq9X5G8nkiLh+9WYVI3sUyxY1uu3g6otM3mWUimeY/YzabAjY9SUmtExa5oDarEkmopA3wOKbTN1s0GBwT7BuFldWQyAqqMiampRBJ+dhQLzl+TKHeBS5N4pegG6AKyVgIXCmBU5NEkgJfVtBc/fvPkhK0+QUeXdLsh4OHFToCFNskJdMO4qM1rKnJ0BKIMZXwkEgrxEZz/GnTC3z7om8ymJ/eoQfNpQSaTVzadN55Qm9CrXSDEISsGpYsSZBWK0hYbhK+apbVjtKbayHncVHlzKC4LPI+FwjBq3YbSypCDOo1uD0p3uvv5SO+X9El4OUxhZip0JsRdFVCl6YSNQURqeHSXaz0OxlVdPJ5m2w6SXs6RSibYcIyyGOjKFm6pJtlsmLez8vJKMXOfablnqfgrBo1QBlzy0+kMElk7dq1b+jEAtMizefz875PLpejr6+Prq6uWa87G7OFz6rwcI29hp1iPzmRpkEP864G6IsvIaIIJrMWR7ISn6LyzmqFvqSbUE6y2WewL+mgQyaozOscHPewrhpeC8KWBtgfgvdUwwu9Kh11kr4c5AyoyRrsDznpWmxxcEzBnVBwO6bj1huW2gwPKCQygrVLbTQEB3sV2pptNLfNwcMqLqdkY5tN7yEVyxZ0LLWJ/t4do1gQHlBY1Wzj1OBQj0I84WXdMheBJpt41uaSuqf5eMcDACyvHeNwdBVti8bQVMmRiRUk8yrrWwdI5V0cCy7GX29Q504yYjeSw8ciX5CM5STW0MQScZgJsxq9xqBZyfLC5GrW1PYzaVTR3JRgKuPFsF1c4nuFvC64q+8ymj1OvBmFnrhNlWt6OESHV7A3I2nXJKtdKiHpQHFoNDglqsiiZDNIK059OsvypAOrvjTFHlCanftsikaEEP8OvBuoFUIMA3/LdGOGnwkhPg4MADfO97pl3bmllBw9epT+/v5ZWywVmO/OLaWkv7+faDRKe3v7nIUNJ4+NLzZ9tPU6SFousroPpyPJ6ppDNGp5lusqF3mnY8sjhsVSJ1xWqeLVXPyBU0NalRyOeVjhy/BqCDp9KUbjJpd4p4W9sVnSOym4aBGIiEk6CR5dsn9Y4ZIm6BmY7uoScEvyYYXWalCEJJcSONICRUiGxwXejILPLcnmBCKq0Nk6fQzTDMiFBKtabBRDkEgKRo8rWFMpnA6LJYtsDvWoJIcSfPGy7/DhLY8zmFwEwOHUMswqD5oqsaUg4fHS1hQCIYiZfiK1TQTcKXKWg7DLzbhVSd7S6LUW0VI/wvOJTjJ+HY+eYyDThN6UYSJfS9TlxKUZjItKlGqTl9MruLj2AH+/+imCyQlwTMf/m3RBJit4LWNzsV9lNCMwEdTqCks1QRIbQ1eoqMnRVunloiGVRCLBq6++WhwMcKbpogVKsXOnUqkz7VmOlPJPpJRNUkqHlLJFSvm/z7ZRA5QxQ800Tfbt24fb7Wbz5s2nbfh+sjG+MykkpgghaG1tnfcvZTZxp9Pp6X7TbW34hckLDKMrCpptsaT2EPvDrQyH6tFthWop6ElMm82rXIIjKcCQrKkXvDbu4R0NIHJOtLDNq3Fo9KTpHnWzvsFk7LhNrcfmcMxDR5MkICXdBxXaGyVHxwWXtEn2HxVkc4J17TaxcYWekGD1chuHCocOKjTUSZa32fTsm95ptm6w2NutYEuBlhb4HTCqSpqrEgweC1BVKWnwWNQsP8TVG37L4sAxQGB6PBw0VtK+cgTDVBgdqyPmqaSlbYz+gWZqjShmHSxyD3Hw2FK0KkFzXYisofO70Q5WLxkimXOhLzEYiVah5SupawphobHXWMwa5zCHU0tpb5igN9pMzaIoz4yvYFNzPx/3WERlNQ/1r+PXUx7eWy84mlU5mLVY71MYM0wqVYFfhyZvCpc3ghcnH040sd8Ks2TJkqK1F4lEOHLkCNlslsrKSmpqaopO2rli2/ZZ79xlMMvPmrKIuzCpc+nSpSxatOi0//5k8eqZZLNZ9u7dS2NjI62trQwODs7p+05kprgLKa9r1qzB6/VyhYRY3qbbEUQTBho26+qGCThj/OfQUmzbyUq/IJ6Dl6I2HW4FzQn9Scn7GwVHRgU+4SBuCSo8kJMulldkmRoEaakcS7rpaDCosDWUvIJhCoJhycVLJXv3qixplgxOSfS0oNYnCYUhGYVqh0DXJJEIVEiF5gab0QlBbFBhRT2MJiX5uGBwTKGxPoHITTubqiuzrGl8gsvf8QKGoRGb8pEznSSqNczw9K8/b7npcSxhXWMfACIgGMlVscQzTjbvZLIiQEfFIJatMGo1412SJ214CKnVVFdFGKMOl2KgKDCUa2ZZa5CeiXYa/GOMJeuoq51iMlvN0rYwe0LLaQyESGRU/vLiX3Flqp6nw01c5PfTn9MwNJt2f5Y6fwqnK40tBA48fCK3mMMHD9LR0VFMFnE4HNTX11NXV4cQglgsxtTUFEePHp21tPNklGLG15utaATKJG6n08m6devmnEQ/F7M8Fouxf//+19V1n4kj7kRxDw8PMzw8TFdXF7quFyc0fpAW4obNYXUKXclhK9DkS3DtykMcH2tlPFxDnQrLqhXyGYGRVWjQ4Pl+QbsP4lmwpKTaI6i2bSITkMp5qPGDKSXObI5wxCSU8NLZmiWTdTDZr1JXDf3DChetsji0X8EwBJ0dNtmo4Ei/wuJWmwq/5FC3iu6UXLzWYu+L07/CtastIgkJKNR4dYZ6nWz7QC9bL3uU+OC0uehwmBzPrKS+fYiAI06UCkbDjeRrYWnnEBNDi8hLDefiKOZUBamslxG7mkUrJjl0uA2316S2JUTe0HhxYAVdy44QjFYhagwSpkr/+HLa28cYCNVT2TJJ/2QDPt1AGj4cvjzRbCXVTVMMJhpQXYLDSTcBd47rV/QQR2eNdJGWGnmpY6oqGaGiSxcfzy2mZ88BOjo6qK6uLv4ubdtGSollWdi2TSAQIBAI0NbWRjabJRKJFEs7q6qqqKmpIRAIvEHIpTLL30x55VAmcauqOq/qmNOJdHR0lIGBATZu3Pi6c42iKBjG3FrpnPg9lmXR09NDJpNh06ZNRVGf6Aj8c7WV/zcv6NUi5CwnDmmhSot1yw7i8Tfy/MEl5CJu2p2CjAU9U7ChHoaioCNZWyeYGLZJWRnGkxWsbYHDg3BJq6D7kB+PC2oCFuGwQq0Sp3+qmgqfScdi6Nun0dokGRqXOA1QFHA4JFYOklGFmmpJNgtDezTWdNgcGxKM9glSCYW1G3JEprJc+ee/ZPOGvWgOE31ZhkQ8wFCinpquceIjFVTXTJEWVYwKlY6qfkBwPF1De8cAqsPG7TPpiyyntfUYpqmSrXLgUAT5vIPxfB2t68foPbIUb2MCryfL4eFGqpakGJ1qJNCQIJd346y1iZsak8lKKrQYwmMQS1VTURNnMlWDz2OT1BWm8tWkLZ28ENgOFaEKJAqYTv4838Tx7gOsWrWKysrKN/wuC88PTIvUsiyklDidThobG2lsbERKSTQaLTZs8Hg8xV1d1/WSifttsXPPl8IY35kUElPS6TRbtmx5Q/z6THZu27aJRqP4/X7WrVtXrCqbLYT2F/pi/lcOBtQYuipxCxhL1xJwZ/nTd7/MviNLeWl/E/XorGsQRFOwrhJyScHBw4Jmb4r+eAUbl0qOjwkubYWX9qmsWmJzeEDQ0Sywow4Gh6tZtdxgeExgjOfQNYvj/S7WrMgxdEQnkxa0LrXRTMHgqILXL+lcbrH/ZY2+vSqr12U4dlRB90iWb9rD6st+i+qwGDvcwuLWfsLhenLSRdOaAQAmMxVMDlVSs3aS6oyDVLyCkXgt9VuD9O9fQoUnjlkv8NZMEB6tJyYd1LWFSUU9DEaaqW0LkoxVYC6VJEMBwvFaFrVPEo1WEHe7CCQg63Dg8WbIWFXoTVnGY/XkpwzqmpJMRutQvVlMXSGX8TFlK3icoIrpeHzOcmDbDv4iV8f4/ukjU0XF6UNfiqIURTpzV6+qqqKyshJFUUin00QiEfbv349t2+RyOeLxOBUVFWfcBTWZTJ62Gei55pyEwk7HbE4uwzDo7u6moqKCrq6uWa95qsKR2chkMnR3d6PrOm1tbacUdoH/7mjmKznBgJ3EwERgUeHJMhGpoq0hROeyYY52tzG4v5Emh4O+URB5i7rKPMfDfjYukZARrA1MC3ttu83+owoXd1gM9qh43ODzSoaHNNYulnTv8VFZaVPVlGPsoIbXnQG3A7dtk4zo1NbZ+HySvhdVVq61CIcshvskG6/qZv31L4KwyMaceANp9JoMPQPt1KydQDEU4sEKkDpWKygJ7fc/EycDkQZa1/QDIKoUkpqXgH+KdNzNpKhiUdMYRk4nZARw1JiEJqqwawUuf5aJaBMONU0q6SPrVvF7M4yHqvDJHOMRN776FMFYAN1voNbC4fFmPG4DM+snlVZxuSUeTZAyHaRtDUPRcNg6f5GrJHJwOnx6JjXSJ9vVbdvG7XbjdrtZtGgRtm3zyiuvMDIyQk9PD36/v1jaOZ8Krzdb/zR4k+zcM3fgZDJJd3c3y5Yto7Gx8ZTfN1eH2om92Pbt28fIyAh1dXWnnQeuKAp3upr5UnqcfiWOU9EIpxU01QJdko76aVk+yrL1xznwymIalACRoJ+pnId3tUmCQypVbkn3gKBjic2BY4J3dlgc2qOyYpnkwGGFJS02flNw4FWFVZ02/f2CBo+OVgnjo146OnKERwWxKQVvRRbNaWOZHsLJCJtvepX6dxwn2R9Ac03nCoQnG5kKChydEeysc3o4tlCYTLYQWDWIS4O8ZhMaaka2pqhqGSfY00jO0PGsD5MP+YiMVGPWKgRWTTHZ14IaSBNYEic95WcyW0Wjb4TQUBPu5VNkIz7CCR8VlWEmwrX4FsUYGanDAaRDLny1aaKGi2xcx9uSJJ70Y9g60m0xKlWk1LCFiiUUyLv4RN5N4nAf69evL9k5duauXviTy+VQVZUVK1YghCCVSjE1NcXw8DBA0Xz3+Xyn3ATeNmdumF+TxBN/aIWEl/Xr15/2jT3XnXtkZITBwcGi42zTpk1MTk7S3d0NQG1tLfX19SctwhdC8LeeRr4UVziiJFGEikMxIeXA68qSlA6cU0461w+jXDREZrCW1GAV+5+pZWXAxaHjgvZmyWRE8M42yb5XVdpap4W9vtMiM6qQscDjheO9gg2dNt0varg9kpWdFsFDToSA1jYbb00Wo2GEbXccxLc0SvzlOjSnSWVHiIl9TeRNB87OJGpCRXPZqCuCDL/citaew7EpSHh/A9VLw4yP1iMVaPLEsLI6KU8F7voQQhXEEx7SGSetVcMkxv1kmy2M/gABHKQqJO7GCIf+czktW4dIRbzkdAc0ZxkeWEz1inEiY/UElsSITtRiunNMTtTj9xo4atLEohWYUmC68pg4QKpYlkbGdIDt4DOmk1zvcbq6ukraGvtECkLPZrMcOnSIlStXFjcKj8eD2+1m8eLFmKZJNBplcHCQZDJJRUUFtbW1VFVVveGIuHDmPgVSSo4dO0Y4HGbLli2zjh6ayel2biklfX19JJNJNm/eXDTBvV4vXq+XpUuXks/nmZycpK+vj2w2S3V1NXV1dVRWVr5O6EII/i7QwJcign1KEpeQmLZCJuNByUsCVUkyGT/6lIa/dRIRSHLZ1mNEx7xsCVUiBgL4jwU41OuiqUEyMCRYt9IiclShrh56DigsarapboD9L2is2mAxcExg5rO0XTNFviFI/Yo4FRumz835Q9OJO57VERIjARLhCqgzCbRMIRRJcqwKVTeJRKvILbKpb4wAgpzqIpSuxbNlCmlJxl5qQSzN4Vg1RXhvDV6nQF0ZpUJJMrF3CWpHBM2bIxHyksv4qWoJEzrSiO+SMBO9LTjqUmj+PPHxGtRVcY51L6FqRYipoTrU5iRW3I+slEx5ITfRgG0rmA6wMyoxAzJCwbRBz8NfJnJkQiNs2rTpjGa2z4dsNsuePXve4Kgr7OiFrLXCzi2EIJFIMDU1xcDAwOv+zuPxlMQsF0JcCdwDqMAPpZTzGh/0huudZnc94/7E+Xx+zju3ZVk8/fTTNDU1FTtezIVUKkVvby8bN258w98Vkmi8Xi/Lli0rekRPZVoVWj1NTk4Si8WoqKigrq6Ompqa1yU5fGUqzO9kCk1IXI4MXsXAzusohqS6JY4VcaGkBQF/GjI6WnMMY6AC55IY2cNV2KqNa6KCZBScWSfxpE1dpUBRDAxh4vBnUevT+BSJtjxE5lCA6pUhkJJcTwUVK6cwxryYEzqG7kDVVFwrp+sK7O4A7kVRwgO1oIC/KzT92fZUk7EEzosiZF6qpbZrlGRvE/k6A18ghm0K4qN12Cmo3jBB7GADRlsKX1yQDHpQ18ex8yocrcKxdhIj5CXhUFAjLnI5ib8zTrCnDk9nnNSxKhwOi5yQSLeN9NhkpnzknGDrknROQ6KRtxTypopuOvjvGYvUQD8ulwshBLW1tdTW1p7WHD4TTibsmRSccgWxFxBCYBgGkUiEcDjM9773PXp6evjEJz7BzTfffCYWhxBCqEAv8D5gGHgJ+BMp5cH5Xqx40XKJ2zCMOSem7Nmzh2w2y7vf/e553SObzXLgwAE2b978uq9nMhn27NlDa2srDQ0Nc3KczURKSSwWY3JyknA4jNPpLLbIdTqdfCsU59FcFAULjy5wCQtXVQYr6MQlJf4lMbIjPjR/Bm9MxzBNKlsSkFfRfDnMqANHwMDKCZyKRLjzKGNe1KYkuQNVeNeEyQ148DbHEQrYB/04G1LEe6txV1nkW9NwxI93XRBpS7QRD8JpETkawLk0id6UwY440G2DzGCAXL2F35tFDeTIjXiwwxWovxd++jd1OFYmUBpz2GkVcbQKuT6MtGHqpUYqVkRQvQZTx2pR29LIQx60VSmkKkmHA6BbOCI6oiNKuLcaR1uK2FgFTpfESKlk8yrOgE1GkRiWg7yhkbNVUhJqLJ0vSkl0dDrfwOFwkM/nCYfDTE5OkkqlCAQC1NXVUV1dfdaZZHMV9mycGGo7sdTYMAw+9KEPsWrVKl599VWeeuqp+ToBhRDincDfSSm3/f4LX/j99b86r0WeeNHzKe6Ck2v16tX09PSwdevWecUb8/k8e/bs4eKLLy5+LRqNcuDAAVavXo3f70dKWbKe1JOTk4RCIWzbRtM0HpM6v6mrQdctPEKiChtV2jir01gTbip0E9WXhZyC1pSBoz70ihzmsI7XZ2DGFSqaM+RHdbxtcdLDHioWpZBJB1ZSQdUUcpMO9IYcOWzcbQmEA+iuQl8fQmYU3HGF1LAH03agd4Wn/35/JZ7OIMlXa1F0HWX973f1/RVYcQ17SwIRd+BxZMkNVpHpjOPu9UEgQyLngQoTT9rEMD2Y7SnUAS+2CbI9hXksQK4+jzbghGobsz6LMVRJrimDesyP0pzASLkxHJKMBiLhIatJMgIsWyNvqBiWQt5UWCwd3CUNQsFRurq6Zi3VtW27+JKdmprC6XQWd/X57pCZTIa9e/eekbBnW1ch1JZMJtm4cSOvvfYaLS1n1DxCCCFuAK6UUn7i91+4GdgqpfzUma7xvJ25h4eHGRoaKg4cKHjM5yPEmWfuQrLLhg0bil7wUo01KpzTW1tbOXjwIJlMhqsdJnWjE/x7ZT05p0DXbTzCxgz7cAlJ2g1K2gOKjbLHjb85TSanoq9NkknqiLY04YgDdWOa5KAfx/o0xqEqlNUx7P1e9M4otOax8xK1yoCeGlgZxqzJovTUkskpTGUErndOARnMF2rxbA5joRLesxj74imQaRx7q9A9gkSjhKyOW5cYCZ3JYAX6O6YQCNJCx8pqiOVJjH4vkYwPvTWOPegl7hWQVVBfq0Wuj2FHXMQqVfSIEyXjwlqcQY74ibfksMMBjLiGq9JCGiopXSIVsAwNM+8gn1dJWyqbNQefJsVkKMjGjRtPuiMXWhoVmmam02lCoRCHDh3CMIyijyQQCJzSMisIu7Ozc16FRiej8FzlcjluueUW7r77bpqb5zZ48FxRVm/5bNi2zeHDh4vDAgu/1IK45xNbLMTHpZQcOXKEeDzOpk2bimfrUp/VCuf4yspK1qxZgxCCdZbFhtEwd0xZZJwCy6Hg0BQMCXZK4MyBTwFzWYpwTsepSqxBF3pGYKtOnFJgOy0cToG0wF6cgbiGsjYGvX4sr4Ey6sXymIRSCq7eBrIrEjgPOFG6Iggk4nAlpisPbgexsRrMdRmYcOBOaTDiZUo6cDYkEQEToUvyrzSS2BiHZWmM39aj1prkVmcgqCGeq8W6KAnONPZL9dirYwinBccrSbYY+I9Xkag1EE5JJK2hOyT516pwdGSwEm4soZBpNohPeBCu6YkqRkYlYwpMW0VaGjd4dK41pwhHIqed1joTj8dDa2srra2tmKbJ1NQUo6OjHDp0CL/fT21tLTU1Na97jkot7ALZbJaPfOQj3Hjjjdx6661n+7yNACcOE2/5/dfOmLKZ5aZpviFMlc/n2bt3L9XV1Sxbtux1P4w9e/awYsWKeccKn3vuuWL4Yvny5Wd0vp4L2WyW7u5uWltbZ429pwyDTxzJMGDnUTUDt2rhMAVuwFyUw5NwoMZB8xu4TUG+PYXruIdcWwpfv4dMWxJfj4dcSwbvuJtMWwIx6UCry4Nu4z7iwepIouzzoK+PgS0Rz1WiV0JGl8iGNAQs1P1e9M44Sl+AxLCO8t7pSkHPIS8iJ4i3GMiEhrcphXWggnR7noqwilWdIR/0kdXAV5Umc8yPsSmN2ONF80iMjjQcqCDZlMM76CLXnAcH5NM6tgbJcQcVqkK2Mo+dc2CjkFEgbahgqRhZFdtUuWuRm7bYKMlkkrVr15bMspJSkkgkij4SRVGora3F7/fT29vL6tWrSyrsXC7HTTfdxJVXXsmnPvWps33ehBBCY9qh9odMi/ol4E+llAfO+KLnStyJRIJ9+/axfPly6uvf2K1y3759LFmyZE5phgWy2Sy//e1v6ezsLOYQl0PY8XicAwcOFJs5noq/60vyeMLA5bCRuomq2jjSAicWWpOBCOmYLSnqxj0Iy0Z3muRMG1e9gekxcGdVjOocjl439so0noM+rKUp5GE3qpA4XQqZIY3cliQqCppmICssfAe8iLo02T4fqibIbo0DUHnQTTYuSDfa+KIOjI0xHMNuckfdWJdFpxf9bABHvUF+RQbtuBsjrqO0J1EnnMSqJEpK4J5wkNyYRu31kWg0UA+5UWpsbAXS1SbuSTdhn4lI6ygZlWROIBwKWVtgWgq1qHxvuU56dIhcLle0fMpFLpdjbGyM48ePo+s6NTU1xRj12TrlDMPglltu4dJLL+X2228vxecQAEKIq4B/ZjoU9v9JKe8+q4uWS9yF3mYAExMTHD16lPXr1580Fnjw4EGamprmPJCgUCVmmiaXXHJJ2YQ9OTlZXPtci/EfG8ty94CBodjoio3iNvEoEtWysVwmekRF91qYLXm0cR2rNYtvxElmcRr9mI5SayIyCg63Td5v4BpwYa3KUHXYTaoziZISuPICo9LA82wFWrVN3G2hhRTMd6QQBrgPulAsjUTAwu20ybdkUQZ0tD4PqT+MA4Lq1zykfSbp5QaOsIZ+yEX8HUmwFLwH/KSb0qi6IDXpxGo0cO/xkH1nHMcRN5FmC1e/G1OXiIiG3WiR0yRGSEfRfy/qvEI2J3inV+ebq5309fUhpWTVqlVlFTb8lym+evVqfD4fkUiEUChEJBLB5XIVnXLzjaebpsnHPvYxNm/ezOc///lSfY6y/DDKKm7DMDh27BiRSIT169efMjHl8OHDxbfr6RgfH+f48eOsW7eOnp4ehBDU19dTW1s7p+SXuTI0NEQwGGT9+vXzniQxmc7xyb0mA5aFrkkcChgeg+qcgqVb5KoMKsIC4bDwxFTwSpSUxG7L4ZvUSa5M4z/qIt2exhN0oGoSK6Lgy2mYqiSfFGTXpTG9kup9btIbk2hxBf8xJ+msQr7JRKsxMH0W3v0e7IggtjWDllMIBFWsmEZ4dYbAK14cDSZTqkAGbPxHHOSaLFKNJv5uL5Zuk2/MY064SDWbuLtdiDoDI6WRac4jJx3YtkpSF2hpDWkKTKmQNsFtq3xuqcZ1LU4OHTqEpmnFNM9ykk6n6e7uZvXq1bNagqlUilAoVJwDVnjuTlc4YlkWt912GytXruSLX/xiKT/HW0vcuVyOPXv24HQ6Wbly5WnPVkeOHMHv99PQcPL2zIW2TbFYrHheE0KQTqcJBoOEQqGi0Ovq6s44fbFQjZbP51mzZs1ZnQvv2Z/jx2MWtmZj6xZ+AbbLRjEkbhs8qiTWkMUdU0g3mQSmFFL1JoFxnXhLnsZ+J+H2LJUHXKTWp3AkFJxSkq82CbzkhtY8zqCOMSWIX5TFGdaQjVmkJvE+6cduNomszlF5zAm1eeR+D8lGE7ffQFgCJaGj5hSSyzO4er1MtuSpH9PImAqRVXl8PU4cAowKm7xQyKsSI6GhxVVMC1wuiKuSvKVgZ8HKqeRthVVOhe9udVKtKxw4cACPx/MGP0s5OJ2wZ2IYRjFxKZFIFFNMa2pqXheasyyL//bf/huLFi3i7rvvLvXneGuJe2JiglQqNefwwPHjx3E6nSft3GJZFvv27cPpdLJixYqTmuG5XI5gMFic41RTU0N9ff2cM50K9/H7/SV7GPvjBn/znza9poXQbWyXTaUtcFiSeEMOdxwMF9SmNYyUxFthYmclqt/Eyqo4hUC4JN6UguG0MRMCq8oiucggMKQTX5PFG1LRXCbKoI47pTG5IodRadPY4yTjz+MI6eRMQXpTGmdMwdXnJtNikGky8b7kQQHiGzJUHnUzWmXSFNRJpyHSlcXzmod0s0Vg0EF6sYEz5iDos1DjGsJQSQG2oWIbAq8p+RN/nEs9Q8C0eGpra8/pjr1mzZozqiSTUhKPx4sxdVVVi3/+5V/+hcrKSr7+9a+XzAl4Am8tcdu2Pa9GCoODgwghWLx48Rv+rpBVVBi1OtfztWEYhEKhYqZTdXU19fX1b8gbL5DL5di7dy8tLS1zag81X/53t8mPjlpEhE1Gl3iQKLaJWSkJRDXcbkm2No8zL4jWG1SMa6QWGbiSgC4xPTZVQw4SK/Is6tNJVBk4Rx1UaBDVJGYGcutz2Bo0vuJCuG2itsAjYWp9Fs+whndYJbjWwPZAzSsupJBMduUJDOnoSZVYk0FFSGMsYFMRV3HGVMKLDZQhnXSthR1SqEDDtCCTFSi6wDRByQs2+1TuvkShzqthWRZ79uxB13UsyyKbzVJVVVXM2y+1QM5W2LORzWZ59tlnueuuuxgbG+Omm27i1ltvpbOzsyTXP4ELW9wjIyMYhsHSpUtf9/V4PM6+fftYtWoVgUDgjB1ntm0XUxoLeeP19fXFlMZkMsn+/fvf0Mqn1ETSNl9+xubZSZOUZpPzQYUpQEgsn40mIFJtsGhSJe+RVKUhbYNLgHRYmLqFZgkmm22ajzgIrs/jCim4sXHZAs+wg5jXIrzCZPFhnamVWaqOObFGFYLvySAsQeteF6lai/Fmi+pxFd+QRv9FWSoGdKyUgltOP23Di03qRp1ENZvajErYBqeqkEaSMRR0S8GVFywWgr/eLLi8bdqMLQyfaGxsLFpulmURiUSYnJwkGo3i8/mKTq0zmYx5IqlUin379pVU2DD9zNx1111kMhn+6Z/+iWeeeYaWlhbWrVtXsnv8nreWuKWU8+pFPj4+TiqVor29vfi1E73spcw4K+SNB4PBovmVzWZZt27dWaclzoXBwUFeOhLjkWAnfVlJ2ilBkeQ0SLptKvLTc7G0JAi/RE+B1WChBxXsBgtlUsHht7FsGyWkEW81kZqN5RNkK22W9TiwnZKsIRApGN+UxzslaBzUGW8xiVfZNO3RcTklR1cYBCY0XEMa0TYDX9jBpM8mEFPRUoJsrSQLODIqCQXcKQWnBbopqRMaH14FN138X6ElwzDYs2cPLS0tNDXNPi9bSkkymSym8yqKQl1dHbW1tfPOc0ilUnR3d59xU4eTIaXkf/7P/0kwGOSHP/xhyXqkn4QLW9yTk5NEIhE6OjqK5Z+RSIS1a9eiqmpZwlzwX00Sa2pqiEQiaJpWLBApddlhIZMum80WHXUv9Er+9wuSo1lJziFJaxaKVFBUG8sJLhMmKm2qwwqDK/IsmVAYW2RRHVVASjRVUhNVyBhgWRKz0mZ8sWTxcZW838YVVHGoFoPLbYSlsPSIRtyGkdUGdX0OqvKCYx156kccxISkJqliCcFIhU1lVCXrkPgMBd0UaPnpAXzVCN67VPD//KGCdsIzX8j1X7p06ay5DCcjm80Wj0+5XO51KaWnepmXU9j/8A//wPHjx/nXf/3XcgsbLnRxT01NMTExQUdHB/v378fhcLBixYrpRZZB2AXPeyqVKr5AYDo+Ojk5yeTkJJZlUVdXR319/Vl32bBtm4MHD6Lr+qzOpdeOwo+egcMxSVyzUIVCXthk3BKPFByvtWiICAbrLeomBcEmG09GoAuI+S1WjmiknTaejIKCxUAVeLLg8Np44ip2UiFVZ5NwSxZPqsgM9C+1aJ7QyAmJxxZYUhB229RFVew8OIVAEeCwpuec1Wrw3pVw87sFM2s8CtGR9vb2OYUzT8bMstuTpZQWhL1u3bqSNkmQUvLNb36Tffv28dOf/vSUc+dKyIUt7lgsRn9/P9lslsbGRlpaWrBtuyzCtiyLAwcO4HK5TunFzefzhEIhgsEg2Wy26HmfbyM90zTp7u6mpqaGJUuWnPLfjk7CT34heHnYJiUh75DkFUBIUl5QbUlOE2DbWDZkKiy8WYWRekn7uELCZ6OOKXgVk6kGQVKX1AUFtimwFPBqkNEFHkshh43PVMiqEksBT0rgthRsU6KoTJ+pLcGyCsG1F0su3zT7mgsJI6X2V8xMKVVVlbq6OrxeL729vWUR9ne+8x1eeOEFfvazn521L2AevLXEDdNv87kyMTHBvn376OrqorKysmwZZ/l8nu7u7uILZK5YlkU4HCYYDJJIJKisrKS+vp6qqqpTmo4FD/zJctJPhm3Dr1+E37woOBKFNGCoEkuR5DUbhxBMaTYVpspYhUlTUiXsl/hjAqkYCKmhKpBxS9JCUBsGHBCqkFQlJGoeDI+CbigIJM6cgkOVSEOgIVjkhs1tsOO9kqrKk6+z4KUuRRnl6chmswwPDzM4OIjL5Soen05XETYXpJT8y7/8C08++SQ7d+48bW+9EvPWE/dcu7EEg0F6e3vRdb3YVaUMscSiV3X58uVnZToW2iMHg0EikQg+n4/6+vo3JD4U7ne2O5ptw3P/CS+8ojAYhmgG8poEBBlN4rAEWYeNJsGwTWxUdIfAlpDRbAKmQt4EywF6fvqlqUiBZdkoho1qgl+XLKpUWNuhcOUVUD2HLOBkMsm+fftKfuY93f3WrVuH2+0uRj/i8Th+v7/YNWe+prSUknvvvZeHH36YBx98sOwtnmbhwhN3YaBfKBSis7OTV155hfr6ehoaGs6qh/RsRCIRDh8+XPJwScF0DAaDhMNhdF0vdms5cuRIWR78bBZefhF6egQTkwqRBCTzNsm0gaLroE6/EKQKqi1AlUhboNo2TlXBo4LPA/XVsKTVZvOWPIo6ffxIp9Ov6yN3spdsIpFg//79JTeNT0ZB2LN1RD0x+SQcDuNwOKitrZ1zluKPf/xj7rvvPn7+85+f8TC/s+TCEndhoJ+iKHR0dBS/NjU1VTR9q6qqikknZ7OTj42NMTQ0xPr168v+Vk6n0xw7doxgMIjX66WhoaHYWbVcTE1N0dvby4YNGzANN9EQZDOQNwRCgu6SBKqhuhZO5/gt/A4K8ejZdsRYLMbBgwfZsGHDORHDqYQ9G5lMpuh9NwyDmpoa6urqZt0wfvazn/Gv//qv7N69+3y2Jn7riftkrZYKIZP6+npaWlpmPV/btk0kEiEYDBKNRotJJzU1NXMWupSS48ePE4vFWLdu3TnxfA4PDzM+Ps6GDRuwbbvoec/n80WHnN/vL5lVEgwG6e/vf133mVIx247o9XoJh8Ns3LixbK2HT2S+wp6JaZpF872QO15ZWYnf7+fJJ5/k+9//Po888sg5OVacggtD3IlEgu7ubjo6OqiqqpqT4+zEpJNwOFzcEU91vrJtm0OHDqGqKh0dHWU5w89c47Fjx4pNCGbGRgsPWTAYJJlMlsQqGRkZYWxsjA0bNpwTz+7o6ChHjhzB7XYjpSyavuXoUAr/ZfqXajhB4Tl6+eWXuf3224nH43z+85/nwx/+cFnSjefBW1/chf7g69atK5rH832wTzzjhkIhXC5XsQqs8IAXRhHV1tbS2tpa9oKFE18kK1euPO39Zlolfr+/aJXMNWGiv7+/WEp7DpIsihZCYbDDbHn7pcwbTyQSHDhwgHXr1pXcXH7iiSf42te+xne+8x2effZZGhoa+PCHP1zSe8yTt664pZQMDAwQDAaL5nGpwlypVKpYBaaqKlVVVUxMTNDe3j6vLKkzxbIsuru7qaqqYsmSJfP+TAXTt2CVzPaymvnvjxw5Qi6XY/Xq1WW3SGA6NXhoaKjYengmM8/pJ4sezJWCsOfTIGOuPPXUU3zpS1/i0UcffTMN7nvrids0TQzD4ODBg8UOHFCejDOY3l0OHTqE0+lEVVXq6+upr68v29mw0BPuVHnU82Xmy6oQyy2YwiceNcptkcC06V/wIcxFqDOjBw6HY17pvOUU9jPPPMOdd97JI488Mq+cg/liWRYXXXQRzc3N7N69ey7f8tYTdzqd5pVXXqGuro7FixeXLTEFppNg+vv7Wb9+PW63m1wux+TkJMFgENM0i/PAShW2KSRvrFixgpqampJccybZbPZ1n8GyLKqqqubU/KIUDA0NMTk5Oe8OpScyM533VOf0eDzOwYMHyyLs559/ns997nM8/PDDZW9B/I1vfIOXX36ZeDx+4Yr7yJEjOJ1Oqquryybsgsk/NTXFunXrZjUbC+fDYDBIJpMpCv1MvdaFUNBc50afLaZpsmfPHlwuF5ZlkclkirXppcjOmo3+/v5ilKFUL5KZ5/QTnYrJZLJswn7xxRf5zGc+w8MPPzxrv4BSMjw8zC233MKdd97JN77xjfMq7rLGhiorK3E6nSWb+jGTQg9027bp6uo66T0cDgdNTU00NTVhWRahUIiBgQGSyeRpGzjMJBQKceTIkbJOoTyRgum/ePHioilZKK4YGRnh0KFDxXE78wkTnoyC1z+dTpdU2PD630PhnD4xMcHBgwcxDIP29vaS9sADePXVV/n0pz/Ngw8+WHZhA3zmM5/h61//OolEouz3Oh1lFff/+B//g9dee42rrrqK7du3z8mTPFdOHBCwdOnSOV9XVVUaGhpoaGgoPmBjY2P09PQQCASKDRxme6hHR0cZGZmeQlnqh3A2stkse/fufUOl1YlncSllMRX2yJEjeL3eYm30fMNjhamohmGwdu3asp7pC33FHQ4H0WiUlStXEo/HeeWVV+Z9Tj8Z+/bt4y//8i/ZtWsXbW1tJVz97OzevZv6+no2b97M008/Xfb7nY6ymuUwnT3185//nF27djEyMsK2bdu47rrrzsrTe7oBAWfCiSKZmpp6XXhKUZTXmannIvRUyEufT0FGoQlCIUxYEEl9ff1pE1yklMVOsqV8CZ+KWCzGoUOH2LBhw+usoJnn9Pn2wYPpVtkf+9jH+NnPflZ05JabL3zhC/z4xz9G0zSy2SzxeJzrr7+en/zkJ6f71rfemXsmsViM3bt3s2vXLo4dO8b73vc+tm/ffkqTeiaFxIa5DAg4U2aGp0zTxOVynbY9c6koDEE427ztgkiCwWAx6WS22nQpZbHWfPny5edV2DOZ7ZxeV1d3ymq8w4cPc8stt/Bv//ZvrF27tlwf4ZQ8/fTT/OM//uOF61A7FclkkkcffZSdO3fS09PDFVdcwfbt29myZctJf2lnMiDgbCjEsN1uNw6Ho7gbFkJs5RD6iXnipTzTn6w23efzceDAAXw+H21tbW8qYc+kkPxT6Nrj8/mKR5BCmO7o0aN85CMf4Uc/+hFdXV1l+gSn520t7hPJZDI88cQT7Ny5k71793LppZeyY8cO3vnOdxZN4KGhISYmJs7Z7lmo+25qanpd6KTQI31ycrLYI72+vr4kBSnlzBM/kYJTsfA5fD4f7e3tp61NLwXRaJSenp6zfnnNbOTw2muvMTIywqOPPsqPfvQjLrroohKuuuxcuOI+kVwux69+9St27tzJSy+9xNatWwmHw1x++eXceuut5yS+W+gs0t7efsosphPj0Gfbkml0dJTR0dFzliduWRZ79+4tNiUs7IZnUxd9OgrC7urqKnl13u9+9zs+//nPA9NJUvfccw9bt24t6T3KyNtD3CcSi8W4+uqrsSyLeDzOxRdfzI4dO7j88svLtnsXzvTznQo50+ytra2loaFhTk6gQpz+XOWJF+LmhT7wBU6sAguFQjidzmIq7Nn+vMsp7LGxMW644QbuueceLrvsMqLRKIqinJMchBLx9hP3vn37ePXVV7nlllswTZNnn32W++67j9/85jd0dXWxY8cOrrjiipI9LOFwmL6+vrM+05umWRR6KpUqnm9nJpycjzzxQuvh1tbWU45ugmmPfcFrLYQoWibzNafLKeyJiQk++MEP8o//+I9cccUVJb12gaGhIT760Y8yMTGBEIJPfvKTfPrTny7lLd5+4j4ZlmXxwgsvsHPnTp566ik6OzvZvn0773//+89YlIWGDqU+7xYSToLBIPF4vNh7rbKyksOHD6MoyjkLPRXq6Nva2uZdNHFiOm9hRNBcwlPlFHYoFOL666/n7rvvZtu2bSW99omMjY0xNjbGpk2bSCQSbN68mQcffJDVq1eX6hYL4p4N27Z5+eWXue+++/jlL39Je3s71157LVdeeeWcCvBPTF9dv359WRs6FHqvTUxMMD4+jtvtZtmyZfMq9TxTCiOZSpELP9MyKZR7VlVVvU7ohdZW5RD21NQU119/PX/7t3/L1VdfXdJrn47t27fzqU99ive9732luuSCuE+Hbdvs3buX++67j8cff5yWlhauvfZarrrqqllj4oVpnqZp0tnZeU7M4kKb48LI2BMbUBTGEJf6BVNwEK5cuXLO88/nyomtsQpjmurq6lBVlb6+vrIIOxqN8sEPfpA77riDHTt2lPTap6O/v5/LLruM/fv3l/JMvyDu+SCl5MCBA+zcuZNHHnmEmpoaduzYwdVXX01NTQ2ZTIbe3l68Xi/t7e3nzCyerUR0ZmaZrus0NDSUZN54oXn/fB2EZ0Kh08ng4CCTk5NUV1fT0NBw0tr0MyEej3PDDTfw6U9/mj/+4z8uyTXnSjKZ5PLLL+fOO+/k+uuvL+WlF8R9phR26J07d/Lwww/jcrkIhUJ89rOf5YYbbjgnwj5ZnvhszKzpLnis57sDnuvWwzBtLvf19U03azTN4gurkA9/NjkByWSSG2+8kVtvvZWPfOQjJV75qTEMg2uuuYZt27Zx++23l/ryC+IuBcPDw1xzzTV0dXXR19eHw+Hg2muvZfv27TQ2NpZF6GeSJ14gm80SDAaLKaQFgZzOcVhIYS1V/7G5UBB2V1fXG5yShc9x4pimwvSQufzM0+k0H/rQh7j55pv5sz/7szJ9gtmRUnLLLbdQXV3NP//zP5fjFgviLgV9fX0Eg0EuueQSpJQMDQ2xa9cuHnzwQSzL4pprruG6666jpaWlJEIviKwUu2c+ny96rPP5fDGWPlMgpcoCmw+nEvZMDMMohtgymUyx9fDJatOz2Sx/8id/wgc/+EFuvfXWc2Jpncizzz7LpZde+roS2K985StcddVVpbrFgrjLiZSSsbEx7r//fh544AHS6TRXX30127dvZ9myZWf0QBXyxMuRCz+zAUUhlm6aZtkcWScjHA4Xa9znG0YsjGkqTA6ZWXaby+W46aab+MAHPsBf/dVfnXNhnyMWxH0uCQaDPPDAA9x///1EIhE+8IEPsGPHjjn3LgsGgxw/fvyMHvj5UhDI4OAgsViMxsZGmpqa3hCaKgcFYW/cuPGsnX+FUOHk5CQjIyN861vfIpfL8f73v58vfOELF6qwYUHc54+pqSkeeughdu3axdjYWLEm/WThs3OdJw7TmVoDAwOsX7+eVCrFxMQEsVjstA0ozoZSCnsm+Xyej33sYySTSaLRKFu2bOE73/lOSe/xJmJB3G8GYrEYDz/8MLt27aK/v5/3vve97Nixgw0bNqAoCj09PWQymXOWJw7TGVQjIyNveJnMbEBRaDlcW1t71msrp7BN0+S2225j1apVfPGLX0QIQTwefyvlis+XBXG/2UgkEsWa9MOHD1NTU0NjYyPf//73z9XQdoaHh5mYmKCrq+uUgp3LMIe5EgqFOHbsWHFAQSmxLItPfepTtLS08Pd///dlNcUff/xxPv3pT2NZFp/4xCeKVWXngQVxv1mxLIvbbruN0dFRqqqq6O7u5rLLLmPHjh284x3vKNsOPjg4SDgcPiMrIZlMFj3WmqYVhX46/0A5hW3bNp/5zGeorKzk61//elkzBi3LoqOjg1/+8pe0tLSwZcsW/v3f/72U+eLz4a3X/fTtgpSS973vfdx4440IIcjlcvzyl7/kpz/9KX/913/NH/zBH3DddddxySWXlGxHP378OPF4vHgcmC8+n6/YfSWTyRAMBunu7j5l9Ve5hf25z30Oj8dTdmHDdLvj5cuXs2zZMgA+/OEP89BDD50vcZeFhZ27zOTzeX7961+za9cunn/++WJN+mWXXXZGApFScvToUbLZbFnKRGcOcygIPZ1OF73/5RD2XXfdRSaT4bvf/e45yfHfuXMnjz/+OD/84Q+B6Rndv/vd7/j2t79d9nvPQll27vL/FM+Af/qnf0IIQSgUOt9LOWt0XWfbtm384Ac/YM+ePdx88808/vjjvOtd7+K2227jscceI5vNzulahTTafD7PmjVryiICp9NJS0sLmzZtYuPGjTidTg4cOEB3dzeBQIBsNnvSmetngpSSL3/5y0SjUb7zne+cE2G/XXjTmeVDQ0P84he/oLW19XwvpeRomsZ73vMe3vOe92BZFs8//zw7d+7kS1/6EqtXr2b79u28733vmzXh5cQ5YZ2dneck5utwOIrOtksuuYRYLHbGwxxmQ0rJP/zDPzA6Osq99957zqILAM3NzQwNDRX/f3h4uOxjhs41bzqz/IYbbuCuu+5i+/btvPzyy6ctsrgQsG2bl156qViTvnz5cnbs2MG2bdvw+XwYhkFPTw9ut/ucVbDBdLfZ48ePs3Hjxtd51G3bLs4aP7EBxXwaLEop+cY3vsH+/fv56U9/es6iCwVM06Sjo4Mnn3yS5uZmtmzZwr/927+xZs2ac7qO33PhO9Qeeughmpub2bBhw/leyjlFURS2bt3K1q1bsW2bPXv2sHPnTr7xjW/Q3NxMOBzmz/7sz7j55pvP2ZqCwSADAwNvEHZhvYXCj0JWWTAYpLe3d06zxqWUfPvb3+a1117j//7f/3vOhQ3TVtS3v/1ttm3bhmVZfOxjHztfwi4b53znfu9738v4+Pgbvn733Xfzla98hV/84hcEAgGWLl36ttm5T0Ymk+HKK6/E5/MxMTFBXV0d27dv55prrqG6urps9y0I+2TzuE/GzGEObre7GGIrCFhKyQ9+8AOeeuopdu3adU7aVL8FuLDj3Pv27eMP//APi+fN4eFhFi1axIsvvljWWcpvZo4fP85vf/tbPvrRjyKl5PDhw+zcuZPdu3dTUVHBtddeyzXXXENdXV3JTPUzFfZMpJTFNNhQKISmaTz//PPAdMP+Bx544JwVtrwFuLDFPZOFnfvkFMJhu3bt4qGHHsLpdPJHf/RHZ12TPjExweDg4FkLezampqb47Gc/y1NPPcXq1av5yEc+wic+8YmS3uMtzNsnFFZKPvvZz7Jq1SrWr1/PddddRzQaPd9LOmuEECxfvpw77riD5557jnvvvReAP//zP+fKK6/kW9/6FkNDQ/MKWRWEPdsZuxT88pe/ZHx8nGPHjvGTn/zkgvNMvxl50+7cpeIXv/gFV1xxBZqmcccddwDwta997TyvqjwUatJ37drFAw88QCaT4ZprrmH79u2nnAN2orDL4dy6//77+cEPfsAjjzxyzto9ffazn+Xhhx9G13Xa29v5P//n/5RtcGQJeHuZ5eXggQceYOfOnfz0pz8930spO1JKJicneeCBB9i1axfRaLQ4J/3EmvTx8XGGh4fp6uoqi7B3797NPffcwyOPPHJOxfUWe6kviPts+aM/+iM+9KEPcdNNN53vpZxzwuFwsSZ9YmKCbdu24XA4sG2bv/mbvymLsJ944gm+9rWv8eijj5bVu3863gIv9QVxn4xThde2b99e/O+XX36Z+++//0Lu6DEnotEod955Jw888ACLFi3iiiuuYMeOHaxfv75k6Z9PPvkkX/7yl3nkkUfmPd2k1LwFXuoXfhLLmfKrX/3qlH9/7733snv3bp588sm3vbBhOgllbGyM3t5epJQ88sgjfPOb36S3t7co9M2bN5+x0J955hm+9KUvsXv37rIKe64vdU3Tznkr5DcDF8TOfSoef/xxbr/9dn7zm9+c9x3kzU46neaxxx7j/vvvp7u7m8svv5wdO3awdevWOed9P/fcc9xxxx3s3r37dRNEzwf33nsv3//+93nyySdL3qCyxJRnx5FSnurPW5729nbZ0tIiN2zYIDds2CBvu+22M77WY489Jjs6OmR7e7v86le/WsJVvvnIZDLy5z//ufzoRz8q16xZIz/5yU/Kxx57TMZiMZlKpWb98+tf/1pu2LBBDg4Onu/ly8cee0x2dnbKYDB4vpcyF06nwzP6c8Hv3KXiTda545ySz+eL6aIvvPACW7duZceOHVx66aXF9NFXX32Vv/qrv+Khhx5i6dKl53fBwPLly8nlcsWhh+94xzv43ve+d55XdVIWHGrnkxdeeIG/+7u/44knngDgq1/9KgBf+MIXzueyzjmmafLMM89w33338dvf/pZNmzaxYcMG7r33Xu6//35WrFhxvpf4VmTBoXY+GRkZYfHixcX/b2lp4Xe/+915XNH5QdM0rrjiCq644gosy+K5557ji1/8It/97ncXhP0mY0HcC5wxqqpy2WWX8fTTT5/vpSwwCxd8bnmpeDt07ljgwmJB3HNky5Yt9PX1cfz4cfL5PP/xH//Btddee76X9bbjQuqvV24WxD1HTuzc0dnZyY033njWnTuGhoZ4z3vew+rVq1mzZg333HNPiVZ7YXIh99crBwve8vPI2NgYY2NjbNq0iUQiwebNm3nwwQffFuG1M+EC7q+3UM99odHU1MSmTZsA8Pv9dHZ2MjIycp5X9ebk7dpf72xY8Ja/Sejv7+e1115j69at53sp54259NdbYO4smOVvApLJJJdffjl33nkn119//flezpuOt0F/vYUMtQsRwzC45ppr2LZtG7fffvv5Xs5bgguwv97CmftCQ0rJxz/+cTo7OxeEvUDJWdi5zyPPPvssl156KevWrSvWTn/lK1/hqquuKsn1Lcvioosuorm5md27d5fkmguUhYXc8guNd73rXSUdqjeTe+65h87OTuLxeNnuscCblwWz/AJleHiYRx55ZKE3+NuY05nlC7xFEULsBL4K+IG/kVJec56XtMA5ZmHnvgARQlwDBKWUr5zvtSxw/lgQ94XJJcC1Qoh+4D+AK4QQPzm/S1rgXLNgll/gCCHezYJZ/rZkYedeYIELlIWde4EFLlAWdu4FFrhAWRD3AgtcoCyIe4EFLlAWxL3AAhcoC+JeYIELlAVxL7DABcqCuBdY4AJlQdwLLHCB8v8DMQaj32i6gu8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 偏导数\n",
    "import matplotlib.pylab as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "\n",
    "def function_2(x):  # f(x0, x1) = x0^2 + x1^2\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "#定义图像和三维格式坐标轴\n",
    "fig = plt.figure()\n",
    "ax3 = plt.axes(projection='3d')\n",
    "\n",
    "#定义三维数据\n",
    "xx = np.arange(-5,5,0.1)\n",
    "yy = np.arange(-5,5,0.1)\n",
    "X, Y = np.meshgrid(xx, yy)\n",
    "Z = function_2([X, Y])\n",
    "\n",
    "#作图\n",
    "ax3.plot_surface(X,Y,Z,cmap='rainbow')\n",
    "#ax3.contour(X,Y,Z, zdim='z',offset=-2，cmap='rainbow)   #等高线图，要设置offset，为Z的最小值\n",
    "plt.show()\n",
    "\n",
    "# 求偏导数 高数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6. 8.]\n"
     ]
    }
   ],
   "source": [
    "# 4.4 \n",
    "# 梯度\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)# 生成和x形状相同的全0数组\n",
    "    \n",
    "    for idx in range(x.size):# 梯度 每个自变量的偏导数集合 求x0偏导数 将x1看做已知常量 求函数导数 \n",
    "        tmp_val = x[idx]\n",
    "        # f(x+h)  f(x-h)\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val# 还原x值\n",
    "        \n",
    "    return grad\n",
    "\n",
    "print(numerical_gradient(function_2, np.array([3.0, 4.0])))# 输出（6， 8）\n",
    "\n",
    "# 梯度指示的方向是函数值减小最快的方向"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.99999994  3.99999992]\n"
     ]
    }
   ],
   "source": [
    "# 梯度法\n",
    "# 学习率\n",
    "\n",
    "# 默认学习率为0.01 梯度法重复次数为100\n",
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "        \n",
    "    return x\n",
    "\n",
    "# 尝试用梯度法求function_2的最小值\n",
    "# 实际可知该函数最小值点为 (0, 0)\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "\n",
    "# 可以看一下学习率 和 训练次数 的影响\n",
    "#print(gradient_descent(function_2, init_x=init_x, lr=1, step_num=10))# [-3.  4.]\n",
    "#print(gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100))# [-6.11110793e-10  8.14814391e-10]\n",
    "#print(gradient_descent(function_2, init_x=init_x, lr=0.01, step_num=100))# [-0.39785867  0.53047822]\n",
    "#print(gradient_descent(function_2, init_x=init_x, lr=0.01, step_num=1000))# [-5.04890207e-09  6.73186943e-09]\n",
    "#print(gradient_descent(function_2, init_x=init_x, lr=0.0001, step_num=10000))# [-1.88812247e-20  2.52844298e-20]\n",
    "\n",
    "# print(gradient_descent(function_2, init_x=init_x, lr=10, step_num=100))# [ 1.19952874e+12 -2.58191483e+12]\n",
    "print(gradient_descent(function_2, init_x=init_x, lr=0.0000000001, step_num=100))# [-2.99999994  3.99999992]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.51779043  1.19072068  0.02618904]\n",
      " [-1.67761803 -0.71644905  2.03080095]]\n",
      "[-1.82053048  0.06962826  1.84343428]\n",
      "2\n",
      "0.17841098655766\n"
     ]
    }
   ],
   "source": [
    "# 神经网络的梯度\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2, 3)# 用高斯分布进行初始化\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "net = simpleNet()\n",
    "print(net.W)# 权重参数\n",
    "\n",
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)\n",
    "print(np.argmax(p))\n",
    "\n",
    "t = np.array([0, 0, 1])# 正解标签\n",
    "print(net.loss(x, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.01286556  0.08517538 -0.09804094]\n",
      " [ 0.01929833  0.12776307 -0.1470614 ]]\n"
     ]
    }
   ],
   "source": [
    "def f(W):\n",
    "    return net.loss(x, t)\n",
    "\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2901839052363413\n",
      "2.2970039626037875\n",
      "2.2915217451886734\n",
      "2.2732237278034826\n",
      "2.294427069652072\n",
      "2.3048960093954385\n",
      "2.2989980440805717\n",
      "2.2861485349305775\n",
      "2.3051354399411497\n",
      "2.2940087591567884\n",
      "2.2869088788080445\n",
      "2.277383293292367\n",
      "2.285823135867449\n",
      "2.2958311441902066\n",
      "2.272714801521791\n"
     ]
    }
   ],
   "source": [
    "# 4.5\n",
    "# 学习算法的实现\n",
    "# 手写体数字识别的神经网络\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 初始化权重\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    # x:输入数据  t:监督数据\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)# np.argmax()详细解释 https://blog.csdn.net/weixin_42755982/article/details/104542538\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "#net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n",
    "\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "train_loss_list = []\n",
    "\n",
    "# 超参数\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):# 一次循环跑了大概60秒 总共一万次 10;45 start  估计得跑一个星期\n",
    "    # 获取mini-batch\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 计算梯度\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    # grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 更新参数\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    # 记录学习过程\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    print(loss)\n",
    "    train_loss_list.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于测试数据的评价\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 初始化权重\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    # x:输入数据  t:监督数据\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)# np.argmax()详细解释 https://blog.csdn.net/weixin_42755982/article/details/104542538\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "#net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n",
    "\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "# 超参数\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "train_size = x_train.shape[0]\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "# 平均每个epoch的重复次数\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):# 一次循环跑了大概  秒 总共一万次 11:30:00 start  估计得跑\n",
    "    # 获取mini-batch\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 计算梯度\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    # grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 更新参数\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    # 记录学习过程\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    #print(loss)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    # 计算每个epoch的识别精度\n",
    "    if i % iter_per_epoch == 0: # \n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第五章\n",
    "# 误差反向传播法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 计算图\n",
    "# 正向传播 \n",
    "# 反向传播\n",
    "# 计算图的有点：简化问题、存储中间计算结果、最重要的 可以通过反向传播高效计算导数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 链式法则\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 反向传播\n",
    "# 加法节点的反向传播\n",
    "# 乘法节点的反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220\n"
     ]
    }
   ],
   "source": [
    "# 5.4 简单层的实现\n",
    "# 乘法层\n",
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        \n",
    "    # 正向求结果\n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x * y\n",
    "        return out\n",
    "    \n",
    "    # 反向求导数\n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.y\n",
    "        dy = dout * self.x\n",
    "        return dx, dy\n",
    "    \n",
    "# 例子\n",
    "apple = 100# 苹果单价\n",
    "apple_num = 2# 购买数量\n",
    "tax = 1.1# 税率\n",
    "# 最终支付的钱 = 100 * 2 * 1.1\n",
    "\n",
    "# layer\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# forward\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "price = mul_tax_layer.forward(apple_price, tax)\n",
    "\n",
    "print(int(price))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加法层\n",
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "        return dx, dy   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price = 715.0000000000001\n",
      "110.00000000000001 2.2 165.0 3.3000000000000003 650\n"
     ]
    }
   ],
   "source": [
    "# 例子 使用乘法层和加法层实现 购买2个苹果 3个橘子\n",
    "apple = 100\n",
    "apple_num = 2\n",
    "orange = 150\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "# layer\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange = AddLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# forward\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "orange_price = mul_orange_layer.forward(orange, orange_num)\n",
    "all_price = add_apple_orange.forward(apple_price, orange_price)\n",
    "price = mul_tax_layer.forward(all_price, tax)\n",
    "\n",
    "# backward\n",
    "dprice = 1\n",
    "dall_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple_price, dorange_price = add_apple_orange.backward(dall_price)\n",
    "dorange, dorange_num = mul_orange_layer.backward(dorange_price)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print('price = ' + str(price))\n",
    "print(dapple_num, dapple, dorange_num, dorange, dtax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  -0.5]\n",
      " [-2.   3. ]]\n",
      "[[1. 0.]\n",
      " [0. 3.]]\n",
      "[[1. 0.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# 5.5 激活函数层的实现\n",
    "# ReLU层\n",
    "# ReLU   x > 0: y = x  x <= 0: y = 0\n",
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        return dx\n",
    "    \n",
    "# 示例\n",
    "import numpy as np\n",
    "x = np.array([[1.0, -0.5], [-2.0, 3.0]])\n",
    "example_relu = ReLU()\n",
    "example_out = example_relu.forward(x)\n",
    "example_dout = example_relu.backward(np.ones_like(x))\n",
    "print(x)\n",
    "print(example_out)\n",
    "print(example_dout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid 层\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.6 Affine/Softmax层的实现\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.x.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    c = np.max(a)\n",
    "    exp_a = np.exp(a - c)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "    return y\n",
    "\n",
    "def cross_entropy_error(y, t): # one-hot-label=True\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y + 1e-7)) / batch_size\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None # 损失\n",
    "        self.y = None # softmax的输出\n",
    "        self.t = None # 监督数据 one-hot vector\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_sizea# softmax反向传播得到的结果： (y1 - t1, y2 - t2, .. yn - tn)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.7 误差反向传播的实现\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 初始化权重\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "        # 生成层\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W : self.loss(x, t)\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        return grads\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "            \n",
    "        # 设定\n",
    "        grads = {}\n",
    "        # 这些值 在计算backward时已经写入类中\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:3.8447021876289604e-10\n",
      "b1:2.2210669621514858e-09\n",
      "W2:6.022951372544388e-09\n",
      "b2:1.3965380227243251e-07\n"
     ]
    }
   ],
   "source": [
    "from dataset.mnist import load_mnist\n",
    "\n",
    "# 读入数据\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "# 求各个权重的绝对误差的平均值\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average(np.abs(grad_backprop[key] - grad_numerical[key]))\n",
    "    print(key + \":\" + str(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10965 0.1094\n",
      "0.9036166666666666 0.9056\n",
      "0.9178833333333334 0.9192\n",
      "0.9329333333333333 0.931\n",
      "0.9413166666666667 0.9395\n",
      "0.9492166666666667 0.9471\n",
      "0.9524166666666667 0.9501\n",
      "0.95895 0.9546\n",
      "0.96395 0.9577\n",
      "0.96605 0.9595\n",
      "0.9687 0.9625\n",
      "0.9712166666666666 0.9642\n",
      "0.9731 0.9659\n",
      "0.9742166666666666 0.9663\n",
      "0.9757 0.9679\n",
      "0.9772333333333333 0.9677\n",
      "0.9772166666666666 0.9688\n",
      "0.9788666666666667 0.9681\n",
      "0.9807 0.9707\n",
      "0.9811666666666666 0.971\n",
      "0.9809666666666667 0.9699\n",
      "0.9826 0.97\n",
      "0.9831666666666666 0.971\n",
      "0.9835666666666667 0.9714\n",
      "0.98435 0.9712\n",
      "0.9851 0.9707\n",
      "0.9852333333333333 0.9716\n",
      "0.9870666666666666 0.9731\n",
      "0.9860666666666666 0.9721\n",
      "0.9883333333333333 0.9731\n",
      "0.9884833333333334 0.9734\n",
      "0.9885166666666667 0.9729\n",
      "0.9887 0.9722\n",
      "0.98945 0.9728\n",
      "0.9898666666666667 0.9728\n",
      "0.9901333333333333 0.974\n",
      "0.9912166666666666 0.9736\n",
      "0.9909 0.9726\n",
      "0.9914333333333334 0.9736\n",
      "0.9911 0.972\n",
      "0.992 0.973\n",
      "0.9921666666666666 0.9726\n",
      "0.9931333333333333 0.9734\n",
      "0.9930166666666667 0.9724\n",
      "0.9935166666666667 0.9734\n",
      "0.9913833333333333 0.972\n",
      "0.9935833333333334 0.9731\n",
      "0.9938666666666667 0.9728\n",
      "0.99445 0.975\n",
      "0.9933666666666666 0.9732\n",
      "0.9948833333333333 0.9727\n",
      "0.99475 0.9729\n",
      "0.99565 0.9732\n",
      "0.9959166666666667 0.9733\n",
      "0.99595 0.9732\n",
      "0.9962666666666666 0.974\n",
      "0.99575 0.9734\n",
      "0.9959666666666667 0.9721\n",
      "0.9955666666666667 0.9733\n",
      "0.9967166666666667 0.9733\n",
      "0.9969666666666667 0.9739\n",
      "0.9968166666666667 0.9717\n",
      "0.99665 0.974\n",
      "0.9970166666666667 0.9737\n",
      "0.9969833333333333 0.9737\n",
      "0.9974166666666666 0.973\n",
      "0.9963666666666666 0.9722\n",
      "0.9975166666666667 0.9735\n",
      "0.9978 0.972\n",
      "0.9979166666666667 0.9737\n",
      "0.99805 0.9737\n",
      "0.9980166666666667 0.9735\n",
      "0.9979166666666667 0.9743\n",
      "0.9984333333333333 0.973\n",
      "0.9985666666666667 0.9729\n",
      "0.9983333333333333 0.9725\n",
      "0.9984833333333333 0.9734\n",
      "0.9986833333333334 0.9731\n",
      "0.9986 0.974\n",
      "0.9985833333333334 0.9738\n",
      "0.9985666666666667 0.9737\n",
      "0.9989333333333333 0.9732\n",
      "0.99895 0.9727\n",
      "0.9991166666666667 0.972\n",
      "0.9993666666666666 0.9729\n",
      "0.9991666666666666 0.9718\n",
      "0.999 0.9732\n",
      "0.9991666666666666 0.9744\n",
      "0.9993333333333333 0.9733\n",
      "0.9992 0.9737\n",
      "0.9994 0.9736\n",
      "0.9995333333333334 0.974\n",
      "0.9994666666666666 0.9734\n",
      "0.9994666666666666 0.9735\n",
      "0.9995666666666667 0.973\n",
      "0.9993666666666666 0.9736\n",
      "0.9996666666666667 0.973\n",
      "0.9995333333333334 0.9725\n",
      "0.9995333333333334 0.9736\n",
      "0.99955 0.9739\n",
      "0.9995666666666667 0.9734\n",
      "0.9996666666666667 0.9731\n",
      "0.9997333333333334 0.9737\n",
      "0.9997833333333334 0.9736\n",
      "0.9997166666666667 0.9733\n",
      "0.9997833333333334 0.9734\n",
      "0.99975 0.9731\n",
      "0.9998166666666667 0.9736\n",
      "0.9998666666666667 0.9731\n",
      "0.9998166666666667 0.9729\n",
      "0.9998333333333334 0.973\n",
      "0.9998333333333334 0.9729\n",
      "0.9998166666666667 0.9728\n",
      "0.9999 0.973\n",
      "0.9998833333333333 0.9733\n",
      "0.9998666666666667 0.9726\n",
      "0.9999 0.9734\n",
      "0.9999333333333333 0.9727\n",
      "0.9999166666666667 0.9736\n",
      "0.9998833333333333 0.973\n",
      "0.9998666666666667 0.9727\n",
      "0.99995 0.9731\n",
      "0.99995 0.9728\n",
      "0.99995 0.9733\n",
      "0.9999666666666667 0.9732\n",
      "0.9999666666666667 0.9731\n",
      "0.9999333333333333 0.9724\n",
      "0.99995 0.9736\n",
      "0.9999833333333333 0.9741\n",
      "0.9999833333333333 0.9727\n",
      "0.99995 0.9731\n",
      "0.99995 0.9732\n",
      "0.9999666666666667 0.9735\n",
      "0.99995 0.9729\n",
      "0.9999666666666667 0.9724\n",
      "0.9999666666666667 0.9721\n",
      "0.99995 0.9728\n",
      "0.9999666666666667 0.9733\n",
      "1.0 0.9733\n",
      "0.9999666666666667 0.9731\n",
      "1.0 0.9731\n",
      "0.9999666666666667 0.9724\n",
      "0.9999666666666667 0.9734\n",
      "0.9999833333333333 0.9732\n",
      "0.9999833333333333 0.9725\n",
      "0.9999833333333333 0.9723\n",
      "0.9999833333333333 0.9727\n",
      "0.9999833333333333 0.9729\n",
      "0.9999833333333333 0.9725\n",
      "1.0 0.9727\n",
      "1.0 0.9727\n",
      "0.9999833333333333 0.9729\n",
      "1.0 0.9728\n",
      "0.9999833333333333 0.9723\n",
      "1.0 0.9728\n",
      "1.0 0.9734\n",
      "1.0 0.9726\n",
      "1.0 0.9735\n",
      "1.0 0.9728\n",
      "1.0 0.9733\n",
      "1.0 0.9727\n",
      "1.0 0.9728\n",
      "0.9999833333333333 0.9729\n",
      "0.9999833333333333 0.9728\n",
      "0.9999833333333333 0.9731\n",
      "1.0 0.9725\n",
      "0.9999833333333333 0.9735\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# 使用误差反向传播法学习\n",
    "#iters_num = 10000\n",
    "iters_num = 100000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_pre_epoch = max(train_size / batch_size, 1)\n",
    "x_ = []\n",
    "\n",
    "for i in range(iters_num): # 16:57:20   比之前数值微分算梯度快了无数倍\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 通过误差反向传播法求梯度\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 更新\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    #train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_pre_epoch == 0:\n",
    "        train_loss_list.append(loss)\n",
    "        x_.append(i)\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(train_acc, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 绘制图形\n",
    "plt.plot(x_, test_acc_list, label='test')\n",
    "plt.plot(x_, train_acc_list, label='train')\n",
    "plt.plot(x_, train_loss_list, label='loss')\n",
    "plt.title('Result-1')\n",
    "plt.xlabel('frequency of training')\n",
    "plt.ylabel('value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600.0\n"
     ]
    }
   ],
   "source": [
    "print(iter_pre_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第六章\n",
    "# 6.1 参数的更新\n",
    "# 最优化\n",
    "# 随机梯度下降法 SGD\n",
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key]\n",
    "\n",
    "# 优化器 optimizer\n",
    "\n",
    "# SGD的缺点：如果函数方向非均向 搜索路径就会十分低效\n",
    "# 根本原因是：梯度的方向并没有指向最小值的方向"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Momentum\n",
    "import numpy as np\n",
    "\n",
    "class Momentum:\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(vall)\n",
    "                \n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]\n",
    "            params[key] += self.v[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaGrad\n",
    "# 学习衰减率\n",
    "class AdaGrad:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "        \n",
    "        for key in params.keys():\n",
    "            self.h[key] += grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam\n",
    "# 组合了Momentum和AdaGrad方法\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 6.2 权重的初始值\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "    \n",
    "input_data = np.random.randn(1000, 100)  # 1000个数据\n",
    "node_num = 100  # 各隐藏层的节点（神经元）数\n",
    "hidden_layer_size = 5  # 隐藏层有5层\n",
    "activations = {}  # 激活值的结果保存在这里\n",
    "\n",
    "x = input_data\n",
    "\n",
    "for i in range(hidden_layer_size):\n",
    "    if i != 0:\n",
    "        x = activations[i-1]\n",
    "\n",
    "    # 改变初始值进行实验！\n",
    "    #w = np.random.randn(node_num, node_num) * 1\n",
    "    w = np.random.randn(node_num, node_num) * 0.01\n",
    "    # w = np.random.randn(node_num, node_num) * np.sqrt(1.0 / node_num)\n",
    "    # w = np.random.randn(node_num, node_num) * np.sqrt(2.0 / node_num)\n",
    "\n",
    "\n",
    "    a = np.dot(x, w)\n",
    "\n",
    "\n",
    "    # 将激活函数的种类也改变，来进行实验！\n",
    "    z = sigmoid(a)\n",
    "    # z = ReLU(a)\n",
    "    # z = tanh(a)\n",
    "\n",
    "    activations[i] = z\n",
    "\n",
    "# 绘制直方图\n",
    "for i, a in activations.items():\n",
    "    plt.subplot(1, len(activations), i+1)\n",
    "    plt.title(str(i+1) + \"-layer\")\n",
    "    if i != 0: plt.yticks([], [])\n",
    "    # plt.xlim(0.1, 1)\n",
    "    # plt.ylim(0, 7000)\n",
    "    plt.hist(a.flatten(), 30, range=(0,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不同激活函数初始值推荐\n",
    "# sigmoid 前一层节点数为n 初始值使用标准差为 1/sqrt(n) 的分布\n",
    "# ReLU 使用ReLU专用的初始值 当前一层节点数为n时 初始值使用标准差为sqrt(2/n)的高斯分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== 1/16 ==============\n",
      "epoch:0 | 0.092 - 0.109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\common\\multi_layer_net_extend.py:101: RuntimeWarning: overflow encountered in square\n",
      "  weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W**2)\n",
      "..\\common\\multi_layer_net_extend.py:101: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W**2)\n",
      "F:\\Anconda\\envs\\OpenCVAndTf\\lib\\site-packages\\numpy\\core\\fromnumeric.py:87: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "..\\common\\functions.py:34: RuntimeWarning: invalid value encountered in subtract\n",
      "  x = x - np.max(x, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 | 0.097 - 0.118\n",
      "epoch:2 | 0.097 - 0.111\n",
      "epoch:3 | 0.097 - 0.127\n",
      "epoch:4 | 0.097 - 0.138\n",
      "epoch:5 | 0.097 - 0.172\n",
      "epoch:6 | 0.097 - 0.189\n",
      "epoch:7 | 0.097 - 0.21\n",
      "epoch:8 | 0.097 - 0.225\n",
      "epoch:9 | 0.097 - 0.241\n",
      "epoch:10 | 0.097 - 0.255\n",
      "epoch:11 | 0.097 - 0.272\n",
      "epoch:12 | 0.097 - 0.288\n",
      "epoch:13 | 0.097 - 0.307\n",
      "epoch:14 | 0.097 - 0.318\n",
      "epoch:15 | 0.097 - 0.33\n",
      "epoch:16 | 0.097 - 0.345\n",
      "epoch:17 | 0.097 - 0.359\n",
      "epoch:18 | 0.097 - 0.361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 | 0.097 - 0.385\n",
      "============== 2/16 ==============\n",
      "epoch:0 | 0.1 - 0.108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\common\\multi_layer_net_extend.py:101: RuntimeWarning: overflow encountered in square\n",
      "  weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W**2)\n",
      "..\\common\\multi_layer_net_extend.py:101: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 | 0.097 - 0.117\n",
      "epoch:2 | 0.097 - 0.121\n",
      "epoch:3 | 0.097 - 0.153\n",
      "epoch:4 | 0.097 - 0.17\n",
      "epoch:5 | 0.097 - 0.193\n",
      "epoch:6 | 0.097 - 0.213\n",
      "epoch:7 | 0.097 - 0.243\n",
      "epoch:8 | 0.097 - 0.263\n",
      "epoch:9 | 0.097 - 0.296\n",
      "epoch:10 | 0.097 - 0.313\n",
      "epoch:11 | 0.097 - 0.336\n",
      "epoch:12 | 0.097 - 0.368\n",
      "epoch:13 | 0.097 - 0.392\n",
      "epoch:14 | 0.097 - 0.4\n",
      "epoch:15 | 0.097 - 0.416\n",
      "epoch:16 | 0.097 - 0.43\n",
      "epoch:17 | 0.097 - 0.444\n",
      "epoch:18 | 0.097 - 0.461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 | 0.097 - 0.474\n",
      "============== 3/16 ==============\n",
      "epoch:0 | 0.115 - 0.119\n",
      "epoch:1 | 0.285 - 0.121\n",
      "epoch:2 | 0.453 - 0.161\n",
      "epoch:3 | 0.54 - 0.195\n",
      "epoch:4 | 0.597 - 0.248\n",
      "epoch:5 | 0.666 - 0.279\n",
      "epoch:6 | 0.702 - 0.308\n",
      "epoch:7 | 0.741 - 0.352\n",
      "epoch:8 | 0.773 - 0.385\n",
      "epoch:9 | 0.811 - 0.412\n",
      "epoch:10 | 0.832 - 0.446\n",
      "epoch:11 | 0.853 - 0.471\n",
      "epoch:12 | 0.878 - 0.492\n",
      "epoch:13 | 0.898 - 0.519\n",
      "epoch:14 | 0.905 - 0.539\n",
      "epoch:15 | 0.92 - 0.565\n",
      "epoch:16 | 0.929 - 0.588\n",
      "epoch:17 | 0.94 - 0.604\n",
      "epoch:18 | 0.946 - 0.617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 | 0.949 - 0.634\n",
      "============== 4/16 ==============\n",
      "epoch:0 | 0.118 - 0.095\n",
      "epoch:1 | 0.329 - 0.117\n",
      "epoch:2 | 0.432 - 0.243\n",
      "epoch:3 | 0.543 - 0.343\n",
      "epoch:4 | 0.609 - 0.389\n",
      "epoch:5 | 0.658 - 0.441\n",
      "epoch:6 | 0.707 - 0.477\n",
      "epoch:7 | 0.725 - 0.518\n",
      "epoch:8 | 0.749 - 0.555\n",
      "epoch:9 | 0.769 - 0.58\n",
      "epoch:10 | 0.792 - 0.612\n",
      "epoch:11 | 0.798 - 0.642\n",
      "epoch:12 | 0.817 - 0.669\n",
      "epoch:13 | 0.824 - 0.69\n",
      "epoch:14 | 0.839 - 0.703\n",
      "epoch:15 | 0.85 - 0.727\n",
      "epoch:16 | 0.854 - 0.739\n",
      "epoch:17 | 0.875 - 0.761\n",
      "epoch:18 | 0.874 - 0.776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 | 0.881 - 0.79\n",
      "============== 5/16 ==============\n",
      "epoch:0 | 0.115 - 0.135\n",
      "epoch:1 | 0.117 - 0.154\n",
      "epoch:2 | 0.112 - 0.298\n",
      "epoch:3 | 0.115 - 0.412\n",
      "epoch:4 | 0.122 - 0.509\n",
      "epoch:5 | 0.128 - 0.558\n",
      "epoch:6 | 0.134 - 0.594\n",
      "epoch:7 | 0.142 - 0.641\n",
      "epoch:8 | 0.157 - 0.679\n",
      "epoch:9 | 0.173 - 0.706\n",
      "epoch:10 | 0.191 - 0.73\n",
      "epoch:11 | 0.215 - 0.751\n",
      "epoch:12 | 0.225 - 0.769\n",
      "epoch:13 | 0.225 - 0.78\n",
      "epoch:14 | 0.251 - 0.788\n",
      "epoch:15 | 0.267 - 0.811\n",
      "epoch:16 | 0.276 - 0.823\n",
      "epoch:17 | 0.291 - 0.829\n",
      "epoch:18 | 0.31 - 0.837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 | 0.334 - 0.849\n",
      "============== 6/16 ==============\n",
      "epoch:0 | 0.072 - 0.081\n",
      "epoch:1 | 0.114 - 0.174\n",
      "epoch:2 | 0.138 - 0.394\n",
      "epoch:3 | 0.116 - 0.588\n",
      "epoch:4 | 0.116 - 0.667\n",
      "epoch:5 | 0.116 - 0.72\n",
      "epoch:6 | 0.116 - 0.746\n",
      "epoch:7 | 0.116 - 0.768\n",
      "epoch:8 | 0.116 - 0.793\n",
      "epoch:9 | 0.116 - 0.806\n",
      "epoch:10 | 0.12 - 0.835\n",
      "epoch:11 | 0.116 - 0.849\n",
      "epoch:12 | 0.145 - 0.871\n",
      "epoch:13 | 0.165 - 0.885\n",
      "epoch:14 | 0.174 - 0.89\n",
      "epoch:15 | 0.119 - 0.899\n",
      "epoch:16 | 0.116 - 0.91\n",
      "epoch:17 | 0.12 - 0.921\n",
      "epoch:18 | 0.151 - 0.928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 | 0.13 - 0.939\n",
      "============== 7/16 ==============\n",
      "epoch:0 | 0.104 - 0.083\n",
      "epoch:1 | 0.117 - 0.195\n",
      "epoch:2 | 0.117 - 0.592\n",
      "epoch:3 | 0.117 - 0.705\n",
      "epoch:4 | 0.117 - 0.763\n",
      "epoch:5 | 0.117 - 0.793\n",
      "epoch:6 | 0.117 - 0.834\n",
      "epoch:7 | 0.117 - 0.868\n",
      "epoch:8 | 0.117 - 0.899\n",
      "epoch:9 | 0.117 - 0.913\n",
      "epoch:10 | 0.117 - 0.931\n",
      "epoch:11 | 0.117 - 0.938\n",
      "epoch:12 | 0.117 - 0.948\n",
      "epoch:13 | 0.117 - 0.955\n",
      "epoch:14 | 0.117 - 0.961\n",
      "epoch:15 | 0.117 - 0.971\n",
      "epoch:16 | 0.117 - 0.974\n",
      "epoch:17 | 0.117 - 0.977\n",
      "epoch:18 | 0.117 - 0.981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 | 0.117 - 0.983\n",
      "============== 8/16 ==============\n",
      "epoch:0 | 0.094 - 0.078\n",
      "epoch:1 | 0.117 - 0.313\n",
      "epoch:2 | 0.117 - 0.579\n",
      "epoch:3 | 0.117 - 0.688\n",
      "epoch:4 | 0.116 - 0.778\n",
      "epoch:5 | 0.117 - 0.84\n",
      "epoch:6 | 0.117 - 0.871\n",
      "epoch:7 | 0.117 - 0.912\n",
      "epoch:8 | 0.117 - 0.932\n",
      "epoch:9 | 0.116 - 0.948\n",
      "epoch:10 | 0.116 - 0.96\n",
      "epoch:11 | 0.116 - 0.975\n",
      "epoch:12 | 0.116 - 0.983\n",
      "epoch:13 | 0.116 - 0.986\n",
      "epoch:14 | 0.117 - 0.99\n",
      "epoch:15 | 0.117 - 0.991\n",
      "epoch:16 | 0.117 - 0.993\n",
      "epoch:17 | 0.117 - 0.995\n",
      "epoch:18 | 0.117 - 0.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 | 0.117 - 0.999\n",
      "============== 9/16 ==============\n",
      "epoch:0 | 0.105 - 0.14\n",
      "epoch:1 | 0.116 - 0.492\n",
      "epoch:2 | 0.117 - 0.796\n",
      "epoch:3 | 0.117 - 0.863\n",
      "epoch:4 | 0.105 - 0.917\n",
      "epoch:5 | 0.105 - 0.95\n",
      "epoch:6 | 0.105 - 0.965\n",
      "epoch:7 | 0.117 - 0.982\n",
      "epoch:8 | 0.117 - 0.989\n",
      "epoch:9 | 0.117 - 0.992\n",
      "epoch:10 | 0.117 - 0.991\n",
      "epoch:11 | 0.117 - 0.993\n",
      "epoch:12 | 0.117 - 0.995\n",
      "epoch:13 | 0.117 - 0.999\n",
      "epoch:14 | 0.117 - 0.999\n",
      "epoch:15 | 0.117 - 0.999\n",
      "epoch:16 | 0.117 - 1.0\n",
      "epoch:17 | 0.117 - 1.0\n",
      "epoch:18 | 0.117 - 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 | 0.117 - 1.0\n",
      "============== 10/16 ==============\n",
      "epoch:0 | 0.087 - 0.104\n",
      "epoch:1 | 0.117 - 0.601\n",
      "epoch:2 | 0.117 - 0.752\n",
      "epoch:3 | 0.117 - 0.788\n",
      "epoch:4 | 0.117 - 0.843\n",
      "epoch:5 | 0.117 - 0.852\n",
      "epoch:6 | 0.117 - 0.901\n",
      "epoch:7 | 0.117 - 0.947\n",
      "epoch:8 | 0.117 - 0.929\n",
      "epoch:9 | 0.117 - 0.986\n",
      "epoch:10 | 0.117 - 0.993\n",
      "epoch:11 | 0.117 - 0.994\n",
      "epoch:12 | 0.117 - 0.999\n",
      "epoch:13 | 0.117 - 0.98\n",
      "epoch:14 | 0.117 - 1.0\n",
      "epoch:15 | 0.117 - 1.0\n",
      "epoch:16 | 0.117 - 1.0\n",
      "epoch:17 | 0.117 - 1.0\n",
      "epoch:18 | 0.117 - 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 | 0.117 - 1.0\n",
      "============== 11/16 ==============\n",
      "epoch:0 | 0.116 - 0.18\n",
      "epoch:1 | 0.116 - 0.484\n",
      "epoch:2 | 0.116 - 0.575\n",
      "epoch:3 | 0.116 - 0.569\n",
      "epoch:4 | 0.116 - 0.676\n",
      "epoch:5 | 0.116 - 0.685\n",
      "epoch:6 | 0.116 - 0.639\n",
      "epoch:7 | 0.116 - 0.705\n",
      "epoch:8 | 0.116 - 0.709\n",
      "epoch:9 | 0.116 - 0.716\n",
      "epoch:10 | 0.116 - 0.718\n",
      "epoch:11 | 0.116 - 0.719\n",
      "epoch:12 | 0.116 - 0.719\n",
      "epoch:13 | 0.116 - 0.806\n",
      "epoch:14 | 0.116 - 0.808\n",
      "epoch:15 | 0.116 - 0.818\n",
      "epoch:16 | 0.116 - 0.815\n",
      "epoch:17 | 0.116 - 0.848\n",
      "epoch:18 | 0.116 - 0.812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 | 0.116 - 0.809\n",
      "============== 12/16 ==============\n",
      "epoch:0 | 0.087 - 0.184\n",
      "epoch:1 | 0.116 - 0.543\n",
      "epoch:2 | 0.116 - 0.704\n",
      "epoch:3 | 0.116 - 0.628\n",
      "epoch:4 | 0.116 - 0.667\n",
      "epoch:5 | 0.116 - 0.685\n",
      "epoch:6 | 0.116 - 0.764\n",
      "epoch:7 | 0.116 - 0.786\n",
      "epoch:8 | 0.116 - 0.791\n",
      "epoch:9 | 0.116 - 0.796\n",
      "epoch:10 | 0.116 - 0.812\n",
      "epoch:11 | 0.116 - 0.825\n",
      "epoch:12 | 0.116 - 0.85\n",
      "epoch:13 | 0.116 - 0.877\n",
      "epoch:14 | 0.116 - 0.857\n",
      "epoch:15 | 0.116 - 0.803\n",
      "epoch:16 | 0.116 - 0.894\n",
      "epoch:17 | 0.116 - 0.99\n",
      "epoch:18 | 0.116 - 0.987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 | 0.116 - 0.99\n",
      "============== 13/16 ==============\n",
      "epoch:0 | 0.117 - 0.093\n",
      "epoch:1 | 0.116 - 0.418\n",
      "epoch:2 | 0.116 - 0.476\n",
      "epoch:3 | 0.116 - 0.541\n",
      "epoch:4 | 0.116 - 0.571\n",
      "epoch:5 | 0.116 - 0.573\n",
      "epoch:6 | 0.116 - 0.629\n",
      "epoch:7 | 0.116 - 0.637\n",
      "epoch:8 | 0.116 - 0.656\n",
      "epoch:9 | 0.116 - 0.674\n",
      "epoch:10 | 0.116 - 0.661\n",
      "epoch:11 | 0.116 - 0.685\n",
      "epoch:12 | 0.116 - 0.694\n",
      "epoch:13 | 0.116 - 0.666\n",
      "epoch:14 | 0.116 - 0.681\n",
      "epoch:15 | 0.116 - 0.7\n",
      "epoch:16 | 0.117 - 0.703\n",
      "epoch:17 | 0.117 - 0.705\n",
      "epoch:18 | 0.117 - 0.649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 | 0.117 - 0.706\n",
      "============== 14/16 ==============\n",
      "epoch:0 | 0.087 - 0.1\n",
      "epoch:1 | 0.105 - 0.33\n",
      "epoch:2 | 0.117 - 0.477\n",
      "epoch:3 | 0.117 - 0.518\n",
      "epoch:4 | 0.117 - 0.554\n",
      "epoch:5 | 0.117 - 0.581\n",
      "epoch:6 | 0.117 - 0.585\n",
      "epoch:7 | 0.117 - 0.585\n",
      "epoch:8 | 0.117 - 0.604\n",
      "epoch:9 | 0.117 - 0.604\n",
      "epoch:10 | 0.117 - 0.59\n",
      "epoch:11 | 0.117 - 0.595\n",
      "epoch:12 | 0.117 - 0.582\n",
      "epoch:13 | 0.117 - 0.612\n",
      "epoch:14 | 0.117 - 0.607\n",
      "epoch:15 | 0.117 - 0.608\n",
      "epoch:16 | 0.117 - 0.615\n",
      "epoch:17 | 0.117 - 0.616\n",
      "epoch:18 | 0.117 - 0.685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 | 0.117 - 0.615\n",
      "============== 15/16 ==============\n",
      "epoch:0 | 0.105 - 0.097\n",
      "epoch:1 | 0.117 - 0.22\n",
      "epoch:2 | 0.117 - 0.32\n",
      "epoch:3 | 0.117 - 0.415\n",
      "epoch:4 | 0.117 - 0.416\n",
      "epoch:5 | 0.117 - 0.424\n",
      "epoch:6 | 0.117 - 0.425\n",
      "epoch:7 | 0.117 - 0.425\n",
      "epoch:8 | 0.117 - 0.426\n",
      "epoch:9 | 0.117 - 0.425\n",
      "epoch:10 | 0.117 - 0.429\n",
      "epoch:11 | 0.117 - 0.431\n",
      "epoch:12 | 0.117 - 0.431\n",
      "epoch:13 | 0.117 - 0.431\n",
      "epoch:14 | 0.117 - 0.432\n",
      "epoch:15 | 0.117 - 0.432\n",
      "epoch:16 | 0.117 - 0.432\n",
      "epoch:17 | 0.117 - 0.431\n",
      "epoch:18 | 0.117 - 0.432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 | 0.117 - 0.433\n",
      "============== 16/16 ==============\n",
      "epoch:0 | 0.117 - 0.239\n",
      "epoch:1 | 0.117 - 0.117\n",
      "epoch:2 | 0.117 - 0.305\n",
      "epoch:3 | 0.117 - 0.308\n",
      "epoch:4 | 0.117 - 0.313\n",
      "epoch:5 | 0.117 - 0.315\n",
      "epoch:6 | 0.117 - 0.311\n",
      "epoch:7 | 0.117 - 0.366\n",
      "epoch:8 | 0.117 - 0.406\n",
      "epoch:9 | 0.117 - 0.427\n",
      "epoch:10 | 0.117 - 0.487\n",
      "epoch:11 | 0.117 - 0.501\n",
      "epoch:12 | 0.117 - 0.479\n",
      "epoch:13 | 0.117 - 0.507\n",
      "epoch:14 | 0.117 - 0.508\n",
      "epoch:15 | 0.117 - 0.508\n",
      "epoch:16 | 0.117 - 0.507\n",
      "epoch:17 | 0.117 - 0.516\n",
      "epoch:18 | 0.117 - 0.519\n",
      "epoch:19 | 0.117 - 0.516\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 6.3 Batch Normalization\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 为了导入父目录的文件而进行的设定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.multi_layer_net_extend import MultiLayerNetExtend\n",
    "from common.optimizer import SGD, Adam\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 减少学习数据\n",
    "x_train = x_train[:1000]\n",
    "t_train = t_train[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "\n",
    "def __train(weight_init_std):\n",
    "    bn_network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100], output_size=10, \n",
    "                                    weight_init_std=weight_init_std, use_batchnorm=True)\n",
    "    network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100], output_size=10,\n",
    "                                weight_init_std=weight_init_std)\n",
    "    optimizer = SGD(lr=learning_rate)\n",
    "    \n",
    "    train_acc_list = []\n",
    "    bn_train_acc_list = []\n",
    "    \n",
    "    iter_per_epoch = max(train_size / batch_size, 1)\n",
    "    epoch_cnt = 0\n",
    "    \n",
    "    for i in range(1000000000):\n",
    "        batch_mask = np.random.choice(train_size, batch_size)\n",
    "        x_batch = x_train[batch_mask]\n",
    "        t_batch = t_train[batch_mask]\n",
    "    \n",
    "        for _network in (bn_network, network):\n",
    "            grads = _network.gradient(x_batch, t_batch)\n",
    "            optimizer.update(_network.params, grads)\n",
    "    \n",
    "        if i % iter_per_epoch == 0:\n",
    "            train_acc = network.accuracy(x_train, t_train)\n",
    "            bn_train_acc = bn_network.accuracy(x_train, t_train)\n",
    "            train_acc_list.append(train_acc)\n",
    "            bn_train_acc_list.append(bn_train_acc)\n",
    "    \n",
    "            print(\"epoch:\" + str(epoch_cnt) + \" | \" + str(train_acc) + \" - \" + str(bn_train_acc))\n",
    "    \n",
    "            epoch_cnt += 1\n",
    "            if epoch_cnt >= max_epochs:\n",
    "                break\n",
    "                \n",
    "    return train_acc_list, bn_train_acc_list\n",
    "\n",
    "\n",
    "# 3.绘制图形==========\n",
    "weight_scale_list = np.logspace(0, -4, num=16)\n",
    "x = np.arange(max_epochs)\n",
    "\n",
    "for i, w in enumerate(weight_scale_list):\n",
    "    print( \"============== \" + str(i+1) + \"/16\" + \" ==============\")\n",
    "    train_acc_list, bn_train_acc_list = __train(w)\n",
    "    \n",
    "    plt.subplot(4,4,i+1)\n",
    "    plt.title(\"W:\" + str(w))\n",
    "    if i == 15:\n",
    "        plt.plot(x, bn_train_acc_list, label='Batch Normalization', markevery=2)\n",
    "        plt.plot(x, train_acc_list, linestyle = \"--\", label='Normal(without BatchNorm)', markevery=2)\n",
    "    else:\n",
    "        plt.plot(x, bn_train_acc_list, markevery=2)\n",
    "        plt.plot(x, train_acc_list, linestyle=\"--\", markevery=2)\n",
    "\n",
    "    plt.ylim(0, 1.0)\n",
    "    if i % 4:\n",
    "        plt.yticks([])\n",
    "    else:\n",
    "        plt.ylabel(\"accuracy\")\n",
    "    if i < 12:\n",
    "        plt.xticks([])\n",
    "    else:\n",
    "        plt.xlabel(\"epochs\")\n",
    "    plt.legend(loc='lower right')\n",
    "    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train acc:0.09, test acc:0.1103\n",
      "epoch:1, train acc:0.12, test acc:0.1231\n",
      "epoch:2, train acc:0.13666666666666666, test acc:0.1334\n",
      "epoch:3, train acc:0.14333333333333334, test acc:0.1524\n",
      "epoch:4, train acc:0.17, test acc:0.1716\n",
      "epoch:5, train acc:0.19666666666666666, test acc:0.1841\n",
      "epoch:6, train acc:0.21333333333333335, test acc:0.1953\n",
      "epoch:7, train acc:0.25666666666666665, test acc:0.2125\n",
      "epoch:8, train acc:0.28, test acc:0.2206\n",
      "epoch:9, train acc:0.30666666666666664, test acc:0.2298\n",
      "epoch:10, train acc:0.33666666666666667, test acc:0.242\n",
      "epoch:11, train acc:0.3466666666666667, test acc:0.243\n",
      "epoch:12, train acc:0.36666666666666664, test acc:0.2565\n",
      "epoch:13, train acc:0.38666666666666666, test acc:0.2675\n",
      "epoch:14, train acc:0.4, test acc:0.2792\n",
      "epoch:15, train acc:0.4066666666666667, test acc:0.2896\n",
      "epoch:16, train acc:0.41, test acc:0.3006\n",
      "epoch:17, train acc:0.44333333333333336, test acc:0.3224\n",
      "epoch:18, train acc:0.4766666666666667, test acc:0.3474\n",
      "epoch:19, train acc:0.5, test acc:0.3512\n",
      "epoch:20, train acc:0.49333333333333335, test acc:0.361\n",
      "epoch:21, train acc:0.5266666666666666, test acc:0.3791\n",
      "epoch:22, train acc:0.5366666666666666, test acc:0.3969\n",
      "epoch:23, train acc:0.5633333333333334, test acc:0.4134\n",
      "epoch:24, train acc:0.5833333333333334, test acc:0.4256\n",
      "epoch:25, train acc:0.6, test acc:0.4349\n",
      "epoch:26, train acc:0.6033333333333334, test acc:0.4405\n",
      "epoch:27, train acc:0.6133333333333333, test acc:0.4352\n",
      "epoch:28, train acc:0.64, test acc:0.4548\n",
      "epoch:29, train acc:0.6533333333333333, test acc:0.4567\n",
      "epoch:30, train acc:0.6866666666666666, test acc:0.4657\n",
      "epoch:31, train acc:0.68, test acc:0.4858\n",
      "epoch:32, train acc:0.7066666666666667, test acc:0.5002\n",
      "epoch:33, train acc:0.7066666666666667, test acc:0.5066\n",
      "epoch:34, train acc:0.7166666666666667, test acc:0.5212\n",
      "epoch:35, train acc:0.7266666666666667, test acc:0.5309\n",
      "epoch:36, train acc:0.7366666666666667, test acc:0.54\n",
      "epoch:37, train acc:0.7433333333333333, test acc:0.541\n",
      "epoch:38, train acc:0.7466666666666667, test acc:0.5495\n",
      "epoch:39, train acc:0.7533333333333333, test acc:0.562\n",
      "epoch:40, train acc:0.7533333333333333, test acc:0.5634\n",
      "epoch:41, train acc:0.7833333333333333, test acc:0.5735\n",
      "epoch:42, train acc:0.76, test acc:0.5761\n",
      "epoch:43, train acc:0.7866666666666666, test acc:0.5936\n",
      "epoch:44, train acc:0.7933333333333333, test acc:0.5794\n",
      "epoch:45, train acc:0.8033333333333333, test acc:0.5926\n",
      "epoch:46, train acc:0.8166666666666667, test acc:0.5939\n",
      "epoch:47, train acc:0.81, test acc:0.5973\n",
      "epoch:48, train acc:0.81, test acc:0.6075\n",
      "epoch:49, train acc:0.8333333333333334, test acc:0.6157\n",
      "epoch:50, train acc:0.8233333333333334, test acc:0.6148\n",
      "epoch:51, train acc:0.8333333333333334, test acc:0.6116\n",
      "epoch:52, train acc:0.86, test acc:0.6337\n",
      "epoch:53, train acc:0.8666666666666667, test acc:0.6342\n",
      "epoch:54, train acc:0.8633333333333333, test acc:0.6234\n",
      "epoch:55, train acc:0.8866666666666667, test acc:0.6381\n",
      "epoch:56, train acc:0.9033333333333333, test acc:0.6421\n",
      "epoch:57, train acc:0.8966666666666666, test acc:0.6558\n",
      "epoch:58, train acc:0.8966666666666666, test acc:0.648\n",
      "epoch:59, train acc:0.91, test acc:0.653\n",
      "epoch:60, train acc:0.9166666666666666, test acc:0.6536\n",
      "epoch:61, train acc:0.9033333333333333, test acc:0.6473\n",
      "epoch:62, train acc:0.9166666666666666, test acc:0.66\n",
      "epoch:63, train acc:0.9266666666666666, test acc:0.6708\n",
      "epoch:64, train acc:0.92, test acc:0.669\n",
      "epoch:65, train acc:0.9233333333333333, test acc:0.6656\n",
      "epoch:66, train acc:0.9233333333333333, test acc:0.6617\n",
      "epoch:67, train acc:0.93, test acc:0.6735\n",
      "epoch:68, train acc:0.94, test acc:0.6817\n",
      "epoch:69, train acc:0.9433333333333334, test acc:0.6646\n",
      "epoch:70, train acc:0.9433333333333334, test acc:0.6777\n",
      "epoch:71, train acc:0.95, test acc:0.6826\n",
      "epoch:72, train acc:0.9533333333333334, test acc:0.6802\n",
      "epoch:73, train acc:0.9533333333333334, test acc:0.6836\n",
      "epoch:74, train acc:0.96, test acc:0.6924\n",
      "epoch:75, train acc:0.9566666666666667, test acc:0.6836\n",
      "epoch:76, train acc:0.96, test acc:0.6942\n",
      "epoch:77, train acc:0.96, test acc:0.6918\n",
      "epoch:78, train acc:0.96, test acc:0.6965\n",
      "epoch:79, train acc:0.96, test acc:0.6922\n",
      "epoch:80, train acc:0.96, test acc:0.6941\n",
      "epoch:81, train acc:0.96, test acc:0.7018\n",
      "epoch:82, train acc:0.9666666666666667, test acc:0.7035\n",
      "epoch:83, train acc:0.97, test acc:0.7045\n",
      "epoch:84, train acc:0.9666666666666667, test acc:0.7048\n",
      "epoch:85, train acc:0.9666666666666667, test acc:0.7014\n",
      "epoch:86, train acc:0.97, test acc:0.7057\n",
      "epoch:87, train acc:0.97, test acc:0.7093\n",
      "epoch:88, train acc:0.9733333333333334, test acc:0.7066\n",
      "epoch:89, train acc:0.9733333333333334, test acc:0.7117\n",
      "epoch:90, train acc:0.97, test acc:0.7084\n",
      "epoch:91, train acc:0.9733333333333334, test acc:0.7101\n",
      "epoch:92, train acc:0.9733333333333334, test acc:0.7134\n",
      "epoch:93, train acc:0.9733333333333334, test acc:0.7078\n",
      "epoch:94, train acc:0.9766666666666667, test acc:0.7125\n",
      "epoch:95, train acc:0.9733333333333334, test acc:0.7147\n",
      "epoch:96, train acc:0.9766666666666667, test acc:0.7133\n",
      "epoch:97, train acc:0.9733333333333334, test acc:0.7159\n",
      "epoch:98, train acc:0.9766666666666667, test acc:0.7195\n",
      "epoch:99, train acc:0.9766666666666667, test acc:0.717\n",
      "epoch:100, train acc:0.9766666666666667, test acc:0.718\n",
      "epoch:101, train acc:0.9766666666666667, test acc:0.7137\n",
      "epoch:102, train acc:0.9766666666666667, test acc:0.7164\n",
      "epoch:103, train acc:0.98, test acc:0.7173\n",
      "epoch:104, train acc:0.98, test acc:0.7233\n",
      "epoch:105, train acc:0.9766666666666667, test acc:0.7277\n",
      "epoch:106, train acc:0.98, test acc:0.7254\n",
      "epoch:107, train acc:0.9866666666666667, test acc:0.7239\n",
      "epoch:108, train acc:0.9866666666666667, test acc:0.7235\n",
      "epoch:109, train acc:0.99, test acc:0.7243\n",
      "epoch:110, train acc:0.99, test acc:0.7249\n",
      "epoch:111, train acc:0.9966666666666667, test acc:0.7253\n",
      "epoch:112, train acc:0.9933333333333333, test acc:0.7248\n",
      "epoch:113, train acc:0.9966666666666667, test acc:0.7305\n",
      "epoch:114, train acc:0.9933333333333333, test acc:0.7243\n",
      "epoch:115, train acc:0.9933333333333333, test acc:0.7281\n",
      "epoch:116, train acc:0.9933333333333333, test acc:0.7271\n",
      "epoch:117, train acc:0.9933333333333333, test acc:0.7284\n",
      "epoch:118, train acc:0.9966666666666667, test acc:0.7295\n",
      "epoch:119, train acc:0.9933333333333333, test acc:0.7266\n",
      "epoch:120, train acc:0.9933333333333333, test acc:0.7291\n",
      "epoch:121, train acc:0.9933333333333333, test acc:0.7267\n",
      "epoch:122, train acc:0.9933333333333333, test acc:0.7358\n",
      "epoch:123, train acc:0.9933333333333333, test acc:0.7301\n",
      "epoch:124, train acc:0.9966666666666667, test acc:0.7329\n",
      "epoch:125, train acc:0.9966666666666667, test acc:0.7361\n",
      "epoch:126, train acc:0.9966666666666667, test acc:0.7345\n",
      "epoch:127, train acc:0.9966666666666667, test acc:0.7327\n",
      "epoch:128, train acc:0.9966666666666667, test acc:0.7312\n",
      "epoch:129, train acc:1.0, test acc:0.7272\n",
      "epoch:130, train acc:0.9966666666666667, test acc:0.732\n",
      "epoch:131, train acc:0.9966666666666667, test acc:0.7361\n",
      "epoch:132, train acc:0.9966666666666667, test acc:0.7377\n",
      "epoch:133, train acc:0.9966666666666667, test acc:0.7338\n",
      "epoch:134, train acc:0.9966666666666667, test acc:0.7341\n",
      "epoch:135, train acc:0.9966666666666667, test acc:0.7333\n",
      "epoch:136, train acc:0.9966666666666667, test acc:0.7342\n",
      "epoch:137, train acc:0.9966666666666667, test acc:0.7356\n",
      "epoch:138, train acc:0.9966666666666667, test acc:0.7338\n",
      "epoch:139, train acc:0.9966666666666667, test acc:0.7381\n",
      "epoch:140, train acc:0.9966666666666667, test acc:0.7353\n",
      "epoch:141, train acc:0.9966666666666667, test acc:0.7336\n",
      "epoch:142, train acc:0.9966666666666667, test acc:0.7348\n",
      "epoch:143, train acc:0.9966666666666667, test acc:0.7368\n",
      "epoch:144, train acc:0.9966666666666667, test acc:0.7348\n",
      "epoch:145, train acc:0.9966666666666667, test acc:0.7354\n",
      "epoch:146, train acc:0.9966666666666667, test acc:0.7391\n",
      "epoch:147, train acc:0.9966666666666667, test acc:0.7386\n",
      "epoch:148, train acc:0.9966666666666667, test acc:0.7378\n",
      "epoch:149, train acc:0.9966666666666667, test acc:0.7376\n",
      "epoch:150, train acc:0.9966666666666667, test acc:0.7383\n",
      "epoch:151, train acc:1.0, test acc:0.7381\n",
      "epoch:152, train acc:1.0, test acc:0.7383\n",
      "epoch:153, train acc:0.9966666666666667, test acc:0.7419\n",
      "epoch:154, train acc:1.0, test acc:0.7394\n",
      "epoch:155, train acc:0.9966666666666667, test acc:0.7408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:156, train acc:1.0, test acc:0.7412\n",
      "epoch:157, train acc:0.9966666666666667, test acc:0.7433\n",
      "epoch:158, train acc:1.0, test acc:0.7424\n",
      "epoch:159, train acc:1.0, test acc:0.7419\n",
      "epoch:160, train acc:1.0, test acc:0.7421\n",
      "epoch:161, train acc:1.0, test acc:0.7414\n",
      "epoch:162, train acc:1.0, test acc:0.7437\n",
      "epoch:163, train acc:1.0, test acc:0.7418\n",
      "epoch:164, train acc:1.0, test acc:0.7424\n",
      "epoch:165, train acc:1.0, test acc:0.7419\n",
      "epoch:166, train acc:1.0, test acc:0.7399\n",
      "epoch:167, train acc:1.0, test acc:0.742\n",
      "epoch:168, train acc:1.0, test acc:0.7414\n",
      "epoch:169, train acc:1.0, test acc:0.7452\n",
      "epoch:170, train acc:1.0, test acc:0.7442\n",
      "epoch:171, train acc:1.0, test acc:0.7444\n",
      "epoch:172, train acc:1.0, test acc:0.7412\n",
      "epoch:173, train acc:1.0, test acc:0.7403\n",
      "epoch:174, train acc:1.0, test acc:0.7428\n",
      "epoch:175, train acc:1.0, test acc:0.7415\n",
      "epoch:176, train acc:1.0, test acc:0.7418\n",
      "epoch:177, train acc:1.0, test acc:0.7423\n",
      "epoch:178, train acc:1.0, test acc:0.7431\n",
      "epoch:179, train acc:1.0, test acc:0.7436\n",
      "epoch:180, train acc:1.0, test acc:0.744\n",
      "epoch:181, train acc:1.0, test acc:0.7448\n",
      "epoch:182, train acc:1.0, test acc:0.7414\n",
      "epoch:183, train acc:1.0, test acc:0.7443\n",
      "epoch:184, train acc:1.0, test acc:0.7456\n",
      "epoch:185, train acc:1.0, test acc:0.7453\n",
      "epoch:186, train acc:1.0, test acc:0.7434\n",
      "epoch:187, train acc:1.0, test acc:0.747\n",
      "epoch:188, train acc:1.0, test acc:0.7457\n",
      "epoch:189, train acc:1.0, test acc:0.746\n",
      "epoch:190, train acc:1.0, test acc:0.7469\n",
      "epoch:191, train acc:1.0, test acc:0.7475\n",
      "epoch:192, train acc:1.0, test acc:0.7473\n",
      "epoch:193, train acc:1.0, test acc:0.7444\n",
      "epoch:194, train acc:1.0, test acc:0.7448\n",
      "epoch:195, train acc:1.0, test acc:0.7466\n",
      "epoch:196, train acc:1.0, test acc:0.7465\n",
      "epoch:197, train acc:1.0, test acc:0.7475\n",
      "epoch:198, train acc:1.0, test acc:0.7496\n",
      "epoch:199, train acc:1.0, test acc:0.7471\n",
      "epoch:200, train acc:1.0, test acc:0.7473\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 6.4 正则化\n",
    "# 过拟合：\n",
    "#     产生原因：模型拥有大量参数，表现力强    训练数据少\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.pardir)  # 为了导入父目录的文件而进行的设定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.multi_layer_net import MultiLayerNet\n",
    "from common.optimizer import SGD\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 为了再现过拟合，减少学习数据\n",
    "x_train = x_train[:300]\n",
    "t_train = t_train[:300]\n",
    "\n",
    "# weight decay（权值衰减）的设定 =======================\n",
    "weight_decay_lambda = 0 # 不使用权值衰减的情况\n",
    "#weight_decay_lambda = 0.1\n",
    "# ====================================================\n",
    "\n",
    "network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10,\n",
    "                        weight_decay_lambda=weight_decay_lambda)\n",
    "optimizer = SGD(lr=0.01)\n",
    "\n",
    "max_epochs = 201\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "epoch_cnt = 0\n",
    "\n",
    "for i in range(1000000000):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    grads = network.gradient(x_batch, t_batch)\n",
    "    optimizer.update(network.params, grads)\n",
    "\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "\n",
    "        print(\"epoch:\" + str(epoch_cnt) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc))\n",
    "\n",
    "        epoch_cnt += 1\n",
    "        if epoch_cnt >= max_epochs:\n",
    "            break\n",
    "\n",
    "\n",
    "# 3.绘制图形==========\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train acc:0.09666666666666666, test acc:0.0998\n",
      "epoch:1, train acc:0.13, test acc:0.112\n",
      "epoch:2, train acc:0.13333333333333333, test acc:0.1213\n",
      "epoch:3, train acc:0.14, test acc:0.1275\n",
      "epoch:4, train acc:0.16, test acc:0.1284\n",
      "epoch:5, train acc:0.17333333333333334, test acc:0.1315\n",
      "epoch:6, train acc:0.18333333333333332, test acc:0.135\n",
      "epoch:7, train acc:0.21666666666666667, test acc:0.1437\n",
      "epoch:8, train acc:0.22333333333333333, test acc:0.1451\n",
      "epoch:9, train acc:0.23333333333333334, test acc:0.1509\n",
      "epoch:10, train acc:0.24666666666666667, test acc:0.1623\n",
      "epoch:11, train acc:0.2733333333333333, test acc:0.1706\n",
      "epoch:12, train acc:0.2833333333333333, test acc:0.1745\n",
      "epoch:13, train acc:0.29, test acc:0.1838\n",
      "epoch:14, train acc:0.32666666666666666, test acc:0.2039\n",
      "epoch:15, train acc:0.33, test acc:0.2151\n",
      "epoch:16, train acc:0.3333333333333333, test acc:0.2149\n",
      "epoch:17, train acc:0.35, test acc:0.2148\n",
      "epoch:18, train acc:0.36, test acc:0.2269\n",
      "epoch:19, train acc:0.37, test acc:0.234\n",
      "epoch:20, train acc:0.39666666666666667, test acc:0.2566\n",
      "epoch:21, train acc:0.38666666666666666, test acc:0.2591\n",
      "epoch:22, train acc:0.42, test acc:0.2879\n",
      "epoch:23, train acc:0.43333333333333335, test acc:0.3034\n",
      "epoch:24, train acc:0.44666666666666666, test acc:0.3155\n",
      "epoch:25, train acc:0.44666666666666666, test acc:0.3257\n",
      "epoch:26, train acc:0.48, test acc:0.341\n",
      "epoch:27, train acc:0.48, test acc:0.3414\n",
      "epoch:28, train acc:0.49, test acc:0.3512\n",
      "epoch:29, train acc:0.52, test acc:0.3845\n",
      "epoch:30, train acc:0.5433333333333333, test acc:0.3987\n",
      "epoch:31, train acc:0.54, test acc:0.3984\n",
      "epoch:32, train acc:0.5533333333333333, test acc:0.4061\n",
      "epoch:33, train acc:0.55, test acc:0.4134\n",
      "epoch:34, train acc:0.56, test acc:0.4235\n",
      "epoch:35, train acc:0.59, test acc:0.4364\n",
      "epoch:36, train acc:0.61, test acc:0.4506\n",
      "epoch:37, train acc:0.6266666666666667, test acc:0.4594\n",
      "epoch:38, train acc:0.6166666666666667, test acc:0.4604\n",
      "epoch:39, train acc:0.6133333333333333, test acc:0.4565\n",
      "epoch:40, train acc:0.62, test acc:0.4622\n",
      "epoch:41, train acc:0.62, test acc:0.4642\n",
      "epoch:42, train acc:0.6466666666666666, test acc:0.4793\n",
      "epoch:43, train acc:0.6233333333333333, test acc:0.4666\n",
      "epoch:44, train acc:0.6333333333333333, test acc:0.4701\n",
      "epoch:45, train acc:0.6266666666666667, test acc:0.474\n",
      "epoch:46, train acc:0.6266666666666667, test acc:0.4732\n",
      "epoch:47, train acc:0.6533333333333333, test acc:0.5042\n",
      "epoch:48, train acc:0.66, test acc:0.5128\n",
      "epoch:49, train acc:0.66, test acc:0.5146\n",
      "epoch:50, train acc:0.66, test acc:0.5268\n",
      "epoch:51, train acc:0.6733333333333333, test acc:0.5336\n",
      "epoch:52, train acc:0.67, test acc:0.5442\n",
      "epoch:53, train acc:0.67, test acc:0.5402\n",
      "epoch:54, train acc:0.6766666666666666, test acc:0.5399\n",
      "epoch:55, train acc:0.6833333333333333, test acc:0.5497\n",
      "epoch:56, train acc:0.6866666666666666, test acc:0.5631\n",
      "epoch:57, train acc:0.69, test acc:0.5603\n",
      "epoch:58, train acc:0.6833333333333333, test acc:0.5589\n",
      "epoch:59, train acc:0.69, test acc:0.5659\n",
      "epoch:60, train acc:0.7, test acc:0.5635\n",
      "epoch:61, train acc:0.7033333333333334, test acc:0.566\n",
      "epoch:62, train acc:0.72, test acc:0.5864\n",
      "epoch:63, train acc:0.7033333333333334, test acc:0.5778\n",
      "epoch:64, train acc:0.71, test acc:0.5801\n",
      "epoch:65, train acc:0.71, test acc:0.5732\n",
      "epoch:66, train acc:0.7166666666666667, test acc:0.5762\n",
      "epoch:67, train acc:0.7266666666666667, test acc:0.5856\n",
      "epoch:68, train acc:0.71, test acc:0.5815\n",
      "epoch:69, train acc:0.7233333333333334, test acc:0.5758\n",
      "epoch:70, train acc:0.7166666666666667, test acc:0.5767\n",
      "epoch:71, train acc:0.7233333333333334, test acc:0.5835\n",
      "epoch:72, train acc:0.7466666666666667, test acc:0.5877\n",
      "epoch:73, train acc:0.7433333333333333, test acc:0.5892\n",
      "epoch:74, train acc:0.75, test acc:0.5964\n",
      "epoch:75, train acc:0.7533333333333333, test acc:0.5967\n",
      "epoch:76, train acc:0.75, test acc:0.597\n",
      "epoch:77, train acc:0.7866666666666666, test acc:0.6386\n",
      "epoch:78, train acc:0.7933333333333333, test acc:0.6333\n",
      "epoch:79, train acc:0.79, test acc:0.6422\n",
      "epoch:80, train acc:0.79, test acc:0.6333\n",
      "epoch:81, train acc:0.78, test acc:0.6343\n",
      "epoch:82, train acc:0.7933333333333333, test acc:0.6261\n",
      "epoch:83, train acc:0.7533333333333333, test acc:0.6032\n",
      "epoch:84, train acc:0.7733333333333333, test acc:0.6063\n",
      "epoch:85, train acc:0.7866666666666666, test acc:0.6078\n",
      "epoch:86, train acc:0.7833333333333333, test acc:0.6165\n",
      "epoch:87, train acc:0.7733333333333333, test acc:0.6143\n",
      "epoch:88, train acc:0.8066666666666666, test acc:0.62\n",
      "epoch:89, train acc:0.8, test acc:0.6493\n",
      "epoch:90, train acc:0.8133333333333334, test acc:0.65\n",
      "epoch:91, train acc:0.8233333333333334, test acc:0.6602\n",
      "epoch:92, train acc:0.8166666666666667, test acc:0.6412\n",
      "epoch:93, train acc:0.83, test acc:0.6411\n",
      "epoch:94, train acc:0.83, test acc:0.64\n",
      "epoch:95, train acc:0.8166666666666667, test acc:0.6281\n",
      "epoch:96, train acc:0.82, test acc:0.6485\n",
      "epoch:97, train acc:0.8233333333333334, test acc:0.6649\n",
      "epoch:98, train acc:0.8333333333333334, test acc:0.6561\n",
      "epoch:99, train acc:0.8233333333333334, test acc:0.6512\n",
      "epoch:100, train acc:0.8433333333333334, test acc:0.6514\n",
      "epoch:101, train acc:0.8333333333333334, test acc:0.6476\n",
      "epoch:102, train acc:0.8366666666666667, test acc:0.6477\n",
      "epoch:103, train acc:0.84, test acc:0.6388\n",
      "epoch:104, train acc:0.84, test acc:0.6548\n",
      "epoch:105, train acc:0.8266666666666667, test acc:0.6376\n",
      "epoch:106, train acc:0.8433333333333334, test acc:0.6546\n",
      "epoch:107, train acc:0.8466666666666667, test acc:0.6612\n",
      "epoch:108, train acc:0.85, test acc:0.6674\n",
      "epoch:109, train acc:0.8466666666666667, test acc:0.6597\n",
      "epoch:110, train acc:0.8433333333333334, test acc:0.6556\n",
      "epoch:111, train acc:0.84, test acc:0.6588\n",
      "epoch:112, train acc:0.83, test acc:0.6451\n",
      "epoch:113, train acc:0.8233333333333334, test acc:0.6419\n",
      "epoch:114, train acc:0.8366666666666667, test acc:0.6459\n",
      "epoch:115, train acc:0.8533333333333334, test acc:0.6675\n",
      "epoch:116, train acc:0.84, test acc:0.6723\n",
      "epoch:117, train acc:0.8366666666666667, test acc:0.6634\n",
      "epoch:118, train acc:0.8533333333333334, test acc:0.6709\n",
      "epoch:119, train acc:0.8433333333333334, test acc:0.6661\n",
      "epoch:120, train acc:0.8533333333333334, test acc:0.671\n",
      "epoch:121, train acc:0.8566666666666667, test acc:0.6876\n",
      "epoch:122, train acc:0.8666666666666667, test acc:0.6828\n",
      "epoch:123, train acc:0.8566666666666667, test acc:0.6816\n",
      "epoch:124, train acc:0.85, test acc:0.6726\n",
      "epoch:125, train acc:0.84, test acc:0.6549\n",
      "epoch:126, train acc:0.8366666666666667, test acc:0.6532\n",
      "epoch:127, train acc:0.8366666666666667, test acc:0.6542\n",
      "epoch:128, train acc:0.84, test acc:0.6568\n",
      "epoch:129, train acc:0.84, test acc:0.6643\n",
      "epoch:130, train acc:0.8366666666666667, test acc:0.6714\n",
      "epoch:131, train acc:0.8366666666666667, test acc:0.6622\n",
      "epoch:132, train acc:0.85, test acc:0.688\n",
      "epoch:133, train acc:0.87, test acc:0.6955\n",
      "epoch:134, train acc:0.87, test acc:0.6962\n",
      "epoch:135, train acc:0.86, test acc:0.6877\n",
      "epoch:136, train acc:0.8566666666666667, test acc:0.6797\n",
      "epoch:137, train acc:0.87, test acc:0.6979\n",
      "epoch:138, train acc:0.86, test acc:0.6896\n",
      "epoch:139, train acc:0.8566666666666667, test acc:0.7073\n",
      "epoch:140, train acc:0.85, test acc:0.7012\n",
      "epoch:141, train acc:0.8566666666666667, test acc:0.6922\n",
      "epoch:142, train acc:0.8666666666666667, test acc:0.6975\n",
      "epoch:143, train acc:0.86, test acc:0.699\n",
      "epoch:144, train acc:0.8533333333333334, test acc:0.7035\n",
      "epoch:145, train acc:0.8633333333333333, test acc:0.7053\n",
      "epoch:146, train acc:0.8733333333333333, test acc:0.7007\n",
      "epoch:147, train acc:0.8666666666666667, test acc:0.7002\n",
      "epoch:148, train acc:0.8666666666666667, test acc:0.7014\n",
      "epoch:149, train acc:0.87, test acc:0.6926\n",
      "epoch:150, train acc:0.86, test acc:0.6989\n",
      "epoch:151, train acc:0.8666666666666667, test acc:0.6898\n",
      "epoch:152, train acc:0.8533333333333334, test acc:0.6756\n",
      "epoch:153, train acc:0.8733333333333333, test acc:0.6812\n",
      "epoch:154, train acc:0.8733333333333333, test acc:0.6896\n",
      "epoch:155, train acc:0.8566666666666667, test acc:0.6907\n",
      "epoch:156, train acc:0.8766666666666667, test acc:0.7111\n",
      "epoch:157, train acc:0.8733333333333333, test acc:0.7006\n",
      "epoch:158, train acc:0.8733333333333333, test acc:0.7055\n",
      "epoch:159, train acc:0.8633333333333333, test acc:0.7015\n",
      "epoch:160, train acc:0.8466666666666667, test acc:0.6798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:161, train acc:0.8633333333333333, test acc:0.6893\n",
      "epoch:162, train acc:0.8733333333333333, test acc:0.7117\n",
      "epoch:163, train acc:0.8766666666666667, test acc:0.7083\n",
      "epoch:164, train acc:0.88, test acc:0.7108\n",
      "epoch:165, train acc:0.8866666666666667, test acc:0.7122\n",
      "epoch:166, train acc:0.8866666666666667, test acc:0.7052\n",
      "epoch:167, train acc:0.8766666666666667, test acc:0.6977\n",
      "epoch:168, train acc:0.89, test acc:0.7191\n",
      "epoch:169, train acc:0.8866666666666667, test acc:0.7217\n",
      "epoch:170, train acc:0.8866666666666667, test acc:0.7202\n",
      "epoch:171, train acc:0.89, test acc:0.7235\n",
      "epoch:172, train acc:0.8866666666666667, test acc:0.7182\n",
      "epoch:173, train acc:0.87, test acc:0.718\n",
      "epoch:174, train acc:0.88, test acc:0.7198\n",
      "epoch:175, train acc:0.8766666666666667, test acc:0.7165\n",
      "epoch:176, train acc:0.8866666666666667, test acc:0.7151\n",
      "epoch:177, train acc:0.8866666666666667, test acc:0.7229\n",
      "epoch:178, train acc:0.89, test acc:0.7087\n",
      "epoch:179, train acc:0.88, test acc:0.7152\n",
      "epoch:180, train acc:0.8833333333333333, test acc:0.7174\n",
      "epoch:181, train acc:0.8833333333333333, test acc:0.7293\n",
      "epoch:182, train acc:0.8833333333333333, test acc:0.7238\n",
      "epoch:183, train acc:0.8833333333333333, test acc:0.72\n",
      "epoch:184, train acc:0.8866666666666667, test acc:0.7236\n",
      "epoch:185, train acc:0.8933333333333333, test acc:0.7239\n",
      "epoch:186, train acc:0.88, test acc:0.7249\n",
      "epoch:187, train acc:0.89, test acc:0.7234\n",
      "epoch:188, train acc:0.9, test acc:0.7247\n",
      "epoch:189, train acc:0.8966666666666666, test acc:0.7235\n",
      "epoch:190, train acc:0.8966666666666666, test acc:0.7194\n",
      "epoch:191, train acc:0.8866666666666667, test acc:0.7263\n",
      "epoch:192, train acc:0.8866666666666667, test acc:0.7224\n",
      "epoch:193, train acc:0.8733333333333333, test acc:0.7158\n",
      "epoch:194, train acc:0.8833333333333333, test acc:0.7118\n",
      "epoch:195, train acc:0.8766666666666667, test acc:0.7154\n",
      "epoch:196, train acc:0.8833333333333333, test acc:0.7143\n",
      "epoch:197, train acc:0.88, test acc:0.7034\n",
      "epoch:198, train acc:0.8866666666666667, test acc:0.7047\n",
      "epoch:199, train acc:0.8866666666666667, test acc:0.7122\n",
      "epoch:200, train acc:0.8833333333333333, test acc:0.7188\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 权值衰减\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.pardir)  # 为了导入父目录的文件而进行的设定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.multi_layer_net import MultiLayerNet\n",
    "from common.optimizer import SGD\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 为了再现过拟合，减少学习数据\n",
    "x_train = x_train[:300]\n",
    "t_train = t_train[:300]\n",
    "\n",
    "# weight decay（权值衰减）的设定 =======================\n",
    "#weight_decay_lambda = 0 # 不使用权值衰减的情况\n",
    "weight_decay_lambda = 0.1\n",
    "# ====================================================\n",
    "\n",
    "network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10,\n",
    "                        weight_decay_lambda=weight_decay_lambda)\n",
    "optimizer = SGD(lr=0.01)\n",
    "\n",
    "max_epochs = 201\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "epoch_cnt = 0\n",
    "\n",
    "for i in range(1000000000):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    grads = network.gradient(x_batch, t_batch)\n",
    "    optimizer.update(network.params, grads)\n",
    "\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "\n",
    "        print(\"epoch:\" + str(epoch_cnt) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc))\n",
    "\n",
    "        epoch_cnt += 1\n",
    "        if epoch_cnt >= max_epochs:\n",
    "            break\n",
    "\n",
    "\n",
    "# 3.绘制图形==========\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout\n",
    "# 在学习的过程中随机删除神经元\n",
    "class Dropout:\n",
    "    def __init__(self, dropout_ratio=0.5):\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, x, train_flg=True):\n",
    "        if train_flg:\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
    "            return x * self.mask\n",
    "        else:\n",
    "            return x * (1.0 - self.dropout_ratio)\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.293568872657596\n",
      "=== epoch:1, train acc:0.09666666666666666, test acc:0.0848 ===\n",
      "train loss:2.291787927186392\n",
      "train loss:2.2993382750627993\n",
      "train loss:2.2884613762616777\n",
      "=== epoch:2, train acc:0.09333333333333334, test acc:0.0865 ===\n",
      "train loss:2.3078543379268233\n",
      "train loss:2.3014305200763863\n",
      "train loss:2.2985121559700996\n",
      "=== epoch:3, train acc:0.09666666666666666, test acc:0.0877 ===\n",
      "train loss:2.2925706246147706\n",
      "train loss:2.280815975327108\n",
      "train loss:2.275756268392726\n",
      "=== epoch:4, train acc:0.10666666666666667, test acc:0.0915 ===\n",
      "train loss:2.2905396633099873\n",
      "train loss:2.2949968039658923\n",
      "train loss:2.294052934967044\n",
      "=== epoch:5, train acc:0.11333333333333333, test acc:0.0929 ===\n",
      "train loss:2.276345635784249\n",
      "train loss:2.2867215753307844\n",
      "train loss:2.2880128076239568\n",
      "=== epoch:6, train acc:0.11333333333333333, test acc:0.0941 ===\n",
      "train loss:2.27467077431825\n",
      "train loss:2.291508171553738\n",
      "train loss:2.288054050624547\n",
      "=== epoch:7, train acc:0.11666666666666667, test acc:0.0956 ===\n",
      "train loss:2.3013194406648183\n",
      "train loss:2.2855544772007033\n",
      "train loss:2.29679929071505\n",
      "=== epoch:8, train acc:0.11666666666666667, test acc:0.0957 ===\n",
      "train loss:2.2943620586712146\n",
      "train loss:2.3058018458734426\n",
      "train loss:2.287662238194143\n",
      "=== epoch:9, train acc:0.12, test acc:0.0972 ===\n",
      "train loss:2.2950000656399423\n",
      "train loss:2.292725675783983\n",
      "train loss:2.282417175045325\n",
      "=== epoch:10, train acc:0.12333333333333334, test acc:0.0996 ===\n",
      "train loss:2.2881023152825946\n",
      "train loss:2.280241368775411\n",
      "train loss:2.281697440083266\n",
      "=== epoch:11, train acc:0.13666666666666666, test acc:0.1008 ===\n",
      "train loss:2.266200993607999\n",
      "train loss:2.2818680401368514\n",
      "train loss:2.274508586661942\n",
      "=== epoch:12, train acc:0.14333333333333334, test acc:0.1027 ===\n",
      "train loss:2.2730607955642106\n",
      "train loss:2.2686690733947272\n",
      "train loss:2.2823158665342755\n",
      "=== epoch:13, train acc:0.14333333333333334, test acc:0.1068 ===\n",
      "train loss:2.288319567263184\n",
      "train loss:2.2701180397032603\n",
      "train loss:2.278456619529702\n",
      "=== epoch:14, train acc:0.15, test acc:0.1115 ===\n",
      "train loss:2.2738855128394055\n",
      "train loss:2.2444507005313263\n",
      "train loss:2.2709555053436197\n",
      "=== epoch:15, train acc:0.15333333333333332, test acc:0.1144 ===\n",
      "train loss:2.2701013837796946\n",
      "train loss:2.2775622997840155\n",
      "train loss:2.2822110212501987\n",
      "=== epoch:16, train acc:0.15, test acc:0.1198 ===\n",
      "train loss:2.2697527098545676\n",
      "train loss:2.26390718028371\n",
      "train loss:2.2736883885857084\n",
      "=== epoch:17, train acc:0.15666666666666668, test acc:0.1246 ===\n",
      "train loss:2.2623644288366376\n",
      "train loss:2.267682304544607\n",
      "train loss:2.2645440193727593\n",
      "=== epoch:18, train acc:0.16, test acc:0.1307 ===\n",
      "train loss:2.2741247314711774\n",
      "train loss:2.283419796147697\n",
      "train loss:2.2501897151520622\n",
      "=== epoch:19, train acc:0.17666666666666667, test acc:0.1363 ===\n",
      "train loss:2.2557232520242034\n",
      "train loss:2.2686258343674184\n",
      "train loss:2.282484489784459\n",
      "=== epoch:20, train acc:0.18, test acc:0.1438 ===\n",
      "train loss:2.287472348887915\n",
      "train loss:2.2687797546612507\n",
      "train loss:2.2432816717547444\n",
      "=== epoch:21, train acc:0.20333333333333334, test acc:0.1496 ===\n",
      "train loss:2.274808690509793\n",
      "train loss:2.2681396957448623\n",
      "train loss:2.2552714402463585\n",
      "=== epoch:22, train acc:0.21666666666666667, test acc:0.156 ===\n",
      "train loss:2.262386841641502\n",
      "train loss:2.2858374915445405\n",
      "train loss:2.2752174594225107\n",
      "=== epoch:23, train acc:0.23, test acc:0.1613 ===\n",
      "train loss:2.263101755881458\n",
      "train loss:2.258443141330687\n",
      "train loss:2.2686558567695982\n",
      "=== epoch:24, train acc:0.24, test acc:0.1683 ===\n",
      "train loss:2.2505660767040374\n",
      "train loss:2.267557056157068\n",
      "train loss:2.257919265270715\n",
      "=== epoch:25, train acc:0.24, test acc:0.1742 ===\n",
      "train loss:2.2721074927836087\n",
      "train loss:2.265897319783703\n",
      "train loss:2.260343229262854\n",
      "=== epoch:26, train acc:0.25, test acc:0.1788 ===\n",
      "train loss:2.2586121282754807\n",
      "train loss:2.2615646712803414\n",
      "train loss:2.240306637975215\n",
      "=== epoch:27, train acc:0.27, test acc:0.1869 ===\n",
      "train loss:2.2513650405340333\n",
      "train loss:2.2621857976642734\n",
      "train loss:2.279026896180152\n",
      "=== epoch:28, train acc:0.2733333333333333, test acc:0.1947 ===\n",
      "train loss:2.246706603644239\n",
      "train loss:2.2687784206981676\n",
      "train loss:2.249791043101978\n",
      "=== epoch:29, train acc:0.28, test acc:0.1973 ===\n",
      "train loss:2.254461623291651\n",
      "train loss:2.2633361821065847\n",
      "train loss:2.2449296134559\n",
      "=== epoch:30, train acc:0.2833333333333333, test acc:0.2025 ===\n",
      "train loss:2.2702620295846208\n",
      "train loss:2.2534900925309524\n",
      "train loss:2.2490530787059084\n",
      "=== epoch:31, train acc:0.2866666666666667, test acc:0.2095 ===\n",
      "train loss:2.2423557157503877\n",
      "train loss:2.2575163348018354\n",
      "train loss:2.280551798928734\n",
      "=== epoch:32, train acc:0.30333333333333334, test acc:0.2157 ===\n",
      "train loss:2.2425187478131936\n",
      "train loss:2.2622019481243725\n",
      "train loss:2.252687642809233\n",
      "=== epoch:33, train acc:0.30333333333333334, test acc:0.2178 ===\n",
      "train loss:2.266144643610572\n",
      "train loss:2.252870962176155\n",
      "train loss:2.2404860940756284\n",
      "=== epoch:34, train acc:0.30666666666666664, test acc:0.2211 ===\n",
      "train loss:2.251997876467869\n",
      "train loss:2.2546825993481416\n",
      "train loss:2.2505981508669284\n",
      "=== epoch:35, train acc:0.31333333333333335, test acc:0.2265 ===\n",
      "train loss:2.233984658863014\n",
      "train loss:2.261122257981548\n",
      "train loss:2.2553225313499916\n",
      "=== epoch:36, train acc:0.3233333333333333, test acc:0.2346 ===\n",
      "train loss:2.242677414632397\n",
      "train loss:2.2583259822592257\n",
      "train loss:2.2646003805358372\n",
      "=== epoch:37, train acc:0.32666666666666666, test acc:0.2427 ===\n",
      "train loss:2.2347225823118704\n",
      "train loss:2.2310796124853063\n",
      "train loss:2.254954948568573\n",
      "=== epoch:38, train acc:0.3333333333333333, test acc:0.2453 ===\n",
      "train loss:2.2608732061510035\n",
      "train loss:2.2159212600546105\n",
      "train loss:2.238020748045442\n",
      "=== epoch:39, train acc:0.3466666666666667, test acc:0.2496 ===\n",
      "train loss:2.231384517726763\n",
      "train loss:2.2462062282945774\n",
      "train loss:2.213318415179121\n",
      "=== epoch:40, train acc:0.3433333333333333, test acc:0.2504 ===\n",
      "train loss:2.2409465259795045\n",
      "train loss:2.234262798334019\n",
      "train loss:2.2377309010116253\n",
      "=== epoch:41, train acc:0.3433333333333333, test acc:0.2516 ===\n",
      "train loss:2.2457067849964223\n",
      "train loss:2.2522392185008244\n",
      "train loss:2.2488993616596926\n",
      "=== epoch:42, train acc:0.35, test acc:0.2566 ===\n",
      "train loss:2.2488884782950906\n",
      "train loss:2.2539052759083877\n",
      "train loss:2.2478985680387455\n",
      "=== epoch:43, train acc:0.3566666666666667, test acc:0.2597 ===\n",
      "train loss:2.2475268155648003\n",
      "train loss:2.2467920108974386\n",
      "train loss:2.2420947028858653\n",
      "=== epoch:44, train acc:0.35333333333333333, test acc:0.2646 ===\n",
      "train loss:2.23314390572381\n",
      "train loss:2.2483724289563405\n",
      "train loss:2.2152850207346755\n",
      "=== epoch:45, train acc:0.37, test acc:0.2674 ===\n",
      "train loss:2.244351156209754\n",
      "train loss:2.213843218543854\n",
      "train loss:2.2340509779009756\n",
      "=== epoch:46, train acc:0.37, test acc:0.271 ===\n",
      "train loss:2.2267016658069125\n",
      "train loss:2.2149489374734572\n",
      "train loss:2.2226583488659064\n",
      "=== epoch:47, train acc:0.38666666666666666, test acc:0.2704 ===\n",
      "train loss:2.2292033539504974\n",
      "train loss:2.2287281939973154\n",
      "train loss:2.242335575827221\n",
      "=== epoch:48, train acc:0.38333333333333336, test acc:0.2724 ===\n",
      "train loss:2.2370421367461315\n",
      "train loss:2.2377532987419637\n",
      "train loss:2.229477988766237\n",
      "=== epoch:49, train acc:0.38666666666666666, test acc:0.2754 ===\n",
      "train loss:2.2236900151343817\n",
      "train loss:2.234702111578951\n",
      "train loss:2.24133081472501\n",
      "=== epoch:50, train acc:0.38666666666666666, test acc:0.2783 ===\n",
      "train loss:2.244909842984438\n",
      "train loss:2.20998074170468\n",
      "train loss:2.2281215694852596\n",
      "=== epoch:51, train acc:0.38666666666666666, test acc:0.2784 ===\n",
      "train loss:2.209671055451805\n",
      "train loss:2.2309231343190716\n",
      "train loss:2.219306908552386\n",
      "=== epoch:52, train acc:0.38, test acc:0.2823 ===\n",
      "train loss:2.2447005272829146\n",
      "train loss:2.239038953868251\n",
      "train loss:2.2269780435346656\n",
      "=== epoch:53, train acc:0.38333333333333336, test acc:0.2871 ===\n",
      "train loss:2.2398516789041456\n",
      "train loss:2.2235938186552766\n",
      "train loss:2.2459217225648604\n",
      "=== epoch:54, train acc:0.38333333333333336, test acc:0.2914 ===\n",
      "train loss:2.2144659638209756\n",
      "train loss:2.223738713057696\n",
      "train loss:2.2142089562643967\n",
      "=== epoch:55, train acc:0.38333333333333336, test acc:0.2947 ===\n",
      "train loss:2.209777446777783\n",
      "train loss:2.1956456512270797\n",
      "train loss:2.2176118722618874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:56, train acc:0.37666666666666665, test acc:0.2926 ===\n",
      "train loss:2.222690839566905\n",
      "train loss:2.2164309454036517\n",
      "train loss:2.214246674588496\n",
      "=== epoch:57, train acc:0.37666666666666665, test acc:0.2933 ===\n",
      "train loss:2.2273656463312217\n",
      "train loss:2.2095971635332234\n",
      "train loss:2.2119097410448436\n",
      "=== epoch:58, train acc:0.39, test acc:0.2935 ===\n",
      "train loss:2.194175127690622\n",
      "train loss:2.2346819248368193\n",
      "train loss:2.2144829391091836\n",
      "=== epoch:59, train acc:0.3933333333333333, test acc:0.2975 ===\n",
      "train loss:2.22821358313087\n",
      "train loss:2.2004244797866765\n",
      "train loss:2.2480666597684342\n",
      "=== epoch:60, train acc:0.3933333333333333, test acc:0.3005 ===\n",
      "train loss:2.208195070530535\n",
      "train loss:2.2221013908658707\n",
      "train loss:2.216291104725049\n",
      "=== epoch:61, train acc:0.4, test acc:0.3028 ===\n",
      "train loss:2.193744561029593\n",
      "train loss:2.1984627383986326\n",
      "train loss:2.2105685073410823\n",
      "=== epoch:62, train acc:0.39666666666666667, test acc:0.3061 ===\n",
      "train loss:2.19350158209181\n",
      "train loss:2.2297465312279057\n",
      "train loss:2.2391696199959794\n",
      "=== epoch:63, train acc:0.39666666666666667, test acc:0.3073 ===\n",
      "train loss:2.2028051958983563\n",
      "train loss:2.2337294449782576\n",
      "train loss:2.2254118686212156\n",
      "=== epoch:64, train acc:0.41, test acc:0.311 ===\n",
      "train loss:2.2002811956852018\n",
      "train loss:2.1940522338728377\n",
      "train loss:2.211668519412174\n",
      "=== epoch:65, train acc:0.4166666666666667, test acc:0.3126 ===\n",
      "train loss:2.1886974335716927\n",
      "train loss:2.2137332246048533\n",
      "train loss:2.1806926158570534\n",
      "=== epoch:66, train acc:0.41333333333333333, test acc:0.3105 ===\n",
      "train loss:2.223323355453889\n",
      "train loss:2.2178073983433495\n",
      "train loss:2.1990565261201502\n",
      "=== epoch:67, train acc:0.4066666666666667, test acc:0.3127 ===\n",
      "train loss:2.2017748456441995\n",
      "train loss:2.204348346815433\n",
      "train loss:2.217968455338231\n",
      "=== epoch:68, train acc:0.4066666666666667, test acc:0.3148 ===\n",
      "train loss:2.230611985753284\n",
      "train loss:2.183151155108662\n",
      "train loss:2.1894260914019243\n",
      "=== epoch:69, train acc:0.4166666666666667, test acc:0.318 ===\n",
      "train loss:2.1983526875371133\n",
      "train loss:2.1935838906467975\n",
      "train loss:2.17980726203001\n",
      "=== epoch:70, train acc:0.41333333333333333, test acc:0.3188 ===\n",
      "train loss:2.208133209371847\n",
      "train loss:2.192881004634258\n",
      "train loss:2.180861101864885\n",
      "=== epoch:71, train acc:0.4166666666666667, test acc:0.3194 ===\n",
      "train loss:2.1893023922843238\n",
      "train loss:2.1913958604274852\n",
      "train loss:2.1774403790790995\n",
      "=== epoch:72, train acc:0.4266666666666667, test acc:0.3194 ===\n",
      "train loss:2.2044387597209685\n",
      "train loss:2.1936616564421483\n",
      "train loss:2.1702633340369077\n",
      "=== epoch:73, train acc:0.43, test acc:0.3215 ===\n",
      "train loss:2.1709919620869296\n",
      "train loss:2.192363248013395\n",
      "train loss:2.1460800496442487\n",
      "=== epoch:74, train acc:0.4266666666666667, test acc:0.3202 ===\n",
      "train loss:2.195654892115906\n",
      "train loss:2.1851588817911773\n",
      "train loss:2.2089318868734082\n",
      "=== epoch:75, train acc:0.4166666666666667, test acc:0.3176 ===\n",
      "train loss:2.200091503325135\n",
      "train loss:2.159739738406916\n",
      "train loss:2.1820948428278464\n",
      "=== epoch:76, train acc:0.4266666666666667, test acc:0.3231 ===\n",
      "train loss:2.173489790348647\n",
      "train loss:2.2099131077812357\n",
      "train loss:2.187826326276506\n",
      "=== epoch:77, train acc:0.4266666666666667, test acc:0.3275 ===\n",
      "train loss:2.19665718405814\n",
      "train loss:2.1846689683079052\n",
      "train loss:2.171928503151045\n",
      "=== epoch:78, train acc:0.4266666666666667, test acc:0.3278 ===\n",
      "train loss:2.174280119978847\n",
      "train loss:2.185184114884736\n",
      "train loss:2.158328903000177\n",
      "=== epoch:79, train acc:0.43, test acc:0.3288 ===\n",
      "train loss:2.199783061284832\n",
      "train loss:2.1685443077879665\n",
      "train loss:2.1595612920704097\n",
      "=== epoch:80, train acc:0.43666666666666665, test acc:0.3319 ===\n",
      "train loss:2.182510405205177\n",
      "train loss:2.1498787500435927\n",
      "train loss:2.1364260600675347\n",
      "=== epoch:81, train acc:0.43666666666666665, test acc:0.3346 ===\n",
      "train loss:2.185293685660956\n",
      "train loss:2.1714811176703708\n",
      "train loss:2.1687516987854205\n",
      "=== epoch:82, train acc:0.43666666666666665, test acc:0.3368 ===\n",
      "train loss:2.172029305320845\n",
      "train loss:2.1595571631153607\n",
      "train loss:2.161049582253106\n",
      "=== epoch:83, train acc:0.43, test acc:0.3357 ===\n",
      "train loss:2.1584808910074926\n",
      "train loss:2.1884175184003163\n",
      "train loss:2.150426080653738\n",
      "=== epoch:84, train acc:0.43, test acc:0.3336 ===\n",
      "train loss:2.1748034918462817\n",
      "train loss:2.157976819441824\n",
      "train loss:2.1984270771775503\n",
      "=== epoch:85, train acc:0.43666666666666665, test acc:0.336 ===\n",
      "train loss:2.18438098113537\n",
      "train loss:2.1720111800285036\n",
      "train loss:2.1792861101345604\n",
      "=== epoch:86, train acc:0.43666666666666665, test acc:0.338 ===\n",
      "train loss:2.1518349440526188\n",
      "train loss:2.1613879035689116\n",
      "train loss:2.1592951020309084\n",
      "=== epoch:87, train acc:0.43666666666666665, test acc:0.3433 ===\n",
      "train loss:2.171798980528349\n",
      "train loss:2.1954701680421795\n",
      "train loss:2.1526931989487097\n",
      "=== epoch:88, train acc:0.4533333333333333, test acc:0.3478 ===\n",
      "train loss:2.1574901376754902\n",
      "train loss:2.163890751162565\n",
      "train loss:2.1435159691332806\n",
      "=== epoch:89, train acc:0.45, test acc:0.3518 ===\n",
      "train loss:2.1294948616219456\n",
      "train loss:2.1392290410853168\n",
      "train loss:2.1434958433992723\n",
      "=== epoch:90, train acc:0.45, test acc:0.3484 ===\n",
      "train loss:2.1516165189165335\n",
      "train loss:2.136344663424364\n",
      "train loss:2.1507618734864873\n",
      "=== epoch:91, train acc:0.44, test acc:0.3446 ===\n",
      "train loss:2.126757795004858\n",
      "train loss:2.1400750046235535\n",
      "train loss:2.140526712657441\n",
      "=== epoch:92, train acc:0.44666666666666666, test acc:0.3444 ===\n",
      "train loss:2.160822396400951\n",
      "train loss:2.117874794639617\n",
      "train loss:2.101059453224438\n",
      "=== epoch:93, train acc:0.44666666666666666, test acc:0.3432 ===\n",
      "train loss:2.159369629491255\n",
      "train loss:2.1571197363465378\n",
      "train loss:2.1205360324467817\n",
      "=== epoch:94, train acc:0.44, test acc:0.3409 ===\n",
      "train loss:2.1239194017236542\n",
      "train loss:2.140330179940562\n",
      "train loss:2.1460252531787045\n",
      "=== epoch:95, train acc:0.44333333333333336, test acc:0.3435 ===\n",
      "train loss:2.1198474133097447\n",
      "train loss:2.128506151075469\n",
      "train loss:2.1185675604503715\n",
      "=== epoch:96, train acc:0.44, test acc:0.3472 ===\n",
      "train loss:2.093120822563347\n",
      "train loss:2.122902728403408\n",
      "train loss:2.150166962278135\n",
      "=== epoch:97, train acc:0.43666666666666665, test acc:0.3419 ===\n",
      "train loss:2.1312061180492807\n",
      "train loss:2.13423884240122\n",
      "train loss:2.1594112853774843\n",
      "=== epoch:98, train acc:0.43666666666666665, test acc:0.3464 ===\n",
      "train loss:2.0942231614671925\n",
      "train loss:2.1480122471926824\n",
      "train loss:2.085839856801579\n",
      "=== epoch:99, train acc:0.43333333333333335, test acc:0.3437 ===\n",
      "train loss:2.1357409721414307\n",
      "train loss:2.114426006767072\n",
      "train loss:2.0829340976079616\n",
      "=== epoch:100, train acc:0.43666666666666665, test acc:0.3446 ===\n",
      "train loss:2.1039935447081417\n",
      "train loss:2.156680221101409\n",
      "train loss:2.0682167700118557\n",
      "=== epoch:101, train acc:0.44, test acc:0.3489 ===\n",
      "train loss:2.0948402155561237\n",
      "train loss:2.120493746586012\n",
      "train loss:2.1306016222498347\n",
      "=== epoch:102, train acc:0.45, test acc:0.354 ===\n",
      "train loss:2.0947524662148664\n",
      "train loss:2.097064587806174\n",
      "train loss:2.125228066025638\n",
      "=== epoch:103, train acc:0.44666666666666666, test acc:0.3569 ===\n",
      "train loss:2.1240364588656635\n",
      "train loss:2.0852244555660304\n",
      "train loss:2.059618279612408\n",
      "=== epoch:104, train acc:0.45, test acc:0.3541 ===\n",
      "train loss:2.114784576780798\n",
      "train loss:2.0780015168621664\n",
      "train loss:2.112271649953958\n",
      "=== epoch:105, train acc:0.45, test acc:0.3528 ===\n",
      "train loss:2.0904389333799713\n",
      "train loss:2.0695791203376945\n",
      "train loss:2.123488324428042\n",
      "=== epoch:106, train acc:0.44666666666666666, test acc:0.3531 ===\n",
      "train loss:2.079038101238581\n",
      "train loss:2.06967918287744\n",
      "train loss:2.0840890792224434\n",
      "=== epoch:107, train acc:0.44666666666666666, test acc:0.347 ===\n",
      "train loss:2.077507923474181\n",
      "train loss:2.0344446109768475\n",
      "train loss:2.0799766810543674\n",
      "=== epoch:108, train acc:0.45, test acc:0.3464 ===\n",
      "train loss:2.1238477852251325\n",
      "train loss:2.110015781564261\n",
      "train loss:2.076637181921898\n",
      "=== epoch:109, train acc:0.44333333333333336, test acc:0.3478 ===\n",
      "train loss:2.054792904293763\n",
      "train loss:2.087930901243468\n",
      "train loss:2.0625251127229256\n",
      "=== epoch:110, train acc:0.44666666666666666, test acc:0.352 ===\n",
      "train loss:2.1178430848697802\n",
      "train loss:2.0683391631971286\n",
      "train loss:2.0380561307934575\n",
      "=== epoch:111, train acc:0.44666666666666666, test acc:0.3521 ===\n",
      "train loss:2.076025453303781\n",
      "train loss:2.0726883748672016\n",
      "train loss:2.0836152616788186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:112, train acc:0.4533333333333333, test acc:0.3517 ===\n",
      "train loss:2.04949085010875\n",
      "train loss:2.0785548599385972\n",
      "train loss:2.0685941730764283\n",
      "=== epoch:113, train acc:0.45, test acc:0.3543 ===\n",
      "train loss:2.082848883662349\n",
      "train loss:2.020495062748058\n",
      "train loss:2.124952890649846\n",
      "=== epoch:114, train acc:0.44666666666666666, test acc:0.355 ===\n",
      "train loss:2.0592936987687915\n",
      "train loss:2.0703626984249386\n",
      "train loss:2.074924685614462\n",
      "=== epoch:115, train acc:0.45, test acc:0.3549 ===\n",
      "train loss:2.114055110550458\n",
      "train loss:2.070845087538554\n",
      "train loss:2.0509989112525644\n",
      "=== epoch:116, train acc:0.4533333333333333, test acc:0.3604 ===\n",
      "train loss:2.015205639017675\n",
      "train loss:2.0135185204549724\n",
      "train loss:2.0639064812254464\n",
      "=== epoch:117, train acc:0.44666666666666666, test acc:0.3603 ===\n",
      "train loss:2.06075562739736\n",
      "train loss:2.0558876454912847\n",
      "train loss:2.1340835687907864\n",
      "=== epoch:118, train acc:0.4666666666666667, test acc:0.3645 ===\n",
      "train loss:2.0184670198626247\n",
      "train loss:2.073017996868727\n",
      "train loss:1.9639841195349463\n",
      "=== epoch:119, train acc:0.4666666666666667, test acc:0.3606 ===\n",
      "train loss:2.0351735384483196\n",
      "train loss:2.040046670583209\n",
      "train loss:1.9885105239437015\n",
      "=== epoch:120, train acc:0.4633333333333333, test acc:0.3578 ===\n",
      "train loss:2.0392432033196504\n",
      "train loss:1.988629238247996\n",
      "train loss:1.983099625443765\n",
      "=== epoch:121, train acc:0.45, test acc:0.358 ===\n",
      "train loss:2.035143044961096\n",
      "train loss:1.987188132117989\n",
      "train loss:1.9650332517628863\n",
      "=== epoch:122, train acc:0.44, test acc:0.3527 ===\n",
      "train loss:2.0468509372490074\n",
      "train loss:2.0143757804783644\n",
      "train loss:2.0010856670210693\n",
      "=== epoch:123, train acc:0.44333333333333336, test acc:0.3537 ===\n",
      "train loss:1.9813315168750298\n",
      "train loss:2.0385033888345827\n",
      "train loss:2.0740935014009185\n",
      "=== epoch:124, train acc:0.45, test acc:0.355 ===\n",
      "train loss:1.930898281045996\n",
      "train loss:2.1141427554395893\n",
      "train loss:1.9326075206761784\n",
      "=== epoch:125, train acc:0.45666666666666667, test acc:0.3575 ===\n",
      "train loss:1.9828304710747946\n",
      "train loss:2.0227036841395774\n",
      "train loss:2.0001094154911128\n",
      "=== epoch:126, train acc:0.44666666666666666, test acc:0.3543 ===\n",
      "train loss:1.957829911086303\n",
      "train loss:2.0404458698310948\n",
      "train loss:1.9452613998215915\n",
      "=== epoch:127, train acc:0.44, test acc:0.3511 ===\n",
      "train loss:1.9954960830898039\n",
      "train loss:1.9725787817682134\n",
      "train loss:1.9994759850654944\n",
      "=== epoch:128, train acc:0.43666666666666665, test acc:0.351 ===\n",
      "train loss:2.022575594346438\n",
      "train loss:1.952516363375467\n",
      "train loss:2.0119591548289386\n",
      "=== epoch:129, train acc:0.43666666666666665, test acc:0.3558 ===\n",
      "train loss:2.011359428728537\n",
      "train loss:1.912938712212959\n",
      "train loss:2.061178468161911\n",
      "=== epoch:130, train acc:0.43666666666666665, test acc:0.3594 ===\n",
      "train loss:1.945694831786746\n",
      "train loss:1.9820819844207584\n",
      "train loss:1.9742856999935066\n",
      "=== epoch:131, train acc:0.43666666666666665, test acc:0.3569 ===\n",
      "train loss:1.9626981677889808\n",
      "train loss:1.9406226560589754\n",
      "train loss:2.011197631890147\n",
      "=== epoch:132, train acc:0.43666666666666665, test acc:0.3612 ===\n",
      "train loss:1.9972240661847456\n",
      "train loss:2.0369167137979396\n",
      "train loss:1.98255246100981\n",
      "=== epoch:133, train acc:0.44, test acc:0.3638 ===\n",
      "train loss:1.912501146561784\n",
      "train loss:1.9955883953007285\n",
      "train loss:1.9170323152136035\n",
      "=== epoch:134, train acc:0.44333333333333336, test acc:0.3654 ===\n",
      "train loss:2.0079707468623633\n",
      "train loss:1.9504034589546329\n",
      "train loss:1.9736874184425333\n",
      "=== epoch:135, train acc:0.4533333333333333, test acc:0.3697 ===\n",
      "train loss:1.9617871178584076\n",
      "train loss:1.9988878689420884\n",
      "train loss:1.9515937217641288\n",
      "=== epoch:136, train acc:0.46, test acc:0.3722 ===\n",
      "train loss:1.9489747464875307\n",
      "train loss:2.0105552921188963\n",
      "train loss:1.8884584730933838\n",
      "=== epoch:137, train acc:0.45666666666666667, test acc:0.3713 ===\n",
      "train loss:1.9821884428138248\n",
      "train loss:1.8952930487566046\n",
      "train loss:1.9293823840344573\n",
      "=== epoch:138, train acc:0.45, test acc:0.37 ===\n",
      "train loss:1.9701589752160276\n",
      "train loss:1.9651934452059978\n",
      "train loss:1.8622642465268178\n",
      "=== epoch:139, train acc:0.45666666666666667, test acc:0.3726 ===\n",
      "train loss:1.9935183913069687\n",
      "train loss:1.8844059994433553\n",
      "train loss:1.9253243301520422\n",
      "=== epoch:140, train acc:0.4633333333333333, test acc:0.3729 ===\n",
      "train loss:1.9574403076651121\n",
      "train loss:1.8716603179582845\n",
      "train loss:1.8612612769052703\n",
      "=== epoch:141, train acc:0.45666666666666667, test acc:0.3706 ===\n",
      "train loss:1.8494167597654012\n",
      "train loss:1.8817445372701582\n",
      "train loss:1.8360659107454902\n",
      "=== epoch:142, train acc:0.44666666666666666, test acc:0.3644 ===\n",
      "train loss:1.9934371346889477\n",
      "train loss:1.9174738518303014\n",
      "train loss:1.8871287881529637\n",
      "=== epoch:143, train acc:0.45666666666666667, test acc:0.3663 ===\n",
      "train loss:1.9923103665651913\n",
      "train loss:1.8704240481252195\n",
      "train loss:1.878468096072856\n",
      "=== epoch:144, train acc:0.44333333333333336, test acc:0.3646 ===\n",
      "train loss:1.863780417721444\n",
      "train loss:1.8744567261024576\n",
      "train loss:1.85170757261451\n",
      "=== epoch:145, train acc:0.44333333333333336, test acc:0.3623 ===\n",
      "train loss:1.8322929819804055\n",
      "train loss:1.8768756791314238\n",
      "train loss:1.8255415432455293\n",
      "=== epoch:146, train acc:0.44, test acc:0.361 ===\n",
      "train loss:1.87372180563831\n",
      "train loss:1.8686283279371352\n",
      "train loss:1.9289221532942498\n",
      "=== epoch:147, train acc:0.44333333333333336, test acc:0.3625 ===\n",
      "train loss:1.9191013815699938\n",
      "train loss:1.84627524499607\n",
      "train loss:1.8430944694023175\n",
      "=== epoch:148, train acc:0.43333333333333335, test acc:0.3592 ===\n",
      "train loss:1.9011158889163609\n",
      "train loss:1.8707415463084371\n",
      "train loss:1.9425198631638378\n",
      "=== epoch:149, train acc:0.44, test acc:0.3623 ===\n",
      "train loss:1.8636789969578935\n",
      "train loss:1.9415147357434654\n",
      "train loss:1.8820947427678518\n",
      "=== epoch:150, train acc:0.44333333333333336, test acc:0.3643 ===\n",
      "train loss:1.896214879832106\n",
      "train loss:1.8336321525334132\n",
      "train loss:1.8554002308952304\n",
      "=== epoch:151, train acc:0.44, test acc:0.3693 ===\n",
      "train loss:1.917534591733871\n",
      "train loss:1.8728690132899428\n",
      "train loss:1.8252953163104342\n",
      "=== epoch:152, train acc:0.44666666666666666, test acc:0.3709 ===\n",
      "train loss:1.9723723238853479\n",
      "train loss:1.7802254886986488\n",
      "train loss:1.8121416312900442\n",
      "=== epoch:153, train acc:0.45, test acc:0.3726 ===\n",
      "train loss:1.8650635292320275\n",
      "train loss:1.8605809722223126\n",
      "train loss:1.8518029853634024\n",
      "=== epoch:154, train acc:0.45, test acc:0.3703 ===\n",
      "train loss:1.828983500393619\n",
      "train loss:1.8529628250589159\n",
      "train loss:1.7935031030034592\n",
      "=== epoch:155, train acc:0.44666666666666666, test acc:0.3694 ===\n",
      "train loss:1.8324425079388902\n",
      "train loss:1.7847477396461964\n",
      "train loss:1.9066543753076084\n",
      "=== epoch:156, train acc:0.44666666666666666, test acc:0.3711 ===\n",
      "train loss:1.8088269872027627\n",
      "train loss:1.8040161077492325\n",
      "train loss:1.8490861435938468\n",
      "=== epoch:157, train acc:0.4533333333333333, test acc:0.3706 ===\n",
      "train loss:1.8026971182998444\n",
      "train loss:1.7967993555480843\n",
      "train loss:1.8052533234730581\n",
      "=== epoch:158, train acc:0.4533333333333333, test acc:0.3728 ===\n",
      "train loss:1.736401997624537\n",
      "train loss:1.8528658478220636\n",
      "train loss:1.7945904477409043\n",
      "=== epoch:159, train acc:0.44333333333333336, test acc:0.368 ===\n",
      "train loss:1.8162465751157364\n",
      "train loss:1.8807514795930171\n",
      "train loss:1.8033206334005578\n",
      "=== epoch:160, train acc:0.44666666666666666, test acc:0.3728 ===\n",
      "train loss:1.6848559159936658\n",
      "train loss:1.7112101772730932\n",
      "train loss:1.7766962337225325\n",
      "=== epoch:161, train acc:0.45, test acc:0.3711 ===\n",
      "train loss:1.7204278128263661\n",
      "train loss:1.8420748873560624\n",
      "train loss:1.7481025590585575\n",
      "=== epoch:162, train acc:0.46, test acc:0.375 ===\n",
      "train loss:1.7293513039287525\n",
      "train loss:1.7718781255617868\n",
      "train loss:1.7560111164615255\n",
      "=== epoch:163, train acc:0.4533333333333333, test acc:0.3755 ===\n",
      "train loss:1.7846543469901355\n",
      "train loss:1.74972965420444\n",
      "train loss:1.8194478393946372\n",
      "=== epoch:164, train acc:0.4633333333333333, test acc:0.3771 ===\n",
      "train loss:1.7542324626108956\n",
      "train loss:1.6437854175420155\n",
      "train loss:1.7522588789562303\n",
      "=== epoch:165, train acc:0.4633333333333333, test acc:0.3728 ===\n",
      "train loss:1.8792519841043416\n",
      "train loss:1.8292692495896283\n",
      "train loss:1.747259941481882\n",
      "=== epoch:166, train acc:0.47333333333333333, test acc:0.3828 ===\n",
      "train loss:1.859013895983199\n",
      "train loss:1.7294803590829857\n",
      "train loss:1.8268829578102375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:167, train acc:0.48333333333333334, test acc:0.3891 ===\n",
      "train loss:1.8144984175677683\n",
      "train loss:1.7432395376201586\n",
      "train loss:1.7940848289594624\n",
      "=== epoch:168, train acc:0.48333333333333334, test acc:0.3923 ===\n",
      "train loss:1.7823091025563387\n",
      "train loss:1.6282671710076175\n",
      "train loss:1.7310865071887493\n",
      "=== epoch:169, train acc:0.48, test acc:0.3922 ===\n",
      "train loss:1.8304292023727238\n",
      "train loss:1.7189573311220834\n",
      "train loss:1.8191262198709284\n",
      "=== epoch:170, train acc:0.4866666666666667, test acc:0.3984 ===\n",
      "train loss:1.7028065773532866\n",
      "train loss:1.7737469513579347\n",
      "train loss:1.7702364112112354\n",
      "=== epoch:171, train acc:0.4866666666666667, test acc:0.4009 ===\n",
      "train loss:1.6980203477415028\n",
      "train loss:1.7779210917821764\n",
      "train loss:1.717705245040946\n",
      "=== epoch:172, train acc:0.5, test acc:0.404 ===\n",
      "train loss:1.7251502999168813\n",
      "train loss:1.7657409353626035\n",
      "train loss:1.8508944328282504\n",
      "=== epoch:173, train acc:0.5033333333333333, test acc:0.4093 ===\n",
      "train loss:1.715960156016732\n",
      "train loss:1.8046519244352015\n",
      "train loss:1.6741920364273426\n",
      "=== epoch:174, train acc:0.51, test acc:0.414 ===\n",
      "train loss:1.7449402061991293\n",
      "train loss:1.7091962884463001\n",
      "train loss:1.6803415517263793\n",
      "=== epoch:175, train acc:0.51, test acc:0.4135 ===\n",
      "train loss:1.6463473953393848\n",
      "train loss:1.6601417675553005\n",
      "train loss:1.6459535794024083\n",
      "=== epoch:176, train acc:0.5066666666666667, test acc:0.4128 ===\n",
      "train loss:1.6513684810353513\n",
      "train loss:1.6565694889033244\n",
      "train loss:1.7004266455045252\n",
      "=== epoch:177, train acc:0.5, test acc:0.4043 ===\n",
      "train loss:1.7885750586122566\n",
      "train loss:1.6948058780471513\n",
      "train loss:1.7022911048771165\n",
      "=== epoch:178, train acc:0.5066666666666667, test acc:0.4082 ===\n",
      "train loss:1.7449399057288637\n",
      "train loss:1.7573627564100578\n",
      "train loss:1.7207624841797218\n",
      "=== epoch:179, train acc:0.5133333333333333, test acc:0.4103 ===\n",
      "train loss:1.728061082595152\n",
      "train loss:1.6265581275089815\n",
      "train loss:1.895067449081089\n",
      "=== epoch:180, train acc:0.52, test acc:0.4183 ===\n",
      "train loss:1.7212731039738065\n",
      "train loss:1.7410113714630515\n",
      "train loss:1.672198971077219\n",
      "=== epoch:181, train acc:0.52, test acc:0.4253 ===\n",
      "train loss:1.633274777543332\n",
      "train loss:1.7469039941311564\n",
      "train loss:1.66009081584494\n",
      "=== epoch:182, train acc:0.52, test acc:0.4228 ===\n",
      "train loss:1.7108157930491514\n",
      "train loss:1.6513439802651515\n",
      "train loss:1.627058369011516\n",
      "=== epoch:183, train acc:0.5266666666666666, test acc:0.4265 ===\n",
      "train loss:1.6588117360842403\n",
      "train loss:1.6318423466056755\n",
      "train loss:1.651823106203929\n",
      "=== epoch:184, train acc:0.5266666666666666, test acc:0.4256 ===\n",
      "train loss:1.6886540686011253\n",
      "train loss:1.6608820463074452\n",
      "train loss:1.8152783332927214\n",
      "=== epoch:185, train acc:0.5366666666666666, test acc:0.4316 ===\n",
      "train loss:1.6553600578391985\n",
      "train loss:1.670202375466723\n",
      "train loss:1.6882550243804848\n",
      "=== epoch:186, train acc:0.5333333333333333, test acc:0.4261 ===\n",
      "train loss:1.5435881292800757\n",
      "train loss:1.600172375840504\n",
      "train loss:1.6048178759405003\n",
      "=== epoch:187, train acc:0.5366666666666666, test acc:0.4322 ===\n",
      "train loss:1.6123573120149675\n",
      "train loss:1.779184535770664\n",
      "train loss:1.5859840620768255\n",
      "=== epoch:188, train acc:0.54, test acc:0.4375 ===\n",
      "train loss:1.6031482480293986\n",
      "train loss:1.7013419764260826\n",
      "train loss:1.5710518694886424\n",
      "=== epoch:189, train acc:0.54, test acc:0.4452 ===\n",
      "train loss:1.697913377682859\n",
      "train loss:1.6542735189151083\n",
      "train loss:1.6690342424761813\n",
      "=== epoch:190, train acc:0.55, test acc:0.4547 ===\n",
      "train loss:1.7103243607845284\n",
      "train loss:1.6053286550751198\n",
      "train loss:1.7018739222624117\n",
      "=== epoch:191, train acc:0.5633333333333334, test acc:0.4661 ===\n",
      "train loss:1.6908418861582515\n",
      "train loss:1.5668069016563906\n",
      "train loss:1.7041989504992885\n",
      "=== epoch:192, train acc:0.5533333333333333, test acc:0.4611 ===\n",
      "train loss:1.572556481691323\n",
      "train loss:1.6017377465503366\n",
      "train loss:1.6131769918664083\n",
      "=== epoch:193, train acc:0.55, test acc:0.4531 ===\n",
      "train loss:1.6334108305950439\n",
      "train loss:1.7312510513105743\n",
      "train loss:1.6171383631742808\n",
      "=== epoch:194, train acc:0.5566666666666666, test acc:0.4557 ===\n",
      "train loss:1.6958425268490047\n",
      "train loss:1.646137378904662\n",
      "train loss:1.6905601021589234\n",
      "=== epoch:195, train acc:0.5666666666666667, test acc:0.4556 ===\n",
      "train loss:1.6783377891489872\n",
      "train loss:1.3735357558849386\n",
      "train loss:1.5700788841433875\n",
      "=== epoch:196, train acc:0.5633333333333334, test acc:0.4445 ===\n",
      "train loss:1.5968415851096287\n",
      "train loss:1.6108452719751645\n",
      "train loss:1.6774666850156046\n",
      "=== epoch:197, train acc:0.5666666666666667, test acc:0.449 ===\n",
      "train loss:1.545711423785111\n",
      "train loss:1.6516713286557194\n",
      "train loss:1.5897976224783161\n",
      "=== epoch:198, train acc:0.5633333333333334, test acc:0.4585 ===\n",
      "train loss:1.4908495197062321\n",
      "train loss:1.6255186123633687\n",
      "train loss:1.5189291905773328\n",
      "=== epoch:199, train acc:0.5666666666666667, test acc:0.4543 ===\n",
      "train loss:1.5846447442214135\n",
      "train loss:1.5499365188977672\n",
      "train loss:1.6388732341484897\n",
      "=== epoch:200, train acc:0.5833333333333334, test acc:0.4594 ===\n",
      "train loss:1.5906697827118854\n",
      "train loss:1.583361070953175\n",
      "train loss:1.5954371192323966\n",
      "=== epoch:201, train acc:0.5733333333333334, test acc:0.4592 ===\n",
      "train loss:1.5800617855854282\n",
      "train loss:1.502624936585879\n",
      "train loss:1.6128553626942255\n",
      "=== epoch:202, train acc:0.5666666666666667, test acc:0.4557 ===\n",
      "train loss:1.7682355550031426\n",
      "train loss:1.6151387111422815\n",
      "train loss:1.670391016567965\n",
      "=== epoch:203, train acc:0.5766666666666667, test acc:0.4687 ===\n",
      "train loss:1.5876245695424733\n",
      "train loss:1.5832637138727594\n",
      "train loss:1.4751562958621642\n",
      "=== epoch:204, train acc:0.5833333333333334, test acc:0.4654 ===\n",
      "train loss:1.6128815766848392\n",
      "train loss:1.5442589097802966\n",
      "train loss:1.5410909625791496\n",
      "=== epoch:205, train acc:0.5766666666666667, test acc:0.4685 ===\n",
      "train loss:1.5828265369393295\n",
      "train loss:1.5931400432971246\n",
      "train loss:1.5410243357475337\n",
      "=== epoch:206, train acc:0.5833333333333334, test acc:0.4741 ===\n",
      "train loss:1.4866106364539933\n",
      "train loss:1.614178459908533\n",
      "train loss:1.4148664943528095\n",
      "=== epoch:207, train acc:0.5733333333333334, test acc:0.4689 ===\n",
      "train loss:1.577454706944396\n",
      "train loss:1.4978435432932744\n",
      "train loss:1.5136877863358678\n",
      "=== epoch:208, train acc:0.5866666666666667, test acc:0.4784 ===\n",
      "train loss:1.487739734404872\n",
      "train loss:1.5776932521227294\n",
      "train loss:1.512586857717152\n",
      "=== epoch:209, train acc:0.5933333333333334, test acc:0.4855 ===\n",
      "train loss:1.5169310493484178\n",
      "train loss:1.5277980631149939\n",
      "train loss:1.6187765204709095\n",
      "=== epoch:210, train acc:0.5966666666666667, test acc:0.4906 ===\n",
      "train loss:1.5020576360283702\n",
      "train loss:1.5055179082112848\n",
      "train loss:1.4889944196932385\n",
      "=== epoch:211, train acc:0.6, test acc:0.4946 ===\n",
      "train loss:1.559568817926036\n",
      "train loss:1.5236302885684248\n",
      "train loss:1.6103392816506539\n",
      "=== epoch:212, train acc:0.6066666666666667, test acc:0.5045 ===\n",
      "train loss:1.5987886945607324\n",
      "train loss:1.402060471945083\n",
      "train loss:1.4988398015177904\n",
      "=== epoch:213, train acc:0.6033333333333334, test acc:0.4994 ===\n",
      "train loss:1.5257040347889097\n",
      "train loss:1.5261659606276663\n",
      "train loss:1.4630935777105691\n",
      "=== epoch:214, train acc:0.6333333333333333, test acc:0.5096 ===\n",
      "train loss:1.5252659015292793\n",
      "train loss:1.4587649390032085\n",
      "train loss:1.5193878331394968\n",
      "=== epoch:215, train acc:0.6233333333333333, test acc:0.509 ===\n",
      "train loss:1.3990816237675219\n",
      "train loss:1.4651395501215077\n",
      "train loss:1.4637347328337333\n",
      "=== epoch:216, train acc:0.6033333333333334, test acc:0.499 ===\n",
      "train loss:1.5571961696745675\n",
      "train loss:1.652004607286775\n",
      "train loss:1.5693198266876038\n",
      "=== epoch:217, train acc:0.6, test acc:0.4988 ===\n",
      "train loss:1.4688921219544289\n",
      "train loss:1.356162021930781\n",
      "train loss:1.4376308007856757\n",
      "=== epoch:218, train acc:0.6, test acc:0.5063 ===\n",
      "train loss:1.5265782696171806\n",
      "train loss:1.485744226571118\n",
      "train loss:1.4967517975684734\n",
      "=== epoch:219, train acc:0.62, test acc:0.515 ===\n",
      "train loss:1.5647712902069353\n",
      "train loss:1.4996076730035222\n",
      "train loss:1.4507253414828143\n",
      "=== epoch:220, train acc:0.6333333333333333, test acc:0.5208 ===\n",
      "train loss:1.4431922653419142\n",
      "train loss:1.4887681319832964\n",
      "train loss:1.6868678556032106\n",
      "=== epoch:221, train acc:0.6433333333333333, test acc:0.5263 ===\n",
      "train loss:1.4802123914970158\n",
      "train loss:1.4946906336599588\n",
      "train loss:1.5339366728711992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:222, train acc:0.6433333333333333, test acc:0.5277 ===\n",
      "train loss:1.5380647020246472\n",
      "train loss:1.400341950382235\n",
      "train loss:1.533208436886318\n",
      "=== epoch:223, train acc:0.6466666666666666, test acc:0.532 ===\n",
      "train loss:1.4232547784008918\n",
      "train loss:1.4748780553011336\n",
      "train loss:1.4023221647896127\n",
      "=== epoch:224, train acc:0.6433333333333333, test acc:0.533 ===\n",
      "train loss:1.3799752553968907\n",
      "train loss:1.4567300732359973\n",
      "train loss:1.5772147188407268\n",
      "=== epoch:225, train acc:0.6566666666666666, test acc:0.5387 ===\n",
      "train loss:1.4331136141505276\n",
      "train loss:1.4166557473189048\n",
      "train loss:1.5039809616436977\n",
      "=== epoch:226, train acc:0.6566666666666666, test acc:0.5423 ===\n",
      "train loss:1.3030135810534227\n",
      "train loss:1.4610436092047514\n",
      "train loss:1.5789000077977144\n",
      "=== epoch:227, train acc:0.6666666666666666, test acc:0.5392 ===\n",
      "train loss:1.3922717277730774\n",
      "train loss:1.4370323964299425\n",
      "train loss:1.514079688694842\n",
      "=== epoch:228, train acc:0.6666666666666666, test acc:0.5418 ===\n",
      "train loss:1.3900217078077355\n",
      "train loss:1.3864344478431903\n",
      "train loss:1.3783276434525014\n",
      "=== epoch:229, train acc:0.6633333333333333, test acc:0.54 ===\n",
      "train loss:1.4852331208905216\n",
      "train loss:1.3646834666463092\n",
      "train loss:1.4629475650696575\n",
      "=== epoch:230, train acc:0.65, test acc:0.5388 ===\n",
      "train loss:1.3206725194227935\n",
      "train loss:1.309212637502522\n",
      "train loss:1.399032992586607\n",
      "=== epoch:231, train acc:0.6533333333333333, test acc:0.5376 ===\n",
      "train loss:1.3643738321475807\n",
      "train loss:1.362123344104178\n",
      "train loss:1.32338510220663\n",
      "=== epoch:232, train acc:0.65, test acc:0.5351 ===\n",
      "train loss:1.530207830639718\n",
      "train loss:1.4794738349357217\n",
      "train loss:1.329325317100581\n",
      "=== epoch:233, train acc:0.65, test acc:0.5362 ===\n",
      "train loss:1.315527346251455\n",
      "train loss:1.3090406128353549\n",
      "train loss:1.33831007328834\n",
      "=== epoch:234, train acc:0.65, test acc:0.5359 ===\n",
      "train loss:1.4083582944608641\n",
      "train loss:1.371124559604872\n",
      "train loss:1.3427532793913295\n",
      "=== epoch:235, train acc:0.6366666666666667, test acc:0.5314 ===\n",
      "train loss:1.4028702776689852\n",
      "train loss:1.2391673542526322\n",
      "train loss:1.35150189518465\n",
      "=== epoch:236, train acc:0.6366666666666667, test acc:0.5294 ===\n",
      "train loss:1.4259576955213333\n",
      "train loss:1.4465522215046838\n",
      "train loss:1.3569054909452678\n",
      "=== epoch:237, train acc:0.63, test acc:0.5305 ===\n",
      "train loss:1.465992776706986\n",
      "train loss:1.4563257618242338\n",
      "train loss:1.1907380316083396\n",
      "=== epoch:238, train acc:0.6333333333333333, test acc:0.526 ===\n",
      "train loss:1.417181203359042\n",
      "train loss:1.4077949272672061\n",
      "train loss:1.2818899101447923\n",
      "=== epoch:239, train acc:0.64, test acc:0.537 ===\n",
      "train loss:1.349377832328386\n",
      "train loss:1.4125551507958114\n",
      "train loss:1.4519469858934757\n",
      "=== epoch:240, train acc:0.6466666666666666, test acc:0.5402 ===\n",
      "train loss:1.311013543933462\n",
      "train loss:1.3196162180775988\n",
      "train loss:1.3516668555261848\n",
      "=== epoch:241, train acc:0.6466666666666666, test acc:0.5401 ===\n",
      "train loss:1.3053122120720524\n",
      "train loss:1.3651985472787573\n",
      "train loss:1.3095034459071724\n",
      "=== epoch:242, train acc:0.66, test acc:0.5482 ===\n",
      "train loss:1.3633547315335515\n",
      "train loss:1.2865169379231478\n",
      "train loss:1.409307864497\n",
      "=== epoch:243, train acc:0.6633333333333333, test acc:0.5474 ===\n",
      "train loss:1.4379972100534075\n",
      "train loss:1.3058182951924961\n",
      "train loss:1.359409700147073\n",
      "=== epoch:244, train acc:0.6466666666666666, test acc:0.5414 ===\n",
      "train loss:1.25514301682557\n",
      "train loss:1.4322347812587097\n",
      "train loss:1.4774382029492674\n",
      "=== epoch:245, train acc:0.64, test acc:0.5236 ===\n",
      "train loss:1.1378342284609038\n",
      "train loss:1.3421692328576704\n",
      "train loss:1.3870539750903859\n",
      "=== epoch:246, train acc:0.65, test acc:0.5329 ===\n",
      "train loss:1.2474626089385303\n",
      "train loss:1.3183571953326199\n",
      "train loss:1.297711449415875\n",
      "=== epoch:247, train acc:0.6466666666666666, test acc:0.5359 ===\n",
      "train loss:1.3703800236318335\n",
      "train loss:1.3030806760592322\n",
      "train loss:1.2648582141085531\n",
      "=== epoch:248, train acc:0.6533333333333333, test acc:0.5347 ===\n",
      "train loss:1.366087520417205\n",
      "train loss:1.3680789327443472\n",
      "train loss:1.265818308716012\n",
      "=== epoch:249, train acc:0.66, test acc:0.5373 ===\n",
      "train loss:1.2389359839329186\n",
      "train loss:1.3322648828232122\n",
      "train loss:1.1876031008381256\n",
      "=== epoch:250, train acc:0.66, test acc:0.5385 ===\n",
      "train loss:1.3325547649040905\n",
      "train loss:1.260858880678106\n",
      "train loss:1.3068812195117234\n",
      "=== epoch:251, train acc:0.67, test acc:0.546 ===\n",
      "train loss:1.3094987172577441\n",
      "train loss:1.295742964263114\n",
      "train loss:1.2941453335706663\n",
      "=== epoch:252, train acc:0.6666666666666666, test acc:0.5433 ===\n",
      "train loss:1.3614006105540153\n",
      "train loss:1.3163167368868491\n",
      "train loss:1.2734577257525292\n",
      "=== epoch:253, train acc:0.6666666666666666, test acc:0.5423 ===\n",
      "train loss:1.2501102830456348\n",
      "train loss:1.3816715037928093\n",
      "train loss:1.354597308942035\n",
      "=== epoch:254, train acc:0.68, test acc:0.5506 ===\n",
      "train loss:1.2825952832925835\n",
      "train loss:1.2804038452830475\n",
      "train loss:1.3421256372765675\n",
      "=== epoch:255, train acc:0.68, test acc:0.5469 ===\n",
      "train loss:1.355575800362192\n",
      "train loss:1.223262607461729\n",
      "train loss:1.4130317415218574\n",
      "=== epoch:256, train acc:0.68, test acc:0.5512 ===\n",
      "train loss:1.468145869515715\n",
      "train loss:1.2099771478570016\n",
      "train loss:1.2945414065102923\n",
      "=== epoch:257, train acc:0.6866666666666666, test acc:0.5527 ===\n",
      "train loss:1.2978607846614816\n",
      "train loss:1.3582044880328639\n",
      "train loss:1.199375957333415\n",
      "=== epoch:258, train acc:0.6833333333333333, test acc:0.5518 ===\n",
      "train loss:1.3693274751940345\n",
      "train loss:1.3547674584910243\n",
      "train loss:1.3263885896077126\n",
      "=== epoch:259, train acc:0.67, test acc:0.5435 ===\n",
      "train loss:1.2730686279208734\n",
      "train loss:1.2044560222713017\n",
      "train loss:1.2780887921115478\n",
      "=== epoch:260, train acc:0.67, test acc:0.5422 ===\n",
      "train loss:1.3040054071879859\n",
      "train loss:1.2338388893621832\n",
      "train loss:1.2839181077451274\n",
      "=== epoch:261, train acc:0.68, test acc:0.5453 ===\n",
      "train loss:1.3332691014974767\n",
      "train loss:1.3053290273416536\n",
      "train loss:1.128828158396603\n",
      "=== epoch:262, train acc:0.6666666666666666, test acc:0.5434 ===\n",
      "train loss:1.13680795837081\n",
      "train loss:1.1222677046465965\n",
      "train loss:1.282097704856654\n",
      "=== epoch:263, train acc:0.6766666666666666, test acc:0.5371 ===\n",
      "train loss:1.2270631393057574\n",
      "train loss:1.2302777346393914\n",
      "train loss:1.1500059684474153\n",
      "=== epoch:264, train acc:0.68, test acc:0.5417 ===\n",
      "train loss:1.1163804291335468\n",
      "train loss:1.2874405162963898\n",
      "train loss:1.283411349115061\n",
      "=== epoch:265, train acc:0.69, test acc:0.5581 ===\n",
      "train loss:1.2686419025768623\n",
      "train loss:1.2887267503129263\n",
      "train loss:1.1028830160122205\n",
      "=== epoch:266, train acc:0.68, test acc:0.5491 ===\n",
      "train loss:1.094701427425939\n",
      "train loss:1.2241214534854128\n",
      "train loss:1.2642458898209874\n",
      "=== epoch:267, train acc:0.68, test acc:0.5529 ===\n",
      "train loss:1.0890623158756867\n",
      "train loss:1.2667601704561298\n",
      "train loss:1.289827226828711\n",
      "=== epoch:268, train acc:0.6866666666666666, test acc:0.5544 ===\n",
      "train loss:1.3442203300157882\n",
      "train loss:1.222674120812925\n",
      "train loss:1.1685615692137583\n",
      "=== epoch:269, train acc:0.67, test acc:0.5522 ===\n",
      "train loss:1.182680854974533\n",
      "train loss:1.208446442160617\n",
      "train loss:1.2741807303937878\n",
      "=== epoch:270, train acc:0.68, test acc:0.552 ===\n",
      "train loss:1.1602129802242571\n",
      "train loss:1.1225506714226745\n",
      "train loss:1.1030406183966535\n",
      "=== epoch:271, train acc:0.68, test acc:0.5489 ===\n",
      "train loss:1.2726686240638496\n",
      "train loss:1.2830200564782503\n",
      "train loss:1.2621660338100553\n",
      "=== epoch:272, train acc:0.6866666666666666, test acc:0.5542 ===\n",
      "train loss:1.2053547675914316\n",
      "train loss:1.0643416547826743\n",
      "train loss:1.0468230453353153\n",
      "=== epoch:273, train acc:0.6933333333333334, test acc:0.5619 ===\n",
      "train loss:1.2150365434640038\n",
      "train loss:1.1999740255730638\n",
      "train loss:1.1008629795747529\n",
      "=== epoch:274, train acc:0.6866666666666666, test acc:0.5604 ===\n",
      "train loss:1.156597998470434\n",
      "train loss:1.182417260092479\n",
      "train loss:1.261743266855795\n",
      "=== epoch:275, train acc:0.6966666666666667, test acc:0.5657 ===\n",
      "train loss:1.1752461486753982\n",
      "train loss:1.202719808479431\n",
      "train loss:1.248138981714367\n",
      "=== epoch:276, train acc:0.7033333333333334, test acc:0.5756 ===\n",
      "train loss:1.1960759431887182\n",
      "train loss:1.2429153902964918\n",
      "train loss:1.131613678330834\n",
      "=== epoch:277, train acc:0.7166666666666667, test acc:0.5807 ===\n",
      "train loss:1.1367833024770793\n",
      "train loss:1.2376291178236938\n",
      "train loss:1.1007284309590304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:278, train acc:0.7166666666666667, test acc:0.5861 ===\n",
      "train loss:1.034892274876656\n",
      "train loss:1.2012139688269714\n",
      "train loss:1.1908553303623612\n",
      "=== epoch:279, train acc:0.71, test acc:0.5836 ===\n",
      "train loss:1.0705609358409642\n",
      "train loss:1.1174578037041674\n",
      "train loss:0.9916876578662871\n",
      "=== epoch:280, train acc:0.7166666666666667, test acc:0.5821 ===\n",
      "train loss:1.072309563183403\n",
      "train loss:1.064894664376811\n",
      "train loss:1.0749785633157998\n",
      "=== epoch:281, train acc:0.7233333333333334, test acc:0.5821 ===\n",
      "train loss:0.9821193453393667\n",
      "train loss:1.0072907925804753\n",
      "train loss:1.1827501297008627\n",
      "=== epoch:282, train acc:0.7233333333333334, test acc:0.5832 ===\n",
      "train loss:1.0717750592694089\n",
      "train loss:1.1289747442065299\n",
      "train loss:1.125163475777574\n",
      "=== epoch:283, train acc:0.7166666666666667, test acc:0.581 ===\n",
      "train loss:1.0501145471987794\n",
      "train loss:1.0941091239737277\n",
      "train loss:1.267317798043375\n",
      "=== epoch:284, train acc:0.7233333333333334, test acc:0.583 ===\n",
      "train loss:1.0292442174378647\n",
      "train loss:1.1414629720517617\n",
      "train loss:1.2519986279295943\n",
      "=== epoch:285, train acc:0.7233333333333334, test acc:0.5828 ===\n",
      "train loss:1.1335706894862916\n",
      "train loss:1.1330981199973384\n",
      "train loss:1.1912178760029644\n",
      "=== epoch:286, train acc:0.7233333333333334, test acc:0.5902 ===\n",
      "train loss:1.0980912176001985\n",
      "train loss:1.204768379245003\n",
      "train loss:1.2089570902361158\n",
      "=== epoch:287, train acc:0.7233333333333334, test acc:0.5922 ===\n",
      "train loss:1.093123312148078\n",
      "train loss:1.2232316120041906\n",
      "train loss:1.0489249495370614\n",
      "=== epoch:288, train acc:0.72, test acc:0.5909 ===\n",
      "train loss:1.1445460513460062\n",
      "train loss:0.9479655966870497\n",
      "train loss:1.139614370833848\n",
      "=== epoch:289, train acc:0.7166666666666667, test acc:0.5912 ===\n",
      "train loss:1.1779962645598783\n",
      "train loss:1.0436610302263256\n",
      "train loss:1.0044158693346972\n",
      "=== epoch:290, train acc:0.72, test acc:0.5904 ===\n",
      "train loss:1.0876305484108117\n",
      "train loss:1.0304992358130807\n",
      "train loss:1.0303747134326406\n",
      "=== epoch:291, train acc:0.7133333333333334, test acc:0.5876 ===\n",
      "train loss:1.1673004226750185\n",
      "train loss:1.1480088322015078\n",
      "train loss:1.0423310145373563\n",
      "=== epoch:292, train acc:0.7233333333333334, test acc:0.5906 ===\n",
      "train loss:1.0398067214741695\n",
      "train loss:1.2068357002251502\n",
      "train loss:0.9386508212307123\n",
      "=== epoch:293, train acc:0.7233333333333334, test acc:0.5918 ===\n",
      "train loss:1.045921206898855\n",
      "train loss:1.1357537860848057\n",
      "train loss:0.9189130982383437\n",
      "=== epoch:294, train acc:0.73, test acc:0.5924 ===\n",
      "train loss:1.096560721726916\n",
      "train loss:1.0176918368925798\n",
      "train loss:1.0662545648590926\n",
      "=== epoch:295, train acc:0.73, test acc:0.5939 ===\n",
      "train loss:1.0390859093687863\n",
      "train loss:1.0352448940490298\n",
      "train loss:1.0567864309555874\n",
      "=== epoch:296, train acc:0.7333333333333333, test acc:0.5933 ===\n",
      "train loss:1.0338520808553053\n",
      "train loss:1.0198681139857733\n",
      "train loss:0.996987516499631\n",
      "=== epoch:297, train acc:0.7333333333333333, test acc:0.5924 ===\n",
      "train loss:1.0840682729733482\n",
      "train loss:1.0225903605119944\n",
      "train loss:0.9775215378352899\n",
      "=== epoch:298, train acc:0.7366666666666667, test acc:0.5964 ===\n",
      "train loss:1.1568099937857959\n",
      "train loss:1.063741773502518\n",
      "train loss:0.9435077283597694\n",
      "=== epoch:299, train acc:0.7366666666666667, test acc:0.5988 ===\n",
      "train loss:1.0209314403317256\n",
      "train loss:0.9472840778299327\n",
      "train loss:0.9894325711762652\n",
      "=== epoch:300, train acc:0.7333333333333333, test acc:0.5999 ===\n",
      "train loss:1.0168989595650872\n",
      "train loss:1.0486453795135215\n",
      "train loss:0.9745136399803471\n",
      "=== epoch:301, train acc:0.7366666666666667, test acc:0.6006 ===\n",
      "train loss:1.1011092243147909\n",
      "train loss:1.1177945625154506\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.6024\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.pardir)  # 为了导入父目录的文件而进行的设定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.multi_layer_net_extend import MultiLayerNetExtend\n",
    "from common.trainer import Trainer\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 为了再现过拟合，减少学习数据\n",
    "x_train = x_train[:300]\n",
    "t_train = t_train[:300]\n",
    "\n",
    "# 设定是否使用Dropuout，以及比例 ========================\n",
    "use_dropout = True  # 不使用Dropout的情况下为False\n",
    "dropout_ratio = 0.2\n",
    "# ====================================================\n",
    "\n",
    "network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
    "                              output_size=10, use_dropout=use_dropout, dropout_ration=dropout_ratio)\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=301, mini_batch_size=100,\n",
    "                  optimizer='sgd', optimizer_param={'lr': 0.01}, verbose=True)\n",
    "trainer.train()\n",
    "\n",
    "train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list\n",
    "\n",
    "# 绘制图形==========\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.5 超参数验证\n",
    "# 超参数对模型性能影响很大 \n",
    "# 不能使用测试数据评估超参数的性能\n",
    "\n",
    "# 训练数据 验证数据 测试数据\n",
    "\n",
    "# 超参数的最优化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第七章 卷积神经网络 CNN\n",
    "# 卷积层\n",
    "# 池化层\n",
    "# 全连接层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.2 卷积层\n",
    "# 全连接层存在的问题\n",
    "# 全连接层忽视了数据的形状信息\n",
    "# 卷积层可以以相同的形状输出数据\n",
    "# 特征图：输入特征图 输出特征图\n",
    "\n",
    "# 卷积运算\n",
    "# 对应相乘再相加\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 填充\n",
    "# 向输入数据周围填入固定的数据\n",
    "\n",
    "# 步幅\n",
    "# \n",
    "\n",
    "# 增大步幅 输出大小变小\n",
    "# 增大填充 输出大小会变大\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.3 池化层\n",
    "# 池化 缩小高、长方向上的空间运算\n",
    "\n",
    "# 一般池化窗口的大小会和步幅设定为相同的值\n",
    "\n",
    "# Max池化 Average池化\n",
    "\n",
    "# 池化层的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.4 卷积层和池化层的实现\n",
    "# CNN中批处理\n",
    "# im2col 函数 将输入的3(4)维数据展开成 2维大矩阵 \n",
    "# 充分利用矩阵计算的库 高速进行大矩阵运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from common.util import im2col\n",
    "import numpy as np\n",
    "\n",
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H + 2 * self.pad - FH) / self.stride)\n",
    "        out_w = int(1 + (W + 2 * self.pad - FW) / self.stride)\n",
    "        \n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        col_W = self.W.reshape(FN, -1)# 滤波器展开\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "        \n",
    "        out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "        #缺省值-1代表我不知道要给行（或者列）设置为几，reshape函数会根据原矩阵的形状自动调整。\n",
    "        # transpose()  转置\n",
    "        \n",
    "        self.x = x\n",
    "        self.col = col\n",
    "        self.col_W = col_W\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
    "\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dout)\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
    "\n",
    "        dcol = np.dot(dout, self.col_W.T)\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0  1  2  3]\n",
      "  [ 4  5  6  7]]\n",
      "\n",
      " [[ 8  9 10 11]\n",
      "  [12 13 14 15]]]\n",
      "[[[ 0  1  2  3]\n",
      "  [ 8  9 10 11]]\n",
      "\n",
      " [[ 4  5  6  7]\n",
      "  [12 13 14 15]]]\n"
     ]
    }
   ],
   "source": [
    "# 实验transpose\n",
    "A = np.arange(16)\n",
    "A = A.reshape(2, 2, 4)\n",
    "B = A.copy()\n",
    "#print(A)\n",
    "B = B.transpose(1, 0, 2)\n",
    "print(A)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 池化层实现\n",
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "        \n",
    "        # 展开\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h * self.pool_w)\n",
    "        \n",
    "        # 最大值\n",
    "        out = np.max(col, axis=1)# 按行取最大值\n",
    "        \n",
    "        # 转换\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "        \n",
    "        self.x = x\n",
    "        self.arg_max = arg_max\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout = dout.transpose(0, 2, 3, 1)\n",
    "        \n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "        dmax = np.zeros((dout.size, pool_size))\n",
    "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
    "        dmax = dmax.reshape(dout.shape + (pool_size,)) \n",
    "        \n",
    "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
    "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.5 CNN的实现\n",
    "# 手写体数字识别的CNN网络结构\n",
    "# Conv->ReLU->Pooling->Affine->ReLU->Affine->Softmax\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 为了导入父目录的文件而进行的设定\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class SimpleConvNet:\n",
    "    \"\"\"简单的ConvNet\n",
    "\n",
    "    conv - relu - pool - affine - relu - affine - softmax\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : 输入大小（MNIST的情况下为784）\n",
    "    hidden_size_list : 隐藏层的神经元数量的列表（e.g. [100, 100, 100]）\n",
    "    output_size : 输出大小（MNIST的情况下为10）\n",
    "    activation : 'relu' or 'sigmoid'\n",
    "    weight_init_std : 指定权重的标准差（e.g. 0.01）\n",
    "        指定'relu'或'he'的情况下设定“He的初始值”\n",
    "        指定'sigmoid'或'xavier'的情况下设定“Xavier的初始值”\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "\n",
    "        # 初始化权重\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        # 生成层\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"求损失函数\n",
    "        参数x是输入数据、t是教师标签\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        \"\"\"求梯度（数值微分）\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 输入数据\n",
    "        t : 教师标签\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        具有各层的梯度的字典变量\n",
    "            grads['W1']、grads['W2']、...是各层的权重\n",
    "            grads['b1']、grads['b2']、...是各层的偏置\n",
    "        \"\"\"\n",
    "        loss_w = lambda w: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in (1, 2, 3):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"求梯度（误差反向传播法）\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 输入数据\n",
    "        t : 教师标签\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        具有各层的梯度的字典变量\n",
    "            grads['W1']、grads['W2']、...是各层的权重\n",
    "            grads['b1']、grads['b2']、...是各层的偏置\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 设定\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "        \n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
    "            self.layers[key].W = self.params['W' + str(i+1)]\n",
    "            self.layers[key].b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2996154112583955\n",
      "=== epoch:1, train acc:0.174, test acc:0.172 ===\n",
      "train loss:2.296484294477489\n",
      "train loss:2.2940575215836954\n",
      "train loss:2.287852870132015\n",
      "train loss:2.276465596549983\n",
      "train loss:2.2623185007432665\n",
      "train loss:2.252986002897277\n",
      "train loss:2.232129582945995\n",
      "train loss:2.2223683359783304\n",
      "train loss:2.2108491320715946\n",
      "train loss:2.1761499285664208\n",
      "train loss:2.1088151893267493\n",
      "train loss:2.0517306847312047\n",
      "train loss:1.9965128538297696\n",
      "train loss:1.9997613052651024\n",
      "train loss:1.934678590305138\n",
      "train loss:1.7959173379785767\n",
      "train loss:1.7521118977800463\n",
      "train loss:1.6635297325049456\n",
      "train loss:1.6740230726940744\n",
      "train loss:1.5547034353647309\n",
      "train loss:1.4939300136716545\n",
      "train loss:1.490622045327292\n",
      "train loss:1.382451184011468\n",
      "train loss:1.2054067115522142\n",
      "train loss:1.1704949343223603\n",
      "train loss:1.1322309325628368\n",
      "train loss:1.0801788304983122\n",
      "train loss:0.9496420639500209\n",
      "train loss:0.999866817631817\n",
      "train loss:0.9987685114409117\n",
      "train loss:0.8766914825256349\n",
      "train loss:0.6882623426329907\n",
      "train loss:0.7834240244707259\n",
      "train loss:0.7854436457575871\n",
      "train loss:0.6288803944056897\n",
      "train loss:0.7562317717693007\n",
      "train loss:0.8333389004911183\n",
      "train loss:0.7050754224265573\n",
      "train loss:0.5612508973504351\n",
      "train loss:0.6644645079476547\n",
      "train loss:0.6897524016108307\n",
      "train loss:0.5493353367293603\n",
      "train loss:0.6859616868034734\n",
      "train loss:0.7453219445309304\n",
      "train loss:0.7132619162611377\n",
      "train loss:0.449539637138051\n",
      "train loss:0.5785655635572821\n",
      "train loss:0.5135567223352656\n",
      "train loss:0.6203382819215217\n",
      "train loss:0.5488944139079518\n",
      "train loss:0.6166316202333572\n",
      "train loss:0.704641085962329\n",
      "train loss:0.5069117381379941\n",
      "train loss:0.42983862750195373\n",
      "train loss:0.476216248293303\n",
      "train loss:0.5144633448535125\n",
      "train loss:0.7546153847253335\n",
      "train loss:0.5075680052019892\n",
      "train loss:0.406538699237432\n",
      "train loss:0.6300457947201215\n",
      "train loss:0.5368729336686145\n",
      "train loss:0.5436135097527761\n",
      "train loss:0.5750727022650248\n",
      "train loss:0.6149128872873271\n",
      "train loss:0.4539237895259657\n",
      "train loss:0.35137020376650463\n",
      "train loss:0.5153628549693559\n",
      "train loss:0.597333029125934\n",
      "train loss:0.5028844021929175\n",
      "train loss:0.6690347107162278\n",
      "train loss:0.49395200095444836\n",
      "train loss:0.3258518559792698\n",
      "train loss:0.46882789552571125\n",
      "train loss:0.4878113576856996\n",
      "train loss:0.5218296977505189\n",
      "train loss:0.37158933841285846\n",
      "train loss:0.5123014635498194\n",
      "train loss:0.45152390059669456\n",
      "train loss:0.4586599779892548\n",
      "train loss:0.3426873766416639\n",
      "train loss:0.5213518097316705\n",
      "train loss:0.414995383469266\n",
      "train loss:0.42526938262320807\n",
      "train loss:0.36707225856147985\n",
      "train loss:0.37409455623864624\n",
      "train loss:0.4026065087291738\n",
      "train loss:0.4158407213200735\n",
      "train loss:0.39732856026017116\n",
      "train loss:0.3969697537317613\n",
      "train loss:0.40148292383927425\n",
      "train loss:0.3962451576123059\n",
      "train loss:0.44399628276890657\n",
      "train loss:0.30714773553437924\n",
      "train loss:0.4688940412508834\n",
      "train loss:0.45213916349101296\n",
      "train loss:0.3139299840972449\n",
      "train loss:0.2825733823307334\n",
      "train loss:0.34707826274232645\n",
      "train loss:0.36466063014281774\n",
      "train loss:0.5400389217520513\n",
      "train loss:0.44058226157955666\n",
      "train loss:0.2995414268546931\n",
      "train loss:0.31079020782622035\n",
      "train loss:0.34924869078751786\n",
      "train loss:0.4147265613184466\n",
      "train loss:0.33429188516090724\n",
      "train loss:0.5777217546873643\n",
      "train loss:0.4489916911431587\n",
      "train loss:0.3665321526517632\n",
      "train loss:0.3937669388483527\n",
      "train loss:0.530802022599473\n",
      "train loss:0.31910379568123315\n",
      "train loss:0.3913142544245067\n",
      "train loss:0.4232237311595732\n",
      "train loss:0.4632814510492141\n",
      "train loss:0.4520118945364113\n",
      "train loss:0.5810923655608558\n",
      "train loss:0.5079908177169447\n",
      "train loss:0.3387216333151859\n",
      "train loss:0.2707590335330735\n",
      "train loss:0.3032759932746276\n",
      "train loss:0.44349296441324215\n",
      "train loss:0.31085033021761743\n",
      "train loss:0.2704185778402139\n",
      "train loss:0.3974364289286999\n",
      "train loss:0.3020595453074825\n",
      "train loss:0.37847227837678077\n",
      "train loss:0.15449474084416243\n",
      "train loss:0.44360877859061865\n",
      "train loss:0.34716605514116045\n",
      "train loss:0.25939760734610573\n",
      "train loss:0.3810045109687074\n",
      "train loss:0.21452283106705863\n",
      "train loss:0.2612498304829732\n",
      "train loss:0.4472309987392641\n",
      "train loss:0.3499699779665308\n",
      "train loss:0.3133363040018781\n",
      "train loss:0.3932719433586886\n",
      "train loss:0.42039820649367265\n",
      "train loss:0.4264197286509497\n",
      "train loss:0.31929209131977127\n",
      "train loss:0.3602261104030615\n",
      "train loss:0.3642967155518213\n",
      "train loss:0.36885820987104756\n",
      "train loss:0.20749363494490086\n",
      "train loss:0.36266335863180615\n",
      "train loss:0.38656703969856926\n",
      "train loss:0.5022156171575392\n",
      "train loss:0.37752171774438814\n",
      "train loss:0.2958491952353229\n",
      "train loss:0.2860296837467179\n",
      "train loss:0.5092469463656186\n",
      "train loss:0.4439775673821961\n",
      "train loss:0.3241502571341253\n",
      "train loss:0.4185669766306597\n",
      "train loss:0.2865360835949346\n",
      "train loss:0.2947115785578012\n",
      "train loss:0.3927708693341722\n",
      "train loss:0.4472243703132681\n",
      "train loss:0.30799041348952605\n",
      "train loss:0.6024778336062273\n",
      "train loss:0.287716396781525\n",
      "train loss:0.4404302313897956\n",
      "train loss:0.4564159305933547\n",
      "train loss:0.32774705568092605\n",
      "train loss:0.5151985702642583\n",
      "train loss:0.36676600836072587\n",
      "train loss:0.3822431639601661\n",
      "train loss:0.297087275413517\n",
      "train loss:0.23233471933784824\n",
      "train loss:0.26472616613066247\n",
      "train loss:0.4728696357191467\n",
      "train loss:0.4369338721657361\n",
      "train loss:0.3949153782058664\n",
      "train loss:0.2917651998615191\n",
      "train loss:0.2941498875588123\n",
      "train loss:0.23849243785600815\n",
      "train loss:0.332038378667431\n",
      "train loss:0.16916668613078212\n",
      "train loss:0.19618167488124577\n",
      "train loss:0.3656149692402416\n",
      "train loss:0.33584662669768833\n",
      "train loss:0.3641861018545769\n",
      "train loss:0.2964496254861439\n",
      "train loss:0.25761203362289603\n",
      "train loss:0.19428822546785945\n",
      "train loss:0.3925838638331774\n",
      "train loss:0.23870450682932592\n",
      "train loss:0.32621252258610584\n",
      "train loss:0.3547613975065088\n",
      "train loss:0.4092986641260686\n",
      "train loss:0.23379834126949153\n",
      "train loss:0.4373291692470857\n",
      "train loss:0.4118145297310602\n",
      "train loss:0.24745594532396006\n",
      "train loss:0.2391792809615323\n",
      "train loss:0.2894712891219882\n",
      "train loss:0.26491843272086996\n",
      "train loss:0.3647549696672291\n",
      "train loss:0.34732094986030604\n",
      "train loss:0.2536967700953599\n",
      "train loss:0.21427002066319809\n",
      "train loss:0.24943129955777882\n",
      "train loss:0.2586378676740546\n",
      "train loss:0.4053313461378141\n",
      "train loss:0.39075796783371963\n",
      "train loss:0.42241939262342043\n",
      "train loss:0.20673263587359297\n",
      "train loss:0.17516628394202005\n",
      "train loss:0.3711487366302405\n",
      "train loss:0.5215183066118512\n",
      "train loss:0.30693935172911396\n",
      "train loss:0.19564661923918997\n",
      "train loss:0.3455883241689465\n",
      "train loss:0.31780170701747495\n",
      "train loss:0.32417043516758204\n",
      "train loss:0.31468360460284683\n",
      "train loss:0.22251584973428673\n",
      "train loss:0.3519897259095663\n",
      "train loss:0.19668316203051117\n",
      "train loss:0.31757530290602604\n",
      "train loss:0.24932769754322698\n",
      "train loss:0.28938292372258545\n",
      "train loss:0.1541648345044204\n",
      "train loss:0.29657072186651307\n",
      "train loss:0.3705143186441039\n",
      "train loss:0.24170526336686396\n",
      "train loss:0.3941194924038291\n",
      "train loss:0.44402651273128463\n",
      "train loss:0.18071258606670182\n",
      "train loss:0.1921993086948758\n",
      "train loss:0.2794757319938276\n",
      "train loss:0.21618148049062655\n",
      "train loss:0.11530083103371488\n",
      "train loss:0.32133385929818814\n",
      "train loss:0.1353743898837548\n",
      "train loss:0.37521557695307395\n",
      "train loss:0.2342314288619111\n",
      "train loss:0.35169926187073663\n",
      "train loss:0.23626476457098214\n",
      "train loss:0.3582910842194352\n",
      "train loss:0.3213531752462127\n",
      "train loss:0.16467119859191914\n",
      "train loss:0.14253106483200184\n",
      "train loss:0.356379380138595\n",
      "train loss:0.2574715246532689\n",
      "train loss:0.29849072205546423\n",
      "train loss:0.14909815036286653\n",
      "train loss:0.26297760634071454\n",
      "train loss:0.2176816778128496\n",
      "train loss:0.33527912083728467\n",
      "train loss:0.3507913825810494\n",
      "train loss:0.2949048894315748\n",
      "train loss:0.2416962291516418\n",
      "train loss:0.2403377743213536\n",
      "train loss:0.35034839752372515\n",
      "train loss:0.3787741519413597\n",
      "train loss:0.3873807956316112\n",
      "train loss:0.22776256910045778\n",
      "train loss:0.16379153623792267\n",
      "train loss:0.42012276347297406\n",
      "train loss:0.2085571467513147\n",
      "train loss:0.40482119151495555\n",
      "train loss:0.3133959984830245\n",
      "train loss:0.4856628574957703\n",
      "train loss:0.31442297001850866\n",
      "train loss:0.24319598491312483\n",
      "train loss:0.18112384069687085\n",
      "train loss:0.1858575355668416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.2140148003485378\n",
      "train loss:0.2077284740336534\n",
      "train loss:0.21536758192742525\n",
      "train loss:0.3207758892439251\n",
      "train loss:0.6599899425114559\n",
      "train loss:0.2208321010002476\n",
      "train loss:0.3035681411813301\n",
      "train loss:0.33323857922262884\n",
      "train loss:0.3963523415030011\n",
      "train loss:0.3274747127796543\n",
      "train loss:0.2447636108620042\n",
      "train loss:0.1854062375116761\n",
      "train loss:0.1814857393464445\n",
      "train loss:0.10031232770626344\n",
      "train loss:0.16715224403346213\n",
      "train loss:0.2024824981913583\n",
      "train loss:0.2797948665397395\n",
      "train loss:0.2238660560711726\n",
      "train loss:0.2292319104596026\n",
      "train loss:0.2073091076133042\n",
      "train loss:0.17218418958212836\n",
      "train loss:0.2407823806699752\n",
      "train loss:0.3154952449710362\n",
      "train loss:0.17377138505142678\n",
      "train loss:0.22794537087260397\n",
      "train loss:0.17107205386978552\n",
      "train loss:0.17580635836967906\n",
      "train loss:0.27712330832121984\n",
      "train loss:0.21058260258529685\n",
      "train loss:0.19603107241748163\n",
      "train loss:0.33450283602062236\n",
      "train loss:0.206040830906436\n",
      "train loss:0.2922048139767408\n",
      "train loss:0.29187867613503654\n",
      "train loss:0.16595025700306187\n",
      "train loss:0.14972493207848536\n",
      "train loss:0.1914300879661566\n",
      "train loss:0.1548823897097626\n",
      "train loss:0.3176361308234156\n",
      "train loss:0.12347704701904647\n",
      "train loss:0.2418969652056958\n",
      "train loss:0.15927518395424603\n",
      "train loss:0.207453727602118\n",
      "train loss:0.1663125342234097\n",
      "train loss:0.22641488992686196\n",
      "train loss:0.22086189708132284\n",
      "train loss:0.19895908457374695\n",
      "train loss:0.2876378269432143\n",
      "train loss:0.24654363429203444\n",
      "train loss:0.19618743558501414\n",
      "train loss:0.13646294093092629\n",
      "train loss:0.1478362345182349\n",
      "train loss:0.13026941998402988\n",
      "train loss:0.22835895149633828\n",
      "train loss:0.41246578986127536\n",
      "train loss:0.16652262488636072\n",
      "train loss:0.21697122191651844\n",
      "train loss:0.2214412837679841\n",
      "train loss:0.23905858723116988\n",
      "train loss:0.1450763059239794\n",
      "train loss:0.11885411135196111\n",
      "train loss:0.21345402460335283\n",
      "train loss:0.13221738098730174\n",
      "train loss:0.3568890007616689\n",
      "train loss:0.4226960957399435\n",
      "train loss:0.10982246458403094\n",
      "train loss:0.32452682472630295\n",
      "train loss:0.22274096289034656\n",
      "train loss:0.1823021922245982\n",
      "train loss:0.19183211931194655\n",
      "train loss:0.40953277103765606\n",
      "train loss:0.12415959079632652\n",
      "train loss:0.18848358732387743\n",
      "train loss:0.23086415297027998\n",
      "train loss:0.17137941187521777\n",
      "train loss:0.432945913727981\n",
      "train loss:0.1958166572461564\n",
      "train loss:0.1677258929027678\n",
      "train loss:0.13805000840624615\n",
      "train loss:0.200658954516114\n",
      "train loss:0.18242222605465752\n",
      "train loss:0.17408661952782328\n",
      "train loss:0.39418988895382673\n",
      "train loss:0.15186251225217448\n",
      "train loss:0.22425621800057313\n",
      "train loss:0.2407469625475249\n",
      "train loss:0.2140104940378996\n",
      "train loss:0.18797485652769527\n",
      "train loss:0.2888116072013815\n",
      "train loss:0.23042534480865953\n",
      "train loss:0.125365393020955\n",
      "train loss:0.19059787851266907\n",
      "train loss:0.21559786038798048\n",
      "train loss:0.21195348067333963\n",
      "train loss:0.24293606663488881\n",
      "train loss:0.3251159053314636\n",
      "train loss:0.27547156702928227\n",
      "train loss:0.17766750415188481\n",
      "train loss:0.2530033793994202\n",
      "train loss:0.29483320544796204\n",
      "train loss:0.11755028469305458\n",
      "train loss:0.20863280015791547\n",
      "train loss:0.21389636859346312\n",
      "train loss:0.2872549703280712\n",
      "train loss:0.2081247318364072\n",
      "train loss:0.1292090601988256\n",
      "train loss:0.30230980299612126\n",
      "train loss:0.1592767169135829\n",
      "train loss:0.26855851667185887\n",
      "train loss:0.14714172398936226\n",
      "train loss:0.13156708316692725\n",
      "train loss:0.09203428336387788\n",
      "train loss:0.1684940495527087\n",
      "train loss:0.2921540380517194\n",
      "train loss:0.28755734679995487\n",
      "train loss:0.21747524445734567\n",
      "train loss:0.21815937153704315\n",
      "train loss:0.16893544330647042\n",
      "train loss:0.07966274324587301\n",
      "train loss:0.25687150588755236\n",
      "train loss:0.1379139462826272\n",
      "train loss:0.11274392496388265\n",
      "train loss:0.15121753272831703\n",
      "train loss:0.1434302066213451\n",
      "train loss:0.160975587115028\n",
      "train loss:0.29058759116387917\n",
      "train loss:0.22402333530661434\n",
      "train loss:0.24082910502803176\n",
      "train loss:0.18075309932659128\n",
      "train loss:0.3655052076589876\n",
      "train loss:0.2096943467174922\n",
      "train loss:0.17718106124972355\n",
      "train loss:0.23472329197186528\n",
      "train loss:0.1547366550743342\n",
      "train loss:0.3308083724581101\n",
      "train loss:0.20166003927391327\n",
      "train loss:0.12944869934672634\n",
      "train loss:0.10607080449511946\n",
      "train loss:0.19875579402996002\n",
      "train loss:0.12227559697275614\n",
      "train loss:0.18642811038634502\n",
      "train loss:0.18698390308068968\n",
      "train loss:0.25971107152587225\n",
      "train loss:0.28044413070725543\n",
      "train loss:0.21107036353830183\n",
      "train loss:0.1928921850715845\n",
      "train loss:0.2283141734917557\n",
      "train loss:0.1653154348710815\n",
      "train loss:0.20469705564987778\n",
      "train loss:0.19441424045711997\n",
      "train loss:0.2509152917463517\n",
      "train loss:0.2921618896263975\n",
      "train loss:0.12961594790571762\n",
      "train loss:0.27878176045471564\n",
      "train loss:0.1854862759457156\n",
      "train loss:0.1316911868905562\n",
      "train loss:0.16427848812931456\n",
      "train loss:0.21490668218004885\n",
      "train loss:0.22477078145033993\n",
      "train loss:0.10581357042443573\n",
      "train loss:0.12269315518480686\n",
      "train loss:0.20852675650520033\n",
      "train loss:0.11141051701150494\n",
      "train loss:0.18919714512881625\n",
      "train loss:0.12910987626163736\n",
      "train loss:0.19663324107627875\n",
      "train loss:0.22455977979041197\n",
      "train loss:0.13266521580219776\n",
      "train loss:0.1719468390985012\n",
      "train loss:0.1513512663320195\n",
      "train loss:0.2485038794230249\n",
      "train loss:0.1896977783307195\n",
      "train loss:0.1617369394829242\n",
      "train loss:0.21883816748673784\n",
      "train loss:0.17575365378044477\n",
      "train loss:0.19660377158410658\n",
      "train loss:0.24795916903450396\n",
      "train loss:0.291003659778576\n",
      "train loss:0.14811719141402413\n",
      "train loss:0.3385685731713488\n",
      "train loss:0.25073960234702247\n",
      "train loss:0.29793315777921725\n",
      "train loss:0.058689939904979595\n",
      "train loss:0.15776888486393828\n",
      "train loss:0.21411972093006185\n",
      "train loss:0.2795864919906132\n",
      "train loss:0.1727507725215473\n",
      "train loss:0.23979033492307175\n",
      "train loss:0.15123446707998567\n",
      "train loss:0.1976915980249019\n",
      "train loss:0.13478437390223894\n",
      "train loss:0.15377305823671092\n",
      "train loss:0.11993554171864415\n",
      "train loss:0.21991356877923263\n",
      "train loss:0.19305307751557335\n",
      "train loss:0.1844308833752768\n",
      "train loss:0.19443348668856042\n",
      "train loss:0.14762344291542306\n",
      "train loss:0.2183818581527192\n",
      "train loss:0.10964997181170145\n",
      "train loss:0.11947496135509064\n",
      "train loss:0.2255411496973975\n",
      "train loss:0.19547430890958406\n",
      "train loss:0.19277252311329185\n",
      "train loss:0.2441762853962059\n",
      "train loss:0.10392306253500916\n",
      "train loss:0.17863667610793357\n",
      "train loss:0.1713791074775822\n",
      "train loss:0.14112625788962305\n",
      "train loss:0.20159762207081544\n",
      "train loss:0.1319690352468268\n",
      "train loss:0.08041025599358465\n",
      "train loss:0.2475349414169964\n",
      "train loss:0.15398073214660143\n",
      "train loss:0.053123840674323096\n",
      "train loss:0.23891222217415525\n",
      "train loss:0.2012043687481194\n",
      "train loss:0.2355821878825096\n",
      "train loss:0.19156563213773453\n",
      "train loss:0.1800620303261303\n",
      "train loss:0.15704750259011557\n",
      "train loss:0.12240707062345944\n",
      "train loss:0.1724631775771745\n",
      "train loss:0.2247075492352935\n",
      "train loss:0.07401924388220796\n",
      "train loss:0.32884338354493226\n",
      "train loss:0.17976759862050606\n",
      "train loss:0.1412071444049608\n",
      "train loss:0.21818755374550589\n",
      "train loss:0.15187573749526032\n",
      "train loss:0.15662753352016398\n",
      "train loss:0.347356213870626\n",
      "train loss:0.07548921829264821\n",
      "train loss:0.16727786335225509\n",
      "train loss:0.17751877088438417\n",
      "train loss:0.08597100175604516\n",
      "train loss:0.06930134376215656\n",
      "train loss:0.1316585665688687\n",
      "train loss:0.19832254761745083\n",
      "train loss:0.13371164368341662\n",
      "train loss:0.23072323115668658\n",
      "train loss:0.16142470796576344\n",
      "train loss:0.23727470573352863\n",
      "train loss:0.2362964807471333\n",
      "train loss:0.12949924421949746\n",
      "train loss:0.15026961351513207\n",
      "train loss:0.18259778335682741\n",
      "train loss:0.14175321223851978\n",
      "train loss:0.22167730895647714\n",
      "train loss:0.176539163887778\n",
      "train loss:0.2077928683761719\n",
      "train loss:0.20179928942731304\n",
      "train loss:0.09163813604676267\n",
      "train loss:0.13985401980803944\n",
      "train loss:0.06972953071971859\n",
      "train loss:0.14670259353034765\n",
      "train loss:0.3216816208051979\n",
      "train loss:0.1369779787278422\n",
      "train loss:0.1392749435614275\n",
      "train loss:0.2067070508460355\n",
      "train loss:0.08930589042482977\n",
      "train loss:0.19387059734551693\n",
      "train loss:0.30352584256538573\n",
      "train loss:0.16908815898835003\n",
      "train loss:0.11299090707006443\n",
      "train loss:0.173382830716826\n",
      "train loss:0.13900759948734176\n",
      "train loss:0.10937342021679075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.18676937350749154\n",
      "train loss:0.14789725855554145\n",
      "train loss:0.2007112971550339\n",
      "train loss:0.14631513710611965\n",
      "train loss:0.1858590900903938\n",
      "train loss:0.07728277404379247\n",
      "train loss:0.18782066355222568\n",
      "train loss:0.0883701795287996\n",
      "train loss:0.09554285718440962\n",
      "train loss:0.12826094808076097\n",
      "train loss:0.13426479712576028\n",
      "train loss:0.28082730964490876\n",
      "train loss:0.12744765640130443\n",
      "train loss:0.210892640599468\n",
      "train loss:0.1465398071534598\n",
      "train loss:0.22252368791406293\n",
      "train loss:0.14605049349976648\n",
      "train loss:0.23795155143563146\n",
      "train loss:0.13507223207514524\n",
      "train loss:0.23428326762987325\n",
      "train loss:0.1386180180805877\n",
      "train loss:0.15837519744290818\n",
      "train loss:0.19638598811710575\n",
      "train loss:0.13039111944457796\n",
      "train loss:0.25985861837045143\n",
      "train loss:0.1289108913703725\n",
      "train loss:0.11306266053850397\n",
      "train loss:0.21085735789432586\n",
      "train loss:0.22512207297845244\n",
      "train loss:0.09797745900764529\n",
      "train loss:0.11422096045365208\n",
      "train loss:0.18986170441681402\n",
      "train loss:0.11780299850310971\n",
      "train loss:0.07275066851189224\n",
      "train loss:0.16126904482072596\n",
      "train loss:0.07887420015873767\n",
      "train loss:0.12806390369772858\n",
      "train loss:0.1544335122690443\n",
      "train loss:0.09936304308382204\n",
      "train loss:0.23237348913526273\n",
      "train loss:0.1264627889371335\n",
      "train loss:0.1738019499055548\n",
      "train loss:0.0906990643083593\n",
      "train loss:0.13146713633062768\n",
      "train loss:0.2486792142458972\n",
      "train loss:0.1658272327557968\n",
      "train loss:0.1533812546905754\n",
      "train loss:0.12824305139998465\n",
      "train loss:0.0860159028960887\n",
      "train loss:0.10282346486804876\n",
      "train loss:0.07775385769651696\n",
      "train loss:0.07652595740831929\n",
      "train loss:0.07869400891331914\n",
      "train loss:0.07675921920978142\n",
      "train loss:0.13144868204656962\n",
      "train loss:0.17856648498417815\n",
      "train loss:0.22199950143567743\n",
      "train loss:0.11121516265332283\n",
      "train loss:0.13345891799617607\n",
      "train loss:0.2476836751257457\n",
      "train loss:0.12445733886552338\n",
      "train loss:0.1023496105296559\n",
      "train loss:0.22781353483556166\n",
      "=== epoch:2, train acc:0.957, test acc:0.96 ===\n",
      "train loss:0.09183178670702873\n",
      "train loss:0.04371772294940882\n",
      "train loss:0.13556317937638407\n",
      "train loss:0.16592349149035865\n",
      "train loss:0.09637666848810371\n",
      "train loss:0.1632928025782011\n",
      "train loss:0.11325585062522717\n",
      "train loss:0.10907217594815403\n",
      "train loss:0.10569483301426966\n",
      "train loss:0.14997138318981573\n",
      "train loss:0.09185740651563697\n",
      "train loss:0.14461057507852937\n",
      "train loss:0.19620057419201353\n",
      "train loss:0.06080252682592198\n",
      "train loss:0.07119650720207997\n",
      "train loss:0.08137584699605972\n",
      "train loss:0.1682642333990096\n",
      "train loss:0.045780434882447624\n",
      "train loss:0.11111492910916324\n",
      "train loss:0.06438393704670724\n",
      "train loss:0.09158543045372763\n",
      "train loss:0.12249117477758098\n",
      "train loss:0.17246005202297507\n",
      "train loss:0.13525119086556361\n",
      "train loss:0.10553692948533541\n",
      "train loss:0.08846898831174055\n",
      "train loss:0.1226550952240962\n",
      "train loss:0.06262348230574609\n",
      "train loss:0.14049299632209175\n",
      "train loss:0.062463971001915614\n",
      "train loss:0.16008632919980403\n",
      "train loss:0.12693318344733714\n",
      "train loss:0.15793167968085953\n",
      "train loss:0.16039264559652314\n",
      "train loss:0.16447273437171767\n",
      "train loss:0.2327803384308979\n",
      "train loss:0.2002426708215417\n",
      "train loss:0.14766974977201466\n",
      "train loss:0.08543226443860194\n",
      "train loss:0.0781943235663999\n",
      "train loss:0.13309253809797808\n",
      "train loss:0.15770310773170776\n",
      "train loss:0.11298044586506335\n",
      "train loss:0.16882106528709506\n",
      "train loss:0.08345830123123463\n",
      "train loss:0.08998219059027163\n",
      "train loss:0.10036935543187625\n",
      "train loss:0.12225535481432148\n",
      "train loss:0.236845147378593\n",
      "train loss:0.09627458740826901\n",
      "train loss:0.08166651305323679\n",
      "train loss:0.07101610469519366\n",
      "train loss:0.13470215713241973\n",
      "train loss:0.09441080973211462\n",
      "train loss:0.14859625582066835\n",
      "train loss:0.04414515641024609\n",
      "train loss:0.2453746357964425\n",
      "train loss:0.1692607999253481\n",
      "train loss:0.10883902438696126\n",
      "train loss:0.21561829351436604\n",
      "train loss:0.1630957446627236\n",
      "train loss:0.06954929276479495\n",
      "train loss:0.08744436261461173\n",
      "train loss:0.0763044602725857\n",
      "train loss:0.06648597798648354\n",
      "train loss:0.1743902001747419\n",
      "train loss:0.13062433580643637\n",
      "train loss:0.07190644599384446\n",
      "train loss:0.14479709302457433\n",
      "train loss:0.14517093715404358\n",
      "train loss:0.15737294920602254\n",
      "train loss:0.1825592386621599\n",
      "train loss:0.11693468860510065\n",
      "train loss:0.1328149812903838\n",
      "train loss:0.15281672828879161\n",
      "train loss:0.07516700075700751\n",
      "train loss:0.16146593851948626\n",
      "train loss:0.1296027505930244\n",
      "train loss:0.08880662865019212\n",
      "train loss:0.08379067842351702\n",
      "train loss:0.23550522143788474\n",
      "train loss:0.058158438726138365\n",
      "train loss:0.13932454791880844\n",
      "train loss:0.19517393926605828\n",
      "train loss:0.09635169515591618\n",
      "train loss:0.08713936677897705\n",
      "train loss:0.2411808604530052\n",
      "train loss:0.2003763324096483\n",
      "train loss:0.0695110187162987\n",
      "train loss:0.04254796505512395\n",
      "train loss:0.09920240981184819\n",
      "train loss:0.1930188260292135\n",
      "train loss:0.22366681670215643\n",
      "train loss:0.11647513174288505\n",
      "train loss:0.18073912587483476\n",
      "train loss:0.1548165829248498\n",
      "train loss:0.12352468968853109\n",
      "train loss:0.17042531683147083\n",
      "train loss:0.12250525182412353\n",
      "train loss:0.05865614951865867\n",
      "train loss:0.07323096500454729\n",
      "train loss:0.07879481035860461\n",
      "train loss:0.14874269618148397\n",
      "train loss:0.17460051596020607\n",
      "train loss:0.12498895015043557\n",
      "train loss:0.08689261599387668\n",
      "train loss:0.1303384126247698\n",
      "train loss:0.0676832425506029\n",
      "train loss:0.056761473019446396\n",
      "train loss:0.15628386607228564\n",
      "train loss:0.1365045473601431\n",
      "train loss:0.08443703215193542\n",
      "train loss:0.15597826208290835\n",
      "train loss:0.08829939062193222\n",
      "train loss:0.12461730788077074\n",
      "train loss:0.04207307920961458\n",
      "train loss:0.02458949535396025\n",
      "train loss:0.1179995278399733\n",
      "train loss:0.08289734968475412\n",
      "train loss:0.18919336730862038\n",
      "train loss:0.1482916824671634\n",
      "train loss:0.11688832433575418\n",
      "train loss:0.08108240094667868\n",
      "train loss:0.10855014644756171\n",
      "train loss:0.13711746394217758\n",
      "train loss:0.2035806751024649\n",
      "train loss:0.09022432517869923\n",
      "train loss:0.060694994901251624\n",
      "train loss:0.059713108494886695\n",
      "train loss:0.1136141066173876\n",
      "train loss:0.04130603666358968\n",
      "train loss:0.1540203229188913\n",
      "train loss:0.0743695151120673\n",
      "train loss:0.2941084927358297\n",
      "train loss:0.0862137306028579\n",
      "train loss:0.084654601262896\n",
      "train loss:0.2070686570621281\n",
      "train loss:0.13363914052426498\n",
      "train loss:0.09754872708821473\n",
      "train loss:0.15179865525533356\n",
      "train loss:0.09330410796802452\n",
      "train loss:0.09235004980310789\n",
      "train loss:0.07015767247894047\n",
      "train loss:0.18136352551127832\n",
      "train loss:0.10972155258241807\n",
      "train loss:0.13823695281406614\n",
      "train loss:0.10456518928697482\n",
      "train loss:0.12067325140143569\n",
      "train loss:0.06382635532474902\n",
      "train loss:0.10580629633761637\n",
      "train loss:0.08463001647408969\n",
      "train loss:0.12500244433711935\n",
      "train loss:0.09955915763403755\n",
      "train loss:0.20404436759062236\n",
      "train loss:0.09006436587272112\n",
      "train loss:0.1906691192102201\n",
      "train loss:0.13904069126567337\n",
      "train loss:0.26642088527317953\n",
      "train loss:0.10699042896853733\n",
      "train loss:0.10933147511205535\n",
      "train loss:0.30105308725655255\n",
      "train loss:0.054290335984762406\n",
      "train loss:0.1039212387856873\n",
      "train loss:0.12529878480807147\n",
      "train loss:0.13815809688507177\n",
      "train loss:0.13689809890852522\n",
      "train loss:0.07800673813273291\n",
      "train loss:0.06574826515763794\n",
      "train loss:0.05685358961776073\n",
      "train loss:0.04378233846794701\n",
      "train loss:0.13554883621292177\n",
      "train loss:0.13600873047651516\n",
      "train loss:0.06855982344291305\n",
      "train loss:0.13732483197141918\n",
      "train loss:0.0984972114470776\n",
      "train loss:0.16977057845324078\n",
      "train loss:0.11585374955077411\n",
      "train loss:0.04756177122599758\n",
      "train loss:0.17660505093402834\n",
      "train loss:0.05617185145370389\n",
      "train loss:0.1198845382031979\n",
      "train loss:0.14151512400346686\n",
      "train loss:0.0893029208691609\n",
      "train loss:0.10599529498879649\n",
      "train loss:0.13278089769586296\n",
      "train loss:0.17079313838661908\n",
      "train loss:0.13864940207790052\n",
      "train loss:0.07011359143974104\n",
      "train loss:0.23379122769984903\n",
      "train loss:0.10371022798004505\n",
      "train loss:0.21700871525303797\n",
      "train loss:0.12531772566727478\n",
      "train loss:0.060654528063742726\n",
      "train loss:0.07112101003575287\n",
      "train loss:0.0930199665501602\n",
      "train loss:0.050531030555401354\n",
      "train loss:0.07743061131613098\n",
      "train loss:0.14045939900549945\n",
      "train loss:0.08989628065089768\n",
      "train loss:0.04195474018494054\n",
      "train loss:0.08725966584558952\n",
      "train loss:0.09785387643237513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.08191680307740176\n",
      "train loss:0.11316735093140108\n",
      "train loss:0.08842695910792113\n",
      "train loss:0.0926062007300647\n",
      "train loss:0.12371244583949481\n",
      "train loss:0.1481109057069382\n",
      "train loss:0.1342547487900317\n",
      "train loss:0.05823717178361333\n",
      "train loss:0.07641964636809098\n",
      "train loss:0.06182390038995821\n",
      "train loss:0.10162794014836272\n",
      "train loss:0.19682436608156575\n",
      "train loss:0.056059580616663614\n",
      "train loss:0.03759587273110222\n",
      "train loss:0.07320095683614922\n",
      "train loss:0.025766103490471183\n",
      "train loss:0.19212177288326193\n",
      "train loss:0.1567569470577396\n",
      "train loss:0.2058882200149299\n",
      "train loss:0.048185507034904325\n",
      "train loss:0.1574117563652336\n",
      "train loss:0.14894403447538662\n",
      "train loss:0.15805484626048621\n",
      "train loss:0.07711097597542406\n",
      "train loss:0.09489000318354643\n",
      "train loss:0.16555771586688203\n",
      "train loss:0.12042564225829705\n",
      "train loss:0.0583734564830007\n",
      "train loss:0.10212105526338908\n",
      "train loss:0.13105595478491888\n",
      "train loss:0.042749911713837456\n",
      "train loss:0.17731100560458615\n",
      "train loss:0.08694426115098812\n",
      "train loss:0.041716747126178504\n",
      "train loss:0.14287356238651297\n",
      "train loss:0.18490473500363713\n",
      "train loss:0.056836274794653044\n",
      "train loss:0.05989685753739099\n",
      "train loss:0.12575087690506234\n",
      "train loss:0.07303204249273672\n",
      "train loss:0.05393997106770903\n",
      "train loss:0.04488907991794257\n",
      "train loss:0.10654803675317927\n",
      "train loss:0.09758188058477435\n",
      "train loss:0.14814387418734676\n",
      "train loss:0.1691319705588445\n",
      "train loss:0.12269957743046685\n",
      "train loss:0.05601280780463461\n",
      "train loss:0.05356535644383829\n",
      "train loss:0.04106750175002525\n",
      "train loss:0.17573588216504352\n",
      "train loss:0.06385902678401927\n",
      "train loss:0.06193599009630653\n",
      "train loss:0.12620287953917836\n",
      "train loss:0.07630682398630506\n",
      "train loss:0.09455724971962304\n",
      "train loss:0.08374003174409854\n",
      "train loss:0.13285695087154445\n",
      "train loss:0.20762004481663854\n",
      "train loss:0.09228655063180279\n",
      "train loss:0.12803096633994515\n",
      "train loss:0.22321956078202004\n",
      "train loss:0.1258589120388216\n",
      "train loss:0.04822183840249717\n",
      "train loss:0.045323244116350754\n",
      "train loss:0.14385461054569557\n",
      "train loss:0.11817938126572396\n",
      "train loss:0.027646588298747898\n",
      "train loss:0.12179071640152932\n",
      "train loss:0.03382893812245366\n",
      "train loss:0.053942749738124324\n",
      "train loss:0.09044612220659429\n",
      "train loss:0.05279151113500156\n",
      "train loss:0.06600327639638132\n",
      "train loss:0.03952076106138455\n",
      "train loss:0.2594787920820699\n",
      "train loss:0.1739268588361927\n",
      "train loss:0.11443533867704693\n",
      "train loss:0.10271714532301868\n",
      "train loss:0.07002704727218424\n",
      "train loss:0.07153273796216776\n",
      "train loss:0.11837693783445453\n",
      "train loss:0.04939349298116261\n",
      "train loss:0.12464010199733715\n",
      "train loss:0.06046655012384795\n",
      "train loss:0.06929954421199634\n",
      "train loss:0.03278716621723235\n",
      "train loss:0.11694211096182164\n",
      "train loss:0.0512049362708664\n",
      "train loss:0.08864719340390902\n",
      "train loss:0.11811343022560378\n",
      "train loss:0.06653433981792568\n",
      "train loss:0.03988999189051327\n",
      "train loss:0.17327203347667577\n",
      "train loss:0.0485099693570325\n",
      "train loss:0.10672869667705022\n",
      "train loss:0.13179647003882708\n",
      "train loss:0.05633058993001134\n",
      "train loss:0.04517016901939744\n",
      "train loss:0.09838513914008637\n",
      "train loss:0.10359639343670247\n",
      "train loss:0.06150425372548787\n",
      "train loss:0.05359176795562833\n",
      "train loss:0.029280188344307276\n",
      "train loss:0.09328494607073554\n",
      "train loss:0.12582961945149004\n",
      "train loss:0.09702535284268698\n",
      "train loss:0.03547180720361925\n",
      "train loss:0.06973675878560957\n",
      "train loss:0.13511178455806777\n",
      "train loss:0.05238954703614338\n",
      "train loss:0.08981576007676788\n",
      "train loss:0.05562445171112182\n",
      "train loss:0.09077186489221743\n",
      "train loss:0.07984495987228352\n",
      "train loss:0.04025189563573034\n",
      "train loss:0.26568131939548817\n",
      "train loss:0.2013524893865544\n",
      "train loss:0.217461908428488\n",
      "train loss:0.16626813139545432\n",
      "train loss:0.07825490707636439\n",
      "train loss:0.047674553945979206\n",
      "train loss:0.21115701628837408\n",
      "train loss:0.06723225421272484\n",
      "train loss:0.06300233875275846\n",
      "train loss:0.0642998846682829\n",
      "train loss:0.06844555427120382\n",
      "train loss:0.0588200334856166\n",
      "train loss:0.06632276376530463\n",
      "train loss:0.06350256153574182\n",
      "train loss:0.06305581414059384\n",
      "train loss:0.04733083873531525\n",
      "train loss:0.03442071534765713\n",
      "train loss:0.17836197318136576\n",
      "train loss:0.11975270895536731\n",
      "train loss:0.027650174983369313\n",
      "train loss:0.05461248020801486\n",
      "train loss:0.08985834820986821\n",
      "train loss:0.026992200654086685\n",
      "train loss:0.07875164372739589\n",
      "train loss:0.05429895819995594\n",
      "train loss:0.13650055214802778\n",
      "train loss:0.09684162737159718\n",
      "train loss:0.07993262893301853\n",
      "train loss:0.05768075437048811\n",
      "train loss:0.033530880016233\n",
      "train loss:0.06338550657042333\n",
      "train loss:0.06391876749248063\n",
      "train loss:0.061308124576564024\n",
      "train loss:0.07123626423205148\n",
      "train loss:0.06258771536991185\n",
      "train loss:0.08622474395927644\n",
      "train loss:0.058238389626115036\n",
      "train loss:0.055013452105023106\n",
      "train loss:0.08371955945511561\n",
      "train loss:0.11577013361913722\n",
      "train loss:0.07546200022031012\n",
      "train loss:0.06485304032496039\n",
      "train loss:0.05662318895026653\n",
      "train loss:0.09001564519859391\n",
      "train loss:0.022799239212001613\n",
      "train loss:0.07989008863842105\n",
      "train loss:0.13069729611372613\n",
      "train loss:0.10358121129462632\n",
      "train loss:0.12828414252737066\n",
      "train loss:0.04702761700812422\n",
      "train loss:0.11516353212733049\n",
      "train loss:0.07872415449426075\n",
      "train loss:0.04670936407371025\n",
      "train loss:0.08519628705565431\n",
      "train loss:0.06320688725734949\n",
      "train loss:0.04584006525337541\n",
      "train loss:0.06706066752958384\n",
      "train loss:0.0316662536602385\n",
      "train loss:0.05577903885952333\n",
      "train loss:0.07930145943082854\n",
      "train loss:0.07607613815245375\n",
      "train loss:0.044829856229403935\n",
      "train loss:0.17312154798562043\n",
      "train loss:0.03337113478765737\n",
      "train loss:0.050998868969781466\n",
      "train loss:0.09660888819159083\n",
      "train loss:0.04524072662579079\n",
      "train loss:0.050763398423551064\n",
      "train loss:0.051490705130384444\n",
      "train loss:0.05850770622201089\n",
      "train loss:0.1889994528409062\n",
      "train loss:0.1005741206651816\n",
      "train loss:0.1292269189374704\n",
      "train loss:0.052546705220014873\n",
      "train loss:0.19332001970564197\n",
      "train loss:0.057657999115129234\n",
      "train loss:0.1187683574927907\n",
      "train loss:0.06743454601376105\n",
      "train loss:0.044311610765721\n",
      "train loss:0.15096015148068015\n",
      "train loss:0.09035076257360711\n",
      "train loss:0.07217213007127538\n",
      "train loss:0.0490965005809275\n",
      "train loss:0.04518573969023393\n",
      "train loss:0.06633609337289928\n",
      "train loss:0.05513132937097349\n",
      "train loss:0.11647630082738678\n",
      "train loss:0.07138277666806682\n",
      "train loss:0.13183493211705427\n",
      "train loss:0.09487795355378592\n",
      "train loss:0.03097917736827767\n",
      "train loss:0.05891523773786982\n",
      "train loss:0.09778145827968304\n",
      "train loss:0.03832233429917266\n",
      "train loss:0.014720642635818399\n",
      "train loss:0.12466392441707416\n",
      "train loss:0.06588487863138541\n",
      "train loss:0.07803169535413561\n",
      "train loss:0.0782160448758502\n",
      "train loss:0.10104845680551529\n",
      "train loss:0.07705947356542606\n",
      "train loss:0.0869689727886814\n",
      "train loss:0.04900894781833137\n",
      "train loss:0.12878103049495776\n",
      "train loss:0.05558904149071043\n",
      "train loss:0.05330961518512118\n",
      "train loss:0.09178725367819492\n",
      "train loss:0.11020503827367105\n",
      "train loss:0.043624046788758315\n",
      "train loss:0.06323238600647618\n",
      "train loss:0.07494747376750928\n",
      "train loss:0.09282695490755538\n",
      "train loss:0.059436410393312426\n",
      "train loss:0.07768826593655725\n",
      "train loss:0.1110538785613721\n",
      "train loss:0.08557051629838539\n",
      "train loss:0.02353804939511595\n",
      "train loss:0.07230332277495116\n",
      "train loss:0.05198330988046505\n",
      "train loss:0.04961171970682923\n",
      "train loss:0.04567157655171556\n",
      "train loss:0.1329416913978533\n",
      "train loss:0.11006855186984602\n",
      "train loss:0.08030338884481597\n",
      "train loss:0.0856403395515329\n",
      "train loss:0.14376807248337672\n",
      "train loss:0.1038570298508644\n",
      "train loss:0.07739810331619353\n",
      "train loss:0.059815683497460526\n",
      "train loss:0.06447093578183838\n",
      "train loss:0.08360966258749956\n",
      "train loss:0.014057334539295235\n",
      "train loss:0.13120955194535588\n",
      "train loss:0.08414266941224394\n",
      "train loss:0.07325724709362982\n",
      "train loss:0.05590723913770381\n",
      "train loss:0.03574169011350329\n",
      "train loss:0.07503806547500491\n",
      "train loss:0.0503661371149706\n",
      "train loss:0.12711964373855034\n",
      "train loss:0.23397791762538733\n",
      "train loss:0.04020799855012234\n",
      "train loss:0.2341270594579968\n",
      "train loss:0.046695611654727937\n",
      "train loss:0.0663889188804319\n",
      "train loss:0.06933510034211414\n",
      "train loss:0.06264191615323131\n",
      "train loss:0.06963839785130324\n",
      "train loss:0.05166171364539138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.035981696072467\n",
      "train loss:0.06996206634282372\n",
      "train loss:0.09613325968149149\n",
      "train loss:0.04087488904190498\n",
      "train loss:0.16425828603970627\n",
      "train loss:0.14550258063523536\n",
      "train loss:0.04401461649999918\n",
      "train loss:0.08840809768500213\n",
      "train loss:0.0926705520042521\n",
      "train loss:0.1193309976286214\n",
      "train loss:0.04591893043828245\n",
      "train loss:0.05218343813047535\n",
      "train loss:0.06211900490861272\n",
      "train loss:0.026000369025635996\n",
      "train loss:0.08476961491977067\n",
      "train loss:0.08503497072835241\n",
      "train loss:0.05708164794254786\n",
      "train loss:0.06271057857905574\n",
      "train loss:0.03854617013698467\n",
      "train loss:0.03397578847748585\n",
      "train loss:0.08689337071565709\n",
      "train loss:0.04503419531069575\n",
      "train loss:0.08383811400686421\n",
      "train loss:0.10964409797972294\n",
      "train loss:0.0751931204755586\n",
      "train loss:0.057825982112089634\n",
      "train loss:0.07457683248634558\n",
      "train loss:0.041118165404752396\n",
      "train loss:0.0520901997820647\n",
      "train loss:0.04156457824804523\n",
      "train loss:0.08692737054170709\n",
      "train loss:0.0681569006867653\n",
      "train loss:0.05636744182130087\n",
      "train loss:0.047833496265748995\n",
      "train loss:0.10011380074432459\n",
      "train loss:0.07687987713600984\n",
      "train loss:0.06883965191054628\n",
      "train loss:0.08495647284920105\n",
      "train loss:0.07362831334568051\n",
      "train loss:0.11555334299074799\n",
      "train loss:0.014354569121199597\n",
      "train loss:0.06521048916657411\n",
      "train loss:0.21538187509742204\n",
      "train loss:0.05667466899443307\n",
      "train loss:0.1933473830755246\n",
      "train loss:0.09779633130665034\n",
      "train loss:0.05709174170879855\n",
      "train loss:0.08846774614100411\n",
      "train loss:0.12485332006456865\n",
      "train loss:0.08415606547510084\n",
      "train loss:0.09863275594114135\n",
      "train loss:0.04133921319992982\n",
      "train loss:0.10478016542626484\n",
      "train loss:0.02734043550768629\n",
      "train loss:0.08303698825066944\n",
      "train loss:0.10655944927350974\n",
      "train loss:0.04195644427371212\n",
      "train loss:0.11455263839658861\n",
      "train loss:0.09332073695949142\n",
      "train loss:0.07978818981241888\n",
      "train loss:0.06275113079293515\n",
      "train loss:0.0625252964657467\n",
      "train loss:0.10036090046693896\n",
      "train loss:0.08487725491815111\n",
      "train loss:0.07274808916145262\n",
      "train loss:0.056632755891047486\n",
      "train loss:0.040040835813296864\n",
      "train loss:0.04489168773973847\n",
      "train loss:0.06122168447258461\n",
      "train loss:0.054997314079576254\n",
      "train loss:0.08071714857949465\n",
      "train loss:0.08843131474038893\n",
      "train loss:0.07744656611045424\n",
      "train loss:0.07827605738454904\n",
      "train loss:0.09456519576384094\n",
      "train loss:0.0327714704160838\n",
      "train loss:0.09969545848533479\n",
      "train loss:0.10530414959614502\n",
      "train loss:0.1756056134970278\n",
      "train loss:0.15090732790995556\n",
      "train loss:0.06055994705130107\n",
      "train loss:0.06182738107125443\n",
      "train loss:0.10565452307731778\n",
      "train loss:0.03144502329152354\n",
      "train loss:0.0445039301067112\n",
      "train loss:0.07108358913857693\n",
      "train loss:0.07200937365957925\n",
      "train loss:0.038492447305645855\n",
      "train loss:0.035086626546716466\n",
      "train loss:0.12356367532003394\n",
      "train loss:0.13206457267123983\n",
      "train loss:0.04126115301958976\n",
      "train loss:0.04567541810960299\n",
      "train loss:0.07621565019815145\n",
      "train loss:0.05759510066825347\n",
      "train loss:0.02007644209192221\n",
      "train loss:0.012140573687308082\n",
      "train loss:0.08129304265418831\n",
      "train loss:0.07910424635864316\n",
      "train loss:0.1039261747222652\n",
      "train loss:0.06829310606843338\n",
      "train loss:0.06857792038639576\n",
      "train loss:0.033479735619063436\n",
      "train loss:0.12065722685698782\n",
      "train loss:0.06064393348402177\n",
      "train loss:0.05756889030030633\n",
      "train loss:0.07493087143323574\n",
      "train loss:0.09702130233990408\n",
      "train loss:0.05573306998644335\n",
      "train loss:0.03669938866565517\n",
      "train loss:0.07406624047078053\n",
      "train loss:0.018993732778112327\n",
      "train loss:0.025607534876351378\n",
      "train loss:0.0477006420864906\n",
      "train loss:0.03953652727777973\n",
      "train loss:0.08405585427323439\n",
      "train loss:0.08254196676968815\n",
      "train loss:0.09736753709334886\n",
      "train loss:0.02828425869798043\n",
      "train loss:0.03672255596846114\n",
      "train loss:0.1024840576799711\n",
      "train loss:0.1075953247253217\n",
      "train loss:0.03985997083439336\n",
      "train loss:0.14568064806729666\n",
      "train loss:0.09229711156141145\n",
      "train loss:0.03667918335054477\n",
      "train loss:0.03570365364282812\n",
      "train loss:0.0630745348126821\n",
      "train loss:0.015985980585263914\n",
      "train loss:0.08511439683567162\n",
      "train loss:0.063624633817865\n",
      "train loss:0.02016409908872587\n",
      "train loss:0.07598912402799196\n",
      "=== epoch:3, train acc:0.971, test acc:0.971 ===\n",
      "train loss:0.08378512580948819\n",
      "train loss:0.020000923150282058\n",
      "train loss:0.058465777564067886\n",
      "train loss:0.12874942870139125\n",
      "train loss:0.042340542399588926\n",
      "train loss:0.06532431575713075\n",
      "train loss:0.03733574320929162\n",
      "train loss:0.10757279043889353\n",
      "train loss:0.13235092234673151\n",
      "train loss:0.09960063911710466\n",
      "train loss:0.06687727362193849\n",
      "train loss:0.031447011210277556\n",
      "train loss:0.07700478758899716\n",
      "train loss:0.09874096171056858\n",
      "train loss:0.07487153361967232\n",
      "train loss:0.02869680636263699\n",
      "train loss:0.09083219878392758\n",
      "train loss:0.04789998650942913\n",
      "train loss:0.11459790300418948\n",
      "train loss:0.06767098473832688\n",
      "train loss:0.11707194867101599\n",
      "train loss:0.025488717396053046\n",
      "train loss:0.06361260772680159\n",
      "train loss:0.037127713819354174\n",
      "train loss:0.0966893528978206\n",
      "train loss:0.07342578640117495\n",
      "train loss:0.07645617132345875\n",
      "train loss:0.12165948971002306\n",
      "train loss:0.035960425834571944\n",
      "train loss:0.043067669959502786\n",
      "train loss:0.14814311850976153\n",
      "train loss:0.027582484005512095\n",
      "train loss:0.1368264064850816\n",
      "train loss:0.10492819486075264\n",
      "train loss:0.049038503371320485\n",
      "train loss:0.09872097348475678\n",
      "train loss:0.050803741705761914\n",
      "train loss:0.04913364609876416\n",
      "train loss:0.039736790785887634\n",
      "train loss:0.08790304112492908\n",
      "train loss:0.04580906898992012\n",
      "train loss:0.06007828435335217\n",
      "train loss:0.04185134662000533\n",
      "train loss:0.10261027287187136\n",
      "train loss:0.08924347579416032\n",
      "train loss:0.08250997818503976\n",
      "train loss:0.06394741316236478\n",
      "train loss:0.12399069190793194\n",
      "train loss:0.042649285446356065\n",
      "train loss:0.08196332496939021\n",
      "train loss:0.08187537252750769\n",
      "train loss:0.06417886449463198\n",
      "train loss:0.16165119073725454\n",
      "train loss:0.04731345420373696\n",
      "train loss:0.23593640468389995\n",
      "train loss:0.021498269088513558\n",
      "train loss:0.09993273824514493\n",
      "train loss:0.052558734651718744\n",
      "train loss:0.07175207986592916\n",
      "train loss:0.03950445046961116\n",
      "train loss:0.02275133160406295\n",
      "train loss:0.11424101362931925\n",
      "train loss:0.037826954428120584\n",
      "train loss:0.10785282585550707\n",
      "train loss:0.04797737112825483\n",
      "train loss:0.06177802659870414\n",
      "train loss:0.12795031971938356\n",
      "train loss:0.0962534962760952\n",
      "train loss:0.0862878954815357\n",
      "train loss:0.05322641767682671\n",
      "train loss:0.07709326413385158\n",
      "train loss:0.05951821992546798\n",
      "train loss:0.06458112255549996\n",
      "train loss:0.09567852512645501\n",
      "train loss:0.059712467832963495\n",
      "train loss:0.05029282335221983\n",
      "train loss:0.05671863592320325\n",
      "train loss:0.06075457302611393\n",
      "train loss:0.03144389044189795\n",
      "train loss:0.029689563938762625\n",
      "train loss:0.061340997326758684\n",
      "train loss:0.0639601175669725\n",
      "train loss:0.04267870859113314\n",
      "train loss:0.17126126395787447\n",
      "train loss:0.05325564673803898\n",
      "train loss:0.1187116154026156\n",
      "train loss:0.13822298113190223\n",
      "train loss:0.037787841864338485\n",
      "train loss:0.032254994800578286\n",
      "train loss:0.07124801197972473\n",
      "train loss:0.10607048683615088\n",
      "train loss:0.20722202234138556\n",
      "train loss:0.039041858176407256\n",
      "train loss:0.050827903924530175\n",
      "train loss:0.07094437710671359\n",
      "train loss:0.042789781444760464\n",
      "train loss:0.05710009055143616\n",
      "train loss:0.033867520481752636\n",
      "train loss:0.09564252056205445\n",
      "train loss:0.04354399717372339\n",
      "train loss:0.12897083913171323\n",
      "train loss:0.030538299263547213\n",
      "train loss:0.0774799322563025\n",
      "train loss:0.05029900123477929\n",
      "train loss:0.06970048545113211\n",
      "train loss:0.023408021997136368\n",
      "train loss:0.04027278425303624\n",
      "train loss:0.15783792373132083\n",
      "train loss:0.05612338293163826\n",
      "train loss:0.058216552900144665\n",
      "train loss:0.07269785398869008\n",
      "train loss:0.09206534971238398\n",
      "train loss:0.10104828766528234\n",
      "train loss:0.14607064868839031\n",
      "train loss:0.07209705971344185\n",
      "train loss:0.03611409089339294\n",
      "train loss:0.06905656236071102\n",
      "train loss:0.048836661518433244\n",
      "train loss:0.06186836495144054\n",
      "train loss:0.06791496772209735\n",
      "train loss:0.11420170253777778\n",
      "train loss:0.09313676749384292\n",
      "train loss:0.0390485870901054\n",
      "train loss:0.0589302158581989\n",
      "train loss:0.04712037923776863\n",
      "train loss:0.06745314615860837\n",
      "train loss:0.09836047671274269\n",
      "train loss:0.08736880794493793\n",
      "train loss:0.04539393950122892\n",
      "train loss:0.08278964858498576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.03665039698235207\n",
      "train loss:0.030104344441617425\n",
      "train loss:0.027905150484556086\n",
      "train loss:0.15684446619898176\n",
      "train loss:0.12965753726441598\n",
      "train loss:0.12282582642466315\n",
      "train loss:0.05001419239875922\n",
      "train loss:0.062086340164527305\n",
      "train loss:0.0333570229587914\n",
      "train loss:0.039693802510326434\n",
      "train loss:0.08126129133918937\n",
      "train loss:0.042990609907294274\n",
      "train loss:0.0363605502508588\n",
      "train loss:0.046532307815893945\n",
      "train loss:0.07379197807457301\n",
      "train loss:0.02815590742073224\n",
      "train loss:0.04418355195638613\n",
      "train loss:0.0583686085089251\n",
      "train loss:0.05048889003672491\n",
      "train loss:0.09029979864381628\n",
      "train loss:0.03789939156326303\n",
      "train loss:0.12483965134452686\n",
      "train loss:0.09574006690398278\n",
      "train loss:0.08544437486367606\n",
      "train loss:0.07288860710563456\n",
      "train loss:0.07610436076527938\n",
      "train loss:0.09287809540407162\n",
      "train loss:0.05469294974595556\n",
      "train loss:0.025533685214503003\n",
      "train loss:0.11264221799619464\n",
      "train loss:0.05948921108044032\n",
      "train loss:0.03174614848849958\n",
      "train loss:0.03038140647761264\n",
      "train loss:0.09461176478053929\n",
      "train loss:0.04379104724348558\n",
      "train loss:0.04941398144531904\n",
      "train loss:0.03909157852538867\n",
      "train loss:0.06296565327008617\n",
      "train loss:0.06094865459765563\n",
      "train loss:0.08334065262064515\n",
      "train loss:0.05939937833423036\n",
      "train loss:0.12368087651735662\n",
      "train loss:0.03376507240667187\n",
      "train loss:0.08085450924579694\n",
      "train loss:0.048260052661651426\n",
      "train loss:0.01830205743169\n",
      "train loss:0.02206246495251558\n",
      "train loss:0.05425145997228076\n",
      "train loss:0.03548438293489032\n",
      "train loss:0.026300415678659764\n",
      "train loss:0.12701985941907074\n",
      "train loss:0.059335230131321796\n",
      "train loss:0.10362904890875595\n",
      "train loss:0.03499961584080723\n",
      "train loss:0.052096419261281196\n",
      "train loss:0.024430768124426624\n",
      "train loss:0.05105374932488791\n",
      "train loss:0.053408552618049346\n",
      "train loss:0.04475659735160398\n",
      "train loss:0.040754292826467406\n",
      "train loss:0.032120888187501515\n",
      "train loss:0.09157731292472959\n",
      "train loss:0.05483668761094914\n",
      "train loss:0.07827457156521447\n",
      "train loss:0.06329936937751654\n",
      "train loss:0.05106625640563978\n",
      "train loss:0.09383415121745912\n",
      "train loss:0.0732224552372226\n",
      "train loss:0.039669584583925006\n",
      "train loss:0.042549237883430686\n",
      "train loss:0.09825534903083506\n",
      "train loss:0.034563212514509056\n",
      "train loss:0.1921162832346035\n",
      "train loss:0.15990561961740454\n",
      "train loss:0.054721780859791715\n",
      "train loss:0.06146590771642664\n",
      "train loss:0.014314395447177697\n",
      "train loss:0.05486673359826353\n",
      "train loss:0.04464872593317974\n",
      "train loss:0.02242740906330839\n",
      "train loss:0.05694175519345323\n",
      "train loss:0.0262760747633945\n",
      "train loss:0.04447269665659337\n",
      "train loss:0.04820980623467563\n",
      "train loss:0.03842983614700186\n",
      "train loss:0.02661246113760945\n",
      "train loss:0.17458834048204136\n",
      "train loss:0.041782786972905786\n",
      "train loss:0.037621220167246644\n",
      "train loss:0.02893719456816348\n",
      "train loss:0.021046995936655398\n",
      "train loss:0.10849419607356386\n",
      "train loss:0.06694723028196088\n",
      "train loss:0.11161404034040873\n",
      "train loss:0.06973074650485532\n",
      "train loss:0.07854631464471405\n",
      "train loss:0.030489395803976402\n",
      "train loss:0.03448137389482999\n",
      "train loss:0.08074507537034561\n",
      "train loss:0.08516068300843184\n",
      "train loss:0.05297124989135558\n",
      "train loss:0.020813545608452368\n",
      "train loss:0.0791658153277403\n",
      "train loss:0.04343620164232817\n",
      "train loss:0.032399789499394725\n",
      "train loss:0.04521081091495835\n",
      "train loss:0.1486454629235218\n",
      "train loss:0.029108264256069633\n",
      "train loss:0.033191209948706064\n",
      "train loss:0.045075899033163246\n",
      "train loss:0.029447873018147202\n",
      "train loss:0.03552276885085322\n",
      "train loss:0.043477086266057896\n",
      "train loss:0.04950650698610007\n",
      "train loss:0.11496226828395924\n",
      "train loss:0.07872354751322078\n",
      "train loss:0.03648519035902346\n",
      "train loss:0.04088172571887193\n",
      "train loss:0.14157501070447687\n",
      "train loss:0.09776037860704015\n",
      "train loss:0.08963416174805448\n",
      "train loss:0.0800306980791468\n",
      "train loss:0.06252361468610207\n",
      "train loss:0.029444537521372372\n",
      "train loss:0.12845710569191782\n",
      "train loss:0.07558391614951424\n",
      "train loss:0.02120981891452064\n",
      "train loss:0.06280065015913108\n",
      "train loss:0.046655742567030103\n",
      "train loss:0.05404666860566897\n",
      "train loss:0.05689031190199967\n",
      "train loss:0.07395890535247093\n",
      "train loss:0.065145817547332\n",
      "train loss:0.06606886147914272\n",
      "train loss:0.08621999260167408\n",
      "train loss:0.030902974712913624\n",
      "train loss:0.03847492724776609\n",
      "train loss:0.025320188600782414\n",
      "train loss:0.16292722459763762\n",
      "train loss:0.02711615665878693\n",
      "train loss:0.07737710851901826\n",
      "train loss:0.02194089046681561\n",
      "train loss:0.01778611526250752\n",
      "train loss:0.05342021310330403\n",
      "train loss:0.018137257790862886\n",
      "train loss:0.0746472171744428\n",
      "train loss:0.06178303777148354\n",
      "train loss:0.021662481825525354\n",
      "train loss:0.05105720903343466\n",
      "train loss:0.05093082169512032\n",
      "train loss:0.03192789027413911\n",
      "train loss:0.059928384939588006\n",
      "train loss:0.09256201665346181\n",
      "train loss:0.047606639165447294\n",
      "train loss:0.019130608366780256\n",
      "train loss:0.11623562392502394\n",
      "train loss:0.04296601793207811\n",
      "train loss:0.03219936845180888\n",
      "train loss:0.040621357843448884\n",
      "train loss:0.02478068425856864\n",
      "train loss:0.047414884285011255\n",
      "train loss:0.06667390251055337\n",
      "train loss:0.020186494742186367\n",
      "train loss:0.11017753237046783\n",
      "train loss:0.055409712602266\n",
      "train loss:0.029445037395923718\n",
      "train loss:0.05988392871525509\n",
      "train loss:0.043642977440765264\n",
      "train loss:0.006114015841554017\n",
      "train loss:0.04119201156296462\n",
      "train loss:0.039346179309779315\n",
      "train loss:0.09701120485963209\n",
      "train loss:0.04999387092014093\n",
      "train loss:0.05662853154684482\n",
      "train loss:0.054515904028005525\n",
      "train loss:0.06930463389656966\n",
      "train loss:0.04771353672941233\n",
      "train loss:0.050628621305456686\n",
      "train loss:0.1276989807286232\n",
      "train loss:0.07877516328894746\n",
      "train loss:0.04002028510183546\n",
      "train loss:0.04832668209036835\n",
      "train loss:0.03286082263645284\n",
      "train loss:0.026917045636841212\n",
      "train loss:0.04756668896065037\n",
      "train loss:0.02883291347331145\n",
      "train loss:0.08597801130085228\n",
      "train loss:0.021176845568348472\n",
      "train loss:0.16200401878836268\n",
      "train loss:0.027852782906404278\n",
      "train loss:0.046945310580285365\n",
      "train loss:0.016298272123263253\n",
      "train loss:0.097615927139344\n",
      "train loss:0.042819780391505906\n",
      "train loss:0.05395997500686787\n",
      "train loss:0.2035837194789508\n",
      "train loss:0.05672863154571824\n",
      "train loss:0.056449455732734834\n",
      "train loss:0.048772900265922174\n",
      "train loss:0.08849058895732276\n",
      "train loss:0.0602121615627518\n",
      "train loss:0.03526132625637293\n",
      "train loss:0.0341839603544437\n",
      "train loss:0.04806486292988518\n",
      "train loss:0.03166803890571868\n",
      "train loss:0.1717264450061707\n",
      "train loss:0.028695247092248678\n",
      "train loss:0.016927460720618407\n",
      "train loss:0.02982258629051525\n",
      "train loss:0.02348256369462977\n",
      "train loss:0.0814210174233105\n",
      "train loss:0.05195886053973694\n",
      "train loss:0.09316434855058091\n",
      "train loss:0.07540095703191967\n",
      "train loss:0.01808221754265834\n",
      "train loss:0.034216235319850366\n",
      "train loss:0.025199888269146785\n",
      "train loss:0.040339193761350305\n",
      "train loss:0.08669049973506991\n",
      "train loss:0.02455631313231437\n",
      "train loss:0.03594810045730233\n",
      "train loss:0.06942870203537792\n",
      "train loss:0.02810296368230238\n",
      "train loss:0.04833200903249369\n",
      "train loss:0.11190785440336494\n",
      "train loss:0.10043014631257371\n",
      "train loss:0.02224026837348936\n",
      "train loss:0.07513259245088882\n",
      "train loss:0.045053820499172524\n",
      "train loss:0.03352731694317117\n",
      "train loss:0.027012228219720574\n",
      "train loss:0.05686224893182012\n",
      "train loss:0.020321826195641074\n",
      "train loss:0.04608828980755011\n",
      "train loss:0.08544196156300597\n",
      "train loss:0.010805621669497058\n",
      "train loss:0.06221448451549374\n",
      "train loss:0.07204201763863485\n",
      "train loss:0.07275338206069715\n",
      "train loss:0.02574522294164476\n",
      "train loss:0.029400852987208\n",
      "train loss:0.020023448185309642\n",
      "train loss:0.015063223026306327\n",
      "train loss:0.0816439967062465\n",
      "train loss:0.08228912426078697\n",
      "train loss:0.057655530921262584\n",
      "train loss:0.04137220786328463\n",
      "train loss:0.1340400836391049\n",
      "train loss:0.02347411514689992\n",
      "train loss:0.023977201965223377\n",
      "train loss:0.029048360349444503\n",
      "train loss:0.14319273449670974\n",
      "train loss:0.05501876412093193\n",
      "train loss:0.031265325696828425\n",
      "train loss:0.0223044111708526\n",
      "train loss:0.04269368736716405\n",
      "train loss:0.07241348664739099\n",
      "train loss:0.051885683001744676\n",
      "train loss:0.035270736693198454\n",
      "train loss:0.08222872595848257\n",
      "train loss:0.019557578026076657\n",
      "train loss:0.019930573678002907\n",
      "train loss:0.02132245094277791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.06959907565923819\n",
      "train loss:0.026148160314749213\n",
      "train loss:0.020467264617377495\n",
      "train loss:0.006620815327260534\n",
      "train loss:0.04049074579429411\n",
      "train loss:0.02127003908571299\n",
      "train loss:0.0475173275743652\n",
      "train loss:0.16600607805545964\n",
      "train loss:0.022342258488338183\n",
      "train loss:0.06005549897907131\n",
      "train loss:0.08058890358443194\n",
      "train loss:0.18342570455252016\n",
      "train loss:0.04753034387241735\n",
      "train loss:0.04881758245487637\n",
      "train loss:0.06318366464596364\n",
      "train loss:0.06265570796145649\n",
      "train loss:0.020314293884442787\n",
      "train loss:0.13798378047854473\n",
      "train loss:0.1233900626063698\n",
      "train loss:0.04884571456345447\n",
      "train loss:0.00821221707430104\n",
      "train loss:0.04594318159272898\n",
      "train loss:0.03135111635831631\n",
      "train loss:0.040263453441796646\n",
      "train loss:0.058498385371771405\n",
      "train loss:0.06779501510207832\n",
      "train loss:0.05056249246570511\n",
      "train loss:0.02979566828132043\n",
      "train loss:0.03434589569998607\n",
      "train loss:0.08616169545784075\n",
      "train loss:0.03713968364728097\n",
      "train loss:0.044952329210031644\n",
      "train loss:0.043987524599143325\n",
      "train loss:0.07894753653090489\n",
      "train loss:0.054533570788804234\n",
      "train loss:0.0600780658231985\n",
      "train loss:0.06863005055537434\n",
      "train loss:0.028514205537064408\n",
      "train loss:0.03338885206670196\n",
      "train loss:0.06472697533270905\n",
      "train loss:0.052200142769626236\n",
      "train loss:0.037526152700698305\n",
      "train loss:0.036777893884126514\n",
      "train loss:0.1903055505264611\n",
      "train loss:0.04535007384202841\n",
      "train loss:0.057866005670853975\n",
      "train loss:0.11049085779082651\n",
      "train loss:0.021984937052236003\n",
      "train loss:0.07186486923020095\n",
      "train loss:0.06917469329606958\n",
      "train loss:0.05477375359358984\n",
      "train loss:0.037613950161900105\n",
      "train loss:0.02224839948131502\n",
      "train loss:0.06575915508067355\n",
      "train loss:0.030192064836513398\n",
      "train loss:0.04739547941150625\n",
      "train loss:0.015488610830714102\n",
      "train loss:0.045440872806651315\n",
      "train loss:0.04637760028764497\n",
      "train loss:0.031786871140089136\n",
      "train loss:0.04157630210150762\n",
      "train loss:0.058287109734742756\n",
      "train loss:0.09040442807603996\n",
      "train loss:0.03270271278321039\n",
      "train loss:0.014191260996851276\n",
      "train loss:0.025923353510312358\n",
      "train loss:0.02451398170766274\n",
      "train loss:0.031460990244956244\n",
      "train loss:0.09056914620773326\n",
      "train loss:0.010422443462776838\n",
      "train loss:0.18413574284539996\n",
      "train loss:0.03781492984422817\n",
      "train loss:0.01815573431418825\n",
      "train loss:0.08903888635021456\n",
      "train loss:0.04277575008242219\n",
      "train loss:0.03303110850041238\n",
      "train loss:0.018830899475865966\n",
      "train loss:0.031935943621296675\n",
      "train loss:0.04435294467714899\n",
      "train loss:0.03302352841556064\n",
      "train loss:0.07549613075973377\n",
      "train loss:0.016447943442039672\n",
      "train loss:0.20811770600817237\n",
      "train loss:0.046876097724559314\n",
      "train loss:0.040815157640461015\n",
      "train loss:0.04450601706643566\n",
      "train loss:0.08451787259968936\n",
      "train loss:0.06449078044922964\n",
      "train loss:0.07481867624937658\n",
      "train loss:0.046541239089240315\n",
      "train loss:0.044171576303877316\n",
      "train loss:0.04961312185591757\n",
      "train loss:0.05884021352286853\n",
      "train loss:0.08481872353780924\n",
      "train loss:0.09150493980998303\n",
      "train loss:0.06224919953478496\n",
      "train loss:0.052799983623060605\n",
      "train loss:0.0467327198660725\n",
      "train loss:0.12336048705411937\n",
      "train loss:0.07603208803238337\n",
      "train loss:0.03523869097811544\n",
      "train loss:0.107295282332034\n",
      "train loss:0.03356997814912195\n",
      "train loss:0.05930905454340369\n",
      "train loss:0.09110138627390711\n",
      "train loss:0.03337204989278553\n",
      "train loss:0.047924114790894035\n",
      "train loss:0.05224747909058819\n",
      "train loss:0.06640878239687426\n",
      "train loss:0.03342389671992347\n",
      "train loss:0.05624643756226898\n",
      "train loss:0.031078548806767713\n",
      "train loss:0.06759842364839068\n",
      "train loss:0.10521403496928464\n",
      "train loss:0.06801166297477272\n",
      "train loss:0.06091637356819796\n",
      "train loss:0.17059831735686648\n",
      "train loss:0.007863609411486306\n",
      "train loss:0.03697789733062143\n",
      "train loss:0.014630665536191039\n",
      "train loss:0.0695257063259093\n",
      "train loss:0.022024995940733857\n",
      "train loss:0.023341380311126668\n",
      "train loss:0.05795306780283206\n",
      "train loss:0.032568057546886034\n",
      "train loss:0.07250686650742497\n",
      "train loss:0.027590014742176868\n",
      "train loss:0.04699559197253872\n",
      "train loss:0.0268182467424193\n",
      "train loss:0.05463500556095558\n",
      "train loss:0.10244652596771678\n",
      "train loss:0.025428648955159675\n",
      "train loss:0.026328535441069608\n",
      "train loss:0.041830651230123424\n",
      "train loss:0.04383169570503123\n",
      "train loss:0.011389146555613641\n",
      "train loss:0.07275255209313608\n",
      "train loss:0.07983906440170778\n",
      "train loss:0.04490494846293487\n",
      "train loss:0.01787739127834818\n",
      "train loss:0.017672758626110386\n",
      "train loss:0.02501798772336932\n",
      "train loss:0.05614310677842406\n",
      "train loss:0.04381326914384645\n",
      "train loss:0.02951390886480874\n",
      "train loss:0.05531317734512262\n",
      "train loss:0.047842203948416415\n",
      "train loss:0.05198156326328794\n",
      "train loss:0.053695774760072264\n",
      "train loss:0.036816676134650936\n",
      "train loss:0.01801997039873748\n",
      "train loss:0.06835006858370497\n",
      "train loss:0.02462557606863458\n",
      "train loss:0.031200058686696997\n",
      "train loss:0.01731562776749659\n",
      "train loss:0.026159308851420544\n",
      "train loss:0.022270455592077073\n",
      "train loss:0.047689353573072364\n",
      "train loss:0.01364554183142071\n",
      "train loss:0.08144947889067591\n",
      "train loss:0.02645237570637864\n",
      "train loss:0.038666459171915815\n",
      "train loss:0.047497015929159066\n",
      "train loss:0.042201451070068854\n",
      "train loss:0.04760122850057006\n",
      "train loss:0.02231717668572475\n",
      "train loss:0.05142361610837656\n",
      "train loss:0.07396270651563666\n",
      "train loss:0.07333333308017481\n",
      "train loss:0.07948491417477087\n",
      "train loss:0.03982172245904132\n",
      "train loss:0.07062732786959161\n",
      "train loss:0.006384506920378069\n",
      "train loss:0.027984936968946977\n",
      "train loss:0.09206423432385655\n",
      "train loss:0.06260165754235772\n",
      "train loss:0.005881348669970376\n",
      "train loss:0.027841320389492642\n",
      "train loss:0.03405102521837142\n",
      "train loss:0.03206901193419259\n",
      "train loss:0.028860344105043713\n",
      "train loss:0.07901864012692618\n",
      "train loss:0.012094175463989863\n",
      "train loss:0.045807369310809705\n",
      "train loss:0.01632698352298586\n",
      "train loss:0.06908839409145122\n",
      "train loss:0.01809527905173885\n",
      "train loss:0.027908469783059\n",
      "train loss:0.016128238522806072\n",
      "train loss:0.0204134210261069\n",
      "train loss:0.06681678479545158\n",
      "train loss:0.023599695237282035\n",
      "train loss:0.020972894342454508\n",
      "train loss:0.08959159125654866\n",
      "train loss:0.01533957574305024\n",
      "train loss:0.010541717969788183\n",
      "train loss:0.05791810224324127\n",
      "train loss:0.11643699892973464\n",
      "train loss:0.005675735333747504\n",
      "train loss:0.0995233744866011\n",
      "train loss:0.11846796224564521\n",
      "train loss:0.05348203847098396\n",
      "train loss:0.012277698432117156\n",
      "train loss:0.03951983040880122\n",
      "train loss:0.048125912990032035\n",
      "train loss:0.13137576351895025\n",
      "train loss:0.02559188982707998\n",
      "=== epoch:4, train acc:0.984, test acc:0.978 ===\n",
      "train loss:0.09723961772593982\n",
      "train loss:0.04487076607864364\n",
      "train loss:0.018780019047825634\n",
      "train loss:0.02081773600446625\n",
      "train loss:0.04180354847643111\n",
      "train loss:0.022728490725196927\n",
      "train loss:0.02655624519573745\n",
      "train loss:0.01507153074619367\n",
      "train loss:0.06101479137905067\n",
      "train loss:0.04492496669812\n",
      "train loss:0.12089861468667239\n",
      "train loss:0.055609866754785645\n",
      "train loss:0.07827703230072491\n",
      "train loss:0.01689169386097364\n",
      "train loss:0.026722284110313188\n",
      "train loss:0.02905800167018278\n",
      "train loss:0.05211244926992053\n",
      "train loss:0.020379431593355687\n",
      "train loss:0.05311160292611047\n",
      "train loss:0.14740984312627028\n",
      "train loss:0.018836736030833343\n",
      "train loss:0.041256967868749925\n",
      "train loss:0.042736508753187546\n",
      "train loss:0.007309627557769965\n",
      "train loss:0.03106320774345521\n",
      "train loss:0.03474789475547563\n",
      "train loss:0.07965879071604272\n",
      "train loss:0.03458767429357447\n",
      "train loss:0.051851777423650075\n",
      "train loss:0.00804001252497917\n",
      "train loss:0.08561741059171277\n",
      "train loss:0.04424880269496773\n",
      "train loss:0.09268959211601899\n",
      "train loss:0.03002764459843089\n",
      "train loss:0.09111921585350288\n",
      "train loss:0.0338827913847187\n",
      "train loss:0.017078534186540994\n",
      "train loss:0.03565266589353505\n",
      "train loss:0.03728010242941649\n",
      "train loss:0.03115108528818582\n",
      "train loss:0.06759794648097904\n",
      "train loss:0.029871034036877176\n",
      "train loss:0.02366453312789626\n",
      "train loss:0.04613132564979644\n",
      "train loss:0.03185829879326778\n",
      "train loss:0.04270101995915098\n",
      "train loss:0.0484850740943679\n",
      "train loss:0.10650418700566164\n",
      "train loss:0.08253348435131704\n",
      "train loss:0.04329394664018435\n",
      "train loss:0.03940803477336938\n",
      "train loss:0.04553164723980223\n",
      "train loss:0.06979659188652075\n",
      "train loss:0.012561053495788777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0208171388291432\n",
      "train loss:0.032953472296596543\n",
      "train loss:0.09135907685546706\n",
      "train loss:0.00934027666775211\n",
      "train loss:0.03148621052916875\n",
      "train loss:0.015078565976090232\n",
      "train loss:0.054377720196247224\n",
      "train loss:0.02299616016078389\n",
      "train loss:0.05109002477598489\n",
      "train loss:0.014287724791199321\n",
      "train loss:0.028673236855199916\n",
      "train loss:0.0314699225787594\n",
      "train loss:0.02905351081523422\n",
      "train loss:0.03439348190199799\n",
      "train loss:0.05427653419038918\n",
      "train loss:0.018772239119155754\n",
      "train loss:0.06248013181459726\n",
      "train loss:0.005149379536514295\n",
      "train loss:0.009954031006532986\n",
      "train loss:0.06844669344544826\n",
      "train loss:0.016618750913350397\n",
      "train loss:0.01154708896774486\n",
      "train loss:0.07555583264193161\n",
      "train loss:0.014324106922139803\n",
      "train loss:0.07487371593456217\n",
      "train loss:0.09766588654526034\n",
      "train loss:0.018307999358524844\n",
      "train loss:0.021536694759518457\n",
      "train loss:0.0412100433159139\n",
      "train loss:0.0072531811296440395\n",
      "train loss:0.020884971297148808\n",
      "train loss:0.017061370504112987\n",
      "train loss:0.07846260899295907\n",
      "train loss:0.08542294015570334\n",
      "train loss:0.02609146906042508\n",
      "train loss:0.0230811101725774\n",
      "train loss:0.01640430232116048\n",
      "train loss:0.019428029994485972\n",
      "train loss:0.019937084634656058\n",
      "train loss:0.012839505859546376\n",
      "train loss:0.03510985583743237\n",
      "train loss:0.055303380498862575\n",
      "train loss:0.07545733021692876\n",
      "train loss:0.049884841090211116\n",
      "train loss:0.08847283354051476\n",
      "train loss:0.05293650601324801\n",
      "train loss:0.00981614544395429\n",
      "train loss:0.0051863632437746865\n",
      "train loss:0.028860197450599107\n",
      "train loss:0.037910524468497034\n",
      "train loss:0.06746584019280374\n",
      "train loss:0.03715475219667879\n",
      "train loss:0.07428369001698167\n",
      "train loss:0.054656880429948436\n",
      "train loss:0.017689305723736056\n",
      "train loss:0.03865483634648339\n",
      "train loss:0.060549180816865486\n",
      "train loss:0.0400110429435461\n",
      "train loss:0.03347479093345806\n",
      "train loss:0.00722093434821084\n",
      "train loss:0.09924925815067066\n",
      "train loss:0.027466142752192703\n",
      "train loss:0.012960157525457452\n",
      "train loss:0.028419050095732302\n",
      "train loss:0.11566024753291756\n",
      "train loss:0.027618458611263493\n",
      "train loss:0.021317545776086074\n",
      "train loss:0.03558902427151628\n",
      "train loss:0.01715989212158596\n",
      "train loss:0.04246224546741421\n",
      "train loss:0.050144049025394065\n",
      "train loss:0.006278656249281141\n",
      "train loss:0.1331802431774283\n",
      "train loss:0.025825848379791582\n",
      "train loss:0.09125616315542974\n",
      "train loss:0.016997023201413046\n",
      "train loss:0.013543651092448115\n",
      "train loss:0.0577503339047075\n",
      "train loss:0.01478478175685204\n",
      "train loss:0.01659896709950384\n",
      "train loss:0.07233701493001861\n",
      "train loss:0.035264580455774276\n",
      "train loss:0.028427613094155896\n",
      "train loss:0.03495289429663831\n",
      "train loss:0.04483659761606474\n",
      "train loss:0.040462898332679756\n",
      "train loss:0.05611799410807042\n",
      "train loss:0.03463754198683827\n",
      "train loss:0.12856847223793288\n",
      "train loss:0.039876128316062326\n",
      "train loss:0.023323994967343976\n",
      "train loss:0.03165503730993455\n",
      "train loss:0.030572071373959574\n",
      "train loss:0.05860544371549528\n",
      "train loss:0.1307955359235771\n",
      "train loss:0.025043784991138743\n",
      "train loss:0.07537254043097795\n",
      "train loss:0.019467394247477055\n",
      "train loss:0.10830499220007893\n",
      "train loss:0.027012130731666827\n",
      "train loss:0.05464875677407794\n",
      "train loss:0.060825973363617496\n",
      "train loss:0.04946671168059118\n",
      "train loss:0.01907910827541248\n",
      "train loss:0.07731522119765369\n",
      "train loss:0.007855281716382244\n",
      "train loss:0.05772794905269329\n",
      "train loss:0.015791163892909327\n",
      "train loss:0.02590226288022392\n",
      "train loss:0.03817688149166109\n",
      "train loss:0.02529356324813437\n",
      "train loss:0.012792958422804321\n",
      "train loss:0.02160991878510602\n",
      "train loss:0.055350733823539035\n",
      "train loss:0.011277752382201401\n",
      "train loss:0.010919393974141474\n",
      "train loss:0.09319666991354192\n",
      "train loss:0.009144050893077673\n",
      "train loss:0.03714910397645978\n",
      "train loss:0.03322920374329459\n",
      "train loss:0.018483323634590713\n",
      "train loss:0.05084406294358232\n",
      "train loss:0.05417115440537039\n",
      "train loss:0.022494640101738694\n",
      "train loss:0.056559491269289754\n",
      "train loss:0.06087219399116802\n",
      "train loss:0.06393908082462728\n",
      "train loss:0.01849364835692412\n",
      "train loss:0.01525547858147776\n",
      "train loss:0.01236987851822252\n",
      "train loss:0.01100864111885238\n",
      "train loss:0.016917514808651085\n",
      "train loss:0.015633998665800286\n",
      "train loss:0.016329654305753935\n",
      "train loss:0.09814241226331155\n",
      "train loss:0.04110692458076309\n",
      "train loss:0.08668562498446435\n",
      "train loss:0.027879678137642246\n",
      "train loss:0.02856437198278825\n",
      "train loss:0.01693712505718104\n",
      "train loss:0.0054069123980088505\n",
      "train loss:0.012245764295667141\n",
      "train loss:0.013182118395548986\n",
      "train loss:0.013954321581383062\n",
      "train loss:0.04766987825393185\n",
      "train loss:0.12330761846649425\n",
      "train loss:0.06496434354720726\n",
      "train loss:0.016418955284044474\n",
      "train loss:0.031670899781671175\n",
      "train loss:0.008300847813127949\n",
      "train loss:0.04327419952431516\n",
      "train loss:0.0870445766794505\n",
      "train loss:0.014372492585488686\n",
      "train loss:0.03895974171217813\n",
      "train loss:0.007014127373586549\n",
      "train loss:0.05577189766787237\n",
      "train loss:0.013597483165387576\n",
      "train loss:0.018455696187033306\n",
      "train loss:0.013102948868658471\n",
      "train loss:0.026881336768427883\n",
      "train loss:0.024830419952269894\n",
      "train loss:0.01126074790329337\n",
      "train loss:0.045130063755332815\n",
      "train loss:0.07954790009165123\n",
      "train loss:0.048342842734185494\n",
      "train loss:0.015270596798905993\n",
      "train loss:0.011242302565756526\n",
      "train loss:0.03986412700290358\n",
      "train loss:0.04800510501444008\n",
      "train loss:0.01896685591796886\n",
      "train loss:0.03877913907168823\n",
      "train loss:0.020318927196652438\n",
      "train loss:0.028386693465055117\n",
      "train loss:0.013255296076563361\n",
      "train loss:0.030617486920587814\n",
      "train loss:0.04836923441460741\n",
      "train loss:0.01646903054784338\n",
      "train loss:0.007126706272391991\n",
      "train loss:0.013384924464524321\n",
      "train loss:0.009055277066723672\n",
      "train loss:0.023240497007545602\n",
      "train loss:0.0751402489696139\n",
      "train loss:0.01306901075011838\n",
      "train loss:0.029594125410150243\n",
      "train loss:0.06468351938968525\n",
      "train loss:0.04069722705764693\n",
      "train loss:0.06429351312913352\n",
      "train loss:0.02651807308485687\n",
      "train loss:0.04977562709840137\n",
      "train loss:0.046530143955767996\n",
      "train loss:0.023170539875500044\n",
      "train loss:0.01438848398466465\n",
      "train loss:0.018490985181789273\n",
      "train loss:0.02457493951947492\n",
      "train loss:0.03739116810026667\n",
      "train loss:0.019767908722654482\n",
      "train loss:0.04301171456348495\n",
      "train loss:0.016404327462861617\n",
      "train loss:0.04500400312036698\n",
      "train loss:0.03472342069197409\n",
      "train loss:0.12129257403467769\n",
      "train loss:0.09837909997827814\n",
      "train loss:0.018428293349940762\n",
      "train loss:0.016514396100379945\n",
      "train loss:0.016185050378625186\n",
      "train loss:0.05068475446669514\n",
      "train loss:0.014722243591513983\n",
      "train loss:0.045085007425198255\n",
      "train loss:0.012176646230799042\n",
      "train loss:0.007272044409731191\n",
      "train loss:0.07336039976480793\n",
      "train loss:0.008677925143667841\n",
      "train loss:0.04759719433383259\n",
      "train loss:0.05435966297733358\n",
      "train loss:0.03585218056444027\n",
      "train loss:0.03389837969809803\n",
      "train loss:0.030490530915286085\n",
      "train loss:0.019143662454234545\n",
      "train loss:0.018863072936213105\n",
      "train loss:0.06383322185121601\n",
      "train loss:0.04206759672129631\n",
      "train loss:0.07485606407464065\n",
      "train loss:0.03556847783484487\n",
      "train loss:0.07408812077985791\n",
      "train loss:0.03093940892948439\n",
      "train loss:0.04694745069135631\n",
      "train loss:0.036840346929832644\n",
      "train loss:0.06133574447011126\n",
      "train loss:0.036159853921462204\n",
      "train loss:0.08824921350347559\n",
      "train loss:0.03247589789299186\n",
      "train loss:0.062281934294810305\n",
      "train loss:0.13804994790435715\n",
      "train loss:0.01665343141084033\n",
      "train loss:0.012579241661063986\n",
      "train loss:0.01367572701842003\n",
      "train loss:0.04006137761720287\n",
      "train loss:0.05974862565028054\n",
      "train loss:0.02823949196428992\n",
      "train loss:0.0573215194277428\n",
      "train loss:0.033724445420553376\n",
      "train loss:0.10641165374931484\n",
      "train loss:0.052313978778954266\n",
      "train loss:0.09915648283236607\n",
      "train loss:0.04982638782692124\n",
      "train loss:0.024016962911501207\n",
      "train loss:0.09403697504708264\n",
      "train loss:0.09664068361067284\n",
      "train loss:0.031930777900091835\n",
      "train loss:0.07210835735018391\n",
      "train loss:0.0208148997763271\n",
      "train loss:0.04732728843238691\n",
      "train loss:0.10514047293267981\n",
      "train loss:0.009049613062453441\n",
      "train loss:0.024408184551737135\n",
      "train loss:0.022359028109569325\n",
      "train loss:0.037968930486078786\n",
      "train loss:0.04187319736552283\n",
      "train loss:0.02296891579065671\n",
      "train loss:0.06315909963731539\n",
      "train loss:0.041646912482633074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.009533335909991425\n",
      "train loss:0.09893606028833889\n",
      "train loss:0.03741496624795681\n",
      "train loss:0.040395172989143795\n",
      "train loss:0.026478201662666567\n",
      "train loss:0.015494769315197247\n",
      "train loss:0.026767148914370105\n",
      "train loss:0.03970863244670078\n",
      "train loss:0.0070521151335153145\n",
      "train loss:0.07294882102355076\n",
      "train loss:0.062402717975725726\n",
      "train loss:0.04171552058217104\n",
      "train loss:0.0100107817259107\n",
      "train loss:0.00868547055646774\n",
      "train loss:0.0226665903483229\n",
      "train loss:0.06544889397281242\n",
      "train loss:0.03214523418348616\n",
      "train loss:0.038002030085193435\n",
      "train loss:0.018465404804803427\n",
      "train loss:0.021353018707526945\n",
      "train loss:0.06009143617372314\n",
      "train loss:0.021493145932609627\n",
      "train loss:0.04254775758230786\n",
      "train loss:0.016609107428644222\n",
      "train loss:0.04032994862590591\n",
      "train loss:0.01967960098831186\n",
      "train loss:0.05261016490618792\n",
      "train loss:0.01511548187155426\n",
      "train loss:0.007269364795056087\n",
      "train loss:0.03603299721514243\n",
      "train loss:0.07225612916149253\n",
      "train loss:0.01944123771183274\n",
      "train loss:0.044224138891042014\n",
      "train loss:0.021267785167936624\n",
      "train loss:0.020491775062153415\n",
      "train loss:0.012040496256860378\n",
      "train loss:0.046588369278806746\n",
      "train loss:0.043670595481714064\n",
      "train loss:0.023796463884725592\n",
      "train loss:0.02933637346626581\n",
      "train loss:0.02915038926400407\n",
      "train loss:0.07356345182220364\n",
      "train loss:0.01376707314532125\n",
      "train loss:0.044714364575023285\n",
      "train loss:0.038992888915948105\n",
      "train loss:0.02127928709151004\n",
      "train loss:0.023688549394742582\n",
      "train loss:0.02742713323062317\n",
      "train loss:0.042162488749071254\n",
      "train loss:0.09942968405752364\n",
      "train loss:0.05215951554553445\n",
      "train loss:0.03967118224068037\n",
      "train loss:0.012622675476805335\n",
      "train loss:0.04603406530869285\n",
      "train loss:0.04452764199076719\n",
      "train loss:0.05411467037084699\n",
      "train loss:0.05562216000800684\n",
      "train loss:0.06051491509108793\n",
      "train loss:0.034792007101373604\n",
      "train loss:0.03825349643832332\n",
      "train loss:0.01418771680960424\n",
      "train loss:0.024948050930157888\n",
      "train loss:0.03187860174982161\n",
      "train loss:0.1001980250755777\n",
      "train loss:0.00811777162524068\n",
      "train loss:0.05892484551393401\n",
      "train loss:0.03224153743533168\n",
      "train loss:0.010235654545814672\n",
      "train loss:0.03412531177556612\n",
      "train loss:0.04147751238171056\n",
      "train loss:0.030560177320310376\n",
      "train loss:0.015575070648722737\n",
      "train loss:0.022309139487881494\n",
      "train loss:0.013908741215052152\n",
      "train loss:0.03882590156866712\n",
      "train loss:0.008090170457990167\n",
      "train loss:0.01683363942190145\n",
      "train loss:0.02518685043028676\n",
      "train loss:0.01705665238254463\n",
      "train loss:0.13072548848362922\n",
      "train loss:0.016889448217467323\n",
      "train loss:0.023105363244983197\n",
      "train loss:0.06907758377221182\n",
      "train loss:0.01186126663528411\n",
      "train loss:0.025781531775442665\n",
      "train loss:0.10722609185757809\n",
      "train loss:0.0232518805382861\n",
      "train loss:0.08132626744237659\n",
      "train loss:0.023294103158761067\n",
      "train loss:0.012814177883930829\n",
      "train loss:0.015350593875991435\n",
      "train loss:0.08434248489273934\n",
      "train loss:0.03920021272720511\n",
      "train loss:0.034453808942736475\n",
      "train loss:0.04652346503637536\n",
      "train loss:0.010687113781165658\n",
      "train loss:0.007340879382761818\n",
      "train loss:0.03914552637460228\n",
      "train loss:0.01889870317991819\n",
      "train loss:0.08743644542134053\n",
      "train loss:0.013420966435615527\n",
      "train loss:0.031009578557050315\n",
      "train loss:0.007223134575243846\n",
      "train loss:0.09612937916283217\n",
      "train loss:0.04135988046890123\n",
      "train loss:0.029750371671794017\n",
      "train loss:0.030472086818167533\n",
      "train loss:0.05085357966584116\n",
      "train loss:0.03521759870095008\n",
      "train loss:0.01944385272815062\n",
      "train loss:0.038741789776713824\n",
      "train loss:0.030378504162352448\n",
      "train loss:0.04832942670886694\n",
      "train loss:0.02644732708064336\n",
      "train loss:0.029596763493510446\n",
      "train loss:0.04850369245663412\n",
      "train loss:0.062213045273434055\n",
      "train loss:0.02779383110367138\n",
      "train loss:0.02321229195501636\n",
      "train loss:0.07014651240132753\n",
      "train loss:0.044439990662689374\n",
      "train loss:0.015582657258324265\n",
      "train loss:0.029815658354687073\n",
      "train loss:0.07240569751686998\n",
      "train loss:0.05109869555708697\n",
      "train loss:0.027590508557762794\n",
      "train loss:0.035985369597754426\n",
      "train loss:0.0382947090988747\n",
      "train loss:0.05536861286572092\n",
      "train loss:0.05041533034767923\n",
      "train loss:0.012680975087609648\n",
      "train loss:0.00836246767586994\n",
      "train loss:0.04270592938077897\n",
      "train loss:0.01075859704283403\n",
      "train loss:0.011653256827603035\n",
      "train loss:0.01435166790032345\n",
      "train loss:0.07414425996089989\n",
      "train loss:0.03441880604793502\n",
      "train loss:0.01158473915993628\n",
      "train loss:0.04563412686944145\n",
      "train loss:0.052686871276893116\n",
      "train loss:0.06599090697932979\n",
      "train loss:0.029012605839015562\n",
      "train loss:0.02927582665236797\n",
      "train loss:0.031208485756271866\n",
      "train loss:0.018073383585581894\n",
      "train loss:0.03896289503632923\n",
      "train loss:0.0400169928021573\n",
      "train loss:0.0057820596129368705\n",
      "train loss:0.17736718652669448\n",
      "train loss:0.00637090405016962\n",
      "train loss:0.016241125282114004\n",
      "train loss:0.020548462276074958\n",
      "train loss:0.011532502594561799\n",
      "train loss:0.026059570877191605\n",
      "train loss:0.020400583480779513\n",
      "train loss:0.05222731004231674\n",
      "train loss:0.04528244447551657\n",
      "train loss:0.017492036714700118\n",
      "train loss:0.010478107402846491\n",
      "train loss:0.029428317028669233\n",
      "train loss:0.014884417371575089\n",
      "train loss:0.03881062549529453\n",
      "train loss:0.018815584445156165\n",
      "train loss:0.010321826105013716\n",
      "train loss:0.030997198469714323\n",
      "train loss:0.02361367540505801\n",
      "train loss:0.030370827807692857\n",
      "train loss:0.005550443291562793\n",
      "train loss:0.014239176002059886\n",
      "train loss:0.06440552585921006\n",
      "train loss:0.023915344618741164\n",
      "train loss:0.05448076100825381\n",
      "train loss:0.019126352771754326\n",
      "train loss:0.023204006645428706\n",
      "train loss:0.011460791769978794\n",
      "train loss:0.01627769495222034\n",
      "train loss:0.07957431003311209\n",
      "train loss:0.02436261187559247\n",
      "train loss:0.03957545766613867\n",
      "train loss:0.02217908062240952\n",
      "train loss:0.02059272630099565\n",
      "train loss:0.03881313212663391\n",
      "train loss:0.05930625417533276\n",
      "train loss:0.04050343445416641\n",
      "train loss:0.046112967116757\n",
      "train loss:0.014101696346656156\n",
      "train loss:0.05607051128314669\n",
      "train loss:0.04534839334284446\n",
      "train loss:0.015098866958013792\n",
      "train loss:0.035680486669640445\n",
      "train loss:0.020527798131740077\n",
      "train loss:0.019700750035035578\n",
      "train loss:0.02770706845198079\n",
      "train loss:0.024134720424613988\n",
      "train loss:0.03898290832348999\n",
      "train loss:0.03032722674840688\n",
      "train loss:0.009849616611739566\n",
      "train loss:0.005417067042385179\n",
      "train loss:0.04386889570422424\n",
      "train loss:0.02149439283922094\n",
      "train loss:0.09978213978082907\n",
      "train loss:0.020047992903669255\n",
      "train loss:0.07658893821052087\n",
      "train loss:0.006866641941966432\n",
      "train loss:0.015111286450676952\n",
      "train loss:0.06689256955797752\n",
      "train loss:0.016723025692543508\n",
      "train loss:0.009274017381612371\n",
      "train loss:0.021408614690552646\n",
      "train loss:0.027650621434280164\n",
      "train loss:0.01780379063406539\n",
      "train loss:0.021619373300823398\n",
      "train loss:0.03800279206309026\n",
      "train loss:0.011253665793494714\n",
      "train loss:0.06552970207835732\n",
      "train loss:0.017380352036270297\n",
      "train loss:0.026977995745784966\n",
      "train loss:0.12517010094549874\n",
      "train loss:0.04978174475179376\n",
      "train loss:0.03213381838997865\n",
      "train loss:0.019025855986794484\n",
      "train loss:0.03237816599270972\n",
      "train loss:0.06370389968971675\n",
      "train loss:0.0132303081852152\n",
      "train loss:0.03776468820044309\n",
      "train loss:0.00891204458013873\n",
      "train loss:0.05332440995774828\n",
      "train loss:0.024013913903073975\n",
      "train loss:0.004987624816116829\n",
      "train loss:0.018501740361872573\n",
      "train loss:0.02708021795953346\n",
      "train loss:0.02567042513726394\n",
      "train loss:0.022574653174882373\n",
      "train loss:0.01435376747190042\n",
      "train loss:0.017854891965851837\n",
      "train loss:0.02698292006677727\n",
      "train loss:0.0065053439203692975\n",
      "train loss:0.024685283076945425\n",
      "train loss:0.027122881916295763\n",
      "train loss:0.06663385039081908\n",
      "train loss:0.020568589726432728\n",
      "train loss:0.006784396012979353\n",
      "train loss:0.01840240271324426\n",
      "train loss:0.004038745032894739\n",
      "train loss:0.006611859364617429\n",
      "train loss:0.007022151039837553\n",
      "train loss:0.0072007779240972234\n",
      "train loss:0.1388203054416918\n",
      "train loss:0.02937758787570748\n",
      "train loss:0.05736680784611044\n",
      "train loss:0.022469952967956255\n",
      "train loss:0.027306809468341898\n",
      "train loss:0.025517664781574304\n",
      "train loss:0.011249853695785446\n",
      "train loss:0.06094776908988679\n",
      "train loss:0.004895091258071937\n",
      "train loss:0.011443548918608833\n",
      "train loss:0.0022663732786113344\n",
      "train loss:0.02513856526101131\n",
      "train loss:0.01735246533881725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.04213087788364597\n",
      "train loss:0.04571652639924162\n",
      "train loss:0.01903678052003905\n",
      "train loss:0.01815706156592059\n",
      "train loss:0.1316875837320773\n",
      "train loss:0.010613260179244934\n",
      "train loss:0.00837352071648604\n",
      "train loss:0.020948747139315733\n",
      "train loss:0.03869586880151555\n",
      "train loss:0.05093933008582325\n",
      "train loss:0.03567796362342891\n",
      "train loss:0.04179610189925075\n",
      "train loss:0.010549956357504492\n",
      "train loss:0.02623254402189549\n",
      "train loss:0.021569903233723143\n",
      "train loss:0.005435923613484105\n",
      "train loss:0.03578993370850775\n",
      "train loss:0.061703220129876196\n",
      "train loss:0.01585301615764183\n",
      "train loss:0.02463459063675887\n",
      "train loss:0.004390653524315618\n",
      "train loss:0.00598060750338206\n",
      "train loss:0.04582500523197327\n",
      "train loss:0.00768854308797054\n",
      "=== epoch:5, train acc:0.989, test acc:0.989 ===\n",
      "train loss:0.022207249556191293\n",
      "train loss:0.011574035933640495\n",
      "train loss:0.010137469686600094\n",
      "train loss:0.006643604608544321\n",
      "train loss:0.006829035613523048\n",
      "train loss:0.007052306241799078\n",
      "train loss:0.027548053004874923\n",
      "train loss:0.021231690790408212\n",
      "train loss:0.03958961215991757\n",
      "train loss:0.013613634184931549\n",
      "train loss:0.003398629199461967\n",
      "train loss:0.04755877386715415\n",
      "train loss:0.008836876676402858\n",
      "train loss:0.01147037937988176\n",
      "train loss:0.03671776717057698\n",
      "train loss:0.04678285747611472\n",
      "train loss:0.02863411492854907\n",
      "train loss:0.01003730672403113\n",
      "train loss:0.021589518387748346\n",
      "train loss:0.0071130906317039885\n",
      "train loss:0.061423273101933615\n",
      "train loss:0.023050970320395606\n",
      "train loss:0.04495773864688572\n",
      "train loss:0.018285920939841188\n",
      "train loss:0.019535100422189512\n",
      "train loss:0.014473241226469946\n",
      "train loss:0.02950420261997765\n",
      "train loss:0.0294566969622614\n",
      "train loss:0.04338598101997516\n",
      "train loss:0.00779892538946145\n",
      "train loss:0.019603005851220382\n",
      "train loss:0.007568564432198372\n",
      "train loss:0.015054416567998162\n",
      "train loss:0.02139564877852269\n",
      "train loss:0.024974010261900482\n",
      "train loss:0.047272930440053554\n",
      "train loss:0.06751393553026411\n",
      "train loss:0.05479950124760955\n",
      "train loss:0.0860773652938723\n",
      "train loss:0.030266373010039326\n",
      "train loss:0.045572359786913846\n",
      "train loss:0.0199214043120857\n",
      "train loss:0.13721272421346428\n",
      "train loss:0.02324143967884864\n",
      "train loss:0.043119049418373356\n",
      "train loss:0.04081146093770923\n",
      "train loss:0.03631420229215208\n",
      "train loss:0.01983660016769014\n",
      "train loss:0.025387043870250606\n",
      "train loss:0.048423814724774694\n",
      "train loss:0.014330114383484326\n",
      "train loss:0.030633043871330753\n",
      "train loss:0.06026638557252092\n",
      "train loss:0.07571473981470345\n",
      "train loss:0.021020635377247996\n",
      "train loss:0.06387250241491678\n",
      "train loss:0.0948789613952679\n",
      "train loss:0.04151453041358285\n",
      "train loss:0.017770028832357287\n",
      "train loss:0.02879978760190571\n",
      "train loss:0.042829902367598274\n",
      "train loss:0.008078969476166013\n",
      "train loss:0.06267474469759585\n",
      "train loss:0.013931964172720503\n",
      "train loss:0.050831548105959114\n",
      "train loss:0.031088372136388032\n",
      "train loss:0.02366574120577534\n",
      "train loss:0.028104822804803867\n",
      "train loss:0.043575917818038795\n",
      "train loss:0.04618341229408572\n",
      "train loss:0.07434927103835369\n",
      "train loss:0.027074934910982652\n",
      "train loss:0.08304251101863137\n",
      "train loss:0.02125568324945212\n",
      "train loss:0.064563484753421\n",
      "train loss:0.02232570352090457\n",
      "train loss:0.04468899396705454\n",
      "train loss:0.034345617167590124\n",
      "train loss:0.0134618539792978\n",
      "train loss:0.009133252492602904\n",
      "train loss:0.017362012530855432\n",
      "train loss:0.06799402237470278\n",
      "train loss:0.019779040750649002\n",
      "train loss:0.008511595945074994\n",
      "train loss:0.044603215515730564\n",
      "train loss:0.04434840151829695\n",
      "train loss:0.07756904744043792\n",
      "train loss:0.018777848346515122\n",
      "train loss:0.043081928055368694\n",
      "train loss:0.017856714984966705\n",
      "train loss:0.06484242832547832\n",
      "train loss:0.04213815993040061\n",
      "train loss:0.045057497304941616\n",
      "train loss:0.03265001430252391\n",
      "train loss:0.0661659629360171\n",
      "train loss:0.05745037991006043\n",
      "train loss:0.06792736361097086\n",
      "train loss:0.015084923740115138\n",
      "train loss:0.0965594066886783\n",
      "train loss:0.054588017310109004\n",
      "train loss:0.02354212335417487\n",
      "train loss:0.06539105834332676\n",
      "train loss:0.019183178808211555\n",
      "train loss:0.016606329755455226\n",
      "train loss:0.04670850572650458\n",
      "train loss:0.027333465350196668\n",
      "train loss:0.06276025814256082\n",
      "train loss:0.045446069659015036\n",
      "train loss:0.07276941463315972\n",
      "train loss:0.03506319732235735\n",
      "train loss:0.03630393048854861\n",
      "train loss:0.024172190913248754\n",
      "train loss:0.03181552243450625\n",
      "train loss:0.03489941190359038\n",
      "train loss:0.00939142109179474\n",
      "train loss:0.04123032956164144\n",
      "train loss:0.006532842054834105\n",
      "train loss:0.012256727876171849\n",
      "train loss:0.017816129805417056\n",
      "train loss:0.006774526088443629\n",
      "train loss:0.01314850160925338\n",
      "train loss:0.024252101191951998\n",
      "train loss:0.030261428362004236\n",
      "train loss:0.006454690305720098\n",
      "train loss:0.01780680744538006\n",
      "train loss:0.0172949788708554\n",
      "train loss:0.014853168141757975\n",
      "train loss:0.03333523504882484\n",
      "train loss:0.015570345243705607\n",
      "train loss:0.012185068876306732\n",
      "train loss:0.023861001372794677\n",
      "train loss:0.02701273737307838\n",
      "train loss:0.00637706960987864\n",
      "train loss:0.01862602458756342\n",
      "train loss:0.01634843418280502\n",
      "train loss:0.010026155342973676\n",
      "train loss:0.02756554623027939\n",
      "train loss:0.10313518181892078\n",
      "train loss:0.03564896198151266\n",
      "train loss:0.03578395102088451\n",
      "train loss:0.00904412656157042\n",
      "train loss:0.006127356548761804\n",
      "train loss:0.01641063399105076\n",
      "train loss:0.009049094285829194\n",
      "train loss:0.04486625586304085\n",
      "train loss:0.014448456860618701\n",
      "train loss:0.004342141369389147\n",
      "train loss:0.011559964485740233\n",
      "train loss:0.023242073589003702\n",
      "train loss:0.11887073753991943\n",
      "train loss:0.01638294753228692\n",
      "train loss:0.01788479276101868\n",
      "train loss:0.035506760563225714\n",
      "train loss:0.012916567398360921\n",
      "train loss:0.03708817813466077\n",
      "train loss:0.010389787132389045\n",
      "train loss:0.05354531910975435\n",
      "train loss:0.038012067777809716\n",
      "train loss:0.02714688766799372\n",
      "train loss:0.05109918755522638\n",
      "train loss:0.00681527247735311\n",
      "train loss:0.023985957540608217\n",
      "train loss:0.1275237329881415\n",
      "train loss:0.021669447414277333\n",
      "train loss:0.009199894941904917\n",
      "train loss:0.019516729875733874\n",
      "train loss:0.03583926179782077\n",
      "train loss:0.015889638523863655\n",
      "train loss:0.01564091788488853\n",
      "train loss:0.009745663596015721\n",
      "train loss:0.015480233594037577\n",
      "train loss:0.004635542704131807\n",
      "train loss:0.09548926315340613\n",
      "train loss:0.017760810228382704\n",
      "train loss:0.01622044135154624\n",
      "train loss:0.005634673295036034\n",
      "train loss:0.015813116056970288\n",
      "train loss:0.013594145793652614\n",
      "train loss:0.06483658950001493\n",
      "train loss:0.03850331468561822\n",
      "train loss:0.022136961704311564\n",
      "train loss:0.035744781398315766\n",
      "train loss:0.02924180726740385\n",
      "train loss:0.013298729895254513\n",
      "train loss:0.005198361497953244\n",
      "train loss:0.044562651041654494\n",
      "train loss:0.006917804273295046\n",
      "train loss:0.057725793796835595\n",
      "train loss:0.029208541445309277\n",
      "train loss:0.0935947604969163\n",
      "train loss:0.014093851951066444\n",
      "train loss:0.006166842056743004\n",
      "train loss:0.013883246438941675\n",
      "train loss:0.013281468168278887\n",
      "train loss:0.034664650596489405\n",
      "train loss:0.022603207138338865\n",
      "train loss:0.012961279471569515\n",
      "train loss:0.01915230196822087\n",
      "train loss:0.003921896104830068\n",
      "train loss:0.011540720232104578\n",
      "train loss:0.01931327969535585\n",
      "train loss:0.02228284455681168\n",
      "train loss:0.024704816330208882\n",
      "train loss:0.002959987857912324\n",
      "train loss:0.03278592920224668\n",
      "train loss:0.013204912871945383\n",
      "train loss:0.008892130624403938\n",
      "train loss:0.14004504306647061\n",
      "train loss:0.039976544599472946\n",
      "train loss:0.009193044534419161\n",
      "train loss:0.015421955365608775\n",
      "train loss:0.021220188026450967\n",
      "train loss:0.00651210923463258\n",
      "train loss:0.017559797632174756\n",
      "train loss:0.009497427678021602\n",
      "train loss:0.03684072520246773\n",
      "train loss:0.012528670270995777\n",
      "train loss:0.009131778631663255\n",
      "train loss:0.013354005425789888\n",
      "train loss:0.041218349014302366\n",
      "train loss:0.016058882748665813\n",
      "train loss:0.06391199058395332\n",
      "train loss:0.02093899378579239\n",
      "train loss:0.027577354558128396\n",
      "train loss:0.0126956487432065\n",
      "train loss:0.01629030479595548\n",
      "train loss:0.015636207434456102\n",
      "train loss:0.020169423003913837\n",
      "train loss:0.023831000375892114\n",
      "train loss:0.025899912999807456\n",
      "train loss:0.01691470662050464\n",
      "train loss:0.005567100368651612\n",
      "train loss:0.061744995776076624\n",
      "train loss:0.062202886145865464\n",
      "train loss:0.018158821835664328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.05015707068944227\n",
      "train loss:0.011914721412982152\n",
      "train loss:0.010949344382133918\n",
      "train loss:0.009356851483828182\n",
      "train loss:0.05305318823194129\n",
      "train loss:0.0017137583914467636\n",
      "train loss:0.025796062330175604\n",
      "train loss:0.02261285876990338\n",
      "train loss:0.009729473488106617\n",
      "train loss:0.014302871253421317\n",
      "train loss:0.040131849282606026\n",
      "train loss:0.010654905650281946\n",
      "train loss:0.02584197838201687\n",
      "train loss:0.007854953859223815\n",
      "train loss:0.004692067611965401\n",
      "train loss:0.03967727176862187\n",
      "train loss:0.048249367721939586\n",
      "train loss:0.0192813922115683\n",
      "train loss:0.037222282483108576\n",
      "train loss:0.04102468577136805\n",
      "train loss:0.10042827477177178\n",
      "train loss:0.09992707779527525\n",
      "train loss:0.009057994299553462\n",
      "train loss:0.03671601075878894\n",
      "train loss:0.008630969785837021\n",
      "train loss:0.03006383304315742\n",
      "train loss:0.04643431400684749\n",
      "train loss:0.01749329014267464\n",
      "train loss:0.011149742222295724\n",
      "train loss:0.022892754448545896\n",
      "train loss:0.06757339866773997\n",
      "train loss:0.02153008823847313\n",
      "train loss:0.009908386286958977\n",
      "train loss:0.015885163937601116\n",
      "train loss:0.010224282781717234\n",
      "train loss:0.01804172603105479\n",
      "train loss:0.02260703840465412\n",
      "train loss:0.09982803910636065\n",
      "train loss:0.03462061713802846\n",
      "train loss:0.011343846389385454\n",
      "train loss:0.013804691777343454\n",
      "train loss:0.016042210893325356\n",
      "train loss:0.03450750670408533\n",
      "train loss:0.0732898535723639\n",
      "train loss:0.01580152415991324\n",
      "train loss:0.007718742549726175\n",
      "train loss:0.026749975330163836\n",
      "train loss:0.010005240599842282\n",
      "train loss:0.027483583735023433\n",
      "train loss:0.010692227473416156\n",
      "train loss:0.05895114145281917\n",
      "train loss:0.014738506072119285\n",
      "train loss:0.03481334745083398\n",
      "train loss:0.04261380929931269\n",
      "train loss:0.07794936393323681\n",
      "train loss:0.01598479798300767\n",
      "train loss:0.05540411073077935\n",
      "train loss:0.009440972926371203\n",
      "train loss:0.011787444280868462\n",
      "train loss:0.019532178758115443\n",
      "train loss:0.003368641540370141\n",
      "train loss:0.022203120716870833\n",
      "train loss:0.012708899743475912\n",
      "train loss:0.0493534486103762\n",
      "train loss:0.005303202959249783\n",
      "train loss:0.010370942530914118\n",
      "train loss:0.005094542837720151\n",
      "train loss:0.0077048331802738465\n",
      "train loss:0.05377900764981845\n",
      "train loss:0.007377132604352933\n",
      "train loss:0.01529398236843862\n",
      "train loss:0.019972606079902144\n",
      "train loss:0.06325342398832469\n",
      "train loss:0.032206166939226614\n",
      "train loss:0.011518762263618452\n",
      "train loss:0.05526806033684993\n",
      "train loss:0.01397092933406241\n",
      "train loss:0.005859980311023119\n",
      "train loss:0.0288405645539578\n",
      "train loss:0.04588860793230743\n",
      "train loss:0.009368899493835996\n",
      "train loss:0.016180455250098626\n",
      "train loss:0.04668129175132632\n",
      "train loss:0.10802417082221845\n",
      "train loss:0.028462489134128157\n",
      "train loss:0.1045549272321916\n",
      "train loss:0.01974396890913674\n",
      "train loss:0.04664785425082118\n",
      "train loss:0.04451537680322577\n",
      "train loss:0.043206120467707186\n",
      "train loss:0.11024990154757272\n",
      "train loss:0.17353305595645258\n",
      "train loss:0.015672682416074703\n",
      "train loss:0.017852599323796528\n",
      "train loss:0.0176345051863434\n",
      "train loss:0.005651199891933977\n",
      "train loss:0.026706573954191615\n",
      "train loss:0.03435326174289792\n",
      "train loss:0.08356332896061644\n",
      "train loss:0.00857160629880624\n",
      "train loss:0.01498059752940756\n",
      "train loss:0.012046233057136656\n",
      "train loss:0.02338967890648054\n",
      "train loss:0.045029169158774994\n",
      "train loss:0.0796681771571459\n",
      "train loss:0.014502143673538841\n",
      "train loss:0.013493902592548915\n",
      "train loss:0.017352260550360425\n",
      "train loss:0.043492664532945584\n",
      "train loss:0.014596267886959358\n",
      "train loss:0.06941578416303744\n",
      "train loss:0.10000487978133302\n",
      "train loss:0.03177774963274972\n",
      "train loss:0.01661112001275185\n",
      "train loss:0.02806413399735801\n",
      "train loss:0.019432533145557193\n",
      "train loss:0.007307214056472804\n",
      "train loss:0.00673325675329198\n",
      "train loss:0.01794303274328185\n",
      "train loss:0.048232797408995026\n",
      "train loss:0.02254127845140708\n",
      "train loss:0.1277464285867602\n",
      "train loss:0.026826268781250166\n",
      "train loss:0.01695882583771289\n",
      "train loss:0.017187160071446207\n",
      "train loss:0.10853859282726944\n",
      "train loss:0.020447801270568288\n",
      "train loss:0.006125923550579233\n",
      "train loss:0.03389011478336271\n",
      "train loss:0.035132304141346465\n",
      "train loss:0.012615846938765543\n",
      "train loss:0.07324793925716244\n",
      "train loss:0.03994873035519232\n",
      "train loss:0.018606836280242168\n",
      "train loss:0.052549463030742906\n",
      "train loss:0.01741389429148168\n",
      "train loss:0.0041088627048004285\n",
      "train loss:0.057422633536714625\n",
      "train loss:0.013163416263378922\n",
      "train loss:0.01185063238914799\n",
      "train loss:0.022375056695636945\n",
      "train loss:0.02170523227525343\n",
      "train loss:0.01590925794337677\n",
      "train loss:0.09766452307347757\n",
      "train loss:0.029131026629794245\n",
      "train loss:0.017177782437115933\n",
      "train loss:0.04064882039989394\n",
      "train loss:0.01139092735247732\n",
      "train loss:0.025690912367856856\n",
      "train loss:0.015703753612608323\n",
      "train loss:0.007483709090178037\n",
      "train loss:0.010195946225174063\n",
      "train loss:0.06758733329857951\n",
      "train loss:0.021007843500507772\n",
      "train loss:0.014055816519562956\n",
      "train loss:0.026919801569948304\n",
      "train loss:0.03741462651013021\n",
      "train loss:0.042239455056392\n",
      "train loss:0.011681922266147863\n",
      "train loss:0.010152192492065766\n",
      "train loss:0.033669478634423274\n",
      "train loss:0.08357443757315217\n",
      "train loss:0.0473817253483464\n",
      "train loss:0.018943832868475386\n",
      "train loss:0.019308511979940074\n",
      "train loss:0.018356540463078698\n",
      "train loss:0.02426688663094424\n",
      "train loss:0.016020446095529457\n",
      "train loss:0.026434444460796694\n",
      "train loss:0.004712806092963692\n",
      "train loss:0.006970210918916749\n",
      "train loss:0.019545958119031752\n",
      "train loss:0.008099399426862112\n",
      "train loss:0.026332160474613824\n",
      "train loss:0.02835784418849134\n",
      "train loss:0.013205606656280438\n",
      "train loss:0.0627166272198063\n",
      "train loss:0.012271002903051267\n",
      "train loss:0.062157195858574626\n",
      "train loss:0.01803666231697082\n",
      "train loss:0.022631374739187193\n",
      "train loss:0.08873263126121717\n",
      "train loss:0.01025026411807134\n",
      "train loss:0.007102924459699776\n",
      "train loss:0.04071752736383054\n",
      "train loss:0.03665142672958095\n",
      "train loss:0.02201821155182663\n",
      "train loss:0.02441697855029696\n",
      "train loss:0.011446174708044038\n",
      "train loss:0.02962252219985887\n",
      "train loss:0.038346731227049145\n",
      "train loss:0.01163691096007425\n",
      "train loss:0.044282530413430575\n",
      "train loss:0.0225756537034098\n",
      "train loss:0.02190273797779076\n",
      "train loss:0.04036685869648014\n",
      "train loss:0.021722753403670664\n",
      "train loss:0.008922657906873075\n",
      "train loss:0.02012101219142957\n",
      "train loss:0.008283125475160887\n",
      "train loss:0.011593972125269041\n",
      "train loss:0.01878387644262172\n",
      "train loss:0.03583600013918449\n",
      "train loss:0.006026221222589089\n",
      "train loss:0.02407810507687969\n",
      "train loss:0.050445881939619824\n",
      "train loss:0.005800064957262984\n",
      "train loss:0.0057588298430988295\n",
      "train loss:0.027503922953986196\n",
      "train loss:0.0666763859752499\n",
      "train loss:0.039347023794811566\n",
      "train loss:0.023486590529880405\n",
      "train loss:0.0060687461394738984\n",
      "train loss:0.02582159289401888\n",
      "train loss:0.021495270015931756\n",
      "train loss:0.0086704901643558\n",
      "train loss:0.05068060212745025\n",
      "train loss:0.021924906490586652\n",
      "train loss:0.013393039161470032\n",
      "train loss:0.022460416431751434\n",
      "train loss:0.12810772760698985\n",
      "train loss:0.0067923810889690805\n",
      "train loss:0.04252849669893693\n",
      "train loss:0.00410094016335123\n",
      "train loss:0.0876093752650304\n",
      "train loss:0.01787149936317707\n",
      "train loss:0.00857229748991943\n",
      "train loss:0.022632816584762613\n",
      "train loss:0.0129967386429202\n",
      "train loss:0.04143828939065728\n",
      "train loss:0.02665594944833575\n",
      "train loss:0.03313438902071911\n",
      "train loss:0.02587229399287047\n",
      "train loss:0.027514474273151474\n",
      "train loss:0.042454208002624234\n",
      "train loss:0.03133740587675937\n",
      "train loss:0.021294795452971457\n",
      "train loss:0.0626138653105965\n",
      "train loss:0.004065702884597732\n",
      "train loss:0.0032011673578766857\n",
      "train loss:0.014290059442064877\n",
      "train loss:0.019139902909882864\n",
      "train loss:0.02733558391590517\n",
      "train loss:0.05401489241841566\n",
      "train loss:0.014157581538806316\n",
      "train loss:0.011299543300330889\n",
      "train loss:0.01111172794027181\n",
      "train loss:0.004174239461990477\n",
      "train loss:0.005497553743346456\n",
      "train loss:0.01651442825566345\n",
      "train loss:0.014235351981675227\n",
      "train loss:0.009304657688213867\n",
      "train loss:0.009226583967022585\n",
      "train loss:0.006299177427767647\n",
      "train loss:0.009501594300145758\n",
      "train loss:0.0648937356160996\n",
      "train loss:0.016661302345233983\n",
      "train loss:0.007860166870836918\n",
      "train loss:0.021116561240490505\n",
      "train loss:0.005664670029005372\n",
      "train loss:0.030442966651835476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.00843089170597091\n",
      "train loss:0.00653066669491477\n",
      "train loss:0.0149498922704444\n",
      "train loss:0.010250062684653071\n",
      "train loss:0.04199624309909245\n",
      "train loss:0.050406497942600756\n",
      "train loss:0.012416898827948308\n",
      "train loss:0.01919291918596435\n",
      "train loss:0.013582763781192029\n",
      "train loss:0.011991726312005534\n",
      "train loss:0.042556634992971035\n",
      "train loss:0.01765998425130982\n",
      "train loss:0.016078345178798886\n",
      "train loss:0.15830365148096215\n",
      "train loss:0.031232568145957044\n",
      "train loss:0.011005862747055659\n",
      "train loss:0.014386204817138381\n",
      "train loss:0.01379380702508431\n",
      "train loss:0.024902938761145296\n",
      "train loss:0.05909553765811843\n",
      "train loss:0.019160377197865987\n",
      "train loss:0.003757471136953275\n",
      "train loss:0.016777842200061364\n",
      "train loss:0.01522109837256397\n",
      "train loss:0.026392241418484695\n",
      "train loss:0.0256853551097819\n",
      "train loss:0.01781824514987324\n",
      "train loss:0.007641384286217054\n",
      "train loss:0.015549920021137737\n",
      "train loss:0.03297910848847955\n",
      "train loss:0.010510802614750028\n",
      "train loss:0.009716025083912597\n",
      "train loss:0.053294438359310047\n",
      "train loss:0.027003773166906827\n",
      "train loss:0.012046950032718561\n",
      "train loss:0.03261761574410594\n",
      "train loss:0.02291850450994664\n",
      "train loss:0.02187655060668697\n",
      "train loss:0.009589747552660432\n",
      "train loss:0.05891004436660883\n",
      "train loss:0.027800052404697982\n",
      "train loss:0.010344313200130016\n",
      "train loss:0.01652530448521757\n",
      "train loss:0.03983087055223845\n",
      "train loss:0.006725648555480281\n",
      "train loss:0.03858824896322339\n",
      "train loss:0.006078695115652835\n",
      "train loss:0.026222700062829552\n",
      "train loss:0.006627632048786152\n",
      "train loss:0.021977773740096282\n",
      "train loss:0.025778682702129126\n",
      "train loss:0.016989891976265433\n",
      "train loss:0.031864138692814986\n",
      "train loss:0.014047629188734612\n",
      "train loss:0.04481255597170133\n",
      "train loss:0.04208673418648971\n",
      "train loss:0.006242691354526795\n",
      "train loss:0.046051549015320735\n",
      "train loss:0.00985284532415347\n",
      "train loss:0.14257596893391672\n",
      "train loss:0.011180434331363474\n",
      "train loss:0.035467533053532375\n",
      "train loss:0.009042257417128602\n",
      "train loss:0.017659158941326695\n",
      "train loss:0.07379482187692987\n",
      "train loss:0.06181386349030493\n",
      "train loss:0.009746560016883806\n",
      "train loss:0.015903726552968823\n",
      "train loss:0.047713309640702854\n",
      "train loss:0.01944210829587953\n",
      "train loss:0.017694274184764453\n",
      "train loss:0.0046083375116877665\n",
      "train loss:0.021462488304787\n",
      "train loss:0.01895247339330393\n",
      "train loss:0.009122243630785955\n",
      "train loss:0.01176641612449943\n",
      "train loss:0.01870157442722246\n",
      "train loss:0.005209464377424988\n",
      "train loss:0.014450162477474618\n",
      "train loss:0.019376449553402914\n",
      "train loss:0.021727339415483753\n",
      "train loss:0.02647389115402505\n",
      "train loss:0.011376926930531843\n",
      "train loss:0.021519480144479533\n",
      "train loss:0.02998823820563139\n",
      "train loss:0.04233083894576575\n",
      "train loss:0.009957536977523503\n",
      "train loss:0.0072225003669323575\n",
      "train loss:0.015559760835036579\n",
      "train loss:0.0024563180126842238\n",
      "train loss:0.013822761086163933\n",
      "train loss:0.01714350172203384\n",
      "train loss:0.07497413198580273\n",
      "train loss:0.04962029404318197\n",
      "train loss:0.022051945668926935\n",
      "train loss:0.026117154326209268\n",
      "train loss:0.036787050236858264\n",
      "train loss:0.05007098041385561\n",
      "train loss:0.07149451676951613\n",
      "train loss:0.025932431603393438\n",
      "train loss:0.03394058869296751\n",
      "train loss:0.04527266278635752\n",
      "train loss:0.024579043413858352\n",
      "train loss:0.013088973646265682\n",
      "=== epoch:6, train acc:0.984, test acc:0.983 ===\n",
      "train loss:0.04677921675967643\n",
      "train loss:0.029307331156666394\n",
      "train loss:0.03455410913583776\n",
      "train loss:0.054852203185518006\n",
      "train loss:0.047271545912201667\n",
      "train loss:0.022185724009323372\n",
      "train loss:0.06239828453721736\n",
      "train loss:0.03736410897920618\n",
      "train loss:0.008043997810560104\n",
      "train loss:0.003436888693273355\n",
      "train loss:0.10458947688284453\n",
      "train loss:0.023907861060715022\n",
      "train loss:0.035406238354917845\n",
      "train loss:0.008010957374571059\n",
      "train loss:0.013644601494853295\n",
      "train loss:0.059499961395427924\n",
      "train loss:0.0459402123665188\n",
      "train loss:0.0363922465111144\n",
      "train loss:0.023538023902357846\n",
      "train loss:0.02743240766445592\n",
      "train loss:0.018406299084192695\n",
      "train loss:0.006398723414619206\n",
      "train loss:0.013627118157835065\n",
      "train loss:0.03291502948504326\n",
      "train loss:0.013648282195445536\n",
      "train loss:0.033558902049754175\n",
      "train loss:0.04149845707536045\n",
      "train loss:0.031057651632577978\n",
      "train loss:0.014107163798414583\n",
      "train loss:0.011800444979957867\n",
      "train loss:0.0340804231208204\n",
      "train loss:0.005596432231477804\n",
      "train loss:0.03874642998691211\n",
      "train loss:0.06387492143059659\n",
      "train loss:0.010830907188822297\n",
      "train loss:0.023727842891211878\n",
      "train loss:0.012094770312995074\n",
      "train loss:0.01239758726892885\n",
      "train loss:0.019194413014945434\n",
      "train loss:0.014287211271453324\n",
      "train loss:0.009413778846550227\n",
      "train loss:0.042494995600508914\n",
      "train loss:0.004026208257475605\n",
      "train loss:0.007052346628035455\n",
      "train loss:0.034648553218482446\n",
      "train loss:0.04279209953599906\n",
      "train loss:0.009739310119293961\n",
      "train loss:0.0180524311815847\n",
      "train loss:0.013206500096167249\n",
      "train loss:0.01208671988578172\n",
      "train loss:0.05488892318234062\n",
      "train loss:0.01806803019226505\n",
      "train loss:0.013085914090998147\n",
      "train loss:0.04701926890666258\n",
      "train loss:0.013778875644121455\n",
      "train loss:0.031355006726239164\n",
      "train loss:0.027081417605385166\n",
      "train loss:0.023168132992462686\n",
      "train loss:0.007224524716602628\n",
      "train loss:0.03205429800064746\n",
      "train loss:0.01906547537668707\n",
      "train loss:0.024173533326184084\n",
      "train loss:0.006516487087267431\n",
      "train loss:0.013740288550250375\n",
      "train loss:0.017284956395371948\n",
      "train loss:0.06688002188795557\n",
      "train loss:0.03696916043579113\n",
      "train loss:0.04389086057079759\n",
      "train loss:0.009110042812720466\n",
      "train loss:0.013309199682198604\n",
      "train loss:0.02374249656947571\n",
      "train loss:0.023797030029180745\n",
      "train loss:0.08696717731100773\n",
      "train loss:0.015809700659277236\n",
      "train loss:0.01797213898570024\n",
      "train loss:0.014745128260726843\n",
      "train loss:0.02529259852809037\n",
      "train loss:0.07824888568535236\n",
      "train loss:0.020721517532891472\n",
      "train loss:0.029251890979086585\n",
      "train loss:0.019931745202689567\n",
      "train loss:0.02624546845231316\n",
      "train loss:0.020954908582426986\n",
      "train loss:0.00961639533793893\n",
      "train loss:0.03572361684859981\n",
      "train loss:0.022618185003983604\n",
      "train loss:0.0034500815206484764\n",
      "train loss:0.017852572447674336\n",
      "train loss:0.03773948170660877\n",
      "train loss:0.04713380028602995\n",
      "train loss:0.00933525775905781\n",
      "train loss:0.018671813457073045\n",
      "train loss:0.02590120653236556\n",
      "train loss:0.015432536332178493\n",
      "train loss:0.01512567566269496\n",
      "train loss:0.04145452962086825\n",
      "train loss:0.02972011535688798\n",
      "train loss:0.006847381430091757\n",
      "train loss:0.017460141381076422\n",
      "train loss:0.0395227956415751\n",
      "train loss:0.0023612917098084353\n",
      "train loss:0.005557534252773543\n",
      "train loss:0.05072727959553254\n",
      "train loss:0.016477389408635117\n",
      "train loss:0.13259303294417626\n",
      "train loss:0.016004529278021932\n",
      "train loss:0.014705956242635647\n",
      "train loss:0.013508126128065867\n",
      "train loss:0.012910273770032041\n",
      "train loss:0.005319298298375425\n",
      "train loss:0.04409979039379294\n",
      "train loss:0.02332481014151397\n",
      "train loss:0.006819359816333141\n",
      "train loss:0.007749012859711797\n",
      "train loss:0.015021413499368424\n",
      "train loss:0.010225572690857025\n",
      "train loss:0.010503654920229626\n",
      "train loss:0.009498952974955264\n",
      "train loss:0.0348107169884968\n",
      "train loss:0.011292834162063815\n",
      "train loss:0.027713154723644964\n",
      "train loss:0.05076554553516778\n",
      "train loss:0.02108825726358921\n",
      "train loss:0.04787161555366141\n",
      "train loss:0.01599344293610148\n",
      "train loss:0.023228170880523074\n",
      "train loss:0.007264504180279146\n",
      "train loss:0.032105353138815566\n",
      "train loss:0.01176150508544687\n",
      "train loss:0.01957296233348999\n",
      "train loss:0.08218538381704098\n",
      "train loss:0.08665012402717717\n",
      "train loss:0.016774557969378278\n",
      "train loss:0.03416201706641893\n",
      "train loss:0.01765560578448976\n",
      "train loss:0.004283339223393482\n",
      "train loss:0.01716671157772576\n",
      "train loss:0.030119528478548496\n",
      "train loss:0.015571138637033211\n",
      "train loss:0.027488510056368112\n",
      "train loss:0.032714389339681335\n",
      "train loss:0.051804278753859026\n",
      "train loss:0.02360688703211798\n",
      "train loss:0.02863812083129895\n",
      "train loss:0.01546485298098049\n",
      "train loss:0.00817036327716154\n",
      "train loss:0.06343199014231549\n",
      "train loss:0.009073922148681073\n",
      "train loss:0.003656191675033431\n",
      "train loss:0.013012086691902465\n",
      "train loss:0.017794823228997447\n",
      "train loss:0.032604454208242754\n",
      "train loss:0.022470859640230243\n",
      "train loss:0.02761725163554827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.02258915827749321\n",
      "train loss:0.014735853233190361\n",
      "train loss:0.03878792049429916\n",
      "train loss:0.0017725197428199685\n",
      "train loss:0.00681895852926343\n",
      "train loss:0.008603387800739616\n",
      "train loss:0.02817274766864664\n",
      "train loss:0.08774770929576697\n",
      "train loss:0.010835405287978848\n",
      "train loss:0.015974867787470835\n",
      "train loss:0.07281870280225063\n",
      "train loss:0.011553522342038127\n",
      "train loss:0.019385973934344467\n",
      "train loss:0.004991827960085652\n",
      "train loss:0.020636561302050983\n",
      "train loss:0.025148414539880787\n",
      "train loss:0.007466740862442693\n",
      "train loss:0.045060397824830414\n",
      "train loss:0.02636575595610128\n",
      "train loss:0.009192284033195889\n",
      "train loss:0.005363789023313733\n",
      "train loss:0.044358273525174734\n",
      "train loss:0.011718211254798305\n",
      "train loss:0.011704784125872043\n",
      "train loss:0.014610581062981956\n",
      "train loss:0.02031947776148644\n",
      "train loss:0.011442598145526069\n",
      "train loss:0.0077138968348841545\n",
      "train loss:0.028475547296276025\n",
      "train loss:0.030610240074765885\n",
      "train loss:0.03788297005674326\n",
      "train loss:0.0033172399217691493\n",
      "train loss:0.02960962037427316\n",
      "train loss:0.011617156669401087\n",
      "train loss:0.005904497441177461\n",
      "train loss:0.023654963455979183\n",
      "train loss:0.014304465345039262\n",
      "train loss:0.07565850078587484\n",
      "train loss:0.02908834961038936\n",
      "train loss:0.11559511354831448\n",
      "train loss:0.022133243364757055\n",
      "train loss:0.02464262153088062\n",
      "train loss:0.031269013020349824\n",
      "train loss:0.0478283660374098\n",
      "train loss:0.004064435060744955\n",
      "train loss:0.015611297928829282\n",
      "train loss:0.005273965601630811\n",
      "train loss:0.008599324563860235\n",
      "train loss:0.02958115601210611\n",
      "train loss:0.013396228205828298\n",
      "train loss:0.023378075424186302\n",
      "train loss:0.017522938013358875\n",
      "train loss:0.016778010486049363\n",
      "train loss:0.012627836830722413\n",
      "train loss:0.007871387230564459\n",
      "train loss:0.01410499154126442\n",
      "train loss:0.009317580159255988\n",
      "train loss:0.06576515761535807\n",
      "train loss:0.01836027649404494\n",
      "train loss:0.01591529985251937\n",
      "train loss:0.022198128175181603\n",
      "train loss:0.008948832686968304\n",
      "train loss:0.02137153685663145\n",
      "train loss:0.01832396444681841\n",
      "train loss:0.041253248514297465\n",
      "train loss:0.028105205110701623\n",
      "train loss:0.01692274515484336\n",
      "train loss:0.002592502649002551\n",
      "train loss:0.013606513522670812\n",
      "train loss:0.03366536592879646\n",
      "train loss:0.005265870935815351\n",
      "train loss:0.002146346434367779\n",
      "train loss:0.0271872457482095\n",
      "train loss:0.006320954588231715\n",
      "train loss:0.016137462596097364\n",
      "train loss:0.020663340584002557\n",
      "train loss:0.024556536507786376\n",
      "train loss:0.01707641185322644\n",
      "train loss:0.01876296120479668\n",
      "train loss:0.018314769710179143\n",
      "train loss:0.004540966489778681\n",
      "train loss:0.007341735005615504\n",
      "train loss:0.04734987950728994\n",
      "train loss:0.012131015793262265\n",
      "train loss:0.004545338732951935\n",
      "train loss:0.01794008105068815\n",
      "train loss:0.08316478950917103\n",
      "train loss:0.03146049891086413\n",
      "train loss:0.06423831973566257\n",
      "train loss:0.0069514316788162915\n",
      "train loss:0.009272878120467828\n",
      "train loss:0.0340098755898529\n",
      "train loss:0.07656897125893186\n",
      "train loss:0.0397438023840065\n",
      "train loss:0.03351171746674392\n",
      "train loss:0.03355627232024465\n",
      "train loss:0.006321951449743962\n",
      "train loss:0.015889986187740693\n",
      "train loss:0.006419755969208344\n",
      "train loss:0.04146403558096469\n",
      "train loss:0.028465394236883403\n",
      "train loss:0.04360745725387647\n",
      "train loss:0.015772806147811368\n",
      "train loss:0.025315208784884198\n",
      "train loss:0.03534818702914887\n",
      "train loss:0.011945152683979528\n",
      "train loss:0.008827656898958602\n",
      "train loss:0.117530111826152\n",
      "train loss:0.05201577861832278\n",
      "train loss:0.023332486306395905\n",
      "train loss:0.030791028578260427\n",
      "train loss:0.04986775579012158\n",
      "train loss:0.012192366999301485\n",
      "train loss:0.03904023322784491\n",
      "train loss:0.013443962654264563\n",
      "train loss:0.035008019620738066\n",
      "train loss:0.069692077883954\n",
      "train loss:0.016791043072243238\n",
      "train loss:0.031215837575117797\n",
      "train loss:0.02111066810811617\n",
      "train loss:0.04045885690465915\n",
      "train loss:0.014042273954574344\n",
      "train loss:0.015843231953842428\n",
      "train loss:0.08004186803107932\n",
      "train loss:0.004044737101224796\n",
      "train loss:0.020048242647713967\n",
      "train loss:0.015915498561532074\n",
      "train loss:0.0023262477192442935\n",
      "train loss:0.06429991713702042\n",
      "train loss:0.006700391366186962\n",
      "train loss:0.0031911497131405253\n",
      "train loss:0.005499524662984848\n",
      "train loss:0.006710511107475353\n",
      "train loss:0.018771249779565572\n",
      "train loss:0.019612167657562813\n",
      "train loss:0.008784021161179364\n",
      "train loss:0.028898762988719243\n",
      "train loss:0.01909991677994315\n",
      "train loss:0.08446042527730413\n",
      "train loss:0.030292261436163894\n",
      "train loss:0.024811194072605635\n",
      "train loss:0.0026912305196734056\n",
      "train loss:0.04446999105097653\n",
      "train loss:0.025781575153011622\n",
      "train loss:0.026394865574835252\n",
      "train loss:0.04714117681549124\n",
      "train loss:0.018293116581286118\n",
      "train loss:0.018383834971358134\n",
      "train loss:0.003866050121660227\n",
      "train loss:0.01564880050588841\n",
      "train loss:0.004028090021298655\n",
      "train loss:0.019614172940457947\n",
      "train loss:0.01569742771277629\n",
      "train loss:0.08468192139821969\n",
      "train loss:0.016419351643458594\n",
      "train loss:0.0493038962484683\n",
      "train loss:0.007144954206503505\n",
      "train loss:0.02057488693699878\n",
      "train loss:0.003329440410631434\n",
      "train loss:0.006144600807589556\n",
      "train loss:0.022075941464643364\n",
      "train loss:0.005018287543451866\n",
      "train loss:0.039447249431315064\n",
      "train loss:0.029866470589261413\n",
      "train loss:0.006622055791422387\n",
      "train loss:0.014948740489248555\n",
      "train loss:0.030415054878455968\n",
      "train loss:0.062210205832788266\n",
      "train loss:0.0069129758479436485\n",
      "train loss:0.02883278764207134\n",
      "train loss:0.037134596113559216\n",
      "train loss:0.004884188798719342\n",
      "train loss:0.005540876833727456\n",
      "train loss:0.03979935839121045\n",
      "train loss:0.00429086383599676\n",
      "train loss:0.005926433882591996\n",
      "train loss:0.059836079680528925\n",
      "train loss:0.0061407737737207875\n",
      "train loss:0.04305480854959191\n",
      "train loss:0.012124829005573302\n",
      "train loss:0.018174877389877428\n",
      "train loss:0.006023780723871197\n",
      "train loss:0.08481666738528036\n",
      "train loss:0.012498196882657252\n",
      "train loss:0.002606689550372513\n",
      "train loss:0.021485250180055336\n",
      "train loss:0.0062939933321088555\n",
      "train loss:0.010414375923703654\n",
      "train loss:0.00502179832310818\n",
      "train loss:0.004578052847477905\n",
      "train loss:0.016093575333218292\n",
      "train loss:0.009040379282823951\n",
      "train loss:0.0128729104837972\n",
      "train loss:0.005700799954124168\n",
      "train loss:0.020674376791944254\n",
      "train loss:0.009929710182183395\n",
      "train loss:0.01384133400381838\n",
      "train loss:0.02661669739820334\n",
      "train loss:0.050317555204152445\n",
      "train loss:0.012126176968488767\n",
      "train loss:0.005538667389108005\n",
      "train loss:0.00797505726931123\n",
      "train loss:0.024933101226828857\n",
      "train loss:0.006334865518211318\n",
      "train loss:0.016999800103288727\n",
      "train loss:0.010565995893154536\n",
      "train loss:0.009931575047901054\n",
      "train loss:0.019484964935498474\n",
      "train loss:0.040476797975060125\n",
      "train loss:0.029075598498679654\n",
      "train loss:0.030735004021273743\n",
      "train loss:0.06586209136350124\n",
      "train loss:0.009105959908986293\n",
      "train loss:0.019245230719532415\n",
      "train loss:0.025428875635418197\n",
      "train loss:0.026835023734420725\n",
      "train loss:0.0054254017726529935\n",
      "train loss:0.008454547274321107\n",
      "train loss:0.01140521411184992\n",
      "train loss:0.07543072397266591\n",
      "train loss:0.008195705711166353\n",
      "train loss:0.013551069546898921\n",
      "train loss:0.00796273120735417\n",
      "train loss:0.01563628830306306\n",
      "train loss:0.022049835269550906\n",
      "train loss:0.02881143103482163\n",
      "train loss:0.027912639432622514\n",
      "train loss:0.025827243784876087\n",
      "train loss:0.024747182725655473\n",
      "train loss:0.02450653691237511\n",
      "train loss:0.004494670445054378\n",
      "train loss:0.007912928505182376\n",
      "train loss:0.00290555665604249\n",
      "train loss:0.010153685417250062\n",
      "train loss:0.006538808707206396\n",
      "train loss:0.019273480062085995\n",
      "train loss:0.024289049546219455\n",
      "train loss:0.0291517884515613\n",
      "train loss:0.01432404956103217\n",
      "train loss:0.002929304423133332\n",
      "train loss:0.029309141244885928\n",
      "train loss:0.025628294757332238\n",
      "train loss:0.012477769264210136\n",
      "train loss:0.008840500223349219\n",
      "train loss:0.005439744513030392\n",
      "train loss:0.02519032183838147\n",
      "train loss:0.060517264172252574\n",
      "train loss:0.01771654562948008\n",
      "train loss:0.020108381067510502\n",
      "train loss:0.0040350665343869795\n",
      "train loss:0.004357710483942504\n",
      "train loss:0.008426747783346945\n",
      "train loss:0.00575908359458539\n",
      "train loss:0.026051018059922\n",
      "train loss:0.008708896892927474\n",
      "train loss:0.04944396525298817\n",
      "train loss:0.018531924906528184\n",
      "train loss:0.014963442485099638\n",
      "train loss:0.029948682191993706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.01469990145800105\n",
      "train loss:0.021332115619337646\n",
      "train loss:0.0045794642589008176\n",
      "train loss:0.044003133882282004\n",
      "train loss:0.029162965095844068\n",
      "train loss:0.020206160125809024\n",
      "train loss:0.013675990052660302\n",
      "train loss:0.008843652092099951\n",
      "train loss:0.019927139525713303\n",
      "train loss:0.025088923604359725\n",
      "train loss:0.02226910419348815\n",
      "train loss:0.019251592006175693\n",
      "train loss:0.016001958099384653\n",
      "train loss:0.01622677053453576\n",
      "train loss:0.029101859254698853\n",
      "train loss:0.06676549841579811\n",
      "train loss:0.019542697134110363\n",
      "train loss:0.002540647501324225\n",
      "train loss:0.07415716470311164\n",
      "train loss:0.005701131014786535\n",
      "train loss:0.035539054115900345\n",
      "train loss:0.007588037809641747\n",
      "train loss:0.037056859168842195\n",
      "train loss:0.020056405998004716\n",
      "train loss:0.02053338559044422\n",
      "train loss:0.005217397659360357\n",
      "train loss:0.004601574375489412\n",
      "train loss:0.009788435654705047\n",
      "train loss:0.016234231137778995\n",
      "train loss:0.0071426031176473215\n",
      "train loss:0.027835652013761033\n",
      "train loss:0.035766533818394834\n",
      "train loss:0.011601684803692463\n",
      "train loss:0.014886724515696644\n",
      "train loss:0.020381618746263996\n",
      "train loss:0.01911369368503936\n",
      "train loss:0.028309329067763973\n",
      "train loss:0.04239941207220476\n",
      "train loss:0.006663731658582558\n",
      "train loss:0.00876361679930724\n",
      "train loss:0.015806954996521155\n",
      "train loss:0.018729315266855034\n",
      "train loss:0.021919362621425905\n",
      "train loss:0.04456251922913851\n",
      "train loss:0.06701342126691599\n",
      "train loss:0.0037894520766731253\n",
      "train loss:0.01402307110514313\n",
      "train loss:0.023462109371532223\n",
      "train loss:0.02182521540898939\n",
      "train loss:0.01169986378091296\n",
      "train loss:0.008609342910014143\n",
      "train loss:0.014949861780316731\n",
      "train loss:0.013934854375105988\n",
      "train loss:0.011539778264462296\n",
      "train loss:0.01128512032161511\n",
      "train loss:0.003947302307227502\n",
      "train loss:0.01922197532003191\n",
      "train loss:0.011948416765949054\n",
      "train loss:0.02203840821094174\n",
      "train loss:0.03129440022143395\n",
      "train loss:0.020757523844959328\n",
      "train loss:0.06675715673822721\n",
      "train loss:0.023319050810073082\n",
      "train loss:0.007762188258125352\n",
      "train loss:0.033747392431521\n",
      "train loss:0.013433641041874674\n",
      "train loss:0.018697509830777406\n",
      "train loss:0.02156032537036328\n",
      "train loss:0.005057552984790829\n",
      "train loss:0.05686968945369943\n",
      "train loss:0.005783573054024061\n",
      "train loss:0.17497592199334425\n",
      "train loss:0.013234213199254939\n",
      "train loss:0.013396114348615809\n",
      "train loss:0.020023347588739047\n",
      "train loss:0.020883630030561547\n",
      "train loss:0.05695423717331604\n",
      "train loss:0.03071014040246356\n",
      "train loss:0.07678997895417879\n",
      "train loss:0.053285789223134046\n",
      "train loss:0.005654165466903809\n",
      "train loss:0.012844360105386912\n",
      "train loss:0.016223469500441134\n",
      "train loss:0.009293822615282627\n",
      "train loss:0.05100232194002884\n",
      "train loss:0.008157076292621182\n",
      "train loss:0.08512531161620382\n",
      "train loss:0.016162910883983396\n",
      "train loss:0.008221382601107646\n",
      "train loss:0.0816270803735062\n",
      "train loss:0.02303560061095762\n",
      "train loss:0.040274572185225975\n",
      "train loss:0.027201609765379667\n",
      "train loss:0.011190453926460781\n",
      "train loss:0.00815846181059427\n",
      "train loss:0.04948779181876332\n",
      "train loss:0.04503811559815205\n",
      "train loss:0.014527545057929222\n",
      "train loss:0.011744072350600578\n",
      "train loss:0.07783195504655631\n",
      "train loss:0.01445445224641583\n",
      "train loss:0.012815325176672247\n",
      "train loss:0.031537682288693206\n",
      "train loss:0.008374809383211852\n",
      "train loss:0.016597886214724446\n",
      "train loss:0.013890755033639332\n",
      "train loss:0.006794015763478997\n",
      "train loss:0.00806321493239765\n",
      "train loss:0.010413399459450065\n",
      "train loss:0.011744437436659604\n",
      "train loss:0.023256425225133076\n",
      "train loss:0.10855106255246827\n",
      "train loss:0.025522595930941323\n",
      "train loss:0.018479885185984164\n",
      "train loss:0.11438222882320663\n",
      "train loss:0.008197622937797114\n",
      "train loss:0.008138294023334243\n",
      "train loss:0.022581879359164808\n",
      "train loss:0.007814773920152758\n",
      "train loss:0.01816335490615584\n",
      "train loss:0.020331061092742547\n",
      "train loss:0.00452955838019995\n",
      "train loss:0.011660421313522589\n",
      "train loss:0.03144011629392632\n",
      "train loss:0.045930832553146075\n",
      "train loss:0.022025045298108017\n",
      "train loss:0.10049928422959932\n",
      "train loss:0.0057150146787134735\n",
      "train loss:0.028040901730464484\n",
      "train loss:0.02138184494302437\n",
      "train loss:0.00477859624749202\n",
      "train loss:0.014105402113819346\n",
      "train loss:0.06082463006668945\n",
      "train loss:0.060898648945509504\n",
      "train loss:0.007979586299401905\n",
      "train loss:0.03339408176391642\n",
      "train loss:0.019072210304960747\n",
      "train loss:0.01050215651373685\n",
      "train loss:0.019883949273773124\n",
      "train loss:0.017375287463442704\n",
      "train loss:0.01591306238370831\n",
      "train loss:0.08084846878468871\n",
      "train loss:0.008083464115028551\n",
      "train loss:0.02378289751084109\n",
      "train loss:0.07139557549881913\n",
      "train loss:0.0036542380821299333\n",
      "train loss:0.017797494222377804\n",
      "train loss:0.018131127356663602\n",
      "train loss:0.0069382047567951065\n",
      "train loss:0.02459685144935275\n",
      "train loss:0.015562417448572447\n",
      "train loss:0.01683818125031006\n",
      "train loss:0.007272347315863242\n",
      "train loss:0.033108114408767465\n",
      "train loss:0.028199485427682903\n",
      "train loss:0.033493469728648806\n",
      "train loss:0.00544962676614684\n",
      "train loss:0.009134505630007499\n",
      "train loss:0.005268317953523365\n",
      "train loss:0.03067855936390104\n",
      "train loss:0.013213485196813795\n",
      "train loss:0.018765935112136936\n",
      "train loss:0.035077782066129035\n",
      "train loss:0.003501667705469972\n",
      "train loss:0.009403894368131467\n",
      "train loss:0.01562201333170946\n",
      "train loss:0.01024658068405254\n",
      "train loss:0.008103147482809306\n",
      "train loss:0.009028962810259097\n",
      "train loss:0.0193417858168457\n",
      "train loss:0.010980311095983228\n",
      "train loss:0.01513803453437047\n",
      "train loss:0.017601524611415974\n",
      "train loss:0.007964708529679143\n",
      "train loss:0.03080363362589057\n",
      "train loss:0.0076053572396589195\n",
      "train loss:0.016291164880704347\n",
      "train loss:0.06073252526031317\n",
      "train loss:0.0303793997139099\n",
      "train loss:0.01428664151801625\n",
      "train loss:0.00899293582254209\n",
      "train loss:0.029840382731051728\n",
      "train loss:0.006659849668930502\n",
      "train loss:0.02925745908728846\n",
      "train loss:0.01620545319128213\n",
      "train loss:0.017997390642984213\n",
      "train loss:0.03604063471172361\n",
      "=== epoch:7, train acc:0.988, test acc:0.981 ===\n",
      "train loss:0.007207450034606488\n",
      "train loss:0.01048674231672716\n",
      "train loss:0.01819423236291714\n",
      "train loss:0.015395743562807967\n",
      "train loss:0.030628134856519815\n",
      "train loss:0.009410331134563915\n",
      "train loss:0.006781603303437005\n",
      "train loss:0.014836463117317393\n",
      "train loss:0.008608199036378774\n",
      "train loss:0.034420228297091064\n",
      "train loss:0.017061294434884106\n",
      "train loss:0.004520346992143757\n",
      "train loss:0.006066449269536364\n",
      "train loss:0.010924771656433413\n",
      "train loss:0.043178076322365914\n",
      "train loss:0.0029778019083592676\n",
      "train loss:0.008099739694999148\n",
      "train loss:0.0027307019572705275\n",
      "train loss:0.0556592616999859\n",
      "train loss:0.017563760199280708\n",
      "train loss:0.008111621813326742\n",
      "train loss:0.0251659798224944\n",
      "train loss:0.003861764975775819\n",
      "train loss:0.005702277499986258\n",
      "train loss:0.04866391343715086\n",
      "train loss:0.04905434299042649\n",
      "train loss:0.03592374465077066\n",
      "train loss:0.006230215215894006\n",
      "train loss:0.007687084589426914\n",
      "train loss:0.018454673241417027\n",
      "train loss:0.01621810550205213\n",
      "train loss:0.00321681082291116\n",
      "train loss:0.019560401477341078\n",
      "train loss:0.058443640841474014\n",
      "train loss:0.013386479761219207\n",
      "train loss:0.003178554696032882\n",
      "train loss:0.005904283124603259\n",
      "train loss:0.009906627661337965\n",
      "train loss:0.019832729431313742\n",
      "train loss:0.005444324841617672\n",
      "train loss:0.018776527601651417\n",
      "train loss:0.010272116737771578\n",
      "train loss:0.022103211862270298\n",
      "train loss:0.018033003985630666\n",
      "train loss:0.018035330832503013\n",
      "train loss:0.00908477756309645\n",
      "train loss:0.03326450647439924\n",
      "train loss:0.002004238942113135\n",
      "train loss:0.02274158980255205\n",
      "train loss:0.007163085757866985\n",
      "train loss:0.006942673099553594\n",
      "train loss:0.0164569950954739\n",
      "train loss:0.005738972490800691\n",
      "train loss:0.006754220299431763\n",
      "train loss:0.013567435174113272\n",
      "train loss:0.005167111042264219\n",
      "train loss:0.014972636765292053\n",
      "train loss:0.01326538794400753\n",
      "train loss:0.07085055943697795\n",
      "train loss:0.017748573702870314\n",
      "train loss:0.004574270590148386\n",
      "train loss:0.04144897308157762\n",
      "train loss:0.0058287075043005595\n",
      "train loss:0.0025255956149749757\n",
      "train loss:0.03129063442949718\n",
      "train loss:0.004579484198491885\n",
      "train loss:0.007584151887979799\n",
      "train loss:0.07074461243380185\n",
      "train loss:0.004643992111578883\n",
      "train loss:0.052934014584590516\n",
      "train loss:0.00389604499291899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.01562270352975002\n",
      "train loss:0.012410505104964975\n",
      "train loss:0.008333609621153103\n",
      "train loss:0.017705983648721457\n",
      "train loss:0.0193823230228908\n",
      "train loss:0.010686046395213828\n",
      "train loss:0.008678975449616811\n",
      "train loss:0.0047774877693584385\n",
      "train loss:0.0099418701918963\n",
      "train loss:0.013929402873047645\n",
      "train loss:0.017861883210078713\n",
      "train loss:0.02179910965106425\n",
      "train loss:0.01780085371990961\n",
      "train loss:0.020213660291483738\n",
      "train loss:0.004031585252098248\n",
      "train loss:0.059221438396801634\n",
      "train loss:0.022892098341803516\n",
      "train loss:0.03536377326992382\n",
      "train loss:0.011093523315090677\n",
      "train loss:0.01727476888803549\n",
      "train loss:0.00244018967186139\n",
      "train loss:0.0052659279230317015\n",
      "train loss:0.021496667322600173\n",
      "train loss:0.01082923049754751\n",
      "train loss:0.008132742123475797\n",
      "train loss:0.010827986304412705\n",
      "train loss:0.01808504874264204\n",
      "train loss:0.033870353528812756\n",
      "train loss:0.016471937139449114\n",
      "train loss:0.0042468037465526074\n",
      "train loss:0.027321751033820712\n",
      "train loss:0.014958655665529376\n",
      "train loss:0.016455523572495637\n",
      "train loss:0.031092900241148344\n",
      "train loss:0.005176577852588452\n",
      "train loss:0.006291341647602238\n",
      "train loss:0.010043605796582165\n",
      "train loss:0.005878692960534029\n",
      "train loss:0.006677065907064362\n",
      "train loss:0.02560037054619634\n",
      "train loss:0.02633415394287735\n",
      "train loss:0.005606796610994384\n",
      "train loss:0.014187879104856771\n",
      "train loss:0.02216135059891716\n",
      "train loss:0.02538194499981393\n",
      "train loss:0.02523686375568811\n",
      "train loss:0.01681795744135813\n",
      "train loss:0.007290662255502396\n",
      "train loss:0.013209480914186065\n",
      "train loss:0.008925614335262087\n",
      "train loss:0.023102664039670394\n",
      "train loss:0.03752116829443369\n",
      "train loss:0.004210670720140129\n",
      "train loss:0.008808316930269657\n",
      "train loss:0.06418164414187645\n",
      "train loss:0.009444268628278523\n",
      "train loss:0.0021397003915459307\n",
      "train loss:0.039621386301053774\n",
      "train loss:0.0035880586509971107\n",
      "train loss:0.04067003370587195\n",
      "train loss:0.010841928843883887\n",
      "train loss:0.001586909069394763\n",
      "train loss:0.011917022541523825\n",
      "train loss:0.045182259444017714\n",
      "train loss:0.0217773045626186\n",
      "train loss:0.12196594968495983\n",
      "train loss:0.017275604048838752\n",
      "train loss:0.027287434689304474\n",
      "train loss:0.0010792829767114285\n",
      "train loss:0.012297463189311712\n",
      "train loss:0.010976196405226138\n",
      "train loss:0.009890725669794797\n",
      "train loss:0.03341916685892809\n",
      "train loss:0.008749949742496169\n",
      "train loss:0.033927908806563774\n",
      "train loss:0.07852960280076875\n",
      "train loss:0.047454272076257374\n",
      "train loss:0.054333282990307145\n",
      "train loss:0.015612088126275998\n",
      "train loss:0.012991278723268827\n",
      "train loss:0.008960002780501537\n",
      "train loss:0.07629203532136619\n",
      "train loss:0.042249031261386065\n",
      "train loss:0.04612675327807638\n",
      "train loss:0.015901051486400034\n",
      "train loss:0.033595543858792876\n",
      "train loss:0.006177877908966975\n",
      "train loss:0.035258493853326224\n",
      "train loss:0.016461447299781433\n",
      "train loss:0.04282300678233395\n",
      "train loss:0.026540284097390572\n",
      "train loss:0.008065566667075216\n",
      "train loss:0.005412146305713832\n",
      "train loss:0.02357896185322514\n",
      "train loss:0.030102886844096594\n",
      "train loss:0.004799840717872948\n",
      "train loss:0.057871745657426074\n",
      "train loss:0.016355300550238514\n",
      "train loss:0.02887864753628479\n",
      "train loss:0.012646344100889906\n",
      "train loss:0.01021937323553719\n",
      "train loss:0.01736888459245232\n",
      "train loss:0.005358608003764813\n",
      "train loss:0.0083365309146828\n",
      "train loss:0.004069622235821311\n",
      "train loss:0.010961005816279337\n",
      "train loss:0.013258846894515874\n",
      "train loss:0.018902352856230445\n",
      "train loss:0.016575592924273602\n",
      "train loss:0.005116136619322145\n",
      "train loss:0.025802069556026076\n",
      "train loss:0.04062254611618807\n",
      "train loss:0.008809853186431186\n",
      "train loss:0.009448864632335718\n",
      "train loss:0.008484386809799644\n",
      "train loss:0.004061991286707841\n",
      "train loss:0.007502836482035702\n",
      "train loss:0.013019241797281\n",
      "train loss:0.013920893527230726\n",
      "train loss:0.017423025211683044\n",
      "train loss:0.004120884564933501\n",
      "train loss:0.0074314227518101\n",
      "train loss:0.011665976886232115\n",
      "train loss:0.02024091903060277\n",
      "train loss:0.011664977172631923\n",
      "train loss:0.009760833526803004\n",
      "train loss:0.028427207408401947\n",
      "train loss:0.012306866119928334\n",
      "train loss:0.0015379216562019502\n",
      "train loss:0.022526039325300982\n",
      "train loss:0.09913182939197313\n",
      "train loss:0.004044710122338785\n",
      "train loss:0.04215977815158365\n",
      "train loss:0.01308524611614714\n",
      "train loss:0.008125277798930576\n",
      "train loss:0.012775758378174294\n",
      "train loss:0.006574717797517502\n",
      "train loss:0.005826501415286584\n",
      "train loss:0.029681878792724783\n",
      "train loss:0.005704934454330976\n",
      "train loss:0.014250616560545364\n",
      "train loss:0.007138908599264039\n",
      "train loss:0.00787702737329786\n",
      "train loss:0.0015938443991009427\n",
      "train loss:0.006004743270955698\n",
      "train loss:0.013857050634848553\n",
      "train loss:0.0068217343262853255\n",
      "train loss:0.0029329872900991172\n",
      "train loss:0.015759856107805767\n",
      "train loss:0.006285505240104068\n",
      "train loss:0.033832681305338064\n",
      "train loss:0.032878090290821185\n",
      "train loss:0.005481634333653831\n",
      "train loss:0.01032414722134135\n",
      "train loss:0.022042727572074025\n",
      "train loss:0.04030520962343608\n",
      "train loss:0.004906071961418724\n",
      "train loss:0.01931747727601395\n",
      "train loss:0.003226739879454807\n",
      "train loss:0.027442634681229636\n",
      "train loss:0.015795960716901408\n",
      "train loss:0.026020205146012696\n",
      "train loss:0.026361497610527147\n",
      "train loss:0.004138301078322912\n",
      "train loss:0.006026162934613455\n",
      "train loss:0.04987164199229492\n",
      "train loss:0.005549228909465598\n",
      "train loss:0.0049370615729689\n",
      "train loss:0.008491965006298086\n",
      "train loss:0.0027522923769446143\n",
      "train loss:0.02040441649186036\n",
      "train loss:0.006170304529668407\n",
      "train loss:0.006675709945899943\n",
      "train loss:0.031937785092856316\n",
      "train loss:0.021747276024675542\n",
      "train loss:0.015447671161534546\n",
      "train loss:0.028354057162346683\n",
      "train loss:0.004923365386891481\n",
      "train loss:0.011742410174713563\n",
      "train loss:0.009715567891846544\n",
      "train loss:0.03104037172053291\n",
      "train loss:0.004089310132212347\n",
      "train loss:0.01196238998205454\n",
      "train loss:0.002972508650384404\n",
      "train loss:0.018031109578875482\n",
      "train loss:0.03776442003914564\n",
      "train loss:0.001407138193396988\n",
      "train loss:0.0034688032799547715\n",
      "train loss:0.0006007128453842271\n",
      "train loss:0.026327006428895943\n",
      "train loss:0.009223829404289563\n",
      "train loss:0.04003727226419617\n",
      "train loss:0.005336593199321792\n",
      "train loss:0.01519240652018066\n",
      "train loss:0.004666916037508525\n",
      "train loss:0.009458216327953661\n",
      "train loss:0.00326867182278287\n",
      "train loss:0.008290557127006534\n",
      "train loss:0.011547491827603968\n",
      "train loss:0.01620349838570919\n",
      "train loss:0.018136690739086705\n",
      "train loss:0.009645150083083368\n",
      "train loss:0.004773614675102907\n",
      "train loss:0.009019539304863102\n",
      "train loss:0.007260666171733194\n",
      "train loss:0.03808908117704914\n",
      "train loss:0.002625940770214718\n",
      "train loss:0.040269995754034185\n",
      "train loss:0.011160885060704553\n",
      "train loss:0.050792900799923206\n",
      "train loss:0.011046552911370584\n",
      "train loss:0.0651897456997999\n",
      "train loss:0.014333166233489826\n",
      "train loss:0.0035075112100010695\n",
      "train loss:0.008567306315123903\n",
      "train loss:0.010360984989084\n",
      "train loss:0.023422696873809653\n",
      "train loss:0.00858372663204459\n",
      "train loss:0.011783840341370843\n",
      "train loss:0.003872294459024053\n",
      "train loss:0.013177958027984995\n",
      "train loss:0.012375034088022768\n",
      "train loss:0.027579947589278578\n",
      "train loss:0.0019193394394567377\n",
      "train loss:0.005207193065259524\n",
      "train loss:0.0012182993994333262\n",
      "train loss:0.0247283550073738\n",
      "train loss:0.010617098397531539\n",
      "train loss:0.008430253540208266\n",
      "train loss:0.01657888943113254\n",
      "train loss:0.003334290695607064\n",
      "train loss:0.02786734648390552\n",
      "train loss:0.007337975109135427\n",
      "train loss:0.008788515163904052\n",
      "train loss:0.00562425427246495\n",
      "train loss:0.03821183851040443\n",
      "train loss:0.03402885992913025\n",
      "train loss:0.006575458583280993\n",
      "train loss:0.0021689928606494803\n",
      "train loss:0.018846244201673988\n",
      "train loss:0.014093003917694424\n",
      "train loss:0.0142032143751887\n",
      "train loss:0.015902276359630194\n",
      "train loss:0.03136655855505477\n",
      "train loss:0.006053499202975289\n",
      "train loss:0.02594864640399912\n",
      "train loss:0.014792374935434574\n",
      "train loss:0.006456604956441416\n",
      "train loss:0.04007400963011415\n",
      "train loss:0.026027103539649984\n",
      "train loss:0.05806130170229945\n",
      "train loss:0.00559498806646641\n",
      "train loss:0.08281199223660128\n",
      "train loss:0.004849240235863137\n",
      "train loss:0.015961913080058683\n",
      "train loss:0.007403758681416311\n",
      "train loss:0.025468121340014617\n",
      "train loss:0.01622930881564406\n",
      "train loss:0.069741399923087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.017854891322071225\n",
      "train loss:0.013680655146360209\n",
      "train loss:0.15589061747095256\n",
      "train loss:0.01244731977146266\n",
      "train loss:0.02584945422354182\n",
      "train loss:0.05654906773545834\n",
      "train loss:0.02460435482211715\n",
      "train loss:0.01533885700888039\n",
      "train loss:0.0058837985238200455\n",
      "train loss:0.03423644451268218\n",
      "train loss:0.015199994448733085\n",
      "train loss:0.012541013255870283\n",
      "train loss:0.008434830844717743\n",
      "train loss:0.00871619043912853\n",
      "train loss:0.03245548446988885\n",
      "train loss:0.019425426734682637\n",
      "train loss:0.03697194139967215\n",
      "train loss:0.0108435126061992\n",
      "train loss:0.008869872264611858\n",
      "train loss:0.012758087032995823\n",
      "train loss:0.01779612561143951\n",
      "train loss:0.008165326641307204\n",
      "train loss:0.017821057417914562\n",
      "train loss:0.030756221554820252\n",
      "train loss:0.006068350306624412\n",
      "train loss:0.02837655093231923\n",
      "train loss:0.015925042854001863\n",
      "train loss:0.027195379354740118\n",
      "train loss:0.04976910586469725\n",
      "train loss:0.010049648083573013\n",
      "train loss:0.011408545544952969\n",
      "train loss:0.016783665750689937\n",
      "train loss:0.01422486851340915\n",
      "train loss:0.007004487976266669\n",
      "train loss:0.05025223664386936\n",
      "train loss:0.02141935629163882\n",
      "train loss:0.0177349504087562\n",
      "train loss:0.0057222119756741375\n",
      "train loss:0.0117111899505182\n",
      "train loss:0.011937435626024211\n",
      "train loss:0.02155607582936214\n",
      "train loss:0.020198019071284823\n",
      "train loss:0.0074611088818471935\n",
      "train loss:0.026473466931108535\n",
      "train loss:0.012191712580044227\n",
      "train loss:0.04963832117785365\n",
      "train loss:0.00710382481205317\n",
      "train loss:0.01095597554875324\n",
      "train loss:0.057497368321269525\n",
      "train loss:0.009513292748704884\n",
      "train loss:0.00602588365383891\n",
      "train loss:0.0029163478911621026\n",
      "train loss:0.007405108728901969\n",
      "train loss:0.011548052749467443\n",
      "train loss:0.014945305775286894\n",
      "train loss:0.003389736601744288\n",
      "train loss:0.024431397028446086\n",
      "train loss:0.019933982815220647\n",
      "train loss:0.02266479333957388\n",
      "train loss:0.0473329387238419\n",
      "train loss:0.014050426837589853\n",
      "train loss:0.018627297638703442\n",
      "train loss:0.004641194492228467\n",
      "train loss:0.010303949291193335\n",
      "train loss:0.0179646215419772\n",
      "train loss:0.007841884116542993\n",
      "train loss:0.033592595287792626\n",
      "train loss:0.015998519973836632\n",
      "train loss:0.04774603598648905\n",
      "train loss:0.019861122884467904\n",
      "train loss:0.043471006008575\n",
      "train loss:0.026588267059969993\n",
      "train loss:0.015479202087740847\n",
      "train loss:0.013914650470150265\n",
      "train loss:0.010060080594098552\n",
      "train loss:0.009060532400115659\n",
      "train loss:0.008476355579015353\n",
      "train loss:0.010319904929262554\n",
      "train loss:0.004482230752531188\n",
      "train loss:0.009673969686176332\n",
      "train loss:0.04393092411496476\n",
      "train loss:0.007353560508325574\n",
      "train loss:0.0065008442715899315\n",
      "train loss:0.0061460630791111345\n",
      "train loss:0.03494688664219032\n",
      "train loss:0.10664325444701134\n",
      "train loss:0.029996269996050855\n",
      "train loss:0.0016092520362943358\n",
      "train loss:0.0028471189308007096\n",
      "train loss:0.0748190516468343\n",
      "train loss:0.010917540494109415\n",
      "train loss:0.011910954874711327\n",
      "train loss:0.01003233761989987\n",
      "train loss:0.006713766590902951\n",
      "train loss:0.01679370726512205\n",
      "train loss:0.01678080393547036\n",
      "train loss:0.01003222862027514\n",
      "train loss:0.007499099338439166\n",
      "train loss:0.00634521619695181\n",
      "train loss:0.005201453541336098\n",
      "train loss:0.02734683019628388\n",
      "train loss:0.03199823961078332\n",
      "train loss:0.024231190988997554\n",
      "train loss:0.010330129993598714\n",
      "train loss:0.012248054935488863\n",
      "train loss:0.018460499300293698\n",
      "train loss:0.01230229244709034\n",
      "train loss:0.010758203712749525\n",
      "train loss:0.0316032064080543\n",
      "train loss:0.011314150289578963\n",
      "train loss:0.009359935401785358\n",
      "train loss:0.01071468164010578\n",
      "train loss:0.009928191942563815\n",
      "train loss:0.0013524848872548884\n",
      "train loss:0.008130527589085373\n",
      "train loss:0.0017047716513046932\n",
      "train loss:0.00704925010521942\n",
      "train loss:0.00720684828275351\n",
      "train loss:0.004269025102796537\n",
      "train loss:0.007819361722575717\n",
      "train loss:0.02475368163157296\n",
      "train loss:0.006784985297596559\n",
      "train loss:0.010586473135932051\n",
      "train loss:0.03817544610315548\n",
      "train loss:0.013175573341466532\n",
      "train loss:0.013030661912317755\n",
      "train loss:0.0017574937594473361\n",
      "train loss:0.0056852662235067795\n",
      "train loss:0.008657917814127671\n",
      "train loss:0.0064097751144419545\n",
      "train loss:0.03056366286032879\n",
      "train loss:0.022990973558268492\n",
      "train loss:0.005344472758937568\n",
      "train loss:0.007811925719550088\n",
      "train loss:0.0009061450867326428\n",
      "train loss:0.013945633862801558\n",
      "train loss:0.026480526060353414\n",
      "train loss:0.015896699519723577\n",
      "train loss:0.006879761708680102\n",
      "train loss:0.04325092261480065\n",
      "train loss:0.004314513230400051\n",
      "train loss:0.010816012233436631\n",
      "train loss:0.020968720135146755\n",
      "train loss:0.014360509143701607\n",
      "train loss:0.008175065162081914\n",
      "train loss:0.019426645293246992\n",
      "train loss:0.02679274823869878\n",
      "train loss:0.003749677964370733\n",
      "train loss:0.008093058437225275\n",
      "train loss:0.0031920278703044314\n",
      "train loss:0.03939666938525626\n",
      "train loss:0.01945126507794692\n",
      "train loss:0.05652205458955821\n",
      "train loss:0.00492131151827729\n",
      "train loss:0.017976017946997682\n",
      "train loss:0.004928045187800422\n",
      "train loss:0.018428107707887578\n",
      "train loss:0.007768608095582042\n",
      "train loss:0.004902237658652988\n",
      "train loss:0.023858113466998634\n",
      "train loss:0.00573730223714803\n",
      "train loss:0.051896341652418645\n",
      "train loss:0.001636378864413808\n",
      "train loss:0.002835785682540107\n",
      "train loss:0.006473756195150693\n",
      "train loss:0.017678477532553874\n",
      "train loss:0.016912710989598158\n",
      "train loss:0.0778307453162121\n",
      "train loss:0.018844634292483682\n",
      "train loss:0.00222207917163406\n",
      "train loss:0.015154264395538727\n",
      "train loss:0.005185967770608436\n",
      "train loss:0.014178942607365956\n",
      "train loss:0.021667505446604552\n",
      "train loss:0.011945779576349133\n",
      "train loss:0.007429436132430769\n",
      "train loss:0.06112183943321245\n",
      "train loss:0.017859030042998963\n",
      "train loss:0.008829625752379252\n",
      "train loss:0.00646629062423494\n",
      "train loss:0.008914022326679052\n",
      "train loss:0.006780643176688864\n",
      "train loss:0.02136935251785148\n",
      "train loss:0.014356648766350058\n",
      "train loss:0.011558769375760634\n",
      "train loss:0.01213058452453764\n",
      "train loss:0.007789794403012067\n",
      "train loss:0.006466835978196132\n",
      "train loss:0.012166708936059013\n",
      "train loss:0.026506028897915597\n",
      "train loss:0.013887493464817078\n",
      "train loss:0.0037634046606745235\n",
      "train loss:0.0202160544886403\n",
      "train loss:0.004276890005229124\n",
      "train loss:0.030862553078939824\n",
      "train loss:0.007587069089593318\n",
      "train loss:0.06582138726299058\n",
      "train loss:0.02383854189978161\n",
      "train loss:0.012913818057441628\n",
      "train loss:0.0547218245783253\n",
      "train loss:0.015934507866241888\n",
      "train loss:0.013253832228926468\n",
      "train loss:0.035293994257267564\n",
      "train loss:0.015210584039825498\n",
      "train loss:0.006115032294997508\n",
      "train loss:0.027420232516164388\n",
      "train loss:0.009328962510202125\n",
      "train loss:0.005347013944405037\n",
      "train loss:0.0066956142963831055\n",
      "train loss:0.005552316854991135\n",
      "train loss:0.038284097358947125\n",
      "train loss:0.010963653672537052\n",
      "train loss:0.02325133749759786\n",
      "train loss:0.012430252475861795\n",
      "train loss:0.02730546274681163\n",
      "train loss:0.009981424522966225\n",
      "train loss:0.011535711268727614\n",
      "train loss:0.010501701592200667\n",
      "train loss:0.010796797754228891\n",
      "train loss:0.011119576231047638\n",
      "train loss:0.0020211231125712424\n",
      "train loss:0.025640386626656356\n",
      "train loss:0.022072730882437438\n",
      "train loss:0.00372995066508938\n",
      "train loss:0.011868196409009872\n",
      "train loss:0.010750149670424035\n",
      "train loss:0.025752678168384766\n",
      "train loss:0.006427378155348173\n",
      "train loss:0.019812257597575607\n",
      "train loss:0.02513233499778583\n",
      "train loss:0.01123968069800166\n",
      "train loss:0.03255533322944955\n",
      "train loss:0.010544822587997087\n",
      "train loss:0.048244824775588606\n",
      "train loss:0.013169001332884607\n",
      "train loss:0.040053380784703975\n",
      "train loss:0.00779813449726653\n",
      "train loss:0.010646397719741423\n",
      "train loss:0.028501173361985318\n",
      "train loss:0.005066999073029974\n",
      "train loss:0.009595904812034334\n",
      "train loss:0.011535868067878676\n",
      "train loss:0.008300202797706564\n",
      "train loss:0.007591563233640439\n",
      "train loss:0.009704395172493437\n",
      "train loss:0.06365463645436344\n",
      "train loss:0.008742494460152234\n",
      "train loss:0.004380930864686253\n",
      "train loss:0.03578223412415619\n",
      "train loss:0.010482098503105133\n",
      "train loss:0.012696296398603117\n",
      "train loss:0.00904878768740752\n",
      "train loss:0.02230932801317743\n",
      "train loss:0.1123077562813829\n",
      "train loss:0.015483435636292071\n",
      "train loss:0.02370506715987489\n",
      "train loss:0.0046507728666181185\n",
      "train loss:0.031473351642095165\n",
      "train loss:0.004189947775287836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.01745763556425251\n",
      "train loss:0.014023855556461698\n",
      "train loss:0.006877938881128729\n",
      "train loss:0.00761964819006114\n",
      "train loss:0.007027196648622341\n",
      "train loss:0.00813045177642389\n",
      "train loss:0.0076555193384751094\n",
      "train loss:0.010940707169605856\n",
      "train loss:0.04832475025293335\n",
      "train loss:0.047351203677695554\n",
      "train loss:0.004136563159681719\n",
      "=== epoch:8, train acc:0.991, test acc:0.989 ===\n",
      "train loss:0.005187778826953839\n",
      "train loss:0.010126463303207815\n",
      "train loss:0.0019988607032402823\n",
      "train loss:0.005753482783770045\n",
      "train loss:0.0030133678042810258\n",
      "train loss:0.017370454866194762\n",
      "train loss:0.0035359632938378087\n",
      "train loss:0.021794877945848466\n",
      "train loss:0.010271605881809965\n",
      "train loss:0.02209777429774594\n",
      "train loss:0.01316378252748682\n",
      "train loss:0.006168715825118\n",
      "train loss:0.010784137004079784\n",
      "train loss:0.0011033316812611865\n",
      "train loss:0.006706172519563399\n",
      "train loss:0.01325306942323554\n",
      "train loss:0.006075217952996961\n",
      "train loss:0.0217796277925819\n",
      "train loss:0.005584805986130753\n",
      "train loss:0.04638456950485269\n",
      "train loss:0.0075855602280691\n",
      "train loss:0.0030998187670653377\n",
      "train loss:0.02007176683609773\n",
      "train loss:0.003738544885718876\n",
      "train loss:0.02738582762750208\n",
      "train loss:0.009631367409779136\n",
      "train loss:0.019868337767517005\n",
      "train loss:0.003521635467405316\n",
      "train loss:0.01811039487551283\n",
      "train loss:0.004405084761354264\n",
      "train loss:0.004334155476034145\n",
      "train loss:0.015837626129727632\n",
      "train loss:0.019475789393707043\n",
      "train loss:0.008496727561809726\n",
      "train loss:0.031597562828068776\n",
      "train loss:0.04690888873826414\n",
      "train loss:0.01577126979248516\n",
      "train loss:0.008093597055385031\n",
      "train loss:0.003884259230311354\n",
      "train loss:0.10025579111738486\n",
      "train loss:0.003147959456383333\n",
      "train loss:0.004417301403150942\n",
      "train loss:0.012230016818742384\n",
      "train loss:0.058565274872891704\n",
      "train loss:0.010529341805137538\n",
      "train loss:0.007390203879019831\n",
      "train loss:0.008978301950622474\n",
      "train loss:0.006065500834258527\n",
      "train loss:0.014591607215878799\n",
      "train loss:0.013321675919919762\n",
      "train loss:0.013361769500367713\n",
      "train loss:0.014198288510496529\n",
      "train loss:0.016391088591551292\n",
      "train loss:0.04887132628378955\n",
      "train loss:0.021600220956906325\n",
      "train loss:0.00818876280372688\n",
      "train loss:0.011915478453270052\n",
      "train loss:0.03295066729713312\n",
      "train loss:0.015213853247694524\n",
      "train loss:0.044295797826636576\n",
      "train loss:0.03540205289705753\n",
      "train loss:0.0036015193239921244\n",
      "train loss:0.0018857087122567066\n",
      "train loss:0.019141559302542934\n",
      "train loss:0.0030529435425170263\n",
      "train loss:0.048843866257540725\n",
      "train loss:0.03767097337252438\n",
      "train loss:0.011557152394302193\n",
      "train loss:0.011954479356635753\n",
      "train loss:0.019996905687928855\n",
      "train loss:0.04550512019113834\n",
      "train loss:0.030055218670250707\n",
      "train loss:0.007355769955764103\n",
      "train loss:0.019573030991416787\n",
      "train loss:0.04229504800450564\n",
      "train loss:0.007258165475045138\n",
      "train loss:0.013544738667968919\n",
      "train loss:0.026124998200360742\n",
      "train loss:0.0014071893192240663\n",
      "train loss:0.009019797449484786\n",
      "train loss:0.023719357220890743\n",
      "train loss:0.009644768687710706\n",
      "train loss:0.022703177015853494\n",
      "train loss:0.018071680564986016\n",
      "train loss:0.03747950455092712\n",
      "train loss:0.010356641566656227\n",
      "train loss:0.0026291383678326186\n",
      "train loss:0.0374538429777217\n",
      "train loss:0.005765956971595591\n",
      "train loss:0.01677485408146741\n",
      "train loss:0.003893183352877826\n",
      "train loss:0.002600969220817298\n",
      "train loss:0.010100756984418868\n",
      "train loss:0.028909172779037862\n",
      "train loss:0.0076846682248381615\n",
      "train loss:0.0865301488494875\n",
      "train loss:0.006859994583733669\n",
      "train loss:0.00842271399452827\n",
      "train loss:0.04757556170487421\n",
      "train loss:0.003188958461099153\n",
      "train loss:0.021563204821581805\n",
      "train loss:0.013494408443016005\n",
      "train loss:0.014567901859877086\n",
      "train loss:0.0071198179828467325\n",
      "train loss:0.005442955476388476\n",
      "train loss:0.008496195684458904\n",
      "train loss:0.05368111118518914\n",
      "train loss:0.04457193996553633\n",
      "train loss:0.015224971194162884\n",
      "train loss:0.002599511455394995\n",
      "train loss:0.011642793449920167\n",
      "train loss:0.011209022771457643\n",
      "train loss:0.06980042035073959\n",
      "train loss:0.01724851217408057\n",
      "train loss:0.01640160254920532\n",
      "train loss:0.04177658165253038\n",
      "train loss:0.012075378643852958\n",
      "train loss:0.01767278389423324\n",
      "train loss:0.01562809373393907\n",
      "train loss:0.006438460269416224\n",
      "train loss:0.01217137493494129\n",
      "train loss:0.008412453401276172\n",
      "train loss:0.01347785372103761\n",
      "train loss:0.03393708406777789\n",
      "train loss:0.010777217497493696\n",
      "train loss:0.016860477332519986\n",
      "train loss:0.034044244813698714\n",
      "train loss:0.0073642687762131\n",
      "train loss:0.004651523651425294\n",
      "train loss:0.0039149991252604485\n",
      "train loss:0.020079354856752118\n",
      "train loss:0.013204702321545381\n",
      "train loss:0.02787838196419093\n",
      "train loss:0.009246726956406603\n",
      "train loss:0.021483696689416833\n",
      "train loss:0.011463051915114512\n",
      "train loss:0.014795624409234676\n",
      "train loss:0.025617380057025737\n",
      "train loss:0.03624022293444036\n",
      "train loss:0.024211677168501814\n",
      "train loss:0.011313693220840868\n",
      "train loss:0.003928040858241727\n",
      "train loss:0.023068226001614026\n",
      "train loss:0.008131313122321312\n",
      "train loss:0.006350643728355813\n",
      "train loss:0.0181548208001091\n",
      "train loss:0.015694622160232555\n",
      "train loss:0.017868131507686666\n",
      "train loss:0.008574418916760395\n",
      "train loss:0.016421562086238056\n",
      "train loss:0.00461968460774348\n",
      "train loss:0.010283572919565543\n",
      "train loss:0.0021442058886390484\n",
      "train loss:0.0014490835502578223\n",
      "train loss:0.004086825203122338\n",
      "train loss:0.024181627465788436\n",
      "train loss:0.004358188173957668\n",
      "train loss:0.01259565350007684\n",
      "train loss:0.0446096346191256\n",
      "train loss:0.006455175344755129\n",
      "train loss:0.004301587510007066\n",
      "train loss:0.0025886234894313786\n",
      "train loss:0.005007666787952069\n",
      "train loss:0.053995727630442206\n",
      "train loss:0.01651530429037159\n",
      "train loss:0.004035678647286674\n",
      "train loss:0.005180717163103418\n",
      "train loss:0.052249290520383546\n",
      "train loss:0.009734727244192559\n",
      "train loss:0.0017006425625578053\n",
      "train loss:0.012817642248410366\n",
      "train loss:0.01817675190445078\n",
      "train loss:0.031873233195114176\n",
      "train loss:0.1628440484002314\n",
      "train loss:0.0163797360347299\n",
      "train loss:0.008483653334720783\n",
      "train loss:0.004142012405898475\n",
      "train loss:0.09401740938220876\n",
      "train loss:0.015310242289203677\n",
      "train loss:0.01590580110052044\n",
      "train loss:0.008721279187276281\n",
      "train loss:0.06458306195221299\n",
      "train loss:0.007693674879920906\n",
      "train loss:0.016374055975232015\n",
      "train loss:0.003920466867551908\n",
      "train loss:0.00826181355529909\n",
      "train loss:0.023768180344622004\n",
      "train loss:0.07282419213232372\n",
      "train loss:0.006079402509533206\n",
      "train loss:0.017179404498635167\n",
      "train loss:0.04751409443413146\n",
      "train loss:0.051863804225324814\n",
      "train loss:0.004026722610016182\n",
      "train loss:0.008810703682340559\n",
      "train loss:0.02435980836739109\n",
      "train loss:0.04320664032323433\n",
      "train loss:0.0122962676776507\n",
      "train loss:0.00697931581219438\n",
      "train loss:0.022544578341911817\n",
      "train loss:0.007643995035393417\n",
      "train loss:0.027756470157790324\n",
      "train loss:0.004356556737556328\n",
      "train loss:0.004969400018742828\n",
      "train loss:0.013757566955542398\n",
      "train loss:0.015962952689128406\n",
      "train loss:0.0156581075355129\n",
      "train loss:0.007157581999033712\n",
      "train loss:0.00393823021038067\n",
      "train loss:0.021647553136302983\n",
      "train loss:0.004000561251336158\n",
      "train loss:0.006055175046244493\n",
      "train loss:0.011608943866948227\n",
      "train loss:0.011430912709364742\n",
      "train loss:0.05512921305142608\n",
      "train loss:0.05066180411548979\n",
      "train loss:0.014214284068141667\n",
      "train loss:0.0017134696538398369\n",
      "train loss:0.009522206191721928\n",
      "train loss:0.00797578910653884\n",
      "train loss:0.007251573955533818\n",
      "train loss:0.00755397587348368\n",
      "train loss:0.005321934607865405\n",
      "train loss:0.055124938078537\n",
      "train loss:0.005350302676276187\n",
      "train loss:0.011778242257751688\n",
      "train loss:0.0025519847022507226\n",
      "train loss:0.05387943496517936\n",
      "train loss:0.004793218171556695\n",
      "train loss:0.011020852889281422\n",
      "train loss:0.00555661187876854\n",
      "train loss:0.0187198180503458\n",
      "train loss:0.01460975403024188\n",
      "train loss:0.010388319798638053\n",
      "train loss:0.0035539481336304573\n",
      "train loss:0.04690165155326059\n",
      "train loss:0.049332893830583036\n",
      "train loss:0.006185468444395805\n",
      "train loss:0.02872434962030972\n",
      "train loss:0.009577085312903867\n",
      "train loss:0.017899911027583697\n",
      "train loss:0.018280685939786337\n",
      "train loss:0.026401630545364062\n",
      "train loss:0.019225375477021855\n",
      "train loss:0.020487981981222557\n",
      "train loss:0.011889917327759538\n",
      "train loss:0.016796998747850428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.02758351274393235\n",
      "train loss:0.007036581650406442\n",
      "train loss:0.017613042436171972\n",
      "train loss:0.01038041627950958\n",
      "train loss:0.016885245148342776\n",
      "train loss:0.02815213758994346\n",
      "train loss:0.03274003475699457\n",
      "train loss:0.0044183727818708175\n",
      "train loss:0.022149475875100903\n",
      "train loss:0.037924684820373034\n",
      "train loss:0.0016305793758308192\n",
      "train loss:0.007436221039717192\n",
      "train loss:0.00689637014115479\n",
      "train loss:0.004114100974770067\n",
      "train loss:0.010264760449853439\n",
      "train loss:0.007774345054857168\n",
      "train loss:0.004251679531359289\n",
      "train loss:0.0051519903705202295\n",
      "train loss:0.00260416059244939\n",
      "train loss:0.01595829452491658\n",
      "train loss:0.03557599287427529\n",
      "train loss:0.012093880946203554\n",
      "train loss:0.019282352226221103\n",
      "train loss:0.00841405989107047\n",
      "train loss:0.007407224526427768\n",
      "train loss:0.011842053595431007\n",
      "train loss:0.003571734633535144\n",
      "train loss:0.03565445109462336\n",
      "train loss:0.00990595795735441\n",
      "train loss:0.030283175171399376\n",
      "train loss:0.030724782319087676\n",
      "train loss:0.013078802486804519\n",
      "train loss:0.029378788886538946\n",
      "train loss:0.021485575796420676\n",
      "train loss:0.00981289490507024\n",
      "train loss:0.00909472892615843\n",
      "train loss:0.005771081210105964\n",
      "train loss:0.0036165799909825725\n",
      "train loss:0.00918253551772007\n",
      "train loss:0.014461281392743075\n",
      "train loss:0.005819505105495833\n",
      "train loss:0.011538666053132274\n",
      "train loss:0.005877032777789195\n",
      "train loss:0.0025765006492954536\n",
      "train loss:0.0037649160541419896\n",
      "train loss:0.019967296345010127\n",
      "train loss:0.006013064776555691\n",
      "train loss:0.0012444242763190749\n",
      "train loss:0.0025141114347831508\n",
      "train loss:0.0014573830138649884\n",
      "train loss:0.0034191153225616566\n",
      "train loss:0.012085979626759457\n",
      "train loss:0.005559098227473865\n",
      "train loss:0.01105666358808369\n",
      "train loss:0.019858497031267155\n",
      "train loss:0.021136196576534618\n",
      "train loss:0.004153149930299954\n",
      "train loss:0.011574030633521137\n",
      "train loss:0.0014620800022308853\n",
      "train loss:0.013189838238760727\n",
      "train loss:0.005193483263088995\n",
      "train loss:0.006793936461515333\n",
      "train loss:0.004665748574101633\n",
      "train loss:0.0016676092851187875\n",
      "train loss:0.01057493987432303\n",
      "train loss:0.01659148305063049\n",
      "train loss:0.030706259164594903\n",
      "train loss:0.07020890882345263\n",
      "train loss:0.009928776603778831\n",
      "train loss:0.004157747640454474\n",
      "train loss:0.003182862540907541\n",
      "train loss:0.006528852625609668\n",
      "train loss:0.0016864824207299845\n",
      "train loss:0.0025424702701708464\n",
      "train loss:0.010810097483286317\n",
      "train loss:0.001964250410251804\n",
      "train loss:0.04602130737570411\n",
      "train loss:0.019423015298120273\n",
      "train loss:0.003242529672651165\n",
      "train loss:0.007633694309065313\n",
      "train loss:0.022575047159864157\n",
      "train loss:0.0034630939454964264\n",
      "train loss:0.007071788459921033\n",
      "train loss:0.006311286403774357\n",
      "train loss:0.012275002759339137\n",
      "train loss:0.0015393817051085903\n",
      "train loss:0.008194528724424447\n",
      "train loss:0.021487574561958152\n",
      "train loss:0.009938424674581408\n",
      "train loss:0.01778757006896152\n",
      "train loss:0.011609288839726799\n",
      "train loss:0.020629285033631057\n",
      "train loss:0.01222489811176837\n",
      "train loss:0.006915086382476605\n",
      "train loss:0.00810170404500521\n",
      "train loss:0.0029161527053751185\n",
      "train loss:0.004000360775009409\n",
      "train loss:0.018891212866988694\n",
      "train loss:0.002290335270903997\n",
      "train loss:0.003609217828058659\n",
      "train loss:0.00843723580708007\n",
      "train loss:0.024687992209973378\n",
      "train loss:0.05314871887485901\n",
      "train loss:0.014685494500909986\n",
      "train loss:0.0025845374931245833\n",
      "train loss:0.046122465142475894\n",
      "train loss:0.005085404331837522\n",
      "train loss:0.10282727830391099\n",
      "train loss:0.005092873207225519\n",
      "train loss:0.006188588757016496\n",
      "train loss:0.010790288218575337\n",
      "train loss:0.009113161192952203\n",
      "train loss:0.02725762795224015\n",
      "train loss:0.01778680039812233\n",
      "train loss:0.01208160347923657\n",
      "train loss:0.013146087293981017\n",
      "train loss:0.019490399816099607\n",
      "train loss:0.023362943423557256\n",
      "train loss:0.010342436974884192\n",
      "train loss:0.002631475165756411\n",
      "train loss:0.03268079315146415\n",
      "train loss:0.007307986149716863\n",
      "train loss:0.006989718936086289\n",
      "train loss:0.011498066133322136\n",
      "train loss:0.011224482750149874\n",
      "train loss:0.007677731014606963\n",
      "train loss:0.009594771201395774\n",
      "train loss:0.012064576103390014\n",
      "train loss:0.007043972020703935\n",
      "train loss:0.015100179739951773\n",
      "train loss:0.0009835872862979907\n",
      "train loss:0.0319839132299947\n",
      "train loss:0.0036432127928066015\n",
      "train loss:0.0058781357840321895\n",
      "train loss:0.03736146834786474\n",
      "train loss:0.002311860341039369\n",
      "train loss:0.0033480886689518548\n",
      "train loss:0.0019919177550499108\n",
      "train loss:0.011914350299855166\n",
      "train loss:0.019971137565946352\n",
      "train loss:0.02260700408425576\n",
      "train loss:0.017057475998416963\n",
      "train loss:0.01109426635063135\n",
      "train loss:0.00956009916657187\n",
      "train loss:0.0037289385958168826\n",
      "train loss:0.010223858338628483\n",
      "train loss:0.0036973883155335297\n",
      "train loss:0.0040638240567942244\n",
      "train loss:0.009312525968668484\n",
      "train loss:0.0012101819925857545\n",
      "train loss:0.023886007108111985\n",
      "train loss:0.012395589527841233\n",
      "train loss:0.011778294199164816\n",
      "train loss:0.008450992059091915\n",
      "train loss:0.010074174125842246\n",
      "train loss:0.004969258752269708\n",
      "train loss:0.002527980225684593\n",
      "train loss:0.00368086085284085\n",
      "train loss:0.003824552461465504\n",
      "train loss:0.014282599892083052\n",
      "train loss:0.003632256152073324\n",
      "train loss:0.005189622314268438\n",
      "train loss:0.0036199064026299368\n",
      "train loss:0.005954059119745733\n",
      "train loss:0.03520185405797991\n",
      "train loss:0.020829167498377602\n",
      "train loss:0.010027462016098108\n",
      "train loss:0.0037216344898614155\n",
      "train loss:0.0030552471182147545\n",
      "train loss:0.01330183722593707\n",
      "train loss:0.0044858184252905865\n",
      "train loss:0.007468185691501087\n",
      "train loss:0.010368623250657145\n",
      "train loss:0.03760866710874498\n",
      "train loss:0.003333167047633596\n",
      "train loss:0.023899484602677802\n",
      "train loss:0.0059791625121531165\n",
      "train loss:0.011774137804416732\n",
      "train loss:0.01875649591877423\n",
      "train loss:0.020424344080473084\n",
      "train loss:0.03341362506332329\n",
      "train loss:0.05722313864553577\n",
      "train loss:0.0375951857975415\n",
      "train loss:0.02445461935622482\n",
      "train loss:0.04395868977922637\n",
      "train loss:0.03458039656813452\n",
      "train loss:0.004926844582460041\n",
      "train loss:0.008140102661498068\n",
      "train loss:0.009185769351175703\n",
      "train loss:0.01905386625843373\n",
      "train loss:0.014009764105948157\n",
      "train loss:0.006637856717302288\n",
      "train loss:0.0035215203200255557\n",
      "train loss:0.0012060669606846328\n",
      "train loss:0.013008563402935927\n",
      "train loss:0.0057407488432586785\n",
      "train loss:0.0027775866356756886\n",
      "train loss:0.029961392362825783\n",
      "train loss:0.01333142536418335\n",
      "train loss:0.026921896829472564\n",
      "train loss:0.003914358634230495\n",
      "train loss:0.004188743151771828\n",
      "train loss:0.009102416688885308\n",
      "train loss:0.05517063289604869\n",
      "train loss:0.05828786357286723\n",
      "train loss:0.005117889371537592\n",
      "train loss:0.0033245550954748605\n",
      "train loss:0.02320286563943836\n",
      "train loss:0.03984733362637038\n",
      "train loss:0.0075632388349150415\n",
      "train loss:0.001949748581113493\n",
      "train loss:0.008376193364988356\n",
      "train loss:0.010034155574247196\n",
      "train loss:0.007911415027172545\n",
      "train loss:0.015215013942340285\n",
      "train loss:0.01122205500665031\n",
      "train loss:0.007780584683556218\n",
      "train loss:0.017154459692987158\n",
      "train loss:0.011213153674288623\n",
      "train loss:0.014155674463074676\n",
      "train loss:0.008225036537851345\n",
      "train loss:0.01246148398291124\n",
      "train loss:0.0076241960109725716\n",
      "train loss:0.007610322552420768\n",
      "train loss:0.013234120089222635\n",
      "train loss:0.013953505488477624\n",
      "train loss:0.0024547385708995244\n",
      "train loss:0.005536681425914226\n",
      "train loss:0.0010935810127438268\n",
      "train loss:0.06984405494138567\n",
      "train loss:0.013032254273309314\n",
      "train loss:0.028091876436273755\n",
      "train loss:0.00492094922570541\n",
      "train loss:0.012076686510340653\n",
      "train loss:0.004720290216458564\n",
      "train loss:0.0058829738563483235\n",
      "train loss:0.01164098238521597\n",
      "train loss:0.013061535707792457\n",
      "train loss:0.007702307617725513\n",
      "train loss:0.032612974900019506\n",
      "train loss:0.006016807146298559\n",
      "train loss:0.023175807421415843\n",
      "train loss:0.01037291449327272\n",
      "train loss:0.014153097049482019\n",
      "train loss:0.00627713454872805\n",
      "train loss:0.0034596131081336464\n",
      "train loss:0.006564004456743434\n",
      "train loss:0.014500227623552324\n",
      "train loss:0.003815993895894803\n",
      "train loss:0.001948386750404944\n",
      "train loss:0.0029407846214663364\n",
      "train loss:0.04501159566781508\n",
      "train loss:0.0055171414029672634\n",
      "train loss:0.009847947373969811\n",
      "train loss:0.0033909180438074947\n",
      "train loss:0.028890554432206943\n",
      "train loss:0.030036581148955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.007473998929199033\n",
      "train loss:0.0378810862520174\n",
      "train loss:0.021934753063606923\n",
      "train loss:0.01789671788744515\n",
      "train loss:0.0030417258943300014\n",
      "train loss:0.009384506547326619\n",
      "train loss:0.0020882842750757713\n",
      "train loss:0.016541646058754828\n",
      "train loss:0.017905141572424916\n",
      "train loss:0.0017066961404204062\n",
      "train loss:0.007574164493049587\n",
      "train loss:0.0019517842403215812\n",
      "train loss:0.014039126345719952\n",
      "train loss:0.0506202899118587\n",
      "train loss:0.037105565054455704\n",
      "train loss:0.0008717770183879519\n",
      "train loss:0.024223510150911964\n",
      "train loss:0.02121929791348639\n",
      "train loss:0.008403680253502325\n",
      "train loss:0.002696669225466256\n",
      "train loss:0.016344311453335113\n",
      "train loss:0.0281040585724449\n",
      "train loss:0.024159913750406624\n",
      "train loss:0.023107570204066813\n",
      "train loss:0.005074499018121608\n",
      "train loss:0.009450428833497731\n",
      "train loss:0.005876940755685991\n",
      "train loss:0.009165736317657087\n",
      "train loss:0.015413525066079146\n",
      "train loss:0.008028193268368412\n",
      "train loss:0.005472063461287986\n",
      "train loss:0.019275833547343857\n",
      "train loss:0.0024872337036136428\n",
      "train loss:0.006908248248973176\n",
      "train loss:0.011275016550440648\n",
      "train loss:0.0031812752595454384\n",
      "train loss:0.0033504749561472863\n",
      "train loss:0.01162732752919995\n",
      "train loss:0.03998372645397215\n",
      "train loss:0.004468548620446412\n",
      "train loss:0.02037476191268926\n",
      "train loss:0.036503845333968624\n",
      "train loss:0.007185188571585596\n",
      "train loss:0.050290062714014365\n",
      "train loss:0.014914791313582096\n",
      "train loss:0.015437874337292833\n",
      "train loss:0.007473440109025595\n",
      "train loss:0.013214206781863384\n",
      "train loss:0.02387690204460382\n",
      "train loss:0.027099730374138015\n",
      "train loss:0.006539447280771493\n",
      "train loss:0.009194871508715102\n",
      "train loss:0.013342347107713225\n",
      "train loss:0.010750578618439574\n",
      "train loss:0.004537870892016093\n",
      "train loss:0.003762070418836383\n",
      "train loss:0.023480787594937803\n",
      "train loss:0.018832558059096588\n",
      "train loss:0.00853805335978258\n",
      "train loss:0.00400904631974106\n",
      "train loss:0.009450644999163198\n",
      "train loss:0.058968740455054354\n",
      "train loss:0.010234528479894705\n",
      "train loss:0.0027763291524586504\n",
      "train loss:0.06434580199173279\n",
      "train loss:0.005405267272759685\n",
      "train loss:0.0032857505017942663\n",
      "train loss:0.023624838145492023\n",
      "train loss:0.006414019913997949\n",
      "train loss:0.009860273049674902\n",
      "train loss:0.0037708686941560197\n",
      "train loss:0.02094048787199143\n",
      "train loss:0.005432458638684408\n",
      "train loss:0.0045678796638283494\n",
      "train loss:0.03215321187270443\n",
      "train loss:0.007264161828183534\n",
      "train loss:0.0016442494509204405\n",
      "train loss:0.019733962163177143\n",
      "train loss:0.012189494604622122\n",
      "train loss:0.005951675697481014\n",
      "train loss:0.0322765542509442\n",
      "train loss:0.0011989852510374778\n",
      "train loss:0.006838304919525898\n",
      "train loss:0.002567837771194065\n",
      "train loss:0.004725798050258504\n",
      "train loss:0.005707887128270202\n",
      "train loss:0.0017263814434945424\n",
      "train loss:0.16578987596458897\n",
      "train loss:0.012917874345919447\n",
      "train loss:0.016809912225698234\n",
      "train loss:0.0017153083818167896\n",
      "train loss:0.009835497483115602\n",
      "train loss:0.03484231998604268\n",
      "train loss:0.004622527524467817\n",
      "train loss:0.053242165899153295\n",
      "train loss:0.0012571728920481337\n",
      "train loss:0.08249725312302157\n",
      "=== epoch:9, train acc:0.993, test acc:0.989 ===\n",
      "train loss:0.00325390061288055\n",
      "train loss:0.013436983283678372\n",
      "train loss:0.01129363105565656\n",
      "train loss:0.015424873857719847\n",
      "train loss:0.004337667177763197\n",
      "train loss:0.013048841997884775\n",
      "train loss:0.023900584694931927\n",
      "train loss:0.06222784703313797\n",
      "train loss:0.007914257631743941\n",
      "train loss:0.02446921672902363\n",
      "train loss:0.01728172242807557\n",
      "train loss:0.005430257217094238\n",
      "train loss:0.0011232539364087193\n",
      "train loss:0.004148249447638543\n",
      "train loss:0.008538095926596047\n",
      "train loss:0.0029145614481565123\n",
      "train loss:0.005520256450659503\n",
      "train loss:0.004078439313595874\n",
      "train loss:0.008257601220730514\n",
      "train loss:0.002184855424065714\n",
      "train loss:0.03444940978171379\n",
      "train loss:0.01712416774845817\n",
      "train loss:0.025183957053828164\n",
      "train loss:0.013769403290535873\n",
      "train loss:0.005873731514127244\n",
      "train loss:0.01077350336887173\n",
      "train loss:0.01506462177841782\n",
      "train loss:0.0010462284132351624\n",
      "train loss:0.0055861436700406055\n",
      "train loss:0.009059500803806088\n",
      "train loss:0.032588432381630755\n",
      "train loss:0.015853179439131657\n",
      "train loss:0.010343941442412628\n",
      "train loss:0.01361004404443878\n",
      "train loss:0.0030282007981978195\n",
      "train loss:0.004590914616856336\n",
      "train loss:0.01086481482421354\n",
      "train loss:0.003663752690815861\n",
      "train loss:0.020095090886176777\n",
      "train loss:0.013666581680300605\n",
      "train loss:0.026818440949194677\n",
      "train loss:0.007399668715504551\n",
      "train loss:0.0033918480293350637\n",
      "train loss:0.007310215286912002\n",
      "train loss:0.01726413504914996\n",
      "train loss:0.02445352060879068\n",
      "train loss:0.0033037261546016274\n",
      "train loss:0.00433186294035691\n",
      "train loss:0.004444526243461047\n",
      "train loss:0.007445852443521817\n",
      "train loss:0.005742003324305668\n",
      "train loss:0.019986091263798147\n",
      "train loss:0.013275144566249628\n",
      "train loss:0.013560940777244972\n",
      "train loss:0.0038632701042560845\n",
      "train loss:0.0079532666267496\n",
      "train loss:0.050758718617321084\n",
      "train loss:0.0013281192872314426\n",
      "train loss:0.018477490826710928\n",
      "train loss:0.0066886338895204775\n",
      "train loss:0.020829234624878512\n",
      "train loss:0.008656675771815846\n",
      "train loss:0.06757416829306447\n",
      "train loss:0.0159337926197355\n",
      "train loss:0.02129477749127817\n",
      "train loss:0.039060130922758715\n",
      "train loss:0.00191546978664954\n",
      "train loss:0.009937939550433483\n",
      "train loss:0.009852630911959533\n",
      "train loss:0.00947581718295886\n",
      "train loss:0.0023930441078242478\n",
      "train loss:0.008397756419457877\n",
      "train loss:0.007692472395299086\n",
      "train loss:0.01100873620071026\n",
      "train loss:0.012878030631101592\n",
      "train loss:0.015597906097084556\n",
      "train loss:0.02816473413152438\n",
      "train loss:0.030689336561001288\n",
      "train loss:0.023632871213237504\n",
      "train loss:0.011691497144840222\n",
      "train loss:0.016002403107831718\n",
      "train loss:0.00223534238496846\n",
      "train loss:0.001122826303287202\n",
      "train loss:0.002380501582629101\n",
      "train loss:0.004519861080604173\n",
      "train loss:0.02662765461316996\n",
      "train loss:0.001273618445482201\n",
      "train loss:0.017827556789044633\n",
      "train loss:0.0034044945601472264\n",
      "train loss:0.041461114932321054\n",
      "train loss:0.022672026529867658\n",
      "train loss:0.0033521392544245314\n",
      "train loss:0.011163844502382147\n",
      "train loss:0.003404410121577936\n",
      "train loss:0.03970793555427665\n",
      "train loss:0.011587532381469352\n",
      "train loss:0.004467931905865616\n",
      "train loss:0.0018384077977906669\n",
      "train loss:0.006579601431281806\n",
      "train loss:0.006818719011638403\n",
      "train loss:0.003034872227073774\n",
      "train loss:0.005122851489567859\n",
      "train loss:0.006559193333247413\n",
      "train loss:0.018216627391127463\n",
      "train loss:0.0056019804929588425\n",
      "train loss:0.009297919919029103\n",
      "train loss:0.023462657387440373\n",
      "train loss:0.007103169202673891\n",
      "train loss:0.0037395717196328105\n",
      "train loss:0.04847906207990322\n",
      "train loss:0.012859817693219913\n",
      "train loss:0.003582688626869702\n",
      "train loss:0.07066952607074393\n",
      "train loss:0.003394747443360723\n",
      "train loss:0.0023103886995260283\n",
      "train loss:0.006059182962896641\n",
      "train loss:0.005781342063159394\n",
      "train loss:0.017720468892798488\n",
      "train loss:0.008818314456283097\n",
      "train loss:0.02421812591868152\n",
      "train loss:0.011331888233425734\n",
      "train loss:0.00617564647411715\n",
      "train loss:0.012382504836961841\n",
      "train loss:0.018083338434492956\n",
      "train loss:0.008998735425057545\n",
      "train loss:0.01182038997831621\n",
      "train loss:0.010694725522711073\n",
      "train loss:0.010662654497433617\n",
      "train loss:0.002670574220012607\n",
      "train loss:0.0016970313514304098\n",
      "train loss:0.000908149918448116\n",
      "train loss:0.008631789283624125\n",
      "train loss:0.018179153794532662\n",
      "train loss:0.014524534295307052\n",
      "train loss:0.0096106640543464\n",
      "train loss:0.013645875491032808\n",
      "train loss:0.036991534237380976\n",
      "train loss:0.04874678313737798\n",
      "train loss:0.0016236936837300188\n",
      "train loss:0.048695676645912365\n",
      "train loss:0.012409590087316666\n",
      "train loss:0.011992711704024297\n",
      "train loss:0.0019744590750728755\n",
      "train loss:0.006216905800803695\n",
      "train loss:0.00420684550149766\n",
      "train loss:0.015567900995694557\n",
      "train loss:0.05517391001693137\n",
      "train loss:0.0061701893858100544\n",
      "train loss:0.0037033999873769467\n",
      "train loss:0.023858156841815782\n",
      "train loss:0.006521233981181452\n",
      "train loss:0.005264015746098146\n",
      "train loss:0.0030876396585364386\n",
      "train loss:0.010693990737088467\n",
      "train loss:0.02681043863541198\n",
      "train loss:0.017406935794366574\n",
      "train loss:0.09610197890613154\n",
      "train loss:0.0023080402231643324\n",
      "train loss:0.006472968419506469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.02276914678207846\n",
      "train loss:0.004888490535007154\n",
      "train loss:0.011480478682424172\n",
      "train loss:0.00124938698577734\n",
      "train loss:0.001379247284145465\n",
      "train loss:0.006341329852169016\n",
      "train loss:0.015052295466664693\n",
      "train loss:0.03838207201929551\n",
      "train loss:0.009624753447259776\n",
      "train loss:0.0028123629635234066\n",
      "train loss:0.04607701191888578\n",
      "train loss:0.011663032660753647\n",
      "train loss:0.0023105564125143795\n",
      "train loss:0.012019272094323857\n",
      "train loss:0.011045175966143697\n",
      "train loss:0.018404457758102077\n",
      "train loss:0.01376470941546733\n",
      "train loss:0.025006367556458957\n",
      "train loss:0.007018239477990082\n",
      "train loss:0.015370068790618686\n",
      "train loss:0.006693999977610323\n",
      "train loss:0.0033747130382549657\n",
      "train loss:0.006841931862624885\n",
      "train loss:0.027610263194925787\n",
      "train loss:0.007866105669516503\n",
      "train loss:0.026317536430462102\n",
      "train loss:0.002220715265468192\n",
      "train loss:0.00818390264870923\n",
      "train loss:0.005828565210071122\n",
      "train loss:0.006499692738254595\n",
      "train loss:0.02222481752553903\n",
      "train loss:0.0007348713178307907\n",
      "train loss:0.03420763154047588\n",
      "train loss:0.027375855734610214\n",
      "train loss:0.0060031697654375936\n",
      "train loss:0.012092605482814399\n",
      "train loss:0.006855588034130982\n",
      "train loss:0.004972476642996014\n",
      "train loss:0.003219605140138174\n",
      "train loss:0.0014176253645710255\n",
      "train loss:0.01744545610723326\n",
      "train loss:0.0039673210492934905\n",
      "train loss:0.002044871574862739\n",
      "train loss:0.03101969343720326\n",
      "train loss:0.0029467177778239972\n",
      "train loss:0.004507328231602242\n",
      "train loss:0.008814810550682456\n",
      "train loss:0.014136711035693854\n",
      "train loss:0.008701506250759293\n",
      "train loss:0.004397599802289097\n",
      "train loss:0.007593897592654819\n",
      "train loss:0.014708362875070116\n",
      "train loss:0.0028344064925158625\n",
      "train loss:0.01735352201201303\n",
      "train loss:0.006318347709869442\n",
      "train loss:0.004789942416648845\n",
      "train loss:0.01075477500049037\n",
      "train loss:0.010719047055748634\n",
      "train loss:0.012572085818573157\n",
      "train loss:0.010206361303425913\n",
      "train loss:0.005002951700558752\n",
      "train loss:0.002876579342950301\n",
      "train loss:0.0046479175021696024\n",
      "train loss:0.006304059436531741\n",
      "train loss:0.01371304386802316\n",
      "train loss:0.01402295667510329\n",
      "train loss:0.003391531713178585\n",
      "train loss:0.007019983047027492\n",
      "train loss:0.0019625069241906022\n",
      "train loss:0.016802560687744498\n",
      "train loss:0.020993003070627413\n",
      "train loss:0.010749710026910036\n",
      "train loss:0.005131646369520872\n",
      "train loss:0.017197293532036383\n",
      "train loss:0.007245013928928413\n",
      "train loss:0.016273994917680043\n",
      "train loss:0.0028184789702606614\n",
      "train loss:0.0066563981928871455\n",
      "train loss:0.006891245017887932\n",
      "train loss:0.0017351439256199335\n",
      "train loss:0.008511952326969162\n",
      "train loss:0.008948680987082142\n",
      "train loss:0.06493668042658146\n",
      "train loss:0.005094828509535508\n",
      "train loss:0.011151418036340937\n",
      "train loss:0.011549391996641527\n",
      "train loss:0.0016871007638926958\n",
      "train loss:0.014007719130668912\n",
      "train loss:0.01269132086376332\n",
      "train loss:0.004242120154068614\n",
      "train loss:0.020593235284072288\n",
      "train loss:0.007574266000023942\n",
      "train loss:0.012579897939902461\n",
      "train loss:0.0016540422542253423\n",
      "train loss:0.005570269755659372\n",
      "train loss:0.0016038478151237754\n",
      "train loss:0.03902691302352424\n",
      "train loss:0.009843648809720206\n",
      "train loss:0.02649233588804914\n",
      "train loss:0.04359027270452352\n",
      "train loss:0.01356245262022849\n",
      "train loss:0.0076202668977282595\n",
      "train loss:0.020251777493729608\n",
      "train loss:0.01169174356994119\n",
      "train loss:0.004591484777582967\n",
      "train loss:0.01171640692674317\n",
      "train loss:0.017553135444773663\n",
      "train loss:0.09042062143059776\n",
      "train loss:0.017329448701888597\n",
      "train loss:0.0044141679380793495\n",
      "train loss:0.0074789681727109635\n",
      "train loss:0.011597779652221111\n",
      "train loss:0.00526387620191569\n",
      "train loss:0.0031544452101873195\n",
      "train loss:0.004000550600777904\n",
      "train loss:0.010328905599255216\n",
      "train loss:0.01993865833899639\n",
      "train loss:0.01879309699493692\n",
      "train loss:0.0006850811537369297\n",
      "train loss:0.014129471544276979\n",
      "train loss:0.003413512112090558\n",
      "train loss:0.0026583800015702874\n",
      "train loss:0.003959804911505522\n",
      "train loss:0.023121724634833904\n",
      "train loss:0.0037489880471469135\n",
      "train loss:0.0042667573565092105\n",
      "train loss:0.010332229102520107\n",
      "train loss:0.006059883427167044\n",
      "train loss:0.07390658155129402\n",
      "train loss:0.00505936857131994\n",
      "train loss:0.012225360309159505\n",
      "train loss:0.0013052658661386219\n",
      "train loss:0.007420590314346224\n",
      "train loss:0.003794840827155352\n",
      "train loss:0.020922886968191258\n",
      "train loss:0.021848377455591084\n",
      "train loss:0.024337851911761833\n",
      "train loss:0.0038089161582596505\n",
      "train loss:0.001379031863124389\n",
      "train loss:0.012153293678784913\n",
      "train loss:0.004894447419591259\n",
      "train loss:0.007934606642912468\n",
      "train loss:0.010676825826018142\n",
      "train loss:0.011709661368651559\n",
      "train loss:0.004949958564298034\n",
      "train loss:0.0013176380447846997\n",
      "train loss:0.013769625723412416\n",
      "train loss:0.0059274248813567946\n",
      "train loss:0.00700479578330834\n",
      "train loss:0.0010593998924597271\n",
      "train loss:0.009891478179727846\n",
      "train loss:0.005176361695490703\n",
      "train loss:0.03764524839941985\n",
      "train loss:0.0076262301490341855\n",
      "train loss:0.0028361134176056895\n",
      "train loss:0.032387494688715726\n",
      "train loss:0.0012704415587927761\n",
      "train loss:0.007890473119347416\n",
      "train loss:0.006320240442162216\n",
      "train loss:0.006385739938284046\n",
      "train loss:0.015853766546371623\n",
      "train loss:0.007449617240640084\n",
      "train loss:0.007393528695848095\n",
      "train loss:0.009306238837763074\n",
      "train loss:0.0016233236535068123\n",
      "train loss:0.012247752243908661\n",
      "train loss:0.03459156692729007\n",
      "train loss:0.006282968592154551\n",
      "train loss:0.012128755489682957\n",
      "train loss:0.002100970982833275\n",
      "train loss:0.004504062551440037\n",
      "train loss:0.019587089112788142\n",
      "train loss:0.0027808241529403838\n",
      "train loss:0.010621238034336988\n",
      "train loss:0.013830216257169245\n",
      "train loss:0.011131581192995808\n",
      "train loss:0.00909483418782182\n",
      "train loss:0.003483535688111207\n",
      "train loss:0.005068561265851058\n",
      "train loss:0.00734007520016797\n",
      "train loss:0.011583388792799973\n",
      "train loss:0.050996397928262625\n",
      "train loss:0.002096070602400214\n",
      "train loss:0.006896055234426406\n",
      "train loss:0.011739820207483731\n",
      "train loss:0.004310664138780287\n",
      "train loss:0.010417588005210934\n",
      "train loss:0.011945296868065464\n",
      "train loss:0.007129091500754604\n",
      "train loss:0.021727453037136957\n",
      "train loss:0.0014355633823989996\n",
      "train loss:0.020064999783434775\n",
      "train loss:0.0030377614397264054\n",
      "train loss:0.008718657346220855\n",
      "train loss:0.0019532999986400855\n",
      "train loss:0.004710056895606407\n",
      "train loss:0.008583201272992428\n",
      "train loss:0.007201361742859849\n",
      "train loss:0.000584638037646908\n",
      "train loss:0.006892889761673516\n",
      "train loss:0.0027530719213385065\n",
      "train loss:0.00032027508095188367\n",
      "train loss:0.003754836520063246\n",
      "train loss:0.010637375570297458\n",
      "train loss:0.01838910913918715\n",
      "train loss:0.016683338770688953\n",
      "train loss:0.03400759070857938\n",
      "train loss:0.0057507979305422815\n",
      "train loss:0.015559056520973651\n",
      "train loss:0.016391738183390663\n",
      "train loss:0.004621489418143154\n",
      "train loss:0.005271691428228506\n",
      "train loss:0.009338100765674259\n",
      "train loss:0.012379224876514988\n",
      "train loss:0.0036040684292311964\n",
      "train loss:0.014678708055303918\n",
      "train loss:0.0034570618387255564\n",
      "train loss:0.0019528342121679506\n",
      "train loss:0.002892395148715125\n",
      "train loss:0.004266716197375857\n",
      "train loss:0.04211061122004179\n",
      "train loss:0.014249127302930103\n",
      "train loss:0.007618678545479856\n",
      "train loss:0.006310653275675086\n",
      "train loss:0.0007027768546576914\n",
      "train loss:0.006501914631190703\n",
      "train loss:0.0074624125048153135\n",
      "train loss:0.020369519092737896\n",
      "train loss:0.008183060622261156\n",
      "train loss:0.0056714081473526355\n",
      "train loss:0.002337152100497101\n",
      "train loss:0.004681246164103546\n",
      "train loss:0.017708326782028015\n",
      "train loss:0.008680003755666597\n",
      "train loss:0.0039768307434811965\n",
      "train loss:0.002079711230455336\n",
      "train loss:0.0034347296639314464\n",
      "train loss:0.012320459745814765\n",
      "train loss:0.005723512204901927\n",
      "train loss:0.0017502958973909205\n",
      "train loss:0.01101497191976869\n",
      "train loss:0.006157252305504004\n",
      "train loss:0.00768869221587458\n",
      "train loss:0.10209239643135223\n",
      "train loss:0.003889489790286075\n",
      "train loss:0.012064376976057483\n",
      "train loss:0.0027728854678860443\n",
      "train loss:0.0014460513816084057\n",
      "train loss:0.007076708384939163\n",
      "train loss:0.053070891945339656\n",
      "train loss:0.007955849378121728\n",
      "train loss:0.0011825052829252235\n",
      "train loss:0.019238889873890094\n",
      "train loss:0.006016967561175845\n",
      "train loss:0.015628353632196633\n",
      "train loss:0.007583301548488045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.004335658771494647\n",
      "train loss:0.012025390403965512\n",
      "train loss:0.009634554376359078\n",
      "train loss:0.010585552897731632\n",
      "train loss:0.0012959813253260151\n",
      "train loss:0.01640941037226634\n",
      "train loss:0.00499389793345406\n",
      "train loss:0.017047290640658123\n",
      "train loss:0.0022560898449755936\n",
      "train loss:0.000989313784644526\n",
      "train loss:0.006269374837065184\n",
      "train loss:0.010929336955785714\n",
      "train loss:0.025873155021290967\n",
      "train loss:0.004638799682866509\n",
      "train loss:0.0023890560529239704\n",
      "train loss:0.019324948147055317\n",
      "train loss:0.06763385334475236\n",
      "train loss:0.0025504367566304208\n",
      "train loss:0.00565004209916785\n",
      "train loss:0.011973470910593835\n",
      "train loss:0.0031003698951905805\n",
      "train loss:0.004736775373825845\n",
      "train loss:0.011864148667264221\n",
      "train loss:0.025956806601808328\n",
      "train loss:0.011452933226289146\n",
      "train loss:0.003984105872449984\n",
      "train loss:0.00309235200914955\n",
      "train loss:0.006817693266887637\n",
      "train loss:0.0020268059589735156\n",
      "train loss:0.0011815474129888025\n",
      "train loss:0.0016669120766036316\n",
      "train loss:0.0032094563703553227\n",
      "train loss:0.005484154914877678\n",
      "train loss:0.005506487841364364\n",
      "train loss:0.0013736081451443233\n",
      "train loss:0.0043707835313667774\n",
      "train loss:0.005400362999713041\n",
      "train loss:0.03307566624117065\n",
      "train loss:0.007687792593964159\n",
      "train loss:0.020528777594102984\n",
      "train loss:0.014288261272865295\n",
      "train loss:0.009264717345644364\n",
      "train loss:0.006663997132886583\n",
      "train loss:0.01597521583287698\n",
      "train loss:0.0025620862787837993\n",
      "train loss:0.005021264484483484\n",
      "train loss:0.012237857936872881\n",
      "train loss:0.007462851914946258\n",
      "train loss:0.003782709952628893\n",
      "train loss:0.010113544564227951\n",
      "train loss:0.006720938183844992\n",
      "train loss:0.01100533967216518\n",
      "train loss:0.004952358882681538\n",
      "train loss:0.01365384937712256\n",
      "train loss:0.006836077627821545\n",
      "train loss:0.0010161712234182064\n",
      "train loss:0.0022354480617465685\n",
      "train loss:0.00608894604885774\n",
      "train loss:0.0069830376507203475\n",
      "train loss:0.030622242773891535\n",
      "train loss:0.0035204117932705094\n",
      "train loss:0.0012685975322529635\n",
      "train loss:0.0038600479153942813\n",
      "train loss:0.015314062686994496\n",
      "train loss:0.0038362196064843468\n",
      "train loss:0.0023081378287729297\n",
      "train loss:0.027818533254574108\n",
      "train loss:0.005454491570435272\n",
      "train loss:0.06929235809747883\n",
      "train loss:0.02296417058413375\n",
      "train loss:0.009388556519002158\n",
      "train loss:0.011996002704017818\n",
      "train loss:0.0035536433646063837\n",
      "train loss:0.005391438291567265\n",
      "train loss:0.024430022663329668\n",
      "train loss:0.030355479431464157\n",
      "train loss:0.00288482015508823\n",
      "train loss:0.010661082173841431\n",
      "train loss:0.0106152847311818\n",
      "train loss:0.0013267272143046357\n",
      "train loss:0.00901047339794455\n",
      "train loss:0.09202946143002905\n",
      "train loss:0.012038794396388336\n",
      "train loss:0.02265238078274522\n",
      "train loss:0.0021198385003427045\n",
      "train loss:0.005728619740972727\n",
      "train loss:0.008729651276161036\n",
      "train loss:0.00906940684049375\n",
      "train loss:0.00978285505505257\n",
      "train loss:0.011874415251441929\n",
      "train loss:0.002059554696881306\n",
      "train loss:0.0026918777269652654\n",
      "train loss:0.00829281298038341\n",
      "train loss:0.01043075411279607\n",
      "train loss:0.003680504531204439\n",
      "train loss:0.008289818330445316\n",
      "train loss:0.01890531798606608\n",
      "train loss:0.07944316538105894\n",
      "train loss:0.003962551761782301\n",
      "train loss:0.002448292709177435\n",
      "train loss:0.009721236326786935\n",
      "train loss:0.0009382768818294695\n",
      "train loss:0.011231990406235563\n",
      "train loss:0.009189397262646393\n",
      "train loss:0.0028299504828333093\n",
      "train loss:0.0011432730387976374\n",
      "train loss:0.00503061294809648\n",
      "train loss:0.02133370499528787\n",
      "train loss:0.02218003580359431\n",
      "train loss:0.0027983907389432716\n",
      "train loss:0.011331905421635311\n",
      "train loss:0.00253364575150484\n",
      "train loss:0.017457409634892673\n",
      "train loss:0.0004095564084946076\n",
      "train loss:0.012885400098368806\n",
      "train loss:0.005335385899470262\n",
      "train loss:0.007336906663948889\n",
      "train loss:0.002633391619594466\n",
      "train loss:0.002061781206616462\n",
      "train loss:0.013238937944608908\n",
      "train loss:0.026865203713481146\n",
      "train loss:0.00213428346898169\n",
      "train loss:0.005237950407427653\n",
      "train loss:0.003477793602587612\n",
      "train loss:0.0036447539024919064\n",
      "train loss:0.013832467737466173\n",
      "train loss:0.0031002991887976446\n",
      "train loss:0.001724502764262302\n",
      "train loss:0.014744317722785281\n",
      "train loss:0.0009910020009774318\n",
      "train loss:0.0024498398693795073\n",
      "train loss:0.008654015850746289\n",
      "train loss:0.008615811598707836\n",
      "train loss:0.0026644902880666797\n",
      "train loss:0.0152236972499031\n",
      "train loss:0.007320551947163784\n",
      "train loss:0.003119505676554319\n",
      "train loss:0.009603067186196907\n",
      "train loss:0.0029898770899509154\n",
      "train loss:0.009724668232397742\n",
      "train loss:0.005568399394416399\n",
      "train loss:0.008361520819941595\n",
      "train loss:0.03256279788570892\n",
      "train loss:0.0052159154596321785\n",
      "train loss:0.0029601305301989018\n",
      "train loss:0.045766836732681775\n",
      "train loss:0.0037586337078635884\n",
      "train loss:0.009691348592276997\n",
      "train loss:0.0015164496139057648\n",
      "train loss:0.009333501067778225\n",
      "train loss:0.0008589576049388059\n",
      "train loss:0.008257942979923294\n",
      "train loss:0.0053788351336887\n",
      "train loss:0.017355911446301726\n",
      "train loss:0.004942099382165105\n",
      "train loss:0.005230945577679827\n",
      "train loss:0.004743761250057497\n",
      "train loss:0.002980307277938729\n",
      "train loss:0.017561584616043787\n",
      "train loss:0.003240042947397195\n",
      "train loss:0.013007789421038196\n",
      "train loss:0.008314702818348188\n",
      "train loss:0.007869101041009284\n",
      "train loss:0.017556012258640032\n",
      "train loss:0.004695444799903004\n",
      "train loss:0.011614988298169267\n",
      "train loss:0.00987980904985617\n",
      "train loss:0.00667651714692088\n",
      "train loss:0.0018430246452633017\n",
      "train loss:0.010292244844808807\n",
      "train loss:0.0029298624149030807\n",
      "train loss:0.00974269992710494\n",
      "train loss:0.015984798914693156\n",
      "train loss:0.005176050913041968\n",
      "train loss:0.001032196932206651\n",
      "train loss:0.025445792812973527\n",
      "train loss:0.010092644827175135\n",
      "train loss:0.00811954782287817\n",
      "train loss:0.0037067330413401607\n",
      "train loss:0.013874258113549927\n",
      "train loss:0.0036480632038922594\n",
      "train loss:0.014390240882901736\n",
      "train loss:0.0032490278636366464\n",
      "train loss:0.00689920951570621\n",
      "train loss:0.002295847407364732\n",
      "=== epoch:10, train acc:0.995, test acc:0.984 ===\n",
      "train loss:0.01562837362543706\n",
      "train loss:0.0022237988171087343\n",
      "train loss:0.006180305234207183\n",
      "train loss:0.022506780939922744\n",
      "train loss:0.005664572311644756\n",
      "train loss:0.004698947227539332\n",
      "train loss:0.0018157212424579994\n",
      "train loss:0.018877366106207175\n",
      "train loss:0.004790243544984816\n",
      "train loss:0.010280589320234547\n",
      "train loss:0.006428293655845771\n",
      "train loss:0.006486936731960418\n",
      "train loss:0.02785517790338866\n",
      "train loss:0.019153811552592194\n",
      "train loss:0.007926936525344173\n",
      "train loss:0.006468912339724143\n",
      "train loss:0.07742373224968815\n",
      "train loss:0.007779990688674551\n",
      "train loss:0.04335491889216971\n",
      "train loss:0.006162176543675244\n",
      "train loss:0.006871623071549287\n",
      "train loss:0.006855723193683237\n",
      "train loss:0.007759265778213229\n",
      "train loss:0.003256096326338845\n",
      "train loss:0.002084488429357915\n",
      "train loss:0.012098813059385457\n",
      "train loss:0.0013995935025811466\n",
      "train loss:0.019259328906462526\n",
      "train loss:0.016079729558148688\n",
      "train loss:0.004389672145973591\n",
      "train loss:0.0043827676749061\n",
      "train loss:0.01009168260907466\n",
      "train loss:0.013766958014268965\n",
      "train loss:0.006259654981165218\n",
      "train loss:0.03193147370575607\n",
      "train loss:0.0068041607594845396\n",
      "train loss:0.01686459992564867\n",
      "train loss:0.013590958798233571\n",
      "train loss:0.012806786598293339\n",
      "train loss:0.00825194767016199\n",
      "train loss:0.003274505893778969\n",
      "train loss:0.004361239811956257\n",
      "train loss:0.00665713676456193\n",
      "train loss:0.002519983288808666\n",
      "train loss:0.003139423586366401\n",
      "train loss:0.015075896309538657\n",
      "train loss:0.003405685272127697\n",
      "train loss:0.0006585140727130273\n",
      "train loss:0.005699084504469054\n",
      "train loss:0.012277633859884434\n",
      "train loss:0.012353728320938616\n",
      "train loss:0.007061353479191237\n",
      "train loss:0.004294633467272844\n",
      "train loss:0.018700675914491857\n",
      "train loss:0.003594694576614242\n",
      "train loss:0.00266016549742486\n",
      "train loss:0.0038661724926724477\n",
      "train loss:0.004693064081874902\n",
      "train loss:0.03039751089272693\n",
      "train loss:0.0022222024045727216\n",
      "train loss:0.011408631383722468\n",
      "train loss:0.053237320857213825\n",
      "train loss:0.006057676906472432\n",
      "train loss:0.01670428431434204\n",
      "train loss:0.002788884746797798\n",
      "train loss:0.015503879586120045\n",
      "train loss:0.0022430445788640656\n",
      "train loss:0.009920190596674537\n",
      "train loss:0.03657825182848369\n",
      "train loss:0.0041307875163187876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.024763159691821576\n",
      "train loss:0.0049577387695535526\n",
      "train loss:0.003341617611756856\n",
      "train loss:0.0736524594820557\n",
      "train loss:0.006586527562901416\n",
      "train loss:0.004811151601636716\n",
      "train loss:0.03462223268464708\n",
      "train loss:0.029969296676167656\n",
      "train loss:0.006947031378946909\n",
      "train loss:0.0015360239854317085\n",
      "train loss:0.001293358578918256\n",
      "train loss:0.003388744142136675\n",
      "train loss:0.0015149623909488973\n",
      "train loss:0.007991140001264163\n",
      "train loss:0.02474523659065576\n",
      "train loss:0.014468361083801905\n",
      "train loss:0.039768280684530526\n",
      "train loss:0.00864843876886232\n",
      "train loss:0.004826261167586442\n",
      "train loss:0.0070622607876085495\n",
      "train loss:0.0070695788806957385\n",
      "train loss:0.011632286103723334\n",
      "train loss:0.00596425846208565\n",
      "train loss:0.0034445372860244395\n",
      "train loss:0.01577896895785838\n",
      "train loss:0.0033816657631947195\n",
      "train loss:0.0049254753953654334\n",
      "train loss:0.007339686796382475\n",
      "train loss:0.005270033207933412\n",
      "train loss:0.023334048190238432\n",
      "train loss:0.0021963981526891643\n",
      "train loss:0.0020385007776504383\n",
      "train loss:0.0012386966262292545\n",
      "train loss:0.008889561578204046\n",
      "train loss:0.015573909018510695\n",
      "train loss:0.009601947786866493\n",
      "train loss:0.0072091464093304684\n",
      "train loss:0.013467638519265359\n",
      "train loss:0.021715580482050426\n",
      "train loss:0.055412280764363546\n",
      "train loss:0.00833977428159263\n",
      "train loss:0.0017540652796604965\n",
      "train loss:0.006931939308380035\n",
      "train loss:0.05589376223146548\n",
      "train loss:0.005800705818278515\n",
      "train loss:0.007352835778320643\n",
      "train loss:0.0008790435971920156\n",
      "train loss:0.01108396540435147\n",
      "train loss:0.021343329255871218\n",
      "train loss:0.008198440510625231\n",
      "train loss:0.0028132528191020857\n",
      "train loss:0.003187210790162721\n",
      "train loss:0.02637347317481923\n",
      "train loss:0.014410659816715602\n",
      "train loss:0.004115973575786192\n",
      "train loss:0.009729181724390946\n",
      "train loss:0.017335783878502382\n",
      "train loss:0.012108542923582391\n",
      "train loss:0.00657812492756466\n",
      "train loss:0.005270287172474008\n",
      "train loss:0.01597577539884952\n",
      "train loss:0.005196011066894989\n",
      "train loss:0.005011052057539195\n",
      "train loss:0.0015993672008408743\n",
      "train loss:0.0054265970679309794\n",
      "train loss:0.010994991403345873\n",
      "train loss:0.015757877282976673\n",
      "train loss:0.0041350957192822545\n",
      "train loss:0.009833302354823487\n",
      "train loss:0.0058251845586644035\n",
      "train loss:0.003128643011032855\n",
      "train loss:0.004123122651962504\n",
      "train loss:0.013843415366195851\n",
      "train loss:0.013182436574807648\n",
      "train loss:0.006658435830572236\n",
      "train loss:0.002489804755795892\n",
      "train loss:0.003543456745394814\n",
      "train loss:0.0014636948741878666\n",
      "train loss:0.06790538345661136\n",
      "train loss:0.0053453785536730965\n",
      "train loss:0.0017939734062254957\n",
      "train loss:0.020553278916712592\n",
      "train loss:0.006226544318903495\n",
      "train loss:0.011496157274221626\n",
      "train loss:0.022786347999840348\n",
      "train loss:0.007627678279423191\n",
      "train loss:0.019152807738941326\n",
      "train loss:0.0028250066208448613\n",
      "train loss:0.008897344440437513\n",
      "train loss:0.008418321440656264\n",
      "train loss:0.006381351114521682\n",
      "train loss:0.005252992028075978\n",
      "train loss:0.016050150581921784\n",
      "train loss:0.006828634645404011\n",
      "train loss:0.00952322001027492\n",
      "train loss:0.00036531392016827033\n",
      "train loss:0.0008501589128220844\n",
      "train loss:0.0017434939576046647\n",
      "train loss:0.009044563301847868\n",
      "train loss:0.013755840669303408\n",
      "train loss:0.008584927108402961\n",
      "train loss:0.004719201773312189\n",
      "train loss:0.003009217863983403\n",
      "train loss:0.007019737467913668\n",
      "train loss:0.007332925877096837\n",
      "train loss:0.005750802525348878\n",
      "train loss:0.010404995094984958\n",
      "train loss:0.02003412593442868\n",
      "train loss:0.005402707419542039\n",
      "train loss:0.0013900696020786771\n",
      "train loss:0.009887127386074812\n",
      "train loss:0.00482336264278923\n",
      "train loss:0.011287969620643268\n",
      "train loss:0.004292107174185792\n",
      "train loss:0.004709705980330745\n",
      "train loss:0.011652809407332392\n",
      "train loss:0.013835495833574714\n",
      "train loss:0.008358023184098477\n",
      "train loss:0.002738606281505189\n",
      "train loss:0.0008537098773446534\n",
      "train loss:0.004775104903229408\n",
      "train loss:0.0031291225393528127\n",
      "train loss:0.005375594298339803\n",
      "train loss:0.009464027825744405\n",
      "train loss:0.0056144831097035665\n",
      "train loss:0.019510287070097408\n",
      "train loss:0.0020785396133090423\n",
      "train loss:0.004253222432637983\n",
      "train loss:0.006411519359084957\n",
      "train loss:0.038325440449037226\n",
      "train loss:0.0018137821537874096\n",
      "train loss:0.035967026308933565\n",
      "train loss:0.005118715571799195\n",
      "train loss:0.008735078833574182\n",
      "train loss:0.0008530527046734462\n",
      "train loss:0.010636928016951012\n",
      "train loss:0.005949009170620934\n",
      "train loss:0.011434939361186714\n",
      "train loss:0.016409436534219286\n",
      "train loss:0.02457970428933633\n",
      "train loss:0.0026024345234245865\n",
      "train loss:0.002990923462189335\n",
      "train loss:0.008841774596453522\n",
      "train loss:0.004541238806916991\n",
      "train loss:0.008807191839654747\n",
      "train loss:0.005702737810489154\n",
      "train loss:0.005328939840745354\n",
      "train loss:0.0046897414526643685\n",
      "train loss:0.016488689885208584\n",
      "train loss:0.007465133294507704\n",
      "train loss:0.004922678669705528\n",
      "train loss:0.0009017626122450325\n",
      "train loss:0.0009416040794039454\n",
      "train loss:0.004306702801534097\n",
      "train loss:0.003188660250147094\n",
      "train loss:0.003093666556130916\n",
      "train loss:0.0011393019097793767\n",
      "train loss:0.030043166019632098\n",
      "train loss:0.009573688668131823\n",
      "train loss:0.007389096761448651\n",
      "train loss:0.0013523580270309115\n",
      "train loss:0.005874890702835592\n",
      "train loss:0.0018744485881592645\n",
      "train loss:0.0034475546514252152\n",
      "train loss:0.008111228131978085\n",
      "train loss:0.025129436165210704\n",
      "train loss:0.003927955580198458\n",
      "train loss:0.004783815800733539\n",
      "train loss:0.009938201493470826\n",
      "train loss:0.003439276419822684\n",
      "train loss:0.0011544502342744649\n",
      "train loss:0.00046287343143384333\n",
      "train loss:0.004023770718896143\n",
      "train loss:0.0051790453103334345\n",
      "train loss:0.00774816516609645\n",
      "train loss:0.0038416126713947834\n",
      "train loss:0.007895228412000764\n",
      "train loss:0.0028984563927503416\n",
      "train loss:0.002572839662491016\n",
      "train loss:0.013634436631248336\n",
      "train loss:0.0017940020240524675\n",
      "train loss:0.006860950433604645\n",
      "train loss:0.0008933773882292547\n",
      "train loss:0.0031205253727448453\n",
      "train loss:0.0015089115572956432\n",
      "train loss:0.0021139185030348466\n",
      "train loss:0.01737761481120535\n",
      "train loss:0.04083878856700788\n",
      "train loss:0.03671080658225816\n",
      "train loss:0.005728913086762415\n",
      "train loss:0.006942081938560842\n",
      "train loss:0.007906009846587393\n",
      "train loss:0.0074831325629750155\n",
      "train loss:0.028340468384834953\n",
      "train loss:0.0031916640618203374\n",
      "train loss:0.03136103468118558\n",
      "train loss:0.003921424818829991\n",
      "train loss:0.007147989187884948\n",
      "train loss:0.005626468886087252\n",
      "train loss:0.005619673202065183\n",
      "train loss:0.0020674312408615375\n",
      "train loss:0.04249997285811932\n",
      "train loss:0.0034711668035081866\n",
      "train loss:0.014082951875767705\n",
      "train loss:0.007594288846423476\n",
      "train loss:0.0042306081983695286\n",
      "train loss:0.017199016612120997\n",
      "train loss:0.014134238903138904\n",
      "train loss:0.004834015481684507\n",
      "train loss:0.007219295509271626\n",
      "train loss:0.003462401446016296\n",
      "train loss:0.001740229908145873\n",
      "train loss:0.029524611228800218\n",
      "train loss:0.0025179756420752437\n",
      "train loss:0.06037252942029173\n",
      "train loss:0.0034766821901710894\n",
      "train loss:0.0027053417819943694\n",
      "train loss:0.006332504762523847\n",
      "train loss:0.01576489009817855\n",
      "train loss:0.003487159116453905\n",
      "train loss:0.011752708549307083\n",
      "train loss:0.0009079368838926415\n",
      "train loss:0.01072999832312751\n",
      "train loss:0.0035997520911308966\n",
      "train loss:0.00790629359674815\n",
      "train loss:0.005955866364363618\n",
      "train loss:0.016752168555566337\n",
      "train loss:0.0014617760063355175\n",
      "train loss:0.0007352989858417278\n",
      "train loss:0.08247083086858939\n",
      "train loss:0.01883277882695555\n",
      "train loss:0.011819340148684188\n",
      "train loss:0.005877721044966885\n",
      "train loss:0.006608434573127241\n",
      "train loss:0.010959665822757515\n",
      "train loss:0.014461456960029276\n",
      "train loss:0.010965717189625614\n",
      "train loss:0.007291321977833455\n",
      "train loss:0.004341661344265374\n",
      "train loss:0.007044142350620604\n",
      "train loss:0.004736155989377796\n",
      "train loss:0.06265359767364016\n",
      "train loss:0.009094415576206794\n",
      "train loss:0.012535968097101788\n",
      "train loss:0.008888598239895807\n",
      "train loss:0.004887429608026151\n",
      "train loss:0.012124554702069554\n",
      "train loss:0.004047970087296296\n",
      "train loss:0.008936699148558498\n",
      "train loss:0.004618578950720068\n",
      "train loss:0.0003901638539844212\n",
      "train loss:0.003444982152347138\n",
      "train loss:0.003855565163518704\n",
      "train loss:0.0037110394105126345\n",
      "train loss:0.009078493008782275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0012942077092509263\n",
      "train loss:0.008160984003857893\n",
      "train loss:0.003932986708793701\n",
      "train loss:0.005948769722905046\n",
      "train loss:0.009445930353966195\n",
      "train loss:0.004778531924163183\n",
      "train loss:0.004429550920619615\n",
      "train loss:0.00919794880980075\n",
      "train loss:0.0031388298526680227\n",
      "train loss:0.005461156767349501\n",
      "train loss:0.01452657702519924\n",
      "train loss:0.03513466641588379\n",
      "train loss:0.04433524757079227\n",
      "train loss:0.023024462519551814\n",
      "train loss:0.007140181198952738\n",
      "train loss:0.016322114046728824\n",
      "train loss:0.00452486495779119\n",
      "train loss:0.007797372316997878\n",
      "train loss:0.00614914952928495\n",
      "train loss:0.019981578771080293\n",
      "train loss:0.011339092467497803\n",
      "train loss:0.002525946356562963\n",
      "train loss:0.004563552415659197\n",
      "train loss:0.006193187158188711\n",
      "train loss:0.0010551293992535039\n",
      "train loss:0.006977927935492913\n",
      "train loss:0.005743476180034401\n",
      "train loss:0.015410095124332052\n",
      "train loss:0.004224676751972\n",
      "train loss:0.002641166689303887\n",
      "train loss:0.0023239310919183328\n",
      "train loss:0.013349111329955941\n",
      "train loss:0.009116118967311482\n",
      "train loss:0.009856297535650458\n",
      "train loss:0.0043422266262930255\n",
      "train loss:0.009501580480688516\n",
      "train loss:0.02574913789901453\n",
      "train loss:0.0029060731444826905\n",
      "train loss:0.025558651601650793\n",
      "train loss:0.00427681405396798\n",
      "train loss:0.0049548670684662535\n",
      "train loss:0.00832281624190759\n",
      "train loss:0.0031503606432797936\n",
      "train loss:0.007521872052669227\n",
      "train loss:0.008892484614685382\n",
      "train loss:0.007749102665964735\n",
      "train loss:0.003980982246632061\n",
      "train loss:0.008356008401630942\n",
      "train loss:0.001278357544563741\n",
      "train loss:0.0016152662963811765\n",
      "train loss:0.0017380015207006463\n",
      "train loss:0.004903693389534641\n",
      "train loss:0.01460194759691701\n",
      "train loss:0.008434206958758409\n",
      "train loss:0.004572116304140702\n",
      "train loss:0.002245641820511628\n",
      "train loss:0.01168156866101332\n",
      "train loss:0.0038716117837805986\n",
      "train loss:0.007003164279528941\n",
      "train loss:0.0567103520457978\n",
      "train loss:0.041989494249522634\n",
      "train loss:0.005182460023829963\n",
      "train loss:0.020211998305900588\n",
      "train loss:0.0025116485559049683\n",
      "train loss:0.03996732744975549\n",
      "train loss:0.012796215402724695\n",
      "train loss:0.003041049802988711\n",
      "train loss:0.014127471301143628\n",
      "train loss:0.01984209422464271\n",
      "train loss:0.0019001772751319787\n",
      "train loss:0.004570564638964408\n",
      "train loss:0.006029353778766369\n",
      "train loss:0.003084318907107735\n",
      "train loss:0.003739634003495941\n",
      "train loss:0.0031826548939538784\n",
      "train loss:0.05580698898877437\n",
      "train loss:0.007230859673859971\n",
      "train loss:0.010973144620979374\n",
      "train loss:0.009842646286585629\n",
      "train loss:0.004396359550193511\n",
      "train loss:0.003030397503031944\n",
      "train loss:0.00973447308747402\n",
      "train loss:0.01458265049082843\n",
      "train loss:0.005779647771062311\n",
      "train loss:0.007514694936524879\n",
      "train loss:0.005031664100690681\n",
      "train loss:0.006093721114154602\n",
      "train loss:0.004318074282665685\n",
      "train loss:0.004062182292631354\n",
      "train loss:0.015501878561710012\n",
      "train loss:0.016069521264691067\n",
      "train loss:0.02423488657978381\n",
      "train loss:0.0032609807048873483\n",
      "train loss:0.028251292316791098\n",
      "train loss:0.19134976201805795\n",
      "train loss:0.0006605338817479832\n",
      "train loss:0.002471366113891935\n",
      "train loss:0.0009878188703632488\n",
      "train loss:0.0022360302612056786\n",
      "train loss:0.0033953385476762294\n",
      "train loss:0.004499162733939025\n",
      "train loss:0.030813508807900538\n",
      "train loss:0.012126593542655315\n",
      "train loss:0.005533917745708637\n",
      "train loss:0.002611961007079045\n",
      "train loss:0.0011557057570000292\n",
      "train loss:0.001993505640946361\n",
      "train loss:0.0039174842990932135\n",
      "train loss:0.005435430548378714\n",
      "train loss:0.006094813562171983\n",
      "train loss:0.0024879158960190006\n",
      "train loss:0.007614779034942213\n",
      "train loss:0.0059892620307815\n",
      "train loss:0.0007900193129856444\n",
      "train loss:0.0029447871663198007\n",
      "train loss:0.005839867244767644\n",
      "train loss:0.0008777085249873094\n",
      "train loss:0.005792570201449742\n",
      "train loss:0.018804276629058606\n",
      "train loss:0.0029666367903846687\n",
      "train loss:0.008816520338299157\n",
      "train loss:0.012017696247191787\n",
      "train loss:0.0017124979926717192\n",
      "train loss:0.0018234322974048488\n",
      "train loss:0.0047961226517462905\n",
      "train loss:0.0017453674029544084\n",
      "train loss:0.002354485589062783\n",
      "train loss:0.005762999046376801\n",
      "train loss:0.009916894252772786\n",
      "train loss:0.001691905367090122\n",
      "train loss:0.002333938575943073\n",
      "train loss:0.019302684026509004\n",
      "train loss:0.03099103725966606\n",
      "train loss:0.002457223974358215\n",
      "train loss:0.0036579623436782194\n",
      "train loss:0.013579306481778262\n",
      "train loss:0.0016638389993439292\n",
      "train loss:0.029076397116454947\n",
      "train loss:0.015798037307343894\n",
      "train loss:0.006690973957655726\n",
      "train loss:0.0014021010472799866\n",
      "train loss:0.01858207247530439\n",
      "train loss:0.043982994981378386\n",
      "train loss:0.0029996065976068843\n",
      "train loss:0.002395774835579738\n",
      "train loss:0.010014877734681769\n",
      "train loss:0.014595256302232273\n",
      "train loss:0.002030264485546066\n",
      "train loss:0.004800804424180947\n",
      "train loss:0.0016977372640450581\n",
      "train loss:0.002957163805802277\n",
      "train loss:0.004695924900575486\n",
      "train loss:0.0006775924295195968\n",
      "train loss:0.0008134910646079249\n",
      "train loss:0.012656597622121369\n",
      "train loss:0.008575432770485081\n",
      "train loss:0.003354976654959268\n",
      "train loss:0.0021516340391173113\n",
      "train loss:0.02431698891012022\n",
      "train loss:0.0008101662477743851\n",
      "train loss:0.003473317716841919\n",
      "train loss:0.017199346659246422\n",
      "train loss:0.017080821203649236\n",
      "train loss:0.0030763188868230718\n",
      "train loss:0.005187991395263788\n",
      "train loss:0.001424189018768921\n",
      "train loss:0.002444455870223937\n",
      "train loss:0.005667009277941761\n",
      "train loss:0.0035525919908181765\n",
      "train loss:0.005914677856635221\n",
      "train loss:0.000803457653212497\n",
      "train loss:0.028911921051379793\n",
      "train loss:0.004624552568062131\n",
      "train loss:0.003262870105109856\n",
      "train loss:0.0009798164479460358\n",
      "train loss:0.0020693082884097897\n",
      "train loss:0.0011554954538700079\n",
      "train loss:0.0008102839947114426\n",
      "train loss:0.005983898125528363\n",
      "train loss:0.0017293945346423673\n",
      "train loss:0.0021104263616807395\n",
      "train loss:0.003986075422540176\n",
      "train loss:0.00412124961507299\n",
      "train loss:0.003826179661676619\n",
      "train loss:0.009086843106788527\n",
      "train loss:0.0041548722325080575\n",
      "train loss:0.004112185220469757\n",
      "train loss:0.005941715320852807\n",
      "train loss:0.0013711769244754372\n",
      "train loss:0.003724550373332458\n",
      "train loss:0.0016603514955207616\n",
      "train loss:0.003657573035247622\n",
      "train loss:0.003129897303427464\n",
      "train loss:0.007855577397795839\n",
      "train loss:0.0009147046135513865\n",
      "train loss:0.007726214415083337\n",
      "train loss:0.004882908263641926\n",
      "train loss:0.004870578222244966\n",
      "train loss:0.011005260045673839\n",
      "train loss:0.0017893926275177232\n",
      "train loss:0.008963320476904525\n",
      "train loss:0.0021582304863901006\n",
      "train loss:0.004747353287808082\n",
      "train loss:0.0033387890064309495\n",
      "train loss:0.0006649020690116447\n",
      "train loss:0.006731089533172073\n",
      "train loss:0.000781172553748691\n",
      "train loss:0.009404271499282617\n",
      "train loss:0.005751460851999053\n",
      "train loss:0.00879745599792678\n",
      "train loss:0.057473895245154914\n",
      "train loss:0.0017062244021496175\n",
      "train loss:0.005034871637048482\n",
      "train loss:0.0028182802596026413\n",
      "train loss:0.0065229479546766046\n",
      "train loss:0.00049191629610118\n",
      "train loss:0.019782210379151587\n",
      "train loss:0.0009350202013925245\n",
      "train loss:0.0002536523029590139\n",
      "train loss:0.003409192752396892\n",
      "train loss:0.022594759502887876\n",
      "train loss:0.009389285871835447\n",
      "train loss:0.003021286219317257\n",
      "train loss:0.007749732417637806\n",
      "train loss:0.0015064746375131955\n",
      "train loss:0.002413872592964378\n",
      "train loss:0.003451017034657076\n",
      "train loss:0.016444502005693976\n",
      "train loss:0.008071370290484372\n",
      "train loss:0.006154295368582546\n",
      "train loss:0.003772237789233476\n",
      "train loss:0.0019330155478602107\n",
      "train loss:0.0010231781239062078\n",
      "train loss:0.004506961393240308\n",
      "train loss:0.006652147723013101\n",
      "train loss:0.003319704100055729\n",
      "train loss:0.0012347040201888192\n",
      "train loss:0.015979550244251853\n",
      "train loss:0.015092897071832794\n",
      "train loss:0.004455708451408549\n",
      "train loss:0.0018968540511077768\n",
      "train loss:0.009913703481794095\n",
      "train loss:0.016716923707389253\n",
      "train loss:0.0029611473900153385\n",
      "train loss:0.003935361866502662\n",
      "train loss:0.007791169384828988\n",
      "train loss:0.0015849433416535589\n",
      "train loss:0.0065345371902725035\n",
      "train loss:0.0018541151516109492\n",
      "train loss:0.002756769682670959\n",
      "train loss:0.0006676819590882048\n",
      "train loss:0.006412716656176932\n",
      "train loss:0.0030044863021627443\n",
      "train loss:0.001983632681999013\n",
      "train loss:0.001541908686844121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.009998291954913169\n",
      "train loss:0.008606713350167218\n",
      "train loss:0.007720888739982109\n",
      "train loss:0.0037900146356257664\n",
      "train loss:0.007243612393092197\n",
      "train loss:0.0014118270709964549\n",
      "train loss:0.0011502880503085162\n",
      "train loss:0.002518589424202794\n",
      "train loss:0.0025073970877200124\n",
      "train loss:0.0022287307455400303\n",
      "train loss:0.007468009062916179\n",
      "train loss:0.020649538505946852\n",
      "train loss:0.0825480975248877\n",
      "train loss:0.007991834321042753\n",
      "train loss:0.013456955343004786\n",
      "train loss:0.0007085655651287881\n",
      "train loss:0.001799867474292464\n",
      "train loss:0.009551158523878236\n",
      "train loss:0.0047584786678726695\n",
      "train loss:0.023826127829394778\n",
      "=== epoch:11, train acc:0.994, test acc:0.989 ===\n",
      "train loss:0.002035159706577147\n",
      "train loss:0.009995786444428695\n",
      "train loss:0.003609834984071446\n",
      "train loss:0.0017790251476316225\n",
      "train loss:0.018022549164679026\n",
      "train loss:0.0011262949940054217\n",
      "train loss:0.0009069203443570023\n",
      "train loss:0.00948226339238609\n",
      "train loss:0.006147581039004857\n",
      "train loss:0.0009920561776095747\n",
      "train loss:0.0005536573265902365\n",
      "train loss:0.011177761220362759\n",
      "train loss:0.00209145135459945\n",
      "train loss:0.0026758350079968806\n",
      "train loss:0.009790447170139617\n",
      "train loss:0.0055199112849398276\n",
      "train loss:0.0017800913726574638\n",
      "train loss:0.017130284004630993\n",
      "train loss:0.019624446915334416\n",
      "train loss:0.00430918784779496\n",
      "train loss:0.006478353297363289\n",
      "train loss:0.008751328986065504\n",
      "train loss:0.008829200302799132\n",
      "train loss:0.002941796001358478\n",
      "train loss:0.0018270683713295618\n",
      "train loss:0.004059774895771713\n",
      "train loss:0.001295903876657092\n",
      "train loss:0.009908508993212967\n",
      "train loss:0.007789043409022287\n",
      "train loss:0.002338679181761004\n",
      "train loss:0.0046626316798196125\n",
      "train loss:0.0016916754596130615\n",
      "train loss:0.000747726696476137\n",
      "train loss:0.005388798490780159\n",
      "train loss:0.0017660858176379638\n",
      "train loss:0.0025924143736349905\n",
      "train loss:0.011352704501989018\n",
      "train loss:0.0016853035801540248\n",
      "train loss:0.00047685347250807554\n",
      "train loss:0.0015211359069713817\n",
      "train loss:0.008491615926386106\n",
      "train loss:0.011720757667021459\n",
      "train loss:0.0019877059686344055\n",
      "train loss:0.0047211471813361165\n",
      "train loss:0.001750657479547567\n",
      "train loss:0.010782677917065372\n",
      "train loss:0.002931322151146124\n",
      "train loss:0.00047994130074152555\n",
      "train loss:0.0028620908499576423\n",
      "train loss:0.002947732306839766\n",
      "train loss:0.0022991515835132903\n",
      "train loss:0.009376648442878688\n",
      "train loss:0.001346431054568563\n",
      "train loss:0.010696304535429324\n",
      "train loss:0.0022526637771839012\n",
      "train loss:0.0008812352161707389\n",
      "train loss:0.002392369563323611\n",
      "train loss:0.0036086946222660225\n",
      "train loss:0.010202199130428363\n",
      "train loss:0.002310820387062031\n",
      "train loss:0.005739341799447358\n",
      "train loss:0.007550362275961218\n",
      "train loss:0.002153114710668884\n",
      "train loss:0.029276992015361318\n",
      "train loss:0.0009060162820030765\n",
      "train loss:0.0014649017311441435\n",
      "train loss:0.005013124174707912\n",
      "train loss:0.00110829705790066\n",
      "train loss:0.00037409943015078497\n",
      "train loss:0.0010051293155678752\n",
      "train loss:0.008580680525423144\n",
      "train loss:0.002238934526244687\n",
      "train loss:0.0002559979434989753\n",
      "train loss:0.004487167195552353\n",
      "train loss:0.0012787114230808968\n",
      "train loss:0.008173406875586304\n",
      "train loss:0.025831108412087116\n",
      "train loss:0.004501765185793188\n",
      "train loss:0.0025881226548103457\n",
      "train loss:0.005179237230146804\n",
      "train loss:0.007041628914901566\n",
      "train loss:0.00032719633851889085\n",
      "train loss:0.008667312780898682\n",
      "train loss:0.001335061745142502\n",
      "train loss:0.004316249175410834\n",
      "train loss:0.002100870304296475\n",
      "train loss:0.0007562367284127239\n",
      "train loss:0.006316641671936781\n",
      "train loss:0.003532354720378532\n",
      "train loss:0.005507818187364197\n",
      "train loss:0.003064450018105395\n",
      "train loss:0.00037616449267996016\n",
      "train loss:0.0013641582622380005\n",
      "train loss:0.02195680889114696\n",
      "train loss:0.0016682881055032843\n",
      "train loss:0.003747390557649198\n",
      "train loss:0.004362632404487915\n",
      "train loss:0.0015383102067102216\n",
      "train loss:0.0014889063427711321\n",
      "train loss:0.0010338333378327761\n",
      "train loss:0.001955297060934707\n",
      "train loss:0.0018621185376937663\n",
      "train loss:0.001085405395932085\n",
      "train loss:0.0038255684754870205\n",
      "train loss:0.014325820276623722\n",
      "train loss:0.002480496268387565\n",
      "train loss:0.003952328807187932\n",
      "train loss:0.0018700087218310984\n",
      "train loss:0.007208280559071554\n",
      "train loss:0.0059276476657315735\n",
      "train loss:0.0011236074710114158\n",
      "train loss:0.00277562483471391\n",
      "train loss:0.0002663762312896644\n",
      "train loss:0.0019465558469587765\n",
      "train loss:0.002626371828511331\n",
      "train loss:0.006224684382678902\n",
      "train loss:0.00021488923569078583\n",
      "train loss:0.001603228842679394\n",
      "train loss:0.0014674782761114583\n",
      "train loss:0.012791003086145692\n",
      "train loss:0.0013703256483703534\n",
      "train loss:0.006623807588605645\n",
      "train loss:0.03135958784577388\n",
      "train loss:0.0006276381439520956\n",
      "train loss:0.007453176984673963\n",
      "train loss:0.0034946716036120358\n",
      "train loss:0.006840533891676757\n",
      "train loss:0.006032916391809434\n",
      "train loss:0.002074226182897137\n",
      "train loss:0.0022297672564205993\n",
      "train loss:0.005361537936932389\n",
      "train loss:0.0083167214404736\n",
      "train loss:0.0027412976014562114\n",
      "train loss:0.003849751571017742\n",
      "train loss:0.02827852966614786\n",
      "train loss:0.0056966445128172685\n",
      "train loss:0.004938789995799457\n",
      "train loss:0.003277531868967953\n",
      "train loss:0.009856897941446325\n",
      "train loss:0.00933535646433639\n",
      "train loss:0.0010871205640799132\n",
      "train loss:0.005381854283483997\n",
      "train loss:0.0002127485053964353\n",
      "train loss:0.00728034890429695\n",
      "train loss:0.0021877813125765\n",
      "train loss:0.0010404255193961155\n",
      "train loss:0.002581193665285624\n",
      "train loss:0.032786990461478274\n",
      "train loss:0.018022749362424192\n",
      "train loss:0.0096790613634062\n",
      "train loss:0.002812572901014623\n",
      "train loss:0.019081656922105245\n",
      "train loss:0.01709253028401891\n",
      "train loss:0.0015791735558024845\n",
      "train loss:0.0022793961888001677\n",
      "train loss:0.005333947113967331\n",
      "train loss:0.00833451660582145\n",
      "train loss:0.0007581985594200579\n",
      "train loss:0.014612856640900966\n",
      "train loss:0.028255965068727535\n",
      "train loss:0.00461971892307289\n",
      "train loss:0.003431500172255514\n",
      "train loss:0.012461560640337967\n",
      "train loss:0.002032005671990992\n",
      "train loss:0.007449546922166923\n",
      "train loss:0.002207860131791076\n",
      "train loss:0.0033218094039605124\n",
      "train loss:0.0017143506497521246\n",
      "train loss:0.0007052186287660624\n",
      "train loss:0.0013198694848273003\n",
      "train loss:0.006554610295153183\n",
      "train loss:0.0038402416641648666\n",
      "train loss:0.003921709396458622\n",
      "train loss:0.001040055646527292\n",
      "train loss:0.005002833089137284\n",
      "train loss:0.0019291292098036086\n",
      "train loss:0.0014662021719266942\n",
      "train loss:0.015720492971292216\n",
      "train loss:0.0039176433803183065\n",
      "train loss:0.0017797367290237197\n",
      "train loss:0.0017891383494748872\n",
      "train loss:0.02304517076765896\n",
      "train loss:0.0023439763151840484\n",
      "train loss:0.00047706134878749425\n",
      "train loss:0.001949076536925299\n",
      "train loss:0.0017638666467068582\n",
      "train loss:0.006958814384202115\n",
      "train loss:0.023198292662626484\n",
      "train loss:0.007722056820184766\n",
      "train loss:0.002171480291459646\n",
      "train loss:0.0005595229441150894\n",
      "train loss:0.001602357574226798\n",
      "train loss:0.033381923855358264\n",
      "train loss:0.021476868218357516\n",
      "train loss:0.016381860921843964\n",
      "train loss:0.001551783999150561\n",
      "train loss:0.004592631200005723\n",
      "train loss:0.004199858238273516\n",
      "train loss:0.0035316638036425266\n",
      "train loss:0.009041681537974328\n",
      "train loss:0.002470985872193547\n",
      "train loss:0.0016911511289084218\n",
      "train loss:0.0016422652545443638\n",
      "train loss:0.012398779827825794\n",
      "train loss:0.026255853999339242\n",
      "train loss:0.008399464210854565\n",
      "train loss:0.011827963832631303\n",
      "train loss:0.0026689238763935095\n",
      "train loss:0.006928127525224665\n",
      "train loss:0.0050619769216960295\n",
      "train loss:0.004833620652491119\n",
      "train loss:0.0009783471232279926\n",
      "train loss:0.0031631273110844893\n",
      "train loss:0.005008138341255193\n",
      "train loss:0.002683428481596435\n",
      "train loss:0.0018536626622803537\n",
      "train loss:0.014400709895079944\n",
      "train loss:0.009280933509827389\n",
      "train loss:0.01067598523778107\n",
      "train loss:0.005862014319887004\n",
      "train loss:0.003789767470496352\n",
      "train loss:0.007259385003773095\n",
      "train loss:0.00584696650533782\n",
      "train loss:0.0035197401111714682\n",
      "train loss:0.005284967303881119\n",
      "train loss:0.0038024018470023983\n",
      "train loss:0.0037878239830649173\n",
      "train loss:0.003600049631507788\n",
      "train loss:0.028421214885815685\n",
      "train loss:0.00558152129607696\n",
      "train loss:0.007408432452213189\n",
      "train loss:0.004007664412183996\n",
      "train loss:0.013741014291564437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0004294647235211582\n",
      "train loss:0.0011788732225402236\n",
      "train loss:0.0006141883850786052\n",
      "train loss:0.004743152677868554\n",
      "train loss:0.014597238283943043\n",
      "train loss:0.008219930422120948\n",
      "train loss:0.002069887212807136\n",
      "train loss:0.005993217430803697\n",
      "train loss:0.004872173098093292\n",
      "train loss:0.0013424304186210663\n",
      "train loss:0.003083322895317102\n",
      "train loss:0.003558446172264926\n",
      "train loss:0.01258929834580037\n",
      "train loss:0.0018120721580805524\n",
      "train loss:0.003460969324805385\n",
      "train loss:0.0008363767243916651\n",
      "train loss:0.01922306733994176\n",
      "train loss:0.009517379935898792\n",
      "train loss:0.00431304024146131\n",
      "train loss:0.0013676549341047229\n",
      "train loss:0.0015074197495109519\n",
      "train loss:0.007850625702016872\n",
      "train loss:0.01060550498941351\n",
      "train loss:0.0006023999414695337\n",
      "train loss:0.0026634061048577246\n",
      "train loss:0.0015446085688728955\n",
      "train loss:0.006459770428763175\n",
      "train loss:0.00030152746714299036\n",
      "train loss:0.012541613760985075\n",
      "train loss:0.0018626099094324862\n",
      "train loss:0.003181176891231767\n",
      "train loss:0.005784728773333883\n",
      "train loss:0.0010966114288310106\n",
      "train loss:0.004462768747482575\n",
      "train loss:0.0092834653153915\n",
      "train loss:0.0038982660167252866\n",
      "train loss:0.001537783546019392\n",
      "train loss:0.0028144149467128616\n",
      "train loss:0.005250269893489886\n",
      "train loss:0.003433397332801691\n",
      "train loss:0.0003785806046041597\n",
      "train loss:0.0041714520841000455\n",
      "train loss:0.0027085849300402137\n",
      "train loss:0.0013863073629654507\n",
      "train loss:0.003446261055583752\n",
      "train loss:0.00447850551219741\n",
      "train loss:0.007845918948315074\n",
      "train loss:0.0009774398368122196\n",
      "train loss:0.0014742828171262364\n",
      "train loss:0.00034342427850233447\n",
      "train loss:0.008198135725180796\n",
      "train loss:0.003971691712242832\n",
      "train loss:0.004410582990662509\n",
      "train loss:0.0009622411086412312\n",
      "train loss:0.0038097142824450696\n",
      "train loss:0.021628529296557203\n",
      "train loss:0.016663349961067154\n",
      "train loss:0.039026270385373686\n",
      "train loss:0.004374006788047353\n",
      "train loss:0.0035335444810190857\n",
      "train loss:0.0011200284616047801\n",
      "train loss:0.01658094554800789\n",
      "train loss:0.008717724914592734\n",
      "train loss:0.012214937428784515\n",
      "train loss:0.005445217734326023\n",
      "train loss:0.0006169165285632658\n",
      "train loss:0.005283073349185452\n",
      "train loss:0.008044895648313882\n",
      "train loss:0.0035351288619463006\n",
      "train loss:0.007464006045449258\n",
      "train loss:0.0042134000705471705\n",
      "train loss:0.007022884552641024\n",
      "train loss:0.0016501605933617808\n",
      "train loss:0.0027886725791856555\n",
      "train loss:0.0026367298802676352\n",
      "train loss:0.0070555554689854414\n",
      "train loss:0.005291561234676476\n",
      "train loss:0.000616320475734446\n",
      "train loss:0.0014221680494865057\n",
      "train loss:0.002563593635495566\n",
      "train loss:0.0006567850928884514\n",
      "train loss:0.0013232345095169848\n",
      "train loss:0.0012688703626638643\n",
      "train loss:0.0017149112508456016\n",
      "train loss:0.0012429394410893133\n",
      "train loss:0.01880337004116648\n",
      "train loss:0.005643897530372288\n",
      "train loss:0.002506801713013256\n",
      "train loss:0.005171395011627955\n",
      "train loss:0.0018352241534435466\n",
      "train loss:0.0026192495036724156\n",
      "train loss:0.0032937990926608287\n",
      "train loss:0.00653517755010766\n",
      "train loss:0.018747167776449507\n",
      "train loss:0.004571601520306412\n",
      "train loss:0.002436207773642648\n",
      "train loss:0.004897878241497962\n",
      "train loss:0.003517987232812813\n",
      "train loss:0.0020404036128696206\n",
      "train loss:0.0019593999101270797\n",
      "train loss:0.0024714199465764104\n",
      "train loss:0.003154906201044291\n",
      "train loss:0.004799034412384723\n",
      "train loss:0.0025111713711872654\n",
      "train loss:0.005788440012298133\n",
      "train loss:0.003440124206955122\n",
      "train loss:0.016981454522387012\n",
      "train loss:0.010776791664255889\n",
      "train loss:0.0013827797186698543\n",
      "train loss:0.0013124582422064796\n",
      "train loss:0.0026306315678311233\n",
      "train loss:0.0008789785239758705\n",
      "train loss:0.001707646166207685\n",
      "train loss:0.004336341229899678\n",
      "train loss:0.0029292116836318106\n",
      "train loss:0.013814212469459339\n",
      "train loss:0.004836909776421541\n",
      "train loss:0.005210990138689348\n",
      "train loss:0.011458565206002252\n",
      "train loss:0.0014807565539354153\n",
      "train loss:0.0020065109948391923\n",
      "train loss:0.003479745323814495\n",
      "train loss:0.009894928282090648\n",
      "train loss:0.005792481428650963\n",
      "train loss:0.0008110275939640148\n",
      "train loss:0.0034080480953390196\n",
      "train loss:0.008550900654968798\n",
      "train loss:0.004829416258933336\n",
      "train loss:0.0026655321629138183\n",
      "train loss:0.004231277916680856\n",
      "train loss:0.012899273745996918\n",
      "train loss:0.006888684073980747\n",
      "train loss:0.006668210562976585\n",
      "train loss:0.003231623407439248\n",
      "train loss:0.0053849646546216936\n",
      "train loss:0.005720365985028022\n",
      "train loss:0.010050388573367859\n",
      "train loss:0.0028570320702604175\n",
      "train loss:0.0013401203206422183\n",
      "train loss:0.005133194732829863\n",
      "train loss:0.0023068736193869034\n",
      "train loss:0.010621355628952646\n",
      "train loss:0.005072301761967006\n",
      "train loss:0.0015425597438941993\n",
      "train loss:0.0011774431527544426\n",
      "train loss:0.0005664766003970024\n",
      "train loss:0.007457876493416438\n",
      "train loss:0.030720607452811896\n",
      "train loss:0.008650616744067018\n",
      "train loss:0.0015321250077735377\n",
      "train loss:0.007373139871715067\n",
      "train loss:0.0037395137261179877\n",
      "train loss:0.006048915036914326\n",
      "train loss:0.0006478656317315096\n",
      "train loss:0.004391717341558697\n",
      "train loss:0.007041363626857639\n",
      "train loss:0.0015466064106539926\n",
      "train loss:0.0027180946368428145\n",
      "train loss:0.002724820530708279\n",
      "train loss:0.008732965983517159\n",
      "train loss:0.001372754546828005\n",
      "train loss:0.012155925100631113\n",
      "train loss:0.0037592676355887355\n",
      "train loss:0.019979337622181112\n",
      "train loss:0.0037129688516428084\n",
      "train loss:0.0029925113258535514\n",
      "train loss:0.005982112686073362\n",
      "train loss:0.006785034038067723\n",
      "train loss:0.03772013083262685\n",
      "train loss:0.00044284317330344603\n",
      "train loss:0.003037452724308667\n",
      "train loss:0.005210233091478701\n",
      "train loss:0.0021076721374490874\n",
      "train loss:0.0027691446457242396\n",
      "train loss:0.01869454900701008\n",
      "train loss:0.007231420013713043\n",
      "train loss:0.00010753014196687605\n",
      "train loss:0.010398423372461954\n",
      "train loss:0.002252214552098178\n",
      "train loss:0.0029283134829697546\n",
      "train loss:0.0034327342691584228\n",
      "train loss:0.0008667295577891408\n",
      "train loss:0.0015769941344491343\n",
      "train loss:0.011663397256589764\n",
      "train loss:0.020993218500302162\n",
      "train loss:0.0022284057592651843\n",
      "train loss:0.006662359476573921\n",
      "train loss:0.0016923250888574875\n",
      "train loss:0.002682703914287592\n",
      "train loss:0.01030543214316336\n",
      "train loss:0.0007277588331368326\n",
      "train loss:0.005192321465200682\n",
      "train loss:0.00750648975842038\n",
      "train loss:0.002412282433136382\n",
      "train loss:0.004310798834486529\n",
      "train loss:0.007704926587224575\n",
      "train loss:0.0037189556281294824\n",
      "train loss:0.0037452910654503023\n",
      "train loss:0.002207968713241511\n",
      "train loss:0.008443117110912203\n",
      "train loss:0.0025869300118948435\n",
      "train loss:0.0023841631712343127\n",
      "train loss:0.014343901778256984\n",
      "train loss:0.004675035957961725\n",
      "train loss:0.004801927853822977\n",
      "train loss:0.0019014277149503617\n",
      "train loss:0.0017108384882409287\n",
      "train loss:0.0009845021655735583\n",
      "train loss:0.014526768552460911\n",
      "train loss:0.001184785836372336\n",
      "train loss:0.003324687807638403\n",
      "train loss:0.008823789938933317\n",
      "train loss:0.002246349381051608\n",
      "train loss:0.0005920879877064269\n",
      "train loss:0.00037009620449210824\n",
      "train loss:0.003588064981280237\n",
      "train loss:0.0032968522991238345\n",
      "train loss:0.010777085969384792\n",
      "train loss:0.0014556238838900526\n",
      "train loss:0.025301217014784452\n",
      "train loss:0.0020500495813502275\n",
      "train loss:0.0005489919690028532\n",
      "train loss:0.002843031594058354\n",
      "train loss:0.0054178475532037085\n",
      "train loss:0.0011063232061074757\n",
      "train loss:0.0029547461677962334\n",
      "train loss:0.0037133396132883276\n",
      "train loss:0.003823188577530663\n",
      "train loss:0.0013749153140117279\n",
      "train loss:0.0021756294348401293\n",
      "train loss:0.001674200431675318\n",
      "train loss:0.009138221282677111\n",
      "train loss:0.014518782075386948\n",
      "train loss:0.003060621493497803\n",
      "train loss:0.008966385993171346\n",
      "train loss:0.017501039349215278\n",
      "train loss:0.0053734316807857885\n",
      "train loss:0.0642369749935991\n",
      "train loss:0.00035999317529697693\n",
      "train loss:0.0006404399525305606\n",
      "train loss:0.011976135079143677\n",
      "train loss:0.001048425662805122\n",
      "train loss:0.02644131779840843\n",
      "train loss:0.006259523736517018\n",
      "train loss:0.0014466463888550194\n",
      "train loss:0.0015279646409080627\n",
      "train loss:0.005000794924894842\n",
      "train loss:0.0008029252452062993\n",
      "train loss:0.0006747386631160376\n",
      "train loss:0.0011185007693139618\n",
      "train loss:0.005900987001165222\n",
      "train loss:0.029304436293299232\n",
      "train loss:0.0017754917950614714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.02217467136759087\n",
      "train loss:0.05280240381317263\n",
      "train loss:0.029275356513687057\n",
      "train loss:0.003500679530031361\n",
      "train loss:0.02340034448797192\n",
      "train loss:0.0025907577489040112\n",
      "train loss:0.0017030996111199884\n",
      "train loss:0.006049592921089305\n",
      "train loss:0.008290163755800444\n",
      "train loss:0.014721780632211454\n",
      "train loss:0.001701860607378614\n",
      "train loss:0.007355253117533666\n",
      "train loss:0.010376417734419488\n",
      "train loss:0.0033748392691546596\n",
      "train loss:0.029477090562273302\n",
      "train loss:0.0033816833442384515\n",
      "train loss:0.000991938944777188\n",
      "train loss:0.01073963138300729\n",
      "train loss:0.0010436914131985628\n",
      "train loss:0.006675791851949825\n",
      "train loss:0.007473604094502893\n",
      "train loss:0.0020957685841776082\n",
      "train loss:0.0678922066435816\n",
      "train loss:0.009772065898841949\n",
      "train loss:0.0005069597726191755\n",
      "train loss:0.0005155898331215429\n",
      "train loss:0.0030308014259051064\n",
      "train loss:0.0071090601744174566\n",
      "train loss:0.02250053218012661\n",
      "train loss:0.004916740338573156\n",
      "train loss:0.004239078574752589\n",
      "train loss:0.006183585097231635\n",
      "train loss:0.015248366033586475\n",
      "train loss:0.002333105353802467\n",
      "train loss:0.009172130963310557\n",
      "train loss:0.0008850464859554206\n",
      "train loss:0.005846032558524659\n",
      "train loss:0.0006059019605075117\n",
      "train loss:0.001005180016044815\n",
      "train loss:0.00891041878306379\n",
      "train loss:0.007772885285051583\n",
      "train loss:0.0017996390038066403\n",
      "train loss:0.0011764068752520068\n",
      "train loss:0.0012191543416597153\n",
      "train loss:0.008942718386980934\n",
      "train loss:0.0008145147183777772\n",
      "train loss:0.004039040857415957\n",
      "train loss:0.007297604896362022\n",
      "train loss:0.010983542035852905\n",
      "train loss:0.0061551653188853\n",
      "train loss:0.0016793824041235945\n",
      "train loss:0.004813734897822276\n",
      "train loss:0.009272070797753175\n",
      "train loss:0.0024420459668818687\n",
      "train loss:0.0029592167160470656\n",
      "train loss:0.006721017595846802\n",
      "train loss:0.0019884572363638177\n",
      "train loss:0.008622926751315742\n",
      "train loss:0.027748524999538008\n",
      "train loss:0.010470712558475282\n",
      "train loss:0.008040240706604305\n",
      "train loss:0.0017930191395550575\n",
      "train loss:0.006275583625016626\n",
      "train loss:0.174843444069871\n",
      "train loss:0.002141140780405101\n",
      "train loss:0.005218992210546256\n",
      "train loss:0.017662151433757198\n",
      "train loss:0.005819803026089651\n",
      "train loss:0.03672683036535942\n",
      "train loss:0.003952377353453124\n",
      "train loss:0.003200559712088897\n",
      "train loss:0.004091779087346522\n",
      "train loss:0.002879237701134724\n",
      "train loss:0.007642754102123422\n",
      "train loss:0.005616699089313376\n",
      "train loss:0.00166081502677365\n",
      "train loss:0.001386920802088412\n",
      "train loss:0.0006087133138379424\n",
      "train loss:0.0033842184057800654\n",
      "train loss:0.013058799413013775\n",
      "train loss:0.006096246395115297\n",
      "train loss:0.011913235563587355\n",
      "train loss:0.005429624304251589\n",
      "train loss:0.005988056894249857\n",
      "train loss:0.003517191268329117\n",
      "train loss:0.001737029925315352\n",
      "train loss:0.001235579727906793\n",
      "train loss:0.0014489826344694847\n",
      "train loss:0.004203024917814694\n",
      "train loss:0.004254316320467431\n",
      "train loss:0.0910731954471748\n",
      "train loss:0.010986056126835446\n",
      "train loss:0.004308260314190949\n",
      "train loss:0.0007589439801429099\n",
      "train loss:0.006108578833617493\n",
      "train loss:0.0012613971047076072\n",
      "train loss:0.00028179449624332847\n",
      "train loss:0.005165675892048194\n",
      "train loss:0.006360222662313306\n",
      "train loss:0.000763881535840702\n",
      "train loss:0.015252916102223079\n",
      "train loss:0.0035888015221187362\n",
      "train loss:0.0014842195508241731\n",
      "train loss:0.0014126975967596626\n",
      "train loss:0.0010854826386836461\n",
      "train loss:0.002990520305228348\n",
      "train loss:0.0020207401883363106\n",
      "train loss:0.0019011188795435392\n",
      "train loss:0.004798027261168687\n",
      "train loss:0.006737977210423545\n",
      "train loss:0.0029467576261261812\n",
      "train loss:0.0008337545849035472\n",
      "train loss:0.0011899566294036753\n",
      "train loss:0.04762385578607302\n",
      "=== epoch:12, train acc:0.995, test acc:0.992 ===\n",
      "train loss:0.0013965013882965354\n",
      "train loss:0.0023541423280902408\n",
      "train loss:0.005543174945576659\n",
      "train loss:0.0004997657447652165\n",
      "train loss:0.005380214152109209\n",
      "train loss:0.00035747126641587725\n",
      "train loss:0.0021293997051107347\n",
      "train loss:0.0038989978426647864\n",
      "train loss:0.006616220836599965\n",
      "train loss:0.004246636942457084\n",
      "train loss:0.0014249849756518928\n",
      "train loss:0.007044542513408924\n",
      "train loss:0.0006457697770675623\n",
      "train loss:0.007980514550988839\n",
      "train loss:0.0101175353135186\n",
      "train loss:0.011947125237471148\n",
      "train loss:0.0014187720418796861\n",
      "train loss:0.003432299958104026\n",
      "train loss:0.03989829294672554\n",
      "train loss:0.0018219262414316203\n",
      "train loss:0.012246380768014466\n",
      "train loss:0.002465608496226448\n",
      "train loss:0.0008395285010000321\n",
      "train loss:0.008836798906977347\n",
      "train loss:0.0032985819577693413\n",
      "train loss:0.00305436084744304\n",
      "train loss:0.00395700751370757\n",
      "train loss:0.031552539292992385\n",
      "train loss:0.004021642344318068\n",
      "train loss:0.006221556223036841\n",
      "train loss:0.0012959637710398867\n",
      "train loss:0.003042196258403004\n",
      "train loss:0.0033072478807552354\n",
      "train loss:0.00867867690126397\n",
      "train loss:0.004249943465191968\n",
      "train loss:0.007315010645164036\n",
      "train loss:0.0014291160294926954\n",
      "train loss:0.027647072788243934\n",
      "train loss:0.009732470291551326\n",
      "train loss:0.0014772709293355577\n",
      "train loss:0.0019972750278571556\n",
      "train loss:0.0004139636072775148\n",
      "train loss:0.0016908624286467583\n",
      "train loss:0.003822456813139154\n",
      "train loss:0.0022555228593895897\n",
      "train loss:0.0012446037829610796\n",
      "train loss:0.002041515336338591\n",
      "train loss:0.0032061197582695434\n",
      "train loss:0.0031977364107184407\n",
      "train loss:0.004793179119335238\n",
      "train loss:0.00251952217180558\n",
      "train loss:0.005860097150546476\n",
      "train loss:0.004820925578956406\n",
      "train loss:0.007137406102227205\n",
      "train loss:0.008091681580565271\n",
      "train loss:0.002054990189559313\n",
      "train loss:0.0015506074211486859\n",
      "train loss:0.0017027937575459467\n",
      "train loss:0.009415837488442231\n",
      "train loss:0.002018127147657479\n",
      "train loss:0.0035247771206232546\n",
      "train loss:0.03048888572263833\n",
      "train loss:0.0070796765989450195\n",
      "train loss:0.008743823757406937\n",
      "train loss:0.009265837378991495\n",
      "train loss:0.0012757334282282465\n",
      "train loss:0.0037633727420684515\n",
      "train loss:0.004763920698810664\n",
      "train loss:0.008472527596469962\n",
      "train loss:0.005080237992078998\n",
      "train loss:0.006354680053645937\n",
      "train loss:0.0014791149849150012\n",
      "train loss:0.0013651305408790958\n",
      "train loss:0.00039256189112736465\n",
      "train loss:0.005799620627940891\n",
      "train loss:0.003903632338908461\n",
      "train loss:0.00439873302356265\n",
      "train loss:0.0010197175115819442\n",
      "train loss:0.0320194678125526\n",
      "train loss:0.00023570800039568918\n",
      "train loss:0.005059536087966095\n",
      "train loss:0.017105858006067294\n",
      "train loss:0.006644274636485532\n",
      "train loss:0.009589126229817429\n",
      "train loss:0.004932802249782531\n",
      "train loss:0.0027193257230576865\n",
      "train loss:0.009399993303300479\n",
      "train loss:0.011089778999198925\n",
      "train loss:0.0015525666803526993\n",
      "train loss:0.002398544322826314\n",
      "train loss:0.004226410232856645\n",
      "train loss:0.005510029958156422\n",
      "train loss:0.0010963731949125046\n",
      "train loss:0.00871509112407295\n",
      "train loss:0.00020675549012087057\n",
      "train loss:0.00385181016007272\n",
      "train loss:0.0019279914319013482\n",
      "train loss:0.00265602624074106\n",
      "train loss:0.010746830786607145\n",
      "train loss:0.003012355657340222\n",
      "train loss:0.0010233755924044581\n",
      "train loss:0.003602440937318103\n",
      "train loss:0.007893056838055216\n",
      "train loss:0.004506161986909613\n",
      "train loss:0.0017184544408817195\n",
      "train loss:0.0027467179951150937\n",
      "train loss:0.0019359120515417213\n",
      "train loss:0.0038424769844594107\n",
      "train loss:0.0006791389160759243\n",
      "train loss:0.0010198805718265932\n",
      "train loss:0.0033041102601484144\n",
      "train loss:0.005616710496344435\n",
      "train loss:0.006989907062023073\n",
      "train loss:0.0015637564376862778\n",
      "train loss:0.10868231377725826\n",
      "train loss:0.0029496562453238183\n",
      "train loss:0.004456684709239567\n",
      "train loss:0.004438375185909684\n",
      "train loss:0.0015540667633437697\n",
      "train loss:0.02223231902581376\n",
      "train loss:0.0008884426604841972\n",
      "train loss:0.000647368237590234\n",
      "train loss:0.0058630358509328215\n",
      "train loss:0.0004903219807532951\n",
      "train loss:0.0023133615308774586\n",
      "train loss:0.0029540900076704397\n",
      "train loss:0.0009691634856459287\n",
      "train loss:0.005894733001581769\n",
      "train loss:0.0201964125808539\n",
      "train loss:0.002481446400365773\n",
      "train loss:0.004163313456777947\n",
      "train loss:0.0049596855596735915\n",
      "train loss:0.0006699958932344308\n",
      "train loss:0.0005814996278950931\n",
      "train loss:0.008880934642792318\n",
      "train loss:0.0012586756963199367\n",
      "train loss:0.0003873194763902933\n",
      "train loss:0.00964140273046606\n",
      "train loss:0.011862880304126869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.00573271744460142\n",
      "train loss:0.0007645595278098663\n",
      "train loss:0.007047666433390779\n",
      "train loss:0.007719292094899418\n",
      "train loss:0.008249517134370688\n",
      "train loss:0.00016253717535765811\n",
      "train loss:0.0024943223116750787\n",
      "train loss:0.006540032270389141\n",
      "train loss:0.0004609453278736781\n",
      "train loss:0.0002130111659008059\n",
      "train loss:0.0009125937664232271\n",
      "train loss:0.0027402498577859965\n",
      "train loss:0.002492770286892044\n",
      "train loss:0.005824986112640245\n",
      "train loss:0.0004281407210499739\n",
      "train loss:0.0015440965691356217\n",
      "train loss:0.0019849355063568407\n",
      "train loss:0.009555265989369673\n",
      "train loss:0.000712929525568425\n",
      "train loss:0.009827075853116311\n",
      "train loss:0.0003722216555297436\n",
      "train loss:0.002835558713724789\n",
      "train loss:0.001945785182850871\n",
      "train loss:0.005811196954441648\n",
      "train loss:0.0028541184979962602\n",
      "train loss:0.003142692643543254\n",
      "train loss:0.06511761367232322\n",
      "train loss:0.0009109814530688366\n",
      "train loss:0.0012507438633614614\n",
      "train loss:0.002628285826923328\n",
      "train loss:0.001709944596699966\n",
      "train loss:0.0005028014248509894\n",
      "train loss:0.0017756049147907014\n",
      "train loss:0.0013396987310373538\n",
      "train loss:0.0037862486902342563\n",
      "train loss:0.0030635719121418947\n",
      "train loss:0.00042466985887648016\n",
      "train loss:0.005502380152121056\n",
      "train loss:0.002262498246086917\n",
      "train loss:0.0008258792640895786\n",
      "train loss:0.0008286615863095867\n",
      "train loss:0.003138879888897158\n",
      "train loss:0.0025909437598860448\n",
      "train loss:0.0018935078477210967\n",
      "train loss:0.005559448724599446\n",
      "train loss:0.0001398785243405628\n",
      "train loss:0.004648050648540664\n",
      "train loss:0.002232863768739133\n",
      "train loss:0.0017450557850863912\n",
      "train loss:0.00083196646200555\n",
      "train loss:0.0003312147236146699\n",
      "train loss:0.001242105235189307\n",
      "train loss:0.001738060007118604\n",
      "train loss:0.007647646266091217\n",
      "train loss:0.0026657957002968287\n",
      "train loss:0.027001401356683816\n",
      "train loss:0.0009136545580689268\n",
      "train loss:0.0005061470051365198\n",
      "train loss:0.006718224830147649\n",
      "train loss:0.00028526894824771834\n",
      "train loss:0.008780188765980482\n",
      "train loss:0.0003641200684625415\n",
      "train loss:0.0010971296511041904\n",
      "train loss:0.000799881056393303\n",
      "train loss:0.004392951798855576\n",
      "train loss:0.008892158208098303\n",
      "train loss:0.04546916150847774\n",
      "train loss:0.006317971212601253\n",
      "train loss:0.0040432670058474325\n",
      "train loss:0.0009929017879275102\n",
      "train loss:0.0010679338849397283\n",
      "train loss:0.0017267186696406434\n",
      "train loss:0.0006677433652722379\n",
      "train loss:0.002738089661726888\n",
      "train loss:0.001806607698237408\n",
      "train loss:0.008617819362798505\n",
      "train loss:0.0037978321782076167\n",
      "train loss:0.010382600358336946\n",
      "train loss:0.008242589885227492\n",
      "train loss:0.0010811867332963336\n",
      "train loss:0.0015846061588978971\n",
      "train loss:0.001963736871184723\n",
      "train loss:0.0006340332030745648\n",
      "train loss:0.003739310259406209\n",
      "train loss:0.01115909805563514\n",
      "train loss:0.0023209936722496482\n",
      "train loss:0.0007072592514436931\n",
      "train loss:0.002419406961778939\n",
      "train loss:0.0034408636619570844\n",
      "train loss:0.004019389356176896\n",
      "train loss:0.0024689387419376298\n",
      "train loss:0.0032010442719993966\n",
      "train loss:0.0023566599769713715\n",
      "train loss:0.004159689710927232\n",
      "train loss:0.0017583869637140632\n",
      "train loss:0.0025894299088647815\n",
      "train loss:0.0016432676351354632\n",
      "train loss:0.0007991468906463756\n",
      "train loss:0.004408543369475397\n",
      "train loss:0.00944036113369304\n",
      "train loss:0.0027010095576075793\n",
      "train loss:0.002129723921785723\n",
      "train loss:0.004335234127292999\n",
      "train loss:0.0016992134409256018\n",
      "train loss:0.000865920014852481\n",
      "train loss:0.0055885512774984415\n",
      "train loss:0.0031559949510165273\n",
      "train loss:0.00039206236026608957\n",
      "train loss:0.0010363433021882288\n",
      "train loss:0.006158063489986403\n",
      "train loss:0.07828596364583261\n",
      "train loss:0.004992888731044972\n",
      "train loss:0.00989622637563209\n",
      "train loss:0.002362433497806374\n",
      "train loss:0.0015277047907457401\n",
      "train loss:0.0012465999248438466\n",
      "train loss:0.005594390737961419\n",
      "train loss:0.0009869756057622392\n",
      "train loss:0.00680887711582959\n",
      "train loss:0.0024011823276039786\n",
      "train loss:0.01011763849223063\n",
      "train loss:0.013217252299749153\n",
      "train loss:0.00905370172649123\n",
      "train loss:0.0011706468330573824\n",
      "train loss:0.001385218505200199\n",
      "train loss:0.00022171857499165982\n",
      "train loss:0.0027694523407967137\n",
      "train loss:0.0009662878663349407\n",
      "train loss:0.013114862554389145\n",
      "train loss:0.0028973552742602754\n",
      "train loss:0.0051116592620083535\n",
      "train loss:0.0016013995242749896\n",
      "train loss:0.002153600967120667\n",
      "train loss:0.009046548090572187\n",
      "train loss:0.001639428614965677\n",
      "train loss:0.009080398518561081\n",
      "train loss:0.029403151888916627\n",
      "train loss:0.00017379030839012935\n",
      "train loss:0.00023202452013558152\n",
      "train loss:0.010019273754867629\n",
      "train loss:0.0013471062795733543\n",
      "train loss:0.0019473172620174566\n",
      "train loss:0.0006141563911543873\n",
      "train loss:0.0026741278275593883\n",
      "train loss:0.003571806213449836\n",
      "train loss:0.0017383428012712665\n",
      "train loss:0.007059362600377703\n",
      "train loss:0.005866643313722414\n",
      "train loss:0.007260360235474115\n",
      "train loss:0.060820627242297046\n",
      "train loss:0.0009642944345788608\n",
      "train loss:0.005258177421611501\n",
      "train loss:0.00567884156708855\n",
      "train loss:0.011688654128759097\n",
      "train loss:0.005035838530746897\n",
      "train loss:0.0024555400007492544\n",
      "train loss:0.014282737636053547\n",
      "train loss:0.004855748419529713\n",
      "train loss:0.0016556183997844948\n",
      "train loss:0.003372474141504422\n",
      "train loss:0.0025027550451053567\n",
      "train loss:0.004180678336295793\n",
      "train loss:0.007736829833910735\n",
      "train loss:0.0006764404238773969\n",
      "train loss:0.0022982107258365716\n",
      "train loss:0.0014360546022060988\n",
      "train loss:0.00035889927404943\n",
      "train loss:0.0044252922932369295\n",
      "train loss:0.01626332533415734\n",
      "train loss:0.0020705862675484557\n",
      "train loss:0.0029914364836391366\n",
      "train loss:0.0004242095350960954\n",
      "train loss:0.0013480724948400696\n",
      "train loss:0.0013293523142902032\n",
      "train loss:0.023064009881242246\n",
      "train loss:0.004734796601733942\n",
      "train loss:0.02099729663259164\n",
      "train loss:0.0006659095173611934\n",
      "train loss:0.0010897203663970054\n",
      "train loss:0.000496227766577529\n",
      "train loss:0.014495929883112067\n",
      "train loss:0.0056574488218504495\n",
      "train loss:0.013414745682366967\n",
      "train loss:0.0006163531107083926\n",
      "train loss:0.0013629733618511133\n",
      "train loss:0.004583804376666685\n",
      "train loss:0.004813550983957317\n",
      "train loss:0.09032062526180328\n",
      "train loss:0.010165001977639579\n",
      "train loss:0.007876943547583153\n",
      "train loss:0.005782602813064966\n",
      "train loss:0.00830571163542567\n",
      "train loss:0.009087480119271373\n",
      "train loss:0.009249877079360296\n",
      "train loss:0.007703457298708171\n",
      "train loss:0.002818032782512021\n",
      "train loss:0.0011087087440952482\n",
      "train loss:0.002231826445072304\n",
      "train loss:0.000634695236938849\n",
      "train loss:0.0015363478311573267\n",
      "train loss:0.0033051060721365753\n",
      "train loss:0.002721739238486219\n",
      "train loss:0.0036498994910305056\n",
      "train loss:0.0060682642530120855\n",
      "train loss:0.0007371540353993664\n",
      "train loss:0.0014801863701499349\n",
      "train loss:0.001377222918350306\n",
      "train loss:0.015778746225073092\n",
      "train loss:0.003464530729394986\n",
      "train loss:0.0036196554573932793\n",
      "train loss:0.003448129135407895\n",
      "train loss:0.0003878242810912859\n",
      "train loss:0.0045522860715294785\n",
      "train loss:0.0033532009420026597\n",
      "train loss:0.005843895595387136\n",
      "train loss:0.012133425664171691\n",
      "train loss:0.0057252575234815\n",
      "train loss:0.0008506474032375672\n",
      "train loss:0.009842590898661414\n",
      "train loss:0.004953658344666017\n",
      "train loss:0.0033489813046784778\n",
      "train loss:0.004330478435812567\n",
      "train loss:0.0018157326055418443\n",
      "train loss:0.005112690390038303\n",
      "train loss:0.0046946093272350504\n",
      "train loss:0.004385194985242524\n",
      "train loss:0.004674897797027421\n",
      "train loss:0.008373233481721728\n",
      "train loss:0.006655977510941693\n",
      "train loss:0.0043747894722296265\n",
      "train loss:0.011967801117600868\n",
      "train loss:0.002745349334626432\n",
      "train loss:0.0042945331105092134\n",
      "train loss:0.012008414505671538\n",
      "train loss:0.007956031177120647\n",
      "train loss:0.007063396111429101\n",
      "train loss:0.0008803843502570569\n",
      "train loss:0.0012392220352958681\n",
      "train loss:0.006097946088665368\n",
      "train loss:0.02208152046697155\n",
      "train loss:0.007113827180089071\n",
      "train loss:0.010429497176912494\n",
      "train loss:0.005012751266911989\n",
      "train loss:0.0013423164877318832\n",
      "train loss:0.0017473564324583467\n",
      "train loss:0.00583669108637174\n",
      "train loss:0.0011223567977824277\n",
      "train loss:0.008784851913722436\n",
      "train loss:0.0271385537448186\n",
      "train loss:0.00032488685414761823\n",
      "train loss:0.0007161406176578705\n",
      "train loss:0.002598735247126877\n",
      "train loss:0.005324628530832653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0015133614739997383\n",
      "train loss:0.0027734784235369397\n",
      "train loss:0.007489742349732281\n",
      "train loss:0.005509451932987701\n",
      "train loss:0.0024314970300739823\n",
      "train loss:0.0029906129033955824\n",
      "train loss:0.00760483350694934\n",
      "train loss:0.0018706343770242364\n",
      "train loss:0.007699772949847295\n",
      "train loss:0.006582676600174792\n",
      "train loss:0.002355542931679514\n",
      "train loss:0.006055221458164954\n",
      "train loss:0.00434518879920134\n",
      "train loss:0.0028070082391771626\n",
      "train loss:0.028262120432353037\n",
      "train loss:0.001627317661978909\n",
      "train loss:0.0017409562596662515\n",
      "train loss:0.006243407185526652\n",
      "train loss:0.003124315226267166\n",
      "train loss:0.0021625702622930423\n",
      "train loss:0.0076143459381099975\n",
      "train loss:0.005286172738262006\n",
      "train loss:0.006427750916056434\n",
      "train loss:0.004367503593210935\n",
      "train loss:0.005749329645751216\n",
      "train loss:0.001756069278591549\n",
      "train loss:0.0020545936568135753\n",
      "train loss:0.00603826649284227\n",
      "train loss:0.002525704556738489\n",
      "train loss:0.0013951652287518765\n",
      "train loss:0.0013790480047221002\n",
      "train loss:0.00035395804089435217\n",
      "train loss:0.003407088471095743\n",
      "train loss:0.004173055941379012\n",
      "train loss:0.007911961123456853\n",
      "train loss:0.0002938151062646489\n",
      "train loss:0.009862887275793082\n",
      "train loss:0.007855934240301976\n",
      "train loss:0.0006695802943523153\n",
      "train loss:0.0030907154267231004\n",
      "train loss:0.009332324641531586\n",
      "train loss:0.0016218427223967816\n",
      "train loss:0.004385882167366977\n",
      "train loss:0.0008621062651444787\n",
      "train loss:0.005572927757955647\n",
      "train loss:0.0025466090120132134\n",
      "train loss:0.004162870954661397\n",
      "train loss:0.015534344441280788\n",
      "train loss:0.005705964254005026\n",
      "train loss:0.0008764727503338298\n",
      "train loss:0.0011717269639565664\n",
      "train loss:0.0005452299477716828\n",
      "train loss:0.0021368626735265863\n",
      "train loss:0.0006484510368335301\n",
      "train loss:0.0027868668939724835\n",
      "train loss:0.02371041879700206\n",
      "train loss:0.026824713773388208\n",
      "train loss:0.0033485116360772627\n",
      "train loss:0.0036957626233994826\n",
      "train loss:0.005592419128528945\n",
      "train loss:0.0011936814966507666\n",
      "train loss:0.008695601328801391\n",
      "train loss:0.004931332988888923\n",
      "train loss:0.001296493230357538\n",
      "train loss:0.03668521960906056\n",
      "train loss:0.0011714837617504423\n",
      "train loss:0.00485245446914593\n",
      "train loss:0.0014032611861373478\n",
      "train loss:0.001023320978853586\n",
      "train loss:0.04016703642164888\n",
      "train loss:0.00254274500216744\n",
      "train loss:0.0024702896081565577\n",
      "train loss:0.024133477567144818\n",
      "train loss:0.0009525394925141785\n",
      "train loss:0.0021196624185982975\n",
      "train loss:0.004647482025700762\n",
      "train loss:0.011775067368592988\n",
      "train loss:0.004354697599597443\n",
      "train loss:0.01635903359324435\n",
      "train loss:0.030582894185402804\n",
      "train loss:0.002107452989341997\n",
      "train loss:0.004331783087983764\n",
      "train loss:0.004506444978431161\n",
      "train loss:0.0029786342914784273\n",
      "train loss:0.0029080963894187444\n",
      "train loss:0.04199165390317179\n",
      "train loss:0.006864033967708417\n",
      "train loss:0.003104283131349347\n",
      "train loss:0.004564755879060308\n",
      "train loss:0.006653207590834459\n",
      "train loss:0.01294150585821326\n",
      "train loss:0.020310540354133887\n",
      "train loss:0.003115675191159987\n",
      "train loss:0.00474452599991983\n",
      "train loss:0.003258975737620862\n",
      "train loss:0.0004123050280164929\n",
      "train loss:0.003992508282768207\n",
      "train loss:0.002003149970615813\n",
      "train loss:0.007703033712164253\n",
      "train loss:0.003856019705434893\n",
      "train loss:0.001341962415206968\n",
      "train loss:0.0017641165310237955\n",
      "train loss:0.000498987498641361\n",
      "train loss:0.0016825956137323137\n",
      "train loss:0.003785514904349869\n",
      "train loss:0.0027796328213485423\n",
      "train loss:0.005343949204919232\n",
      "train loss:0.0010114890817792621\n",
      "train loss:0.0028871045044279774\n",
      "train loss:0.010378696530479315\n",
      "train loss:0.0173523923842333\n",
      "train loss:0.009701352432721477\n",
      "train loss:0.004220971648674754\n",
      "train loss:0.012626492110390844\n",
      "train loss:0.0015296605540912464\n",
      "train loss:0.004711570034652573\n",
      "train loss:0.0023781422080127036\n",
      "train loss:0.001878988318019217\n",
      "train loss:0.08930452580561254\n",
      "train loss:0.008861678592345679\n",
      "train loss:0.011154734287486064\n",
      "train loss:0.008951906752400334\n",
      "train loss:0.00204569710188034\n",
      "train loss:0.02774176493797352\n",
      "train loss:0.008386295723229898\n",
      "train loss:0.0002659570736253852\n",
      "train loss:0.0019128560111887085\n",
      "train loss:0.0024476266137318857\n",
      "train loss:0.014572626582541834\n",
      "train loss:0.004619155969180693\n",
      "train loss:0.011738581736162816\n",
      "train loss:0.004406839974317077\n",
      "train loss:0.007569469862387113\n",
      "train loss:0.007698062021504613\n",
      "train loss:0.015740599355563194\n",
      "train loss:0.004546680386909103\n",
      "train loss:0.005060895817125302\n",
      "train loss:0.0015432630914070894\n",
      "train loss:0.0007740206919379996\n",
      "train loss:0.01506108596760229\n",
      "train loss:0.022227588310682115\n",
      "train loss:0.0032648139048303843\n",
      "train loss:0.004860289459897513\n",
      "train loss:0.006630862002953933\n",
      "train loss:0.01114443317032058\n",
      "train loss:0.0008279377508352371\n",
      "train loss:0.009963224346099574\n",
      "train loss:0.0021393346238106135\n",
      "train loss:0.07241462014356055\n",
      "train loss:0.005689300275783883\n",
      "train loss:0.005882134534051302\n",
      "train loss:0.012580825998378007\n",
      "train loss:0.015495242997544113\n",
      "train loss:0.0001700737401609754\n",
      "train loss:0.003574042048051059\n",
      "train loss:0.006344381999805385\n",
      "train loss:0.002949142616009379\n",
      "train loss:0.009329690856000734\n",
      "train loss:0.016247496426788067\n",
      "train loss:0.0005310503090305537\n",
      "train loss:0.0045693853289642\n",
      "train loss:0.004133622625713355\n",
      "train loss:0.008653206728492952\n",
      "train loss:0.021078241728977053\n",
      "train loss:0.007850926433328624\n",
      "train loss:0.0020877778099428644\n",
      "train loss:0.013954952952590447\n",
      "train loss:0.0016847046872236313\n",
      "train loss:0.0013240392408733026\n",
      "train loss:0.019444659506642384\n",
      "train loss:0.000823128144166995\n",
      "train loss:0.04599255067000524\n",
      "train loss:0.023769229579288554\n",
      "train loss:0.02727201036618238\n",
      "train loss:0.006363260014483714\n",
      "train loss:0.0038987799399397127\n",
      "train loss:0.007837000776769763\n",
      "train loss:0.018499881931567208\n",
      "train loss:0.006345778486299905\n",
      "train loss:0.016198184073168188\n",
      "train loss:0.014277523080816826\n",
      "train loss:0.0013748760110159422\n",
      "train loss:0.012866728976764705\n",
      "train loss:0.001036174039411149\n",
      "train loss:0.016069863029309593\n",
      "train loss:0.0012265627563615527\n",
      "train loss:0.006813079389275395\n",
      "train loss:0.005485682980917351\n",
      "train loss:0.0026895006056532355\n",
      "train loss:0.008980039811719659\n",
      "train loss:0.01845092358419983\n",
      "train loss:0.014734118691885017\n",
      "train loss:0.002001052491069302\n",
      "train loss:0.004018197358673899\n",
      "train loss:0.0025683468193606557\n",
      "train loss:0.003144875748327354\n",
      "train loss:0.004722963413633933\n",
      "train loss:0.004757512179622485\n",
      "train loss:0.002348703293410512\n",
      "train loss:0.01490086073942689\n",
      "train loss:0.0007479822868868811\n",
      "train loss:0.0041080182495079305\n",
      "train loss:0.02957692268668155\n",
      "train loss:0.0018712983535596921\n",
      "train loss:7.956953550634295e-05\n",
      "train loss:0.000592781374468976\n",
      "train loss:0.005951942421769103\n",
      "train loss:0.0012728562944496103\n",
      "=== epoch:13, train acc:0.994, test acc:0.988 ===\n",
      "train loss:0.004489008470633246\n",
      "train loss:0.0003005019514416557\n",
      "train loss:0.0007781738204267891\n",
      "train loss:0.001481733350945463\n",
      "train loss:0.0011717052605990294\n",
      "train loss:0.0047380933658173405\n",
      "train loss:0.003495662859078859\n",
      "train loss:0.001407432922086012\n",
      "train loss:0.0004759279738004876\n",
      "train loss:0.025485813871816934\n",
      "train loss:0.0008230891041092841\n",
      "train loss:0.001889282746926365\n",
      "train loss:0.0014996767583849865\n",
      "train loss:0.010167709681128862\n",
      "train loss:0.002204683245078312\n",
      "train loss:0.008219648937441256\n",
      "train loss:0.003235279920171395\n",
      "train loss:0.0023595809499748206\n",
      "train loss:0.018379291419538233\n",
      "train loss:0.0008517323025392787\n",
      "train loss:0.0015542680632579906\n",
      "train loss:0.0041290894265778\n",
      "train loss:0.002587453997678372\n",
      "train loss:0.013826241534307535\n",
      "train loss:0.010522339879195047\n",
      "train loss:0.0015409638040272185\n",
      "train loss:0.0037013047769913067\n",
      "train loss:0.010795533616735102\n",
      "train loss:0.003367800249133135\n",
      "train loss:0.0023867122714889295\n",
      "train loss:0.006695326819229877\n",
      "train loss:0.007396565072787044\n",
      "train loss:0.0012165405529402102\n",
      "train loss:0.005411326357756607\n",
      "train loss:0.0012171559861582242\n",
      "train loss:0.004346950155425544\n",
      "train loss:0.021062209739686585\n",
      "train loss:0.0039381695110029145\n",
      "train loss:0.0005573783048969621\n",
      "train loss:0.0064862187280368385\n",
      "train loss:0.013295558015983535\n",
      "train loss:0.0034629719594527135\n",
      "train loss:0.006406051566716702\n",
      "train loss:0.002291921321884938\n",
      "train loss:0.009981867148992766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.031567366445999404\n",
      "train loss:0.00215251353783909\n",
      "train loss:0.005071796016723343\n",
      "train loss:0.01603177550700372\n",
      "train loss:0.0020822062022235388\n",
      "train loss:0.0006296452542701775\n",
      "train loss:0.005770564708415666\n",
      "train loss:0.001360098007495288\n",
      "train loss:0.006130068498112436\n",
      "train loss:0.014960009446006511\n",
      "train loss:0.0005768912035027334\n",
      "train loss:0.0018796831742830867\n",
      "train loss:0.0010723116864904943\n",
      "train loss:0.008115484109004381\n",
      "train loss:0.0034557957355538967\n",
      "train loss:0.0012939366800704458\n",
      "train loss:0.0008381955815634504\n",
      "train loss:0.003284380874159679\n",
      "train loss:0.009319424047209894\n",
      "train loss:0.00408041033472546\n",
      "train loss:0.0007024993854823563\n",
      "train loss:0.004550417976405151\n",
      "train loss:0.004590760320073227\n",
      "train loss:0.0025371234074363087\n",
      "train loss:0.002313268582565962\n",
      "train loss:0.0011150378847479782\n",
      "train loss:0.006385154868098903\n",
      "train loss:0.013290073505911097\n",
      "train loss:0.0017365313753120348\n",
      "train loss:0.005270799059791395\n",
      "train loss:0.0027180300289806696\n",
      "train loss:0.003948327524687326\n",
      "train loss:0.00387820564996673\n",
      "train loss:0.0015047031149669505\n",
      "train loss:0.0008080715589638553\n",
      "train loss:0.0028661571334640734\n",
      "train loss:0.008063276375368187\n",
      "train loss:0.0008688185800634844\n",
      "train loss:0.00033705507394952815\n",
      "train loss:0.00709676911981853\n",
      "train loss:0.0004154135901027039\n",
      "train loss:0.005362292902521392\n",
      "train loss:0.0010714186490015686\n",
      "train loss:0.0014075633943733726\n",
      "train loss:0.007203977120238303\n",
      "train loss:0.004353259760013314\n",
      "train loss:0.006617998691612245\n",
      "train loss:0.0007242866517559954\n",
      "train loss:0.0032898277603731047\n",
      "train loss:0.01250765433776202\n",
      "train loss:0.00982442095269165\n",
      "train loss:0.007793771745945932\n",
      "train loss:0.0004868933710501944\n",
      "train loss:0.0010630735457831172\n",
      "train loss:0.002258782512004259\n",
      "train loss:0.00269442376998954\n",
      "train loss:0.005723610033111924\n",
      "train loss:5.4167580800816266e-05\n",
      "train loss:0.020267112780464024\n",
      "train loss:0.005161973677654134\n",
      "train loss:0.004024860442840605\n",
      "train loss:0.0065676423541085336\n",
      "train loss:0.003467612052521746\n",
      "train loss:0.0026338514511013216\n",
      "train loss:0.0009616586600241995\n",
      "train loss:0.009537874722877147\n",
      "train loss:0.002848739694860281\n",
      "train loss:0.0009302975008847156\n",
      "train loss:0.0008437415327167493\n",
      "train loss:0.0007625034125064655\n",
      "train loss:0.005528204834169857\n",
      "train loss:0.00027639174763485015\n",
      "train loss:0.012771172525055623\n",
      "train loss:0.003964609813626612\n",
      "train loss:0.0010754144955438588\n",
      "train loss:0.004555277064815847\n",
      "train loss:0.004723930808009145\n",
      "train loss:0.000927023418882295\n",
      "train loss:0.023127680508268494\n",
      "train loss:0.0017012477820687712\n",
      "train loss:0.003487800803318218\n",
      "train loss:0.0028720078226354417\n",
      "train loss:0.001497602153401407\n",
      "train loss:0.00859569905384929\n",
      "train loss:0.0013415126016591075\n",
      "train loss:0.003147887043960599\n",
      "train loss:0.006120468926796337\n",
      "train loss:0.008585182598665322\n",
      "train loss:0.00225165455098222\n",
      "train loss:0.04245590618657854\n",
      "train loss:0.0005637897515529592\n",
      "train loss:0.043211475091265585\n",
      "train loss:0.0003355335769906926\n",
      "train loss:0.0007979021519493722\n",
      "train loss:0.004901309507566683\n",
      "train loss:0.005694735928556864\n",
      "train loss:0.002709944320892336\n",
      "train loss:0.0023565631775896477\n",
      "train loss:0.0020614164878891227\n",
      "train loss:0.003356542855014091\n",
      "train loss:0.005102224862385913\n",
      "train loss:0.0007719342004746849\n",
      "train loss:0.0020464169048850615\n",
      "train loss:0.00761521182168596\n",
      "train loss:0.011226481610620287\n",
      "train loss:0.0005631361452186067\n",
      "train loss:0.002840761350233825\n",
      "train loss:0.0006774923012244602\n",
      "train loss:0.008242861751711296\n",
      "train loss:0.01984356402984498\n",
      "train loss:0.003320724796411597\n",
      "train loss:0.013336532502825378\n",
      "train loss:0.004786081025401682\n",
      "train loss:0.007242306297906968\n",
      "train loss:0.002030110347579933\n",
      "train loss:0.013504635950143052\n",
      "train loss:0.0030505332564615215\n",
      "train loss:0.0014294003851125026\n",
      "train loss:0.0023466782496681\n",
      "train loss:0.011865996078163792\n",
      "train loss:0.002607626474915384\n",
      "train loss:0.009477759772910058\n",
      "train loss:0.012262685578476065\n",
      "train loss:0.006076377438499609\n",
      "train loss:0.007160622221216538\n",
      "train loss:0.03478362588706232\n",
      "train loss:0.003193256505591064\n",
      "train loss:0.0013866969247766894\n",
      "train loss:0.0006598763178014801\n",
      "train loss:0.003616982119125241\n",
      "train loss:0.06804072483604807\n",
      "train loss:0.003563924833610528\n",
      "train loss:0.0031953813380672796\n",
      "train loss:0.0010631972870465787\n",
      "train loss:0.003003042265387556\n",
      "train loss:0.004040686942612894\n",
      "train loss:0.005246441185357402\n",
      "train loss:0.00486827527142963\n",
      "train loss:0.005092632391941318\n",
      "train loss:0.025040190087893932\n",
      "train loss:0.0005804957671889496\n",
      "train loss:0.007314648953793145\n",
      "train loss:0.0030700604515940103\n",
      "train loss:0.0035653775183251022\n",
      "train loss:0.011900273131627628\n",
      "train loss:0.0009385606790770817\n",
      "train loss:0.0008550348325989203\n",
      "train loss:0.010086607296233245\n",
      "train loss:0.009149265596876696\n",
      "train loss:0.0018847555710965747\n",
      "train loss:0.002073177847465208\n",
      "train loss:0.0033834216842085433\n",
      "train loss:0.0015867566993330237\n",
      "train loss:0.0010995218725486041\n",
      "train loss:0.0009823850190234535\n",
      "train loss:0.0002317402763714347\n",
      "train loss:0.0032559979666595875\n",
      "train loss:0.0008624171661082609\n",
      "train loss:0.0011048960785450647\n",
      "train loss:0.013165874056951615\n",
      "train loss:0.0018884036989698831\n",
      "train loss:0.0027392705636854354\n",
      "train loss:0.005985136690116547\n",
      "train loss:0.0045819811593735185\n",
      "train loss:0.0023261772245479952\n",
      "train loss:0.006643271556300901\n",
      "train loss:0.01397808056604706\n",
      "train loss:0.0032911185226136477\n",
      "train loss:0.0010782908420569359\n",
      "train loss:0.007696183673743326\n",
      "train loss:0.0007240472354168756\n",
      "train loss:0.0017650311208895655\n",
      "train loss:0.0025105342406385933\n",
      "train loss:0.0009163355856903691\n",
      "train loss:0.001916897847415152\n",
      "train loss:0.0013692176898199104\n",
      "train loss:0.001965214978481196\n",
      "train loss:0.0012564646842861018\n",
      "train loss:0.00266799299558678\n",
      "train loss:0.0005021341726217784\n",
      "train loss:0.0015170694560284375\n",
      "train loss:0.002288395898192078\n",
      "train loss:0.0024987155933034093\n",
      "train loss:0.004379188937616436\n",
      "train loss:0.007643202561868821\n",
      "train loss:0.00551959996742933\n",
      "train loss:0.0010278269115743977\n",
      "train loss:0.0012392571151690248\n",
      "train loss:0.0007537921318142243\n",
      "train loss:0.0011428288697577361\n",
      "train loss:0.0012833849686448712\n",
      "train loss:0.010104038327059816\n",
      "train loss:0.00278170815170675\n",
      "train loss:0.023294233338090137\n",
      "train loss:0.012928642904336825\n",
      "train loss:0.0023151608422058013\n",
      "train loss:0.0018182110184854312\n",
      "train loss:0.00476754680941861\n",
      "train loss:0.0018570293692267098\n",
      "train loss:0.004986275519582872\n",
      "train loss:0.0021008211084033086\n",
      "train loss:0.002303596707228692\n",
      "train loss:0.003405719630849576\n",
      "train loss:0.010173567713919878\n",
      "train loss:0.0019766499030427777\n",
      "train loss:0.0020311302224166133\n",
      "train loss:0.0008035206321185398\n",
      "train loss:0.006382697311458035\n",
      "train loss:0.0041346372680351315\n",
      "train loss:0.005817178350281873\n",
      "train loss:0.0006191913435530802\n",
      "train loss:0.004287125757679306\n",
      "train loss:0.0021172046981130753\n",
      "train loss:0.01504952333254324\n",
      "train loss:0.013713208624894873\n",
      "train loss:0.0027302386193585127\n",
      "train loss:0.0016977881444378315\n",
      "train loss:0.0012838245257653475\n",
      "train loss:0.0008135519667628295\n",
      "train loss:0.03081806621532742\n",
      "train loss:0.00504130986421402\n",
      "train loss:0.0030291200636205096\n",
      "train loss:0.0037503740179696027\n",
      "train loss:0.004033389222710431\n",
      "train loss:0.0017416585198721502\n",
      "train loss:0.004517182645606328\n",
      "train loss:0.05414953664654032\n",
      "train loss:0.001724477952312132\n",
      "train loss:0.0024332136893461813\n",
      "train loss:0.004858393058936655\n",
      "train loss:0.002866810633529381\n",
      "train loss:0.0013693623827029336\n",
      "train loss:0.0002946084449471033\n",
      "train loss:0.0036149304993205013\n",
      "train loss:0.0056882534820194754\n",
      "train loss:0.00029483013334226724\n",
      "train loss:0.004959255525461459\n",
      "train loss:0.00027762129522063036\n",
      "train loss:0.004286167319805246\n",
      "train loss:0.002589983116524566\n",
      "train loss:0.0009816071632042643\n",
      "train loss:0.012582226391096098\n",
      "train loss:0.0003433144047550663\n",
      "train loss:0.004386439491012569\n",
      "train loss:0.0018455709221201135\n",
      "train loss:0.0035585858950081058\n",
      "train loss:0.0011586286239977526\n",
      "train loss:0.0027117561521797316\n",
      "train loss:0.006407478791922277\n",
      "train loss:0.00038497507019429213\n",
      "train loss:0.0020842314992358564\n",
      "train loss:0.0020197076069264925\n",
      "train loss:0.004811647136554802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.001197584961774611\n",
      "train loss:0.004109033629830326\n",
      "train loss:0.0028103666369177945\n",
      "train loss:0.004697903692011281\n",
      "train loss:0.0009241952712851993\n",
      "train loss:0.02006712883233264\n",
      "train loss:0.0011543756383487085\n",
      "train loss:0.06645673266348079\n",
      "train loss:0.0008293402010580975\n",
      "train loss:0.0005684085360860107\n",
      "train loss:0.0027448832283720635\n",
      "train loss:0.002395432848988823\n",
      "train loss:0.045467159586093535\n",
      "train loss:0.0012907092002648682\n",
      "train loss:0.0016431240792335608\n",
      "train loss:0.0008705416676397367\n",
      "train loss:0.000658944958508659\n",
      "train loss:0.001861045435442047\n",
      "train loss:0.0017959634771990921\n",
      "train loss:0.00032715749681516454\n",
      "train loss:0.00134191481902718\n",
      "train loss:0.002343300192973465\n",
      "train loss:0.016140026544598732\n",
      "train loss:0.04058452222399822\n",
      "train loss:0.0005562624283748222\n",
      "train loss:0.004668160259285311\n",
      "train loss:0.0013703054605946929\n",
      "train loss:0.027294761209251103\n",
      "train loss:0.007300077554385966\n",
      "train loss:0.0008509835698086159\n",
      "train loss:0.00319862306189344\n",
      "train loss:0.0028337714831817695\n",
      "train loss:0.0036323811205370917\n",
      "train loss:0.004847610393042554\n",
      "train loss:0.001031476142157025\n",
      "train loss:0.004254071071278746\n",
      "train loss:0.0021930127710790303\n",
      "train loss:0.0014811586751866323\n",
      "train loss:0.0016735750101895494\n",
      "train loss:0.0008328720221413642\n",
      "train loss:0.0010049742660917477\n",
      "train loss:0.0011520354200760384\n",
      "train loss:0.0026136779941380423\n",
      "train loss:0.0026070429305637393\n",
      "train loss:0.1164012040635325\n",
      "train loss:0.0004928570169111679\n",
      "train loss:0.0015523669136563098\n",
      "train loss:0.0017021031700864297\n",
      "train loss:0.006769439516033992\n",
      "train loss:0.003588326964157875\n",
      "train loss:0.0042814294082843055\n",
      "train loss:0.016835335788677942\n",
      "train loss:0.0008366819380806362\n",
      "train loss:0.004897314563057612\n",
      "train loss:0.005368644793043303\n",
      "train loss:0.005503837494329483\n",
      "train loss:0.0019765007114506093\n",
      "train loss:0.02051018420943769\n",
      "train loss:0.008288108205841933\n",
      "train loss:0.004514129885192879\n",
      "train loss:0.0005166193441626798\n",
      "train loss:0.0019828115020488735\n",
      "train loss:0.0009537114707423326\n",
      "train loss:0.004224346967765593\n",
      "train loss:0.0019960848076490656\n",
      "train loss:0.0054289955532338\n",
      "train loss:0.006423707497531001\n",
      "train loss:0.0034320740342981344\n",
      "train loss:0.00411677469394335\n",
      "train loss:0.0024415831044550893\n",
      "train loss:0.005737980966964431\n",
      "train loss:0.001579696748500988\n",
      "train loss:0.0003176648694151658\n",
      "train loss:0.0011764230195481257\n",
      "train loss:0.005353750095445579\n",
      "train loss:0.0006769049052807146\n",
      "train loss:0.0015443455582949905\n",
      "train loss:0.003936299188425777\n",
      "train loss:0.0004795017020546563\n",
      "train loss:0.0009129922253895297\n",
      "train loss:0.0024160832493481475\n",
      "train loss:0.0003737728408860412\n",
      "train loss:0.00036461834806653077\n",
      "train loss:0.0008965510062260361\n",
      "train loss:0.008628249639476412\n",
      "train loss:0.0011514737453455492\n",
      "train loss:0.0019735983763095196\n",
      "train loss:0.0024975503870194415\n",
      "train loss:0.0013838288225039219\n",
      "train loss:0.008741729077600237\n",
      "train loss:0.0026661237444694603\n",
      "train loss:0.0010588075066173788\n",
      "train loss:0.015420016854718686\n",
      "train loss:0.001286092539538912\n",
      "train loss:0.015105548770414812\n",
      "train loss:0.0005686907893481707\n",
      "train loss:0.002284014637565338\n",
      "train loss:0.0019650061641195283\n",
      "train loss:0.0031829647111470782\n",
      "train loss:0.0017439084163045604\n",
      "train loss:0.00896261505985154\n",
      "train loss:0.0016377003227956146\n",
      "train loss:0.00024658072356426557\n",
      "train loss:0.004527520235654308\n",
      "train loss:0.004126128077704998\n",
      "train loss:0.003623733806207053\n",
      "train loss:0.0001465278447641057\n",
      "train loss:0.012519390600080109\n",
      "train loss:0.003873405864481713\n",
      "train loss:0.0006914334189098355\n",
      "train loss:0.0011557569061195977\n",
      "train loss:0.003172058784176236\n",
      "train loss:0.0017032658013772862\n",
      "train loss:0.0013223632093829036\n",
      "train loss:0.0005851717953986731\n",
      "train loss:0.004019423438836421\n",
      "train loss:0.00777802871924512\n",
      "train loss:0.004092981016168026\n",
      "train loss:0.00508312228105493\n",
      "train loss:0.000472802905082567\n",
      "train loss:0.004104107428817475\n",
      "train loss:0.0015534720409115638\n",
      "train loss:0.007672735906924549\n",
      "train loss:0.005389546065950013\n",
      "train loss:0.0020959019823116524\n",
      "train loss:0.001211379370248753\n",
      "train loss:0.0018800554339298003\n",
      "train loss:0.0012403380123801782\n",
      "train loss:0.00124243959779541\n",
      "train loss:0.0076197581018072535\n",
      "train loss:0.0042462905937436105\n",
      "train loss:0.0018283596521235282\n",
      "train loss:0.0010965131931577588\n",
      "train loss:0.0003441170157062738\n",
      "train loss:0.00943706568450744\n",
      "train loss:0.0018279291621376584\n",
      "train loss:0.0020790353437934593\n",
      "train loss:0.005982861198566285\n",
      "train loss:0.0003228907532594358\n",
      "train loss:0.003936370334287109\n",
      "train loss:0.006643577206146392\n",
      "train loss:0.0005463305281999083\n",
      "train loss:0.008063730049464245\n",
      "train loss:0.008080554224946713\n",
      "train loss:0.04741330651418993\n",
      "train loss:0.003608902927879929\n",
      "train loss:0.0028672425085616354\n",
      "train loss:0.0009581674250535628\n",
      "train loss:0.005349048594111727\n",
      "train loss:0.0009277056661050804\n",
      "train loss:0.001450303548206257\n",
      "train loss:0.009340713792409585\n",
      "train loss:0.003337362814667832\n",
      "train loss:0.0009148494038008308\n",
      "train loss:0.0029815865853472607\n",
      "train loss:0.02426781314896627\n",
      "train loss:0.0034399746534901003\n",
      "train loss:0.0034105519468239924\n",
      "train loss:0.03504784657772558\n",
      "train loss:0.0005176166834149226\n",
      "train loss:0.012176908482437345\n",
      "train loss:0.002652629659957097\n",
      "train loss:0.016691930993870643\n",
      "train loss:0.033933614354315494\n",
      "train loss:0.004111629581127867\n",
      "train loss:0.006942406553652541\n",
      "train loss:0.0016460436923867521\n",
      "train loss:0.0004601842866434445\n",
      "train loss:0.0004772054185684985\n",
      "train loss:0.004487125131094005\n",
      "train loss:0.002523514775774523\n",
      "train loss:0.0033932124238955103\n",
      "train loss:0.006922930267148214\n",
      "train loss:0.0008042215419492555\n",
      "train loss:0.005870865317692498\n",
      "train loss:0.0034857588277354815\n",
      "train loss:0.0033326210255709542\n",
      "train loss:0.005287155741081415\n",
      "train loss:0.007272530692239186\n",
      "train loss:0.0010217219127983883\n",
      "train loss:0.01419502070788629\n",
      "train loss:0.0018777181334074444\n",
      "train loss:0.0013297988576217925\n",
      "train loss:0.001550005523887469\n",
      "train loss:0.0011013487508107777\n",
      "train loss:0.0002536082113863902\n",
      "train loss:0.004217383010320767\n",
      "train loss:0.009526144334633379\n",
      "train loss:0.005733453382438842\n",
      "train loss:0.0018573426367642205\n",
      "train loss:0.0026490400383276\n",
      "train loss:0.003320830311355293\n",
      "train loss:0.002127149167595743\n",
      "train loss:0.0107305974551293\n",
      "train loss:0.0019729661905416736\n",
      "train loss:0.015000342246398743\n",
      "train loss:0.0015228844020898733\n",
      "train loss:0.0009941235770062073\n",
      "train loss:0.00020617636680312238\n",
      "train loss:0.0005985276165253841\n",
      "train loss:0.009677823872909067\n",
      "train loss:0.0002249434979331124\n",
      "train loss:0.003471575328454617\n",
      "train loss:0.008085482143483008\n",
      "train loss:0.005890930424955219\n",
      "train loss:0.000251332968609908\n",
      "train loss:0.004798127315582455\n",
      "train loss:0.0013514327959430529\n",
      "train loss:0.005511347954464849\n",
      "train loss:0.0038683394038625247\n",
      "train loss:0.003247752089065167\n",
      "train loss:0.0031926073588870525\n",
      "train loss:0.0016213238852930442\n",
      "train loss:0.0027414817640728215\n",
      "train loss:0.004097043439564619\n",
      "train loss:0.00012874890363797267\n",
      "train loss:0.0021394645900106504\n",
      "train loss:0.0034375174659029427\n",
      "train loss:0.0009067688856493828\n",
      "train loss:0.001825197065680188\n",
      "train loss:0.009676623439600223\n",
      "train loss:0.001961262283227808\n",
      "train loss:0.003354128367483028\n",
      "train loss:0.0023299253796398582\n",
      "train loss:0.0011434870627160777\n",
      "train loss:0.001483493124399649\n",
      "train loss:0.0032900838930043012\n",
      "train loss:0.000336390263504279\n",
      "train loss:0.004917196395519485\n",
      "train loss:0.0006783479527244898\n",
      "train loss:0.003596017756935467\n",
      "train loss:0.0033257869926803085\n",
      "train loss:0.007964576609307251\n",
      "train loss:0.017459151703242935\n",
      "train loss:0.00235771333589562\n",
      "train loss:0.0005121680286219788\n",
      "train loss:0.002937999085491384\n",
      "train loss:0.003185803157752928\n",
      "train loss:0.0018279146525624378\n",
      "train loss:0.0004975584293899833\n",
      "train loss:0.00018525432660820566\n",
      "train loss:0.007566049243478329\n",
      "train loss:0.0032399766394248077\n",
      "train loss:0.003756194591963494\n",
      "train loss:0.0030851856470687057\n",
      "train loss:0.0007633999525451804\n",
      "train loss:0.0013429704207372614\n",
      "train loss:0.0010945563734656714\n",
      "train loss:0.006633031458807025\n",
      "train loss:0.0027130059127184404\n",
      "train loss:0.00011345847419891333\n",
      "train loss:0.0018970166473683962\n",
      "train loss:0.0011400900592526723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.005235513730904595\n",
      "train loss:0.005575218209774501\n",
      "train loss:0.007370108180662971\n",
      "train loss:0.00037445034665387745\n",
      "train loss:0.0030350230198571996\n",
      "train loss:0.002332807677375807\n",
      "train loss:0.00038462836094612334\n",
      "train loss:0.006581296007257815\n",
      "train loss:0.0005528576098273447\n",
      "train loss:0.0017154166657426667\n",
      "train loss:0.0006911683033560454\n",
      "train loss:0.0009152209973910932\n",
      "train loss:0.0009142289806479624\n",
      "train loss:0.0016120925989672264\n",
      "train loss:0.0020721365105855942\n",
      "train loss:0.017864663555528636\n",
      "train loss:0.008481409472253587\n",
      "train loss:0.0062866359167178\n",
      "train loss:0.0050362832669622985\n",
      "train loss:0.006055311966142963\n",
      "train loss:0.004641494429092185\n",
      "train loss:0.0008767025208626577\n",
      "train loss:0.0005578974106377398\n",
      "train loss:0.002845143812402592\n",
      "train loss:0.0010750128918772858\n",
      "train loss:0.0034218090076719447\n",
      "train loss:0.005646014708298185\n",
      "train loss:0.0017309863276375086\n",
      "train loss:0.0020618706036391473\n",
      "train loss:0.0018573216936851958\n",
      "train loss:0.010896871861373985\n",
      "train loss:0.001893040168493101\n",
      "train loss:0.002077655578106769\n",
      "train loss:0.001674453937092061\n",
      "train loss:0.0025536993202181775\n",
      "train loss:0.00015336094880388765\n",
      "train loss:0.006490554432387335\n",
      "train loss:0.001114940037577598\n",
      "train loss:0.002069033648104249\n",
      "train loss:0.002699040685245222\n",
      "train loss:0.002798164520670614\n",
      "train loss:0.01132513044245894\n",
      "train loss:0.0003954532132489361\n",
      "train loss:0.0005863694137392033\n",
      "train loss:0.004602692970580386\n",
      "train loss:0.011505587926777498\n",
      "train loss:0.00016218284902351303\n",
      "train loss:0.002025559587302817\n",
      "train loss:0.0006413441647515691\n",
      "=== epoch:14, train acc:0.994, test acc:0.987 ===\n",
      "train loss:0.0032934268468198225\n",
      "train loss:0.0038303938352423563\n",
      "train loss:0.0009893786897990449\n",
      "train loss:0.00229796424487443\n",
      "train loss:0.004345321338895735\n",
      "train loss:0.007431155246412829\n",
      "train loss:0.0003029860824814626\n",
      "train loss:0.0004422381348250384\n",
      "train loss:0.008171008213959262\n",
      "train loss:0.0006736942230707404\n",
      "train loss:0.019361917952347946\n",
      "train loss:0.002175229589585864\n",
      "train loss:0.0010370501175446874\n",
      "train loss:0.0006057170501394878\n",
      "train loss:0.003737568376565364\n",
      "train loss:0.0018283914451766175\n",
      "train loss:0.0007699402078861893\n",
      "train loss:0.002036881487624595\n",
      "train loss:0.000659028504503281\n",
      "train loss:0.002274029452934571\n",
      "train loss:0.01172988046756574\n",
      "train loss:0.0017089610272197078\n",
      "train loss:0.017237035547865046\n",
      "train loss:0.003914832715461735\n",
      "train loss:0.0017913548325880226\n",
      "train loss:0.0018012960552133627\n",
      "train loss:0.008343197282125592\n",
      "train loss:0.0004787259672754027\n",
      "train loss:0.0011555766200714323\n",
      "train loss:0.010535429578058808\n",
      "train loss:0.0002524517922921509\n",
      "train loss:0.0019213943504679009\n",
      "train loss:0.002384894185807197\n",
      "train loss:0.001625575956914605\n",
      "train loss:0.00509650127297459\n",
      "train loss:0.001134103367234031\n",
      "train loss:0.004555229797226325\n",
      "train loss:0.0004055365147276472\n",
      "train loss:0.0035682100406918805\n",
      "train loss:0.0018161754492215488\n",
      "train loss:0.0006269686532528305\n",
      "train loss:0.0008847344082024383\n",
      "train loss:0.000220636155835189\n",
      "train loss:0.0006401424389211001\n",
      "train loss:0.006082343391446017\n",
      "train loss:0.004054778304419888\n",
      "train loss:0.0018629685516201185\n",
      "train loss:0.0035083031235894862\n",
      "train loss:0.0005650820912026348\n",
      "train loss:0.0018586033711217767\n",
      "train loss:0.025167364296057978\n",
      "train loss:0.0020413194294122473\n",
      "train loss:0.0002687187652781086\n",
      "train loss:0.00737830242074099\n",
      "train loss:0.0009039761879577578\n",
      "train loss:0.0007458149758828643\n",
      "train loss:0.0009612737529561683\n",
      "train loss:0.00033046166659496585\n",
      "train loss:0.007366076353185483\n",
      "train loss:0.023055430796268404\n",
      "train loss:0.0019004859344209002\n",
      "train loss:0.0008471108745649671\n",
      "train loss:0.00031357812577848394\n",
      "train loss:0.0009919415407516157\n",
      "train loss:0.006108306175111615\n",
      "train loss:0.0038607648554643765\n",
      "train loss:0.001868876976864352\n",
      "train loss:0.0026513051740339206\n",
      "train loss:0.001172784669218965\n",
      "train loss:0.002992238523376785\n",
      "train loss:0.0010341591278414657\n",
      "train loss:0.009162961095461027\n",
      "train loss:0.0463360678813317\n",
      "train loss:0.0013007453274374709\n",
      "train loss:0.0024865728985635615\n",
      "train loss:0.006384698819255995\n",
      "train loss:0.0025287087915721812\n",
      "train loss:0.001783039203930115\n",
      "train loss:0.004102146818224633\n",
      "train loss:0.0019528828765036157\n",
      "train loss:0.0010605692247069075\n",
      "train loss:0.004248067705877256\n",
      "train loss:0.0012925655699578314\n",
      "train loss:0.004024151998659863\n",
      "train loss:0.0033905906703751406\n",
      "train loss:0.0007581022346735923\n",
      "train loss:0.02597873776897855\n",
      "train loss:0.0057306721382965435\n",
      "train loss:0.0008394617240490127\n",
      "train loss:0.00262015763666334\n",
      "train loss:0.06913621271718493\n",
      "train loss:0.00033772659764862774\n",
      "train loss:0.0022388496564587045\n",
      "train loss:0.0017858883582771278\n",
      "train loss:0.0012573748378333297\n",
      "train loss:0.0061025711673450165\n",
      "train loss:0.0037336945698174286\n",
      "train loss:0.0033365124018729197\n",
      "train loss:0.0010730665494035146\n",
      "train loss:0.00630838539921157\n",
      "train loss:0.01105039983211435\n",
      "train loss:0.001925999234504516\n",
      "train loss:0.0004332102624964585\n",
      "train loss:0.005295127530088261\n",
      "train loss:0.0037596669180683688\n",
      "train loss:0.005179202403507432\n",
      "train loss:0.0013327011812123515\n",
      "train loss:0.0011684469393072696\n",
      "train loss:0.002968250900392962\n",
      "train loss:0.0016780919119360651\n",
      "train loss:0.0013754641179836585\n",
      "train loss:0.002079897818571809\n",
      "train loss:0.01630639850002553\n",
      "train loss:0.0037832917383544985\n",
      "train loss:0.0025074461678327147\n",
      "train loss:0.00039905085830705707\n",
      "train loss:0.0017189222593120418\n",
      "train loss:0.0022989429905150844\n",
      "train loss:0.000557403868284128\n",
      "train loss:0.0011163435214964434\n",
      "train loss:0.006076817489534999\n",
      "train loss:0.0019459521414299725\n",
      "train loss:0.0008094790826386074\n",
      "train loss:0.0010625092136556978\n",
      "train loss:0.0010290316820235343\n",
      "train loss:0.001364417007136512\n",
      "train loss:0.00021964914113488648\n",
      "train loss:0.004622649964330177\n",
      "train loss:0.0005697269270283236\n",
      "train loss:0.0006160625815811181\n",
      "train loss:0.0033417423923377742\n",
      "train loss:0.0042841774163446585\n",
      "train loss:0.00033647578761316\n",
      "train loss:0.0001966480881999208\n",
      "train loss:0.00010431892180244178\n",
      "train loss:0.0038321693846017496\n",
      "train loss:0.007636784144833334\n",
      "train loss:0.0002568183802612939\n",
      "train loss:0.004533864804557456\n",
      "train loss:0.004051738616366168\n",
      "train loss:0.0003152717215826165\n",
      "train loss:0.07291041328961478\n",
      "train loss:0.0015731371449391966\n",
      "train loss:0.0008392916000470665\n",
      "train loss:0.0005757917608539212\n",
      "train loss:0.0066225391520455335\n",
      "train loss:0.00011236642775669114\n",
      "train loss:0.0017427296164650573\n",
      "train loss:0.02919047965765672\n",
      "train loss:0.004294647925962377\n",
      "train loss:0.004487988861325166\n",
      "train loss:0.0037924853522059193\n",
      "train loss:0.0015180478485152108\n",
      "train loss:0.004524264005472451\n",
      "train loss:0.002560487193007421\n",
      "train loss:0.0035754131662482946\n",
      "train loss:0.00599545902400029\n",
      "train loss:0.0016436563266004481\n",
      "train loss:0.008858387945853023\n",
      "train loss:0.0005225003691109148\n",
      "train loss:0.0024217473547537877\n",
      "train loss:0.0008491134718322902\n",
      "train loss:0.0013930627471946447\n",
      "train loss:0.005674610277830826\n",
      "train loss:0.0023917804415161813\n",
      "train loss:0.0006998596689071154\n",
      "train loss:0.0009806200373853574\n",
      "train loss:0.028780584406853316\n",
      "train loss:0.0031732574202270767\n",
      "train loss:0.0022885306401729615\n",
      "train loss:0.0021016639423313225\n",
      "train loss:0.0006755625396419935\n",
      "train loss:0.0026527629059118446\n",
      "train loss:0.0002036018862562549\n",
      "train loss:0.0022528308162259587\n",
      "train loss:0.02336609492693647\n",
      "train loss:0.0004363799590990293\n",
      "train loss:0.002529554228517158\n",
      "train loss:0.0029511963136729723\n",
      "train loss:0.0007575851056279614\n",
      "train loss:0.002315650228754856\n",
      "train loss:0.0014280919493760366\n",
      "train loss:0.001040974861025644\n",
      "train loss:0.002129583566449692\n",
      "train loss:0.002031691787872576\n",
      "train loss:0.0015990717097705926\n",
      "train loss:0.0010072745974510381\n",
      "train loss:0.001896720821023028\n",
      "train loss:0.0026588340545363586\n",
      "train loss:0.00041484733432924775\n",
      "train loss:0.0018121785661687204\n",
      "train loss:0.0006472127074254026\n",
      "train loss:0.004349672855375087\n",
      "train loss:0.0014031856629176385\n",
      "train loss:0.0007011612417370187\n",
      "train loss:0.0007128304199292339\n",
      "train loss:0.0035338083620572135\n",
      "train loss:0.0005349527706220728\n",
      "train loss:0.0033424401288797657\n",
      "train loss:0.004478926379601603\n",
      "train loss:0.003885317727759627\n",
      "train loss:0.005436088087308005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0005908948008677735\n",
      "train loss:0.0031078125964126013\n",
      "train loss:0.0013632128709045988\n",
      "train loss:0.00190726678580841\n",
      "train loss:0.0002960138101740747\n",
      "train loss:0.001953863701567464\n",
      "train loss:0.0013482773847652415\n",
      "train loss:0.0026460389534103796\n",
      "train loss:0.0014079883937656145\n",
      "train loss:0.00047062148674477844\n",
      "train loss:0.007493197491508126\n",
      "train loss:0.001612141685402098\n",
      "train loss:0.01629252449905951\n",
      "train loss:0.016827897838845928\n",
      "train loss:0.0005529119011962979\n",
      "train loss:0.0005841735904192871\n",
      "train loss:0.0003461347489642494\n",
      "train loss:0.0006066118118915962\n",
      "train loss:0.023562973784434758\n",
      "train loss:0.004249546242455832\n",
      "train loss:0.0006094933077490891\n",
      "train loss:0.009283278551376247\n",
      "train loss:0.011018510632671112\n",
      "train loss:0.0017616363677848148\n",
      "train loss:0.0019765265456944776\n",
      "train loss:0.004296510559758377\n",
      "train loss:0.037812456351126816\n",
      "train loss:0.004357577870391435\n",
      "train loss:0.005201506675202513\n",
      "train loss:0.0032976784773665114\n",
      "train loss:0.0050304774681172604\n",
      "train loss:0.001723366507931865\n",
      "train loss:0.0022554706396651068\n",
      "train loss:0.007062247654834378\n",
      "train loss:0.000964477611256858\n",
      "train loss:0.004765492304473117\n",
      "train loss:0.0024800545182229604\n",
      "train loss:0.0015661334550151555\n",
      "train loss:0.0002235932121424244\n",
      "train loss:0.0007186605191113732\n",
      "train loss:0.0019973913585529\n",
      "train loss:0.0028327296259299944\n",
      "train loss:0.0036142103893858244\n",
      "train loss:0.002319322240096932\n",
      "train loss:0.015624864180512663\n",
      "train loss:0.0001485725376103854\n",
      "train loss:0.001343830801882077\n",
      "train loss:0.004431912614289648\n",
      "train loss:0.0036378803267320676\n",
      "train loss:0.0017720478273951063\n",
      "train loss:0.004456454422002168\n",
      "train loss:0.005309527365065919\n",
      "train loss:0.006577918277196238\n",
      "train loss:0.01482043333249667\n",
      "train loss:0.004844834265930182\n",
      "train loss:0.0003958061085674984\n",
      "train loss:0.0013576741715487794\n",
      "train loss:0.005535748417826261\n",
      "train loss:0.01068778896692829\n",
      "train loss:0.004565878777907767\n",
      "train loss:0.0007670622734806816\n",
      "train loss:0.0014069016339871632\n",
      "train loss:0.004106707574758427\n",
      "train loss:0.0001196514744132917\n",
      "train loss:0.001867017789963239\n",
      "train loss:0.011725397192154551\n",
      "train loss:0.002321878440865775\n",
      "train loss:0.0034131917526819076\n",
      "train loss:0.004299879391788511\n",
      "train loss:0.0004973594471212752\n",
      "train loss:0.00723965408627317\n",
      "train loss:0.004742457807197472\n",
      "train loss:0.006689503944048707\n",
      "train loss:0.0014714498520496128\n",
      "train loss:0.0031252621400568764\n",
      "train loss:0.0008912443101635191\n",
      "train loss:0.017113397011520322\n",
      "train loss:0.012835633996087448\n",
      "train loss:0.0005960257612745924\n",
      "train loss:0.0006002826748572267\n",
      "train loss:0.0015297056868179045\n",
      "train loss:0.008480588034055929\n",
      "train loss:0.0014235505211968717\n",
      "train loss:0.009330709222189799\n",
      "train loss:0.05122003525754117\n",
      "train loss:0.004043285092035026\n",
      "train loss:0.0037472849917813316\n",
      "train loss:0.00548022650527742\n",
      "train loss:0.0009134643305936473\n",
      "train loss:0.0008952984788931418\n",
      "train loss:0.0017642768913952009\n",
      "train loss:0.0006235203008589946\n",
      "train loss:0.003442305522426402\n",
      "train loss:0.0035405561333626623\n",
      "train loss:0.0069196176304648915\n",
      "train loss:0.0006515053382951734\n",
      "train loss:0.0008587527269982121\n",
      "train loss:0.0026387721751619736\n",
      "train loss:0.0015572866469547183\n",
      "train loss:0.002810257164288341\n",
      "train loss:0.006373426936976955\n",
      "train loss:0.0010038874591061448\n",
      "train loss:0.004848946282081531\n",
      "train loss:0.0029978757153929893\n",
      "train loss:0.003824669926573493\n",
      "train loss:0.01573969108990967\n",
      "train loss:0.0047417595126919366\n",
      "train loss:0.0026080453276136717\n",
      "train loss:0.00015601090867658875\n",
      "train loss:0.006876785415276968\n",
      "train loss:0.001239274016952905\n",
      "train loss:0.004000226933657686\n",
      "train loss:0.0025116170153461222\n",
      "train loss:0.0012191401907244463\n",
      "train loss:0.0020167387079873473\n",
      "train loss:0.008351851924657948\n",
      "train loss:0.0036605947045564103\n",
      "train loss:0.001510102065323614\n",
      "train loss:0.0014524941907599418\n",
      "train loss:0.015970088217289635\n",
      "train loss:0.0034058334329769456\n",
      "train loss:0.002772125087567832\n",
      "train loss:0.0015565178016316158\n",
      "train loss:0.0020941934428854625\n",
      "train loss:0.007231659625143933\n",
      "train loss:0.003457047845791916\n",
      "train loss:0.006272677708637715\n",
      "train loss:0.0075001455318921195\n",
      "train loss:0.0011157078942808727\n",
      "train loss:0.0026852537286520413\n",
      "train loss:0.0018452315654528858\n",
      "train loss:0.0038343821387166006\n",
      "train loss:0.0032330130933800527\n",
      "train loss:0.010634924300701162\n",
      "train loss:0.03562244225966283\n",
      "train loss:0.005015938880953224\n",
      "train loss:0.007498125668040832\n",
      "train loss:0.0002505155473832943\n",
      "train loss:0.0004284278700151707\n",
      "train loss:0.00028096004064345916\n",
      "train loss:0.0074898655797937365\n",
      "train loss:0.0017560960174116608\n",
      "train loss:0.0009577357403432454\n",
      "train loss:0.0004715729666103937\n",
      "train loss:0.002003026240884954\n",
      "train loss:0.005781426553504809\n",
      "train loss:0.0006006998835169261\n",
      "train loss:0.00330440520269285\n",
      "train loss:0.0063948103426292235\n",
      "train loss:0.0024918145022855007\n",
      "train loss:0.0012709668593655784\n",
      "train loss:0.000543810896357052\n",
      "train loss:0.002519228234397192\n",
      "train loss:0.00012551670779395517\n",
      "train loss:0.005514804397107346\n",
      "train loss:0.0016055464892582502\n",
      "train loss:0.012072333840616704\n",
      "train loss:0.0009462966048562202\n",
      "train loss:0.0005325406166468224\n",
      "train loss:0.001249815162626902\n",
      "train loss:0.0019058893777642327\n",
      "train loss:0.0015160952979394538\n",
      "train loss:0.0013471194605776651\n",
      "train loss:0.0036370807237653237\n",
      "train loss:0.006338169658875045\n",
      "train loss:0.0031354203124260233\n",
      "train loss:0.0004595086126799765\n",
      "train loss:0.025693947744619337\n",
      "train loss:0.009725363836297683\n",
      "train loss:0.00030453062321644315\n",
      "train loss:0.0022463201485080912\n",
      "train loss:0.0018026479678563353\n",
      "train loss:0.00049123335764649\n",
      "train loss:0.0002171912001951587\n",
      "train loss:0.0028050423974050427\n",
      "train loss:0.006515097181025556\n",
      "train loss:0.0045411873671448956\n",
      "train loss:0.0028038861394962685\n",
      "train loss:0.00260258965751382\n",
      "train loss:0.011792669758172699\n",
      "train loss:0.006111577876019108\n",
      "train loss:0.0007717419156853177\n",
      "train loss:0.000248972036853753\n",
      "train loss:0.007643687721565443\n",
      "train loss:0.002963541501303182\n",
      "train loss:0.0007354751144265152\n",
      "train loss:0.0002613359048743218\n",
      "train loss:0.002335654505633811\n",
      "train loss:0.0019948055333487273\n",
      "train loss:0.05561864152714807\n",
      "train loss:0.000977129140009139\n",
      "train loss:0.005877417111470701\n",
      "train loss:0.006377171437484662\n",
      "train loss:0.0003584917468129818\n",
      "train loss:0.0064998383884848875\n",
      "train loss:0.001907797796314881\n",
      "train loss:0.002953859644922035\n",
      "train loss:0.0003228602974265199\n",
      "train loss:0.000181009155572601\n",
      "train loss:0.0020306743654261096\n",
      "train loss:0.010013123780624553\n",
      "train loss:0.0025609685241112863\n",
      "train loss:0.0007379999258308668\n",
      "train loss:0.0035542327594273145\n",
      "train loss:0.001255124576814215\n",
      "train loss:0.001003812814290518\n",
      "train loss:0.003921493945053878\n",
      "train loss:0.0018799134927872784\n",
      "train loss:0.009896196900623704\n",
      "train loss:0.001742845861986963\n",
      "train loss:0.003288109741790996\n",
      "train loss:0.0060790253240050095\n",
      "train loss:0.002250139621779174\n",
      "train loss:0.0012184999576313443\n",
      "train loss:0.0006609271614213506\n",
      "train loss:0.0011061111935098892\n",
      "train loss:0.00404462232357076\n",
      "train loss:0.0013194400410849933\n",
      "train loss:0.0002679153395316446\n",
      "train loss:0.0006874694670177865\n",
      "train loss:0.0006150897890812802\n",
      "train loss:0.021361379220467262\n",
      "train loss:0.0005566209830812596\n",
      "train loss:0.0008759744716564139\n",
      "train loss:0.0005048226285617216\n",
      "train loss:0.0010775161440267434\n",
      "train loss:0.0007279616440908408\n",
      "train loss:0.0036650037596191575\n",
      "train loss:0.0021644639924217747\n",
      "train loss:0.0010237457033854577\n",
      "train loss:0.0058321682712747566\n",
      "train loss:0.0013688889802595467\n",
      "train loss:0.0018796904385579875\n",
      "train loss:0.00382661056114196\n",
      "train loss:0.007071985956529574\n",
      "train loss:0.0001969227670478066\n",
      "train loss:0.002739058690843643\n",
      "train loss:0.00014376723400751578\n",
      "train loss:0.0033362514748053822\n",
      "train loss:0.002186207385615768\n",
      "train loss:0.0012394659482577754\n",
      "train loss:0.00044877612313512993\n",
      "train loss:0.001052118525560488\n",
      "train loss:0.016321185547555916\n",
      "train loss:0.001553927326876868\n",
      "train loss:0.000188596277359047\n",
      "train loss:0.004459899011166474\n",
      "train loss:0.0028455067776773403\n",
      "train loss:0.0016359165679071899\n",
      "train loss:0.006436318559765422\n",
      "train loss:0.011619052730734365\n",
      "train loss:0.0013421508572135077\n",
      "train loss:0.0020900936419859088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.00020244584649378814\n",
      "train loss:0.003761217813662428\n",
      "train loss:0.0007587951078279519\n",
      "train loss:0.0035655367975827735\n",
      "train loss:0.0012445380527679537\n",
      "train loss:0.002467835876040451\n",
      "train loss:0.028640290864179775\n",
      "train loss:0.0005339507039960594\n",
      "train loss:0.0008225738388826373\n",
      "train loss:0.0015528525843662411\n",
      "train loss:0.0006560742869993476\n",
      "train loss:0.002353205579219099\n",
      "train loss:0.004193718915773215\n",
      "train loss:0.0036279048980310076\n",
      "train loss:0.000987925834558959\n",
      "train loss:0.0011353396277531324\n",
      "train loss:0.0021966947658953206\n",
      "train loss:0.003609332898281169\n",
      "train loss:0.002365943015055102\n",
      "train loss:0.01581196265779989\n",
      "train loss:0.0016439023446933447\n",
      "train loss:0.0005495607664285753\n",
      "train loss:0.010977118098198305\n",
      "train loss:0.0005303807015905804\n",
      "train loss:0.005266527647132562\n",
      "train loss:0.001970330595300184\n",
      "train loss:0.0051845618158398886\n",
      "train loss:0.002399819605807638\n",
      "train loss:0.002131963690676628\n",
      "train loss:0.0017146521551942356\n",
      "train loss:0.00013503611030985313\n",
      "train loss:0.0007759337731956256\n",
      "train loss:0.005728842489728998\n",
      "train loss:0.00012093662957352506\n",
      "train loss:0.0025936860076264173\n",
      "train loss:0.018974310851496236\n",
      "train loss:0.0005778119650304138\n",
      "train loss:0.004040276418566052\n",
      "train loss:0.0039824313978098595\n",
      "train loss:0.0007909170440844776\n",
      "train loss:0.011765560481211391\n",
      "train loss:0.0008133039083386389\n",
      "train loss:0.003786762264078431\n",
      "train loss:0.0008930648956855446\n",
      "train loss:0.001312719373588612\n",
      "train loss:0.0008883247459302331\n",
      "train loss:0.0007415959259016279\n",
      "train loss:0.00017728609923748926\n",
      "train loss:0.006205433490166622\n",
      "train loss:0.006077163177250607\n",
      "train loss:0.0025603506323068005\n",
      "train loss:0.000204758434806786\n",
      "train loss:0.0003048366427558064\n",
      "train loss:0.0005826931547973289\n",
      "train loss:0.0016039920446273932\n",
      "train loss:0.0015337837385619338\n",
      "train loss:0.0003725024884289292\n",
      "train loss:0.0005357103164077504\n",
      "train loss:0.0032059917004916487\n",
      "train loss:0.0026287073386414843\n",
      "train loss:0.000985203044975928\n",
      "train loss:0.009698069779714456\n",
      "train loss:0.00047521978094868764\n",
      "train loss:0.003157482674870592\n",
      "train loss:0.0004410551396165537\n",
      "train loss:0.002257728675978433\n",
      "train loss:0.0001410646025975319\n",
      "train loss:0.0001912491680558397\n",
      "train loss:0.003210121799429346\n",
      "train loss:0.0005655870015450282\n",
      "train loss:0.00011723694255467397\n",
      "train loss:0.0015287354445660635\n",
      "train loss:0.010172149029091741\n",
      "train loss:0.0007886568146343416\n",
      "train loss:0.0017471474544488762\n",
      "train loss:0.004660112670969845\n",
      "train loss:0.002273725028544717\n",
      "train loss:0.001728549706327567\n",
      "train loss:0.013775735093005366\n",
      "train loss:0.0007463481312301155\n",
      "train loss:0.008291651105080854\n",
      "train loss:0.005123811734026505\n",
      "train loss:0.0014330637291247387\n",
      "train loss:0.018050114063061822\n",
      "train loss:0.00046141749263976675\n",
      "train loss:0.003994987651279756\n",
      "train loss:0.0009525159606835803\n",
      "train loss:0.000568075956808813\n",
      "train loss:0.00955627208878717\n",
      "train loss:0.0046281799200915165\n",
      "train loss:0.0002583971716252565\n",
      "train loss:0.0011407216019201929\n",
      "train loss:0.003992098673147865\n",
      "train loss:0.000756902883013735\n",
      "train loss:0.002589843895905443\n",
      "train loss:0.0003655638836923074\n",
      "train loss:0.00017389016480841925\n",
      "train loss:0.0024211531967781043\n",
      "train loss:0.0033842973886002714\n",
      "train loss:0.0036344583806356195\n",
      "train loss:0.0014676421550108374\n",
      "train loss:0.0006884811435102746\n",
      "train loss:0.002298801482786914\n",
      "train loss:0.0011034426130046295\n",
      "train loss:0.01031268308853611\n",
      "train loss:0.00028158624777032765\n",
      "train loss:0.00016212277520953895\n",
      "train loss:0.007159249110630595\n",
      "train loss:0.000867591251263039\n",
      "train loss:0.0013798583963832358\n",
      "train loss:0.005207536781939107\n",
      "train loss:0.0021836393458161906\n",
      "train loss:0.008400561753594403\n",
      "train loss:0.00087825280820976\n",
      "train loss:0.0023981171203057284\n",
      "train loss:0.0022435186118390927\n",
      "train loss:0.0023797361955906985\n",
      "train loss:0.0012127346557972015\n",
      "train loss:0.0009633300061152802\n",
      "train loss:0.0018076151792231305\n",
      "train loss:0.006391420478920546\n",
      "train loss:0.0019914547644538\n",
      "train loss:0.006429134568853542\n",
      "train loss:0.005266125217222113\n",
      "train loss:0.006393201072215801\n",
      "train loss:0.00426640312563175\n",
      "train loss:0.0006462999453452103\n",
      "train loss:0.007413188954737957\n",
      "train loss:0.0016950538285253431\n",
      "train loss:0.012698187842368284\n",
      "train loss:0.0007150842602312467\n",
      "train loss:0.0018736713761280282\n",
      "train loss:0.006611347444288234\n",
      "train loss:0.003367487778998218\n",
      "train loss:0.0006706506853743806\n",
      "train loss:0.00820236854437076\n",
      "train loss:0.0019255090526837223\n",
      "train loss:0.007879149585202189\n",
      "train loss:0.0002704552334900953\n",
      "train loss:0.005481355686378545\n",
      "train loss:0.006177595650969692\n",
      "train loss:0.000554356349215344\n",
      "train loss:0.0011006269852525039\n",
      "train loss:0.003780461235296942\n",
      "train loss:0.004176007930280883\n",
      "=== epoch:15, train acc:0.995, test acc:0.988 ===\n",
      "train loss:0.00914117744824201\n",
      "train loss:0.019994488192411695\n",
      "train loss:0.021547202842118373\n",
      "train loss:0.0016841186075435793\n",
      "train loss:0.0032568603073146376\n",
      "train loss:0.0004881150201379524\n",
      "train loss:0.00200588932027381\n",
      "train loss:0.0013723555197864455\n",
      "train loss:0.0004082361956700639\n",
      "train loss:0.0005869840043519416\n",
      "train loss:0.008434916600452983\n",
      "train loss:0.00028083170750473367\n",
      "train loss:0.001679178787497311\n",
      "train loss:0.0023064878744696576\n",
      "train loss:0.00018966627648297511\n",
      "train loss:0.003287922506912251\n",
      "train loss:0.001163787289863741\n",
      "train loss:0.00255310445207659\n",
      "train loss:0.005431202846807972\n",
      "train loss:0.004276174184933937\n",
      "train loss:0.002966983108461453\n",
      "train loss:0.004840489129724159\n",
      "train loss:0.0005576625112066711\n",
      "train loss:0.0029921066963260285\n",
      "train loss:0.0007083154209103974\n",
      "train loss:0.00942532937429787\n",
      "train loss:0.0005383697071206742\n",
      "train loss:0.003159170845359378\n",
      "train loss:0.01696739392748712\n",
      "train loss:0.0006265462880790498\n",
      "train loss:0.0012054614875091738\n",
      "train loss:0.009209590582756517\n",
      "train loss:0.0007396236847831819\n",
      "train loss:0.002777428977203614\n",
      "train loss:0.0018115149124036459\n",
      "train loss:0.0005669114313155006\n",
      "train loss:0.003072255246522418\n",
      "train loss:0.0065388816458413915\n",
      "train loss:0.008357573481493607\n",
      "train loss:0.0009313975535635334\n",
      "train loss:0.004191308673392232\n",
      "train loss:0.005466702843969945\n",
      "train loss:0.0017107055456498106\n",
      "train loss:0.000773773931188205\n",
      "train loss:0.001816832520544799\n",
      "train loss:0.0005855306181893146\n",
      "train loss:0.0007582309519740837\n",
      "train loss:0.000993900002484164\n",
      "train loss:0.000858568392547481\n",
      "train loss:0.0009879179924114562\n",
      "train loss:0.007387285103843432\n",
      "train loss:0.0007310870593560398\n",
      "train loss:0.007604241651703856\n",
      "train loss:0.0006299626746416064\n",
      "train loss:0.0015993972778314725\n",
      "train loss:0.0024600024921838295\n",
      "train loss:0.009456462671628451\n",
      "train loss:0.0012125168322508852\n",
      "train loss:0.0016482463986851879\n",
      "train loss:0.0070471236223231745\n",
      "train loss:0.00011789071131344831\n",
      "train loss:0.0009477877932973594\n",
      "train loss:0.0007422198959639764\n",
      "train loss:0.002162892063588455\n",
      "train loss:0.0052870190484697556\n",
      "train loss:0.00914316873122773\n",
      "train loss:0.003380502177653784\n",
      "train loss:0.004011328518505618\n",
      "train loss:0.006080242886220697\n",
      "train loss:0.0021195751839444667\n",
      "train loss:0.002125293068586032\n",
      "train loss:0.0016224070901849025\n",
      "train loss:0.00010589999434234582\n",
      "train loss:0.0004990788593289169\n",
      "train loss:0.002086772916119283\n",
      "train loss:0.008537697584640123\n",
      "train loss:0.0006041086124859462\n",
      "train loss:0.004275462234251759\n",
      "train loss:0.00029713334044641204\n",
      "train loss:0.00024001235236873085\n",
      "train loss:0.001237974621092407\n",
      "train loss:0.0005437112957006827\n",
      "train loss:0.001184279613156245\n",
      "train loss:0.00945401833735351\n",
      "train loss:0.004481200420757153\n",
      "train loss:0.0009335733763735401\n",
      "train loss:0.00433732079735097\n",
      "train loss:0.0016323408589101268\n",
      "train loss:0.0031565705998394516\n",
      "train loss:0.00012689754209544926\n",
      "train loss:0.0005790391674744323\n",
      "train loss:0.0009924417902051747\n",
      "train loss:0.0005495937697417114\n",
      "train loss:0.002226350635738592\n",
      "train loss:0.011547326017369847\n",
      "train loss:0.006340893997867632\n",
      "train loss:0.0006453908786765705\n",
      "train loss:0.0036066070085604292\n",
      "train loss:0.00038380414867448945\n",
      "train loss:0.008956138488283518\n",
      "train loss:0.00235313116730924\n",
      "train loss:0.0023284640762157296\n",
      "train loss:0.0039067274380493065\n",
      "train loss:0.007588529220396048\n",
      "train loss:0.00012471866279054554\n",
      "train loss:0.0011632179063443635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.00029128739791072286\n",
      "train loss:0.019794483498307675\n",
      "train loss:0.0018237328106446465\n",
      "train loss:0.0007224852963379301\n",
      "train loss:0.002418685456951494\n",
      "train loss:0.0005298178683812779\n",
      "train loss:0.0012044556873812024\n",
      "train loss:0.0031855071609728767\n",
      "train loss:0.003094154883275273\n",
      "train loss:0.002396339507617442\n",
      "train loss:0.0002748087686306187\n",
      "train loss:0.0006577561896736482\n",
      "train loss:0.001134583724918554\n",
      "train loss:0.0007005344189413075\n",
      "train loss:0.0024692447313760402\n",
      "train loss:0.01952655656976419\n",
      "train loss:0.001662937072236853\n",
      "train loss:0.003371417358051518\n",
      "train loss:0.003247214651739821\n",
      "train loss:0.0018641054377229026\n",
      "train loss:0.004277085945829787\n",
      "train loss:0.045547874533305714\n",
      "train loss:0.0028854153005787187\n",
      "train loss:0.003438172112551662\n",
      "train loss:0.0003765405693749391\n",
      "train loss:0.00015572068156359482\n",
      "train loss:0.0033591366511798704\n",
      "train loss:0.0027881131967786993\n",
      "train loss:0.0018364045606441687\n",
      "train loss:0.00047102345306141715\n",
      "train loss:0.0003236269730067495\n",
      "train loss:0.0024311519973911015\n",
      "train loss:0.0024731924357124496\n",
      "train loss:0.0014375347982404452\n",
      "train loss:0.002366053714293798\n",
      "train loss:0.007329827532525602\n",
      "train loss:0.0014437913667119076\n",
      "train loss:0.008970557961863784\n",
      "train loss:0.004145832280144629\n",
      "train loss:0.0015473759905402912\n",
      "train loss:0.010123024845877602\n",
      "train loss:0.01744868922849691\n",
      "train loss:0.0012959397491481485\n",
      "train loss:0.002440369279620662\n",
      "train loss:0.006485486968769009\n",
      "train loss:0.002746956720089798\n",
      "train loss:0.00491096527323071\n",
      "train loss:0.002149834477051635\n",
      "train loss:0.0033284305428392365\n",
      "train loss:0.012451290134572723\n",
      "train loss:0.0036551654020679893\n",
      "train loss:0.003945193342668491\n",
      "train loss:0.00165008036695961\n",
      "train loss:0.0013926412885055912\n",
      "train loss:0.005485591254223043\n",
      "train loss:0.0007943627983707736\n",
      "train loss:0.0009281323871990891\n",
      "train loss:0.010886226337444998\n",
      "train loss:0.0005577250067120081\n",
      "train loss:0.00465331263197016\n",
      "train loss:0.0028208993833749906\n",
      "train loss:0.007977972153052026\n",
      "train loss:0.031254260531643666\n",
      "train loss:0.026830152940402637\n",
      "train loss:0.000293613832406265\n",
      "train loss:0.010852580082861541\n",
      "train loss:0.05079267134325832\n",
      "train loss:0.0009266287571790226\n",
      "train loss:0.0011148673016635998\n",
      "train loss:0.0061255986573746654\n",
      "train loss:0.00912984889144598\n",
      "train loss:0.0017251106826631588\n",
      "train loss:0.051146027412080707\n",
      "train loss:0.020693174762623\n",
      "train loss:0.006687006857805425\n",
      "train loss:0.009076542914048113\n",
      "train loss:0.00038450590501077\n",
      "train loss:0.0007081995529113385\n",
      "train loss:0.0033949730470645774\n",
      "train loss:0.008003502363760481\n",
      "train loss:4.6162025762094514e-05\n",
      "train loss:0.001795220074592812\n",
      "train loss:0.005077015118796504\n",
      "train loss:0.016358649421859436\n",
      "train loss:0.0012743610873504297\n",
      "train loss:0.0011031451003523773\n",
      "train loss:0.011421950576282805\n",
      "train loss:0.0002638405355217162\n",
      "train loss:0.0012797708813045833\n",
      "train loss:0.004711922677895165\n",
      "train loss:0.0010398608432454922\n",
      "train loss:0.0004954370428033917\n",
      "train loss:0.002748802929375073\n",
      "train loss:0.0007462378313198405\n",
      "train loss:0.005639636634595681\n",
      "train loss:0.0020947122995053076\n",
      "train loss:0.011622683576748949\n",
      "train loss:0.004864969011684663\n",
      "train loss:0.0016218720742598315\n",
      "train loss:0.001024530547358198\n",
      "train loss:0.00396751262808473\n",
      "train loss:0.004101815772726176\n",
      "train loss:0.002220610604134697\n",
      "train loss:0.004008202822831257\n",
      "train loss:0.0010211360749857786\n",
      "train loss:0.0010960807534085306\n",
      "train loss:0.0065220718096848625\n",
      "train loss:0.0007887972769529443\n",
      "train loss:0.0009247017651185845\n",
      "train loss:0.0038606823328121154\n",
      "train loss:0.0006600966356282333\n",
      "train loss:0.011472480797769559\n",
      "train loss:0.0007168612923158988\n",
      "train loss:0.004362785037209105\n",
      "train loss:0.00923176983220066\n",
      "train loss:0.005289377902129416\n",
      "train loss:0.0010721558693113278\n",
      "train loss:0.004793988908735825\n",
      "train loss:0.0030546147605086723\n",
      "train loss:0.0034801710085305905\n",
      "train loss:0.0003404364900565004\n",
      "train loss:0.003357194392411027\n",
      "train loss:0.001186522152099702\n",
      "train loss:0.0010546908635300416\n",
      "train loss:0.007958516423338828\n",
      "train loss:0.0039004465511967403\n",
      "train loss:0.0007560493886532277\n",
      "train loss:0.0011070047413643984\n",
      "train loss:0.0022197884031214233\n",
      "train loss:0.006896950303020785\n",
      "train loss:0.00423947682510674\n",
      "train loss:0.009446099874385277\n",
      "train loss:0.00033693318681349906\n",
      "train loss:0.0013218638962539935\n",
      "train loss:0.003315068727634292\n",
      "train loss:0.0024253255518475935\n",
      "train loss:0.0014301433568951168\n",
      "train loss:0.00267700290383978\n",
      "train loss:0.0019116031997220841\n",
      "train loss:0.0003681813245397407\n",
      "train loss:0.000777022109062769\n",
      "train loss:0.009894544499427543\n",
      "train loss:0.0011444418724147955\n",
      "train loss:0.002750021998577322\n",
      "train loss:0.0007582819707290314\n",
      "train loss:0.0036441696490911507\n",
      "train loss:0.015192795035495253\n",
      "train loss:0.0053368186293462115\n",
      "train loss:0.0005957470989763903\n",
      "train loss:0.009433704482781325\n",
      "train loss:0.0010034484393626419\n",
      "train loss:0.002949662223244304\n",
      "train loss:0.01707455545044757\n",
      "train loss:0.0030072289788865697\n",
      "train loss:0.0005411995375581674\n",
      "train loss:0.0006000132918127384\n",
      "train loss:0.0006592054216305416\n",
      "train loss:0.0006168138166180908\n",
      "train loss:0.0005136982650092\n",
      "train loss:0.039324893895801884\n",
      "train loss:0.009520896606721013\n",
      "train loss:0.0047711885477977325\n",
      "train loss:0.0018090253096350703\n",
      "train loss:0.0013371870808419472\n",
      "train loss:0.0019270511973911136\n",
      "train loss:0.0006674465122006025\n",
      "train loss:0.001466772380309763\n",
      "train loss:0.0012887744316636251\n",
      "train loss:0.027998403750491573\n",
      "train loss:0.000655570697016822\n",
      "train loss:0.0019374201087342952\n",
      "train loss:0.0012941823280698595\n",
      "train loss:0.0022254901089363197\n",
      "train loss:0.00036923133012992543\n",
      "train loss:0.00736854448102607\n",
      "train loss:0.0010376809508289644\n",
      "train loss:0.00035953695650631126\n",
      "train loss:0.05124830988696538\n",
      "train loss:0.0005489940083177834\n",
      "train loss:0.00034793340479245675\n",
      "train loss:0.0033393649279912074\n",
      "train loss:0.007944070946917337\n",
      "train loss:0.000599581926261427\n",
      "train loss:0.00402742879547312\n",
      "train loss:0.005600490659908157\n",
      "train loss:0.0012800497101112176\n",
      "train loss:0.0029718207055613507\n",
      "train loss:0.0003070646203826481\n",
      "train loss:0.007320387065863575\n",
      "train loss:0.0020698162220209953\n",
      "train loss:0.0006393756765831234\n",
      "train loss:0.0018281801391973826\n",
      "train loss:0.0016844384361308262\n",
      "train loss:0.0024250551189175383\n",
      "train loss:0.005707703915518811\n",
      "train loss:0.0005842337980409156\n",
      "train loss:0.0033172641456099415\n",
      "train loss:0.002271315259159326\n",
      "train loss:0.0002126559217759922\n",
      "train loss:0.0002454244980676204\n",
      "train loss:0.002316585986007958\n",
      "train loss:0.013117278608207375\n",
      "train loss:0.0022705825007946485\n",
      "train loss:4.24132012487105e-05\n",
      "train loss:0.0012511051509389263\n",
      "train loss:0.002184097838956858\n",
      "train loss:0.0024845145959448527\n",
      "train loss:0.002053770316050846\n",
      "train loss:0.0022103898383408933\n",
      "train loss:0.009162720428456748\n",
      "train loss:0.003199428153195999\n",
      "train loss:0.0016720296803876692\n",
      "train loss:0.0027475324714486397\n",
      "train loss:0.0015595606417056736\n",
      "train loss:0.0009057424660581431\n",
      "train loss:0.003911811114004546\n",
      "train loss:0.003221776322394623\n",
      "train loss:0.0028864541115121863\n",
      "train loss:0.0012901733371166206\n",
      "train loss:0.004212154145901034\n",
      "train loss:0.002888075384773965\n",
      "train loss:0.00247131918276226\n",
      "train loss:0.0007018199264802902\n",
      "train loss:0.0019039728123908916\n",
      "train loss:0.0019821809592073265\n",
      "train loss:0.002909493906470301\n",
      "train loss:0.00015801418320532525\n",
      "train loss:0.0014608400217640495\n",
      "train loss:0.0001859420688411108\n",
      "train loss:0.006607668833071443\n",
      "train loss:0.0002557824182326777\n",
      "train loss:0.0014121099611013844\n",
      "train loss:0.0007517624143658048\n",
      "train loss:0.005066292727837613\n",
      "train loss:0.0003790477308733962\n",
      "train loss:0.002641076561243261\n",
      "train loss:0.0004778122344440017\n",
      "train loss:0.002031751195625977\n",
      "train loss:0.0013923656950252917\n",
      "train loss:0.002221449663700569\n",
      "train loss:0.0015356997851176833\n",
      "train loss:0.004208111641729643\n",
      "train loss:0.0005320571618599977\n",
      "train loss:0.008955233569329502\n",
      "train loss:0.0032434645518233485\n",
      "train loss:0.006957904976407156\n",
      "train loss:0.0008758314679844755\n",
      "train loss:0.00042224809196862007\n",
      "train loss:0.0005075045823137662\n",
      "train loss:0.006478456588230786\n",
      "train loss:0.011710906075363833\n",
      "train loss:0.007024163925982684\n",
      "train loss:0.0014812288872387794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0012245525554848663\n",
      "train loss:0.005598705194763054\n",
      "train loss:0.004395488074953315\n",
      "train loss:0.0008074551695342338\n",
      "train loss:0.0004680305145787839\n",
      "train loss:0.0008659073226659904\n",
      "train loss:0.0015969908553878434\n",
      "train loss:0.0011577351069220867\n",
      "train loss:0.0019070060183580578\n",
      "train loss:0.00026195833816363323\n",
      "train loss:0.0017383122963590114\n",
      "train loss:0.0002916078328111903\n",
      "train loss:0.0022924831019942135\n",
      "train loss:0.0008124197580413144\n",
      "train loss:0.0024771439474771525\n",
      "train loss:0.0008362714900142674\n",
      "train loss:1.954307270655159e-05\n",
      "train loss:0.002406277149891474\n",
      "train loss:0.003831266568462247\n",
      "train loss:0.0006395403374587762\n",
      "train loss:0.0005428941696061357\n",
      "train loss:0.01111246761673184\n",
      "train loss:0.005019886823446651\n",
      "train loss:0.00024095149517585447\n",
      "train loss:0.02331252013876307\n",
      "train loss:0.0002044033791699059\n",
      "train loss:0.006887938948822193\n",
      "train loss:0.00019210241275533985\n",
      "train loss:0.002162471497996623\n",
      "train loss:0.0006817863769512406\n",
      "train loss:0.005952584269631554\n",
      "train loss:0.001139497862322219\n",
      "train loss:0.0014018480901281508\n",
      "train loss:0.05421997820164937\n",
      "train loss:0.02237983800464205\n",
      "train loss:0.004175548708943801\n",
      "train loss:0.0015300720129502116\n",
      "train loss:0.0014497429758156843\n",
      "train loss:0.0012778011507251278\n",
      "train loss:0.00031557018196250923\n",
      "train loss:0.0012449544178334394\n",
      "train loss:0.0015694571044645581\n",
      "train loss:0.008938578604587027\n",
      "train loss:0.013999375837086249\n",
      "train loss:0.0021107754786945303\n",
      "train loss:0.0028877092324920747\n",
      "train loss:0.010349453978059088\n",
      "train loss:0.004753281956220211\n",
      "train loss:0.0030462863895826275\n",
      "train loss:0.0003718040696684095\n",
      "train loss:0.004611276603582005\n",
      "train loss:0.0014352047945689666\n",
      "train loss:0.0005026532419796913\n",
      "train loss:0.004352617547212542\n",
      "train loss:0.0009252205661832192\n",
      "train loss:0.0002894066383708727\n",
      "train loss:0.0029432121855751696\n",
      "train loss:0.006034103178351418\n",
      "train loss:0.007347441443179543\n",
      "train loss:0.0010588329252627607\n",
      "train loss:0.00023579163342874997\n",
      "train loss:0.0006024567353947692\n",
      "train loss:0.006820726266488899\n",
      "train loss:0.009257477774778535\n",
      "train loss:0.0059867025369031825\n",
      "train loss:0.001027051183232638\n",
      "train loss:0.0014463803263310196\n",
      "train loss:0.0027223560361315114\n",
      "train loss:0.0005499019201700321\n",
      "train loss:0.005125822853441649\n",
      "train loss:0.001320051117544025\n",
      "train loss:0.0010950701778121134\n",
      "train loss:0.0030544681152365875\n",
      "train loss:0.006071774602897805\n",
      "train loss:0.0006320258375273241\n",
      "train loss:0.0052628161495830205\n",
      "train loss:0.0015446778785117173\n",
      "train loss:0.004140172876104382\n",
      "train loss:0.00520205424254942\n",
      "train loss:0.0036401522373078073\n",
      "train loss:0.004490111803957127\n",
      "train loss:0.0002854980249065445\n",
      "train loss:5.4949985073583134e-05\n",
      "train loss:0.0435286158676504\n",
      "train loss:0.0007972922088789459\n",
      "train loss:0.00011267426047311137\n",
      "train loss:0.00536761188559462\n",
      "train loss:0.042002655932934424\n",
      "train loss:0.0006151689460749311\n",
      "train loss:0.010266687672373973\n",
      "train loss:0.0008402502894130004\n",
      "train loss:0.0014503706096161695\n",
      "train loss:0.051528379583617286\n",
      "train loss:0.00022552580103802\n",
      "train loss:0.0012405662404067037\n",
      "train loss:0.0043428604355830415\n",
      "train loss:0.0032395170927342785\n",
      "train loss:0.002074212824156606\n",
      "train loss:0.03919651853135009\n",
      "train loss:0.004328611267169162\n",
      "train loss:0.001534457698536528\n",
      "train loss:0.0049160114637433585\n",
      "train loss:0.002743971416277605\n",
      "train loss:0.0027798433232005797\n",
      "train loss:0.005626989069490919\n",
      "train loss:0.004786160471726994\n",
      "train loss:0.00327629101718575\n",
      "train loss:0.002162125306050247\n",
      "train loss:0.0037736381701634473\n",
      "train loss:0.003306983520890053\n",
      "train loss:0.0037079824642280807\n",
      "train loss:0.001191093068782738\n",
      "train loss:0.0009369478963406455\n",
      "train loss:0.0007201593058819021\n",
      "train loss:0.00013762233362995567\n",
      "train loss:0.007333751358295933\n",
      "train loss:0.0004072355969641821\n",
      "train loss:0.0015882653638036564\n",
      "train loss:0.012958792545122125\n",
      "train loss:0.004637004960945063\n",
      "train loss:0.0025687329691687437\n",
      "train loss:0.0020233374278891283\n",
      "train loss:0.009679372213121653\n",
      "train loss:0.0012753079168555037\n",
      "train loss:0.011711559123014623\n",
      "train loss:0.0005780181722376686\n",
      "train loss:0.0013056860036394146\n",
      "train loss:0.0008667888507652453\n",
      "train loss:0.0072771994743097935\n",
      "train loss:0.0006403207920763898\n",
      "train loss:0.0007787396139417457\n",
      "train loss:0.004119027673585786\n",
      "train loss:0.0003047155205828278\n",
      "train loss:0.003053307556886054\n",
      "train loss:0.0016560602172414043\n",
      "train loss:0.0104868500182919\n",
      "train loss:0.0044311247463867465\n",
      "train loss:0.0026081163924991254\n",
      "train loss:0.0003334306268005858\n",
      "train loss:0.00012162092646881443\n",
      "train loss:0.0005672030000193315\n",
      "train loss:0.0030180363325078403\n",
      "train loss:0.0002947922543454102\n",
      "train loss:0.006285952924527612\n",
      "train loss:0.002990622159469299\n",
      "train loss:0.0023013509169988768\n",
      "train loss:0.0006045737959163521\n",
      "train loss:7.126827087991184e-05\n",
      "train loss:0.0089764565403277\n",
      "train loss:0.0008219077014879291\n",
      "train loss:0.0015412261962390007\n",
      "train loss:0.0008852587167170308\n",
      "train loss:0.00570985715494583\n",
      "train loss:0.004075975696065777\n",
      "train loss:0.006165714027898794\n",
      "train loss:0.006754988171601607\n",
      "train loss:0.0003524587668150003\n",
      "train loss:0.003938291814726851\n",
      "train loss:0.0009552959734341828\n",
      "train loss:0.0033688916545576526\n",
      "train loss:0.000361899867590098\n",
      "train loss:0.003429047759866292\n",
      "train loss:0.0021474979114112142\n",
      "train loss:0.0022957727336337954\n",
      "train loss:0.006499302197731912\n",
      "train loss:0.0026794729701314114\n",
      "train loss:0.004562342390414661\n",
      "train loss:0.0028391832225962066\n",
      "train loss:0.006052742123974089\n",
      "train loss:0.0013263078427895364\n",
      "train loss:0.00515710799885192\n",
      "train loss:0.0009172213212146858\n",
      "train loss:0.0018180090887280355\n",
      "train loss:0.0015262729003759092\n",
      "train loss:0.010037341957084863\n",
      "train loss:0.0012959468428818247\n",
      "train loss:0.0037098270802750194\n",
      "train loss:0.0007286014118703061\n",
      "train loss:0.00016120126564039963\n",
      "train loss:0.004463678772384278\n",
      "train loss:0.003025037911830774\n",
      "train loss:0.0013506325906804393\n",
      "train loss:0.001582538692900652\n",
      "train loss:0.004525481022112716\n",
      "train loss:0.004814723231505892\n",
      "train loss:0.002648712496620473\n",
      "train loss:0.0037929797203705633\n",
      "train loss:0.0007069078542138607\n",
      "train loss:0.0021049686452429446\n",
      "train loss:0.0007129778728721759\n",
      "train loss:0.004614767848112125\n",
      "train loss:0.0005117190764629693\n",
      "train loss:0.0064028110095652345\n",
      "train loss:0.0015994237054418115\n",
      "train loss:0.0006559526660306793\n",
      "train loss:0.0034616272255459657\n",
      "train loss:0.003539500055780226\n",
      "train loss:0.00025716135493445596\n",
      "train loss:0.0054461525137887755\n",
      "train loss:0.00013478555536900554\n",
      "train loss:0.003026527642818367\n",
      "train loss:0.002825247227923887\n",
      "train loss:0.003632220188645105\n",
      "train loss:0.0006843256684338084\n",
      "train loss:0.00039891201604957456\n",
      "train loss:0.018197130959093973\n",
      "train loss:0.002806476026421265\n",
      "train loss:0.0008598444312097557\n",
      "train loss:0.00028584048981664876\n",
      "train loss:0.0020591363010495625\n",
      "train loss:0.00965991576640468\n",
      "train loss:0.0029478966519076023\n",
      "train loss:0.0028880169569306803\n",
      "train loss:0.0009055315930640048\n",
      "train loss:0.0002313557534402824\n",
      "train loss:0.002855665379600702\n",
      "train loss:0.00041785385268686804\n",
      "train loss:0.005489051136789166\n",
      "train loss:0.005301165206754398\n",
      "train loss:0.003724287728827765\n",
      "train loss:0.003738179145286418\n",
      "train loss:0.0016154577249844176\n",
      "train loss:0.010209222599828134\n",
      "train loss:0.0008674348653183504\n",
      "train loss:0.003454233020925708\n",
      "train loss:0.00031333733170065215\n",
      "train loss:0.003712474681436944\n",
      "train loss:0.0016402439625944807\n",
      "train loss:0.005540597422395849\n",
      "train loss:0.0016010748703620807\n",
      "train loss:0.0031641573638306573\n",
      "train loss:0.00045391064602724155\n",
      "train loss:0.0009921500472825405\n",
      "train loss:0.00024599485708850426\n",
      "train loss:0.000587891747604884\n",
      "train loss:0.0028144876536770826\n",
      "train loss:0.02412141654987722\n",
      "train loss:0.0044713805873597854\n",
      "train loss:0.0055266232982007215\n",
      "train loss:0.011175446056460982\n",
      "train loss:0.002288680176413349\n",
      "=== epoch:16, train acc:0.996, test acc:0.987 ===\n",
      "train loss:0.0003316277025832238\n",
      "train loss:0.0007727493941849259\n",
      "train loss:0.0008315332524812104\n",
      "train loss:0.0003459285597837952\n",
      "train loss:0.0002752392810509797\n",
      "train loss:0.0023627686243156646\n",
      "train loss:0.0043010502078616696\n",
      "train loss:0.0028130991493230117\n",
      "train loss:0.00016483586523224444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:5.204805348120246e-05\n",
      "train loss:0.0024195430334912013\n",
      "train loss:0.0011100849990535115\n",
      "train loss:0.0028800252010417714\n",
      "train loss:0.003568886179150946\n",
      "train loss:0.0022740805216234805\n",
      "train loss:0.0004578707979837705\n",
      "train loss:0.0007178397304699567\n",
      "train loss:0.0005901531414163441\n",
      "train loss:0.0007927660280174699\n",
      "train loss:0.010460053287925599\n",
      "train loss:0.0014783854928958472\n",
      "train loss:0.002315648034751376\n",
      "train loss:0.019144610354238388\n",
      "train loss:0.0018157161916979676\n",
      "train loss:0.008308871744722019\n",
      "train loss:0.0017703187913798895\n",
      "train loss:0.0009732484304464671\n",
      "train loss:0.00019618643793729328\n",
      "train loss:0.0007660753195246712\n",
      "train loss:0.0018083463084626395\n",
      "train loss:0.009344182683777562\n",
      "train loss:0.0005157542868973308\n",
      "train loss:0.021830167197639704\n",
      "train loss:0.0004051715256028798\n",
      "train loss:0.0026998982926946073\n",
      "train loss:0.0036757611449781953\n",
      "train loss:0.00606846895575465\n",
      "train loss:0.0006114766203967752\n",
      "train loss:0.000244418435255278\n",
      "train loss:0.0019318772746049605\n",
      "train loss:0.0007886050019210558\n",
      "train loss:0.0020657896364400115\n",
      "train loss:0.0015932000584888166\n",
      "train loss:0.002969994549457424\n",
      "train loss:0.004533449131307023\n",
      "train loss:0.0012280814804980886\n",
      "train loss:0.00032190620912653266\n",
      "train loss:0.002108369398309004\n",
      "train loss:0.0013581002280097737\n",
      "train loss:0.0015503830420498504\n",
      "train loss:0.0006496068694982107\n",
      "train loss:0.002194403066686802\n",
      "train loss:0.006346679995264979\n",
      "train loss:2.944425807672082e-05\n",
      "train loss:0.0025444412127528077\n",
      "train loss:0.0004165251206899027\n",
      "train loss:0.0008361373448269846\n",
      "train loss:9.485447728090802e-05\n",
      "train loss:0.0010694222634818988\n",
      "train loss:0.00039337373896393017\n",
      "train loss:0.002452872254733596\n",
      "train loss:0.0013885435412053684\n",
      "train loss:0.0022837946885864817\n",
      "train loss:0.0006237280925637319\n",
      "train loss:0.005182065551750976\n",
      "train loss:0.001293280710754784\n",
      "train loss:0.0006637670424294485\n",
      "train loss:0.0006222867875844124\n",
      "train loss:0.0013609315676716455\n",
      "train loss:0.0008072810177148929\n",
      "train loss:0.0020036508387241358\n",
      "train loss:0.002000256755224474\n",
      "train loss:0.0034413264906944345\n",
      "train loss:0.00544975967224394\n",
      "train loss:0.00021803273266882478\n",
      "train loss:0.0013480120210551088\n",
      "train loss:0.0019217871306868453\n",
      "train loss:0.002636617362901072\n",
      "train loss:0.00431719885184852\n",
      "train loss:0.001053010531012386\n",
      "train loss:0.0002944270330383437\n",
      "train loss:0.0019477022373583976\n",
      "train loss:0.004108572858425116\n",
      "train loss:0.004770226127738394\n",
      "train loss:0.0037493737877448063\n",
      "train loss:0.0014764165147069283\n",
      "train loss:0.0019875296004032064\n",
      "train loss:0.002271204018987099\n",
      "train loss:0.0020657125380483896\n",
      "train loss:0.002403042357107716\n",
      "train loss:0.002645551889132618\n",
      "train loss:0.00012240875354001466\n",
      "train loss:0.0004026603508076675\n",
      "train loss:0.0006698562296872706\n",
      "train loss:0.030310514718599517\n",
      "train loss:0.0007125676229488978\n",
      "train loss:0.009139065532290706\n",
      "train loss:0.0014986601945116034\n",
      "train loss:0.0033761043451341396\n",
      "train loss:0.0014590358139858423\n",
      "train loss:0.001832855239276682\n",
      "train loss:0.0029180287250349605\n",
      "train loss:0.00048150214945103897\n",
      "train loss:0.0018621050106587423\n",
      "train loss:0.0010973272888760346\n",
      "train loss:0.0015960166861196023\n",
      "train loss:0.0018173148370846388\n",
      "train loss:0.0014518190134257866\n",
      "train loss:0.0016863888869181668\n",
      "train loss:0.0007186198679232735\n",
      "train loss:0.00010874712569242192\n",
      "train loss:0.0010261599034273494\n",
      "train loss:0.0001360599281121886\n",
      "train loss:0.0016633370658652436\n",
      "train loss:0.002454895592006774\n",
      "train loss:9.906154413242322e-06\n",
      "train loss:0.0008630294434608882\n",
      "train loss:0.00243738661868124\n",
      "train loss:0.0012771393500962949\n",
      "train loss:3.287463086699655e-05\n",
      "train loss:0.0016094331379788243\n",
      "train loss:0.013848502276289722\n",
      "train loss:0.006235638700637274\n",
      "train loss:2.758497081973626e-05\n",
      "train loss:0.0003429113539772647\n",
      "train loss:0.0013433074303162962\n",
      "train loss:0.013585031285928296\n",
      "train loss:0.0010698095254275495\n",
      "train loss:0.0028923032314026372\n",
      "train loss:1.4348553934541374e-05\n",
      "train loss:0.0007684820102213718\n",
      "train loss:0.0006437007952930964\n",
      "train loss:0.0038279669559558747\n",
      "train loss:0.0003386369221889088\n",
      "train loss:0.03796806909426299\n",
      "train loss:0.001642223109132696\n",
      "train loss:6.314692737188578e-05\n",
      "train loss:0.006013387537206618\n",
      "train loss:0.003998440534421259\n",
      "train loss:0.002249022174847849\n",
      "train loss:0.0009561558857607245\n",
      "train loss:0.002334476832726314\n",
      "train loss:0.016338155579278587\n",
      "train loss:0.00045032913023756256\n",
      "train loss:0.0011980984902316802\n",
      "train loss:0.004992135944872913\n",
      "train loss:0.0011173508354138236\n",
      "train loss:0.00023170668762816736\n",
      "train loss:0.05019874650115867\n",
      "train loss:0.0035526313647331457\n",
      "train loss:0.0012057219474782466\n",
      "train loss:0.002992879212772876\n",
      "train loss:0.002737177893801867\n",
      "train loss:0.0019123714550714158\n",
      "train loss:0.003524406605654903\n",
      "train loss:0.0019741961564156358\n",
      "train loss:0.0005945201416249866\n",
      "train loss:0.0041536396801157074\n",
      "train loss:0.0016041013303404418\n",
      "train loss:0.0006685604125373426\n",
      "train loss:0.000726936261642825\n",
      "train loss:0.00048173142683690366\n",
      "train loss:0.0026249354517091433\n",
      "train loss:0.0016564678032575642\n",
      "train loss:0.008201626579142987\n",
      "train loss:0.001381496073238781\n",
      "train loss:9.311639931778096e-05\n",
      "train loss:0.005937209208264255\n",
      "train loss:0.001069596314897945\n",
      "train loss:0.0007403695100327731\n",
      "train loss:0.0003009634928133225\n",
      "train loss:0.0014667517114637881\n",
      "train loss:0.008813129741123513\n",
      "train loss:0.0005067509986539717\n",
      "train loss:0.00041896572371270954\n",
      "train loss:0.07891532131282603\n",
      "train loss:0.0007448154979834095\n",
      "train loss:0.0019406947824477736\n",
      "train loss:0.008211901626135984\n",
      "train loss:0.0003494575947732554\n",
      "train loss:0.006113244120968644\n",
      "train loss:0.0021645534971102484\n",
      "train loss:0.0020219704245913648\n",
      "train loss:0.001176543589097898\n",
      "train loss:0.003026318168841918\n",
      "train loss:0.0041928517748887305\n",
      "train loss:0.0014077030950311739\n",
      "train loss:0.004619925467230924\n",
      "train loss:0.004715111079273172\n",
      "train loss:0.0013589264856444295\n",
      "train loss:0.003522479131761535\n",
      "train loss:0.0020976546855152704\n",
      "train loss:0.0009408290493051797\n",
      "train loss:0.0018068290207491405\n",
      "train loss:0.00010902504377969543\n",
      "train loss:0.001830971381728208\n",
      "train loss:0.008744970261291117\n",
      "train loss:0.0011215269579411915\n",
      "train loss:0.0014658568401710857\n",
      "train loss:0.0012321692995499848\n",
      "train loss:0.0019341727503060968\n",
      "train loss:0.0009240417201610417\n",
      "train loss:0.00021608613774495332\n",
      "train loss:0.0010823694449275784\n",
      "train loss:0.0014561280802765988\n",
      "train loss:0.001429259538725226\n",
      "train loss:0.004818388693728596\n",
      "train loss:0.004636017377005244\n",
      "train loss:0.00037012559646991373\n",
      "train loss:0.012585219297410523\n",
      "train loss:0.0013665209197869428\n",
      "train loss:0.00016706790550141698\n",
      "train loss:0.0015361587527693591\n",
      "train loss:0.0012289526288455763\n",
      "train loss:0.001566723325744021\n",
      "train loss:0.004065382541766029\n",
      "train loss:0.0007502021693447588\n",
      "train loss:0.002283574679063712\n",
      "train loss:0.0008211590219287852\n",
      "train loss:0.0007950932261591367\n",
      "train loss:0.002092301802650128\n",
      "train loss:0.0018043806362771056\n",
      "train loss:0.0009372548133501909\n",
      "train loss:0.00047505181907057123\n",
      "train loss:0.000647260179906715\n",
      "train loss:0.0006075846022409546\n",
      "train loss:0.0005082342954108887\n",
      "train loss:0.0017289899800082922\n",
      "train loss:0.0005686309025570546\n",
      "train loss:0.002337709425690061\n",
      "train loss:0.04214198851738041\n",
      "train loss:0.0009856754152996307\n",
      "train loss:0.0015714800906590184\n",
      "train loss:0.00018571364930428962\n",
      "train loss:1.3608193654120081e-05\n",
      "train loss:0.001386045758239963\n",
      "train loss:0.0023671443486114403\n",
      "train loss:0.0018224608642969692\n",
      "train loss:0.00030518124755050924\n",
      "train loss:0.003130186471637585\n",
      "train loss:0.0015976544318924988\n",
      "train loss:0.004944366766660196\n",
      "train loss:0.00012020311663182803\n",
      "train loss:0.011284545510474581\n",
      "train loss:0.002948690056508258\n",
      "train loss:0.00046818331861783405\n",
      "train loss:0.0013962911508630979\n",
      "train loss:0.004253936152149387\n",
      "train loss:0.0017680914757467116\n",
      "train loss:0.00538849420180902\n",
      "train loss:0.0003526622940605107\n",
      "train loss:0.00035715608108831024\n",
      "train loss:0.003698708457284941\n",
      "train loss:0.00611817302108783\n",
      "train loss:0.00011565138717795043\n",
      "train loss:0.0003821126090173293\n",
      "train loss:0.0007568332208407572\n",
      "train loss:0.0014458296315810768\n",
      "train loss:0.0011580129977759848\n",
      "train loss:0.00011940088435663396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0009731534865819494\n",
      "train loss:0.0030245814358023926\n",
      "train loss:0.0014781635838685017\n",
      "train loss:0.0002504433561310286\n",
      "train loss:0.0013597145579410425\n",
      "train loss:0.006902752115193645\n",
      "train loss:0.000744201521292633\n",
      "train loss:0.01405296994753597\n",
      "train loss:0.0037250269658712924\n",
      "train loss:0.0004376012518441856\n",
      "train loss:0.002920262378393346\n",
      "train loss:0.00018788006581399151\n",
      "train loss:0.0014896754751578315\n",
      "train loss:0.00398254342034142\n",
      "train loss:0.0010542320877762648\n",
      "train loss:0.00024068959712079594\n",
      "train loss:0.005390758257833001\n",
      "train loss:0.0028601281411998252\n",
      "train loss:0.0008435911171675105\n",
      "train loss:0.0017387725122116103\n",
      "train loss:0.0005270656475496722\n",
      "train loss:0.00026681309976670723\n",
      "train loss:0.0011862284283556128\n",
      "train loss:0.0016060895588426987\n",
      "train loss:0.00015930889417690937\n",
      "train loss:0.009437057996980995\n",
      "train loss:0.0015921453604648326\n",
      "train loss:0.0018242854868733879\n",
      "train loss:0.00027875295898420726\n",
      "train loss:0.0008156368920091414\n",
      "train loss:0.0013049851340741667\n",
      "train loss:0.0009927908599340716\n",
      "train loss:0.0012520576494496434\n",
      "train loss:0.0065047928746854175\n",
      "train loss:0.0009046928581702692\n",
      "train loss:0.00016398767553481842\n",
      "train loss:0.00012806902033484782\n",
      "train loss:0.0005745018118219599\n",
      "train loss:0.0008698322545711971\n",
      "train loss:0.0051988794007633695\n",
      "train loss:0.0019005942826360457\n",
      "train loss:0.009525347713371523\n",
      "train loss:0.00047005684639473924\n",
      "train loss:0.0006612101549734982\n",
      "train loss:0.004426228451748719\n",
      "train loss:0.0001021349702099246\n",
      "train loss:0.0007925244128808973\n",
      "train loss:0.0029644140931210316\n",
      "train loss:0.0008433638978168205\n",
      "train loss:0.0009279625427274984\n",
      "train loss:0.0016907519557717088\n",
      "train loss:0.003762380830603844\n",
      "train loss:0.0019582566463591635\n",
      "train loss:0.008200636165926414\n",
      "train loss:0.001264862516652384\n",
      "train loss:7.818739239093584e-06\n",
      "train loss:0.0002006730889584386\n",
      "train loss:2.4721740595546568e-05\n",
      "train loss:0.0021256784126626573\n",
      "train loss:0.0004069375388825027\n",
      "train loss:0.0006680681887679653\n",
      "train loss:0.000285860995412917\n",
      "train loss:0.002408490848549885\n",
      "train loss:0.0017933165784085165\n",
      "train loss:0.00021793514139032807\n",
      "train loss:0.0028570016271250937\n",
      "train loss:0.001989458201936288\n",
      "train loss:0.0026431772310791545\n",
      "train loss:0.0024706316101465323\n",
      "train loss:0.02000539025783362\n",
      "train loss:0.005109693418967526\n",
      "train loss:0.00026520340641297607\n",
      "train loss:0.001023344145125293\n",
      "train loss:0.0010734227714294764\n",
      "train loss:0.0027491641352666934\n",
      "train loss:0.003755105490295938\n",
      "train loss:0.0013292739227657904\n",
      "train loss:0.00021991046518603328\n",
      "train loss:8.80694957821165e-05\n",
      "train loss:0.0007132600488499057\n",
      "train loss:0.005486298950847245\n",
      "train loss:0.0009501270600681696\n",
      "train loss:0.0037554242482014224\n",
      "train loss:0.005209226067842449\n",
      "train loss:0.0016603601397910436\n",
      "train loss:0.003413115267693318\n",
      "train loss:0.0006027921533127608\n",
      "train loss:6.0187028266535906e-05\n",
      "train loss:0.00045808428790554515\n",
      "train loss:0.005908601125454433\n",
      "train loss:0.004290815631860508\n",
      "train loss:0.000322650125261594\n",
      "train loss:0.00018602758909391448\n",
      "train loss:0.0025521651681336198\n",
      "train loss:0.0001683316697419647\n",
      "train loss:0.0018153030602942757\n",
      "train loss:0.00019147509490186242\n",
      "train loss:0.0028330591448640263\n",
      "train loss:0.009179105485982688\n",
      "train loss:0.00036634981281556504\n",
      "train loss:0.00038753031139731\n",
      "train loss:0.0014677048006922462\n",
      "train loss:0.00020963463538550448\n",
      "train loss:0.0018934847529225246\n",
      "train loss:0.00095727960444651\n",
      "train loss:0.0030525862556008572\n",
      "train loss:0.009156756337719084\n",
      "train loss:0.00010616753449302128\n",
      "train loss:0.0002894628421184754\n",
      "train loss:0.0009555415936050256\n",
      "train loss:0.0046292460591330275\n",
      "train loss:0.0005453408474353516\n",
      "train loss:0.008121238997373036\n",
      "train loss:0.00030217542307921474\n",
      "train loss:0.0012623296304121261\n",
      "train loss:0.0007951495637354689\n",
      "train loss:0.0005118866667575111\n",
      "train loss:0.04342842083794246\n",
      "train loss:0.002030141683763277\n",
      "train loss:0.0015773091203331205\n",
      "train loss:0.00014556748209981998\n",
      "train loss:0.0004496871913348036\n",
      "train loss:0.0006935080773280744\n",
      "train loss:0.0011065394061356485\n",
      "train loss:0.00101816308868437\n",
      "train loss:0.0008617197547507856\n",
      "train loss:5.7781923141147876e-05\n",
      "train loss:0.001431628513994943\n",
      "train loss:0.00428833739036188\n",
      "train loss:0.00581037537813099\n",
      "train loss:0.008045574389998218\n",
      "train loss:0.008976794341648008\n",
      "train loss:0.00031704870754910937\n",
      "train loss:0.0010211039200815253\n",
      "train loss:0.0038873114221687085\n",
      "train loss:0.002603549107691116\n",
      "train loss:0.0008164433552453505\n",
      "train loss:0.009101748340603686\n",
      "train loss:0.0029920092772981527\n",
      "train loss:0.00014603698453238929\n",
      "train loss:0.0014856720813260938\n",
      "train loss:0.009399881899710133\n",
      "train loss:0.003004249068765163\n",
      "train loss:0.003099521092474587\n",
      "train loss:0.002039226043994898\n",
      "train loss:0.00024185755345885202\n",
      "train loss:0.007123478417072918\n",
      "train loss:0.0007381704291369635\n",
      "train loss:0.0014935433006072795\n",
      "train loss:0.0012386103898338729\n",
      "train loss:0.0007566379774474702\n",
      "train loss:0.00016167892508207833\n",
      "train loss:0.000499173118191079\n",
      "train loss:0.003397046877802318\n",
      "train loss:0.0021523931591704783\n",
      "train loss:0.0019486947208657284\n",
      "train loss:0.0001174855120095745\n",
      "train loss:0.0015924053930933683\n",
      "train loss:0.0022413321337632483\n",
      "train loss:0.0014285275084223615\n",
      "train loss:0.0006575005501681109\n",
      "train loss:0.0005180108193402218\n",
      "train loss:0.0011162763798676075\n",
      "train loss:0.000977920867082257\n",
      "train loss:0.0007178458471578093\n",
      "train loss:0.0016542137088377668\n",
      "train loss:0.0026388626604677175\n",
      "train loss:0.00013572274451448661\n",
      "train loss:0.018018483430373985\n",
      "train loss:0.015903570659316964\n",
      "train loss:0.007789649727388164\n",
      "train loss:0.005149606706482402\n",
      "train loss:0.0215396004133505\n",
      "train loss:0.000100786029103654\n",
      "train loss:0.0004565125958969667\n",
      "train loss:0.0002872081310475797\n",
      "train loss:0.0014453769759273667\n",
      "train loss:0.0032372896642306943\n",
      "train loss:0.0016264903330077487\n",
      "train loss:0.0005375857231760457\n",
      "train loss:0.0012079064799453518\n",
      "train loss:0.0006260326654630393\n",
      "train loss:0.008550967333829157\n",
      "train loss:0.0021434530803421245\n",
      "train loss:0.010530392535065712\n",
      "train loss:2.3640748370465005e-05\n",
      "train loss:0.007564027185273665\n",
      "train loss:0.0001305099564649189\n",
      "train loss:0.0009132584253920509\n",
      "train loss:0.0020799227417737814\n",
      "train loss:0.00033921424942654475\n",
      "train loss:0.0016145246876584263\n",
      "train loss:0.0010090854012867784\n",
      "train loss:0.0005962413676025266\n",
      "train loss:0.0031619379498976376\n",
      "train loss:0.0019769047651162445\n",
      "train loss:0.0010275558388658837\n",
      "train loss:0.0007863084579898712\n",
      "train loss:0.01593613562951844\n",
      "train loss:0.0016834514670395703\n",
      "train loss:0.022297733015545485\n",
      "train loss:0.00019199309604504098\n",
      "train loss:0.0029150977470552607\n",
      "train loss:0.00103049474403976\n",
      "train loss:5.3012799883633416e-05\n",
      "train loss:0.0014191745672840648\n",
      "train loss:0.016079241354403594\n",
      "train loss:0.0007982176656423878\n",
      "train loss:0.004014209845210259\n",
      "train loss:0.000587220880200322\n",
      "train loss:0.002950915730303756\n",
      "train loss:0.00023920476622768278\n",
      "train loss:0.0013068374345583257\n",
      "train loss:0.005244832306126099\n",
      "train loss:0.0004513459519557955\n",
      "train loss:0.002154601386348549\n",
      "train loss:0.000304306043287054\n",
      "train loss:0.0027863590625912867\n",
      "train loss:0.0003338969107543771\n",
      "train loss:0.009978099943756605\n",
      "train loss:0.001977914001641958\n",
      "train loss:0.006409685129873968\n",
      "train loss:0.00046226269527305523\n",
      "train loss:0.006320688433198891\n",
      "train loss:0.004067456706812234\n",
      "train loss:0.00457828149083645\n",
      "train loss:0.0032252162242181093\n",
      "train loss:0.00038597789631839024\n",
      "train loss:0.0007954845623609375\n",
      "train loss:0.0020616278074940646\n",
      "train loss:0.005629012028266117\n",
      "train loss:0.001351179467282763\n",
      "train loss:0.000262368759694637\n",
      "train loss:0.0022891158739386072\n",
      "train loss:0.0014390043933429498\n",
      "train loss:0.0015903904762491084\n",
      "train loss:0.01160711031440197\n",
      "train loss:0.0008146458891722234\n",
      "train loss:0.0004408163489793658\n",
      "train loss:0.0014248923275313804\n",
      "train loss:0.006306642028525583\n",
      "train loss:0.00272494460559426\n",
      "train loss:0.001464921810116828\n",
      "train loss:0.00343994661316412\n",
      "train loss:0.003513111938081201\n",
      "train loss:0.0004577930020514673\n",
      "train loss:0.0014845367196417434\n",
      "train loss:8.183911481604911e-05\n",
      "train loss:0.00448144945645586\n",
      "train loss:0.0027607633567481972\n",
      "train loss:0.0008865878537734123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.00020908901389676723\n",
      "train loss:0.0005472159912608139\n",
      "train loss:0.00011758737440917812\n",
      "train loss:0.004700833611343005\n",
      "train loss:0.0006241871978870832\n",
      "train loss:0.0028045019340108505\n",
      "train loss:0.00022740368854568799\n",
      "train loss:0.00021061647632964712\n",
      "train loss:0.00198182186478527\n",
      "train loss:0.0018631659084706804\n",
      "train loss:5.104243391121019e-05\n",
      "train loss:0.002133109648088358\n",
      "train loss:0.00038233493558666897\n",
      "train loss:0.0015860282648089152\n",
      "train loss:0.0017995230333910067\n",
      "train loss:0.0011129492047506741\n",
      "train loss:0.00190957847281635\n",
      "train loss:0.00041806205567598603\n",
      "train loss:0.0005771797657431652\n",
      "train loss:0.00026811164164860694\n",
      "train loss:0.0013052048070712507\n",
      "train loss:0.0025014839616017874\n",
      "train loss:0.00026420524256850396\n",
      "train loss:0.0015141184956867775\n",
      "train loss:0.0005318818652626788\n",
      "train loss:0.005203083611545133\n",
      "train loss:0.0035343207720804444\n",
      "train loss:0.015514704461806073\n",
      "train loss:0.00325313071391677\n",
      "train loss:0.0005813332533873432\n",
      "train loss:0.0034561384945541963\n",
      "train loss:0.00021030540321200885\n",
      "train loss:0.0007485689206093506\n",
      "train loss:0.0032118943455168403\n",
      "train loss:0.003561721753394507\n",
      "train loss:0.004712821673540171\n",
      "train loss:0.00021713154675250584\n",
      "train loss:0.0008387525942066379\n",
      "train loss:0.005592616634762193\n",
      "train loss:0.0009759124485864124\n",
      "train loss:0.001429500308190417\n",
      "train loss:0.005654324995098777\n",
      "train loss:0.0015086214015782547\n",
      "train loss:0.0016696926241285865\n",
      "train loss:0.00012909613061834767\n",
      "train loss:0.0009516239431510323\n",
      "train loss:0.00051897591087193\n",
      "train loss:0.0017871634858281867\n",
      "train loss:0.0006919979188550108\n",
      "train loss:0.0020558358373933866\n",
      "train loss:0.0018169442929322338\n",
      "train loss:4.645532872224343e-05\n",
      "train loss:0.005553671144482822\n",
      "train loss:0.0005244617445645496\n",
      "train loss:0.00268073735588009\n",
      "train loss:0.0008701165727459634\n",
      "train loss:0.00278650929434357\n",
      "train loss:0.010178060873751414\n",
      "train loss:0.0017600748202639537\n",
      "train loss:0.003228048002504095\n",
      "train loss:0.0076996822318893465\n",
      "train loss:9.919374365755804e-05\n",
      "train loss:0.001888142618176695\n",
      "train loss:0.00017605303192002494\n",
      "train loss:0.001156649574649736\n",
      "train loss:0.0018926055747576038\n",
      "train loss:0.0007614892522041586\n",
      "train loss:0.0003557527877793231\n",
      "train loss:0.006128566630976474\n",
      "train loss:0.002686038558271007\n",
      "train loss:0.01610721532122716\n",
      "train loss:0.0028547868571570057\n",
      "train loss:0.0006590007976133862\n",
      "train loss:0.0016888671143442768\n",
      "train loss:0.0005584673560749854\n",
      "train loss:0.0006726305512546983\n",
      "train loss:0.0003919535932650102\n",
      "train loss:0.0019017612813534916\n",
      "train loss:0.0011523668369645649\n",
      "train loss:0.00012804918586898167\n",
      "train loss:0.007515018299085101\n",
      "train loss:0.005451979314379966\n",
      "train loss:0.0039599736130403315\n",
      "train loss:0.0007134100032433774\n",
      "train loss:0.00194115206151093\n",
      "train loss:0.0016414339698800785\n",
      "train loss:0.0018230367425036067\n",
      "train loss:0.0017488848408259011\n",
      "train loss:0.002569685095674227\n",
      "=== epoch:17, train acc:0.998, test acc:0.988 ===\n",
      "train loss:0.000532135851410477\n",
      "train loss:0.00016011300293352696\n",
      "train loss:0.0003839004011176594\n",
      "train loss:0.0009034125233011285\n",
      "train loss:0.001090014072943121\n",
      "train loss:0.00209869177599955\n",
      "train loss:0.0020744714816658292\n",
      "train loss:0.03763354761603664\n",
      "train loss:0.0037018977870401345\n",
      "train loss:0.00039548104928341875\n",
      "train loss:0.0008841851287553164\n",
      "train loss:0.004005991391495276\n",
      "train loss:0.006791130867197507\n",
      "train loss:0.001230984986173345\n",
      "train loss:0.001886575897394363\n",
      "train loss:0.0023319320104560784\n",
      "train loss:0.004236557885485461\n",
      "train loss:0.005550970142592782\n",
      "train loss:0.003419439992587879\n",
      "train loss:0.003547053527689663\n",
      "train loss:0.00579058076057905\n",
      "train loss:0.0028743275529669763\n",
      "train loss:0.0027496972361198174\n",
      "train loss:0.0049521065306122845\n",
      "train loss:0.0015206213335948234\n",
      "train loss:0.00942814823816953\n",
      "train loss:0.003949637674237974\n",
      "train loss:0.0026429461366789544\n",
      "train loss:0.002925887986687555\n",
      "train loss:0.002872350818537789\n",
      "train loss:0.0011541132925629095\n",
      "train loss:0.005924394926434632\n",
      "train loss:0.00682002354351079\n",
      "train loss:0.015009360581383436\n",
      "train loss:0.0018096987927381859\n",
      "train loss:0.0013184035739067703\n",
      "train loss:0.0002473362047249722\n",
      "train loss:0.0023068671079269046\n",
      "train loss:0.003702268625415761\n",
      "train loss:0.02451717514408973\n",
      "train loss:0.003002665881911287\n",
      "train loss:0.008473666943797633\n",
      "train loss:0.005738838989361199\n",
      "train loss:0.0023986571247426526\n",
      "train loss:0.0010668283777452252\n",
      "train loss:0.00020696367817913338\n",
      "train loss:0.0013425493752751698\n",
      "train loss:0.0009179465331609112\n",
      "train loss:0.007867740443013598\n",
      "train loss:0.0030891896325280876\n",
      "train loss:0.001449856399349091\n",
      "train loss:0.0014153713699940737\n",
      "train loss:0.00039576115102950933\n",
      "train loss:0.0008084710693279187\n",
      "train loss:0.0011532355269256258\n",
      "train loss:0.00018823399328345535\n",
      "train loss:0.0041739839121867186\n",
      "train loss:0.001786422787515654\n",
      "train loss:0.003720998182615692\n",
      "train loss:0.0008348623644017135\n",
      "train loss:0.0010027804053513228\n",
      "train loss:0.008063541172379752\n",
      "train loss:0.018190196371151317\n",
      "train loss:0.0009488756083412271\n",
      "train loss:0.0014206681805066623\n",
      "train loss:0.012497212974761931\n",
      "train loss:0.004769826709642389\n",
      "train loss:0.001412035828473088\n",
      "train loss:0.0007703744854079794\n",
      "train loss:0.002263356464481328\n",
      "train loss:0.0008841817886416978\n",
      "train loss:0.0016457852819439145\n",
      "train loss:0.005094269269770607\n",
      "train loss:0.0016169342040754937\n",
      "train loss:0.0005213432142395942\n",
      "train loss:0.001061847209602374\n",
      "train loss:0.00016346387389319165\n",
      "train loss:0.0007378662667818816\n",
      "train loss:0.0008838188969041145\n",
      "train loss:0.0016752742987547146\n",
      "train loss:0.0018261764173631387\n",
      "train loss:0.002837201183345836\n",
      "train loss:0.01160841638831298\n",
      "train loss:0.0023844918228039915\n",
      "train loss:0.00028835358950201413\n",
      "train loss:0.00042876096644593915\n",
      "train loss:0.005264288000484031\n",
      "train loss:0.00037709654158973605\n",
      "train loss:0.0030987458999294703\n",
      "train loss:0.0032539413851123923\n",
      "train loss:0.0011254077784548782\n",
      "train loss:0.0027331721665293713\n",
      "train loss:0.0013807403245816388\n",
      "train loss:0.0034709845076395978\n",
      "train loss:0.001987443580479712\n",
      "train loss:0.0050859531684372405\n",
      "train loss:0.00043126479335322155\n",
      "train loss:0.007949342094252253\n",
      "train loss:0.0032816714577296037\n",
      "train loss:0.0017153438975961823\n",
      "train loss:0.000435262511061566\n",
      "train loss:0.0014403007914099869\n",
      "train loss:0.01043532964239064\n",
      "train loss:8.082912986837393e-05\n",
      "train loss:0.0009066452587545516\n",
      "train loss:0.001210608429709467\n",
      "train loss:0.011487401703083615\n",
      "train loss:0.0017938945791246591\n",
      "train loss:0.0013706147924366857\n",
      "train loss:0.0016133378187484244\n",
      "train loss:0.0021211440597881607\n",
      "train loss:0.000693199376842491\n",
      "train loss:0.0067913144606022826\n",
      "train loss:0.0039044489188181626\n",
      "train loss:0.002880018294744931\n",
      "train loss:0.009229699081819835\n",
      "train loss:0.0629269517454702\n",
      "train loss:0.003293643007939021\n",
      "train loss:0.0012578489879814393\n",
      "train loss:0.003593339432409638\n",
      "train loss:0.0004524796851041709\n",
      "train loss:0.00031682526408265734\n",
      "train loss:0.0016922776081328738\n",
      "train loss:0.0006231989446444027\n",
      "train loss:0.0009833108450025242\n",
      "train loss:0.005510546167547921\n",
      "train loss:0.0006135416964057873\n",
      "train loss:0.0006574228477355183\n",
      "train loss:0.0076625640968818074\n",
      "train loss:0.006265349466028986\n",
      "train loss:0.0014880478253308479\n",
      "train loss:0.0022706893438296754\n",
      "train loss:0.0010051226065930525\n",
      "train loss:0.0030223887065377354\n",
      "train loss:0.0013330818162647334\n",
      "train loss:0.002300237167774705\n",
      "train loss:0.0024542527917568594\n",
      "train loss:0.0015882559748174587\n",
      "train loss:0.0031093829778090967\n",
      "train loss:0.0022003669876108855\n",
      "train loss:0.0019536831668500753\n",
      "train loss:0.0019409540639431563\n",
      "train loss:0.0011228254940443077\n",
      "train loss:0.0023644855527895194\n",
      "train loss:0.0025525046161289666\n",
      "train loss:0.0017167075621450814\n",
      "train loss:0.0009586582545372109\n",
      "train loss:0.0035392180770069865\n",
      "train loss:0.00028806871365052425\n",
      "train loss:0.003930831817245967\n",
      "train loss:0.001972279943851849\n",
      "train loss:0.0017330555234330947\n",
      "train loss:0.00048044377528410823\n",
      "train loss:0.003277144026336145\n",
      "train loss:0.0006613702044606815\n",
      "train loss:0.00035009364006414204\n",
      "train loss:0.00028454188468860376\n",
      "train loss:0.0020523546204869515\n",
      "train loss:0.00042208321963486495\n",
      "train loss:0.0007557742581130705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0015550029193675083\n",
      "train loss:0.038498042604064316\n",
      "train loss:0.002462603813321431\n",
      "train loss:0.00016474195438460023\n",
      "train loss:0.0003643145250123548\n",
      "train loss:0.0014809082846351657\n",
      "train loss:0.003045064484115222\n",
      "train loss:0.0013406312892199881\n",
      "train loss:0.0006228423806448708\n",
      "train loss:0.0007446581772835914\n",
      "train loss:0.005124182841178726\n",
      "train loss:0.005124168851969876\n",
      "train loss:0.0007532686738867746\n",
      "train loss:0.000726493187462767\n",
      "train loss:0.006258631689939877\n",
      "train loss:3.2971332213162084e-05\n",
      "train loss:0.004620534990727804\n",
      "train loss:0.0010773297071776883\n",
      "train loss:0.0001739318725137255\n",
      "train loss:0.0020404563866759378\n",
      "train loss:0.004291094412478689\n",
      "train loss:4.345176698464752e-05\n",
      "train loss:0.0010058632422908084\n",
      "train loss:0.0034132384732176073\n",
      "train loss:0.00021096533553880656\n",
      "train loss:0.012104540475327753\n",
      "train loss:0.002910998471011617\n",
      "train loss:0.001454781056579394\n",
      "train loss:0.00027535362162415503\n",
      "train loss:0.0015273248829938614\n",
      "train loss:0.004709579556676503\n",
      "train loss:0.0021311585103953585\n",
      "train loss:0.007321300203547548\n",
      "train loss:0.0012507316421703398\n",
      "train loss:0.00452338961961426\n",
      "train loss:0.00015586251350937607\n",
      "train loss:0.0013084886495974765\n",
      "train loss:0.0012644741704698661\n",
      "train loss:0.003150346057983953\n",
      "train loss:0.00021037213119600814\n",
      "train loss:0.0006454839252126837\n",
      "train loss:0.003642925404300402\n",
      "train loss:0.0006503465960245473\n",
      "train loss:0.007583490281647769\n",
      "train loss:0.0018902470373237449\n",
      "train loss:0.002281464804205234\n",
      "train loss:0.004137999089824306\n",
      "train loss:0.003463628547129757\n",
      "train loss:0.00040004677741707645\n",
      "train loss:0.0023616180953428974\n",
      "train loss:0.0016658534386810404\n",
      "train loss:0.0015938554862990578\n",
      "train loss:0.005498401140126057\n",
      "train loss:0.004262733544998649\n",
      "train loss:0.007506202382359137\n",
      "train loss:0.005298686354768922\n",
      "train loss:0.0018262700429920256\n",
      "train loss:0.001414085432814802\n",
      "train loss:0.0027926283601967593\n",
      "train loss:0.025027634309990304\n",
      "train loss:0.0006008600081884085\n",
      "train loss:0.002521849110107146\n",
      "train loss:0.002019424450287611\n",
      "train loss:0.0006130820237115837\n",
      "train loss:0.00040761112255599275\n",
      "train loss:0.005274495947053044\n",
      "train loss:0.0018731492708524355\n",
      "train loss:0.005284857909104545\n",
      "train loss:0.00019184192180530946\n",
      "train loss:0.0009829061554828668\n",
      "train loss:0.0019704241028274437\n",
      "train loss:7.87261071651216e-05\n",
      "train loss:0.002597351577478938\n",
      "train loss:0.0027671277425971026\n",
      "train loss:0.0029599702244578955\n",
      "train loss:0.004366250491632183\n",
      "train loss:0.0010148390248708831\n",
      "train loss:0.0005675845035147732\n",
      "train loss:0.0833849476944752\n",
      "train loss:0.0033354558274056446\n",
      "train loss:0.0020186090641412338\n",
      "train loss:0.0006187861092875122\n",
      "train loss:0.0017540167999938845\n",
      "train loss:0.003962755187597372\n",
      "train loss:0.0017925629712270902\n",
      "train loss:0.00037506775329553015\n",
      "train loss:0.0005618181660379782\n",
      "train loss:0.00023016214108640483\n",
      "train loss:0.006528761019060383\n",
      "train loss:0.0020879759277927564\n",
      "train loss:0.0002164101527877709\n",
      "train loss:0.0008820967426902585\n",
      "train loss:0.006436848074013635\n",
      "train loss:0.00018350938451933076\n",
      "train loss:0.0012372731714377244\n",
      "train loss:0.0006386774866252805\n",
      "train loss:0.0006431821562157707\n",
      "train loss:0.0020353741345093958\n",
      "train loss:0.0021450287189510283\n",
      "train loss:0.0006103323931768649\n",
      "train loss:0.0009759353725932758\n",
      "train loss:0.000651031208643467\n",
      "train loss:0.008447101445020202\n",
      "train loss:0.0009086746777061609\n",
      "train loss:0.0003678600217981125\n",
      "train loss:0.0018725943202772253\n",
      "train loss:0.0004652178362042852\n",
      "train loss:0.00033027272380689696\n",
      "train loss:0.05310928946721097\n",
      "train loss:0.007427619801392841\n",
      "train loss:0.0019297551484521618\n",
      "train loss:0.015067089184901817\n",
      "train loss:0.0034150283168651097\n",
      "train loss:0.003135822808183897\n",
      "train loss:0.0038884954017747242\n",
      "train loss:0.0003272050307473315\n",
      "train loss:0.00780866675797985\n",
      "train loss:0.0007395195741169012\n",
      "train loss:0.00565979253736274\n",
      "train loss:0.00508718663186792\n",
      "train loss:0.005350839607454147\n",
      "train loss:0.0007324569805773604\n",
      "train loss:0.002112801155187981\n",
      "train loss:0.0007463089421903272\n",
      "train loss:0.0027190498662896497\n",
      "train loss:0.00035025183689231445\n",
      "train loss:0.002697324857972105\n",
      "train loss:0.00012688542591647296\n",
      "train loss:0.005760446075503042\n",
      "train loss:0.0015978464372026202\n",
      "train loss:0.011996900263716258\n",
      "train loss:0.003002213757709805\n",
      "train loss:0.006761493816378033\n",
      "train loss:0.0021927149443152346\n",
      "train loss:0.0014364332330368086\n",
      "train loss:0.005778680540642455\n",
      "train loss:0.03230373139919538\n",
      "train loss:0.0032160229094067567\n",
      "train loss:0.001439929771830271\n",
      "train loss:0.000333143399134243\n",
      "train loss:0.0007354715894139195\n",
      "train loss:0.0008091853558212507\n",
      "train loss:0.00036554865683631076\n",
      "train loss:7.065712327148141e-05\n",
      "train loss:0.0026226123261388345\n",
      "train loss:0.00015384961123899798\n",
      "train loss:0.0075240571902116145\n",
      "train loss:0.0073200113961849785\n",
      "train loss:0.002350523349983519\n",
      "train loss:0.0001040802152505441\n",
      "train loss:0.0011877351201649202\n",
      "train loss:0.0013103357495003786\n",
      "train loss:0.0031887467207090824\n",
      "train loss:0.0015378108104652701\n",
      "train loss:0.00017762095290707223\n",
      "train loss:0.001391372353375448\n",
      "train loss:0.003211445220901735\n",
      "train loss:0.0006409162272903139\n",
      "train loss:0.02272215356792717\n",
      "train loss:0.0009803820349036348\n",
      "train loss:0.0001198607797778601\n",
      "train loss:0.00017140132577324403\n",
      "train loss:0.005035114821994194\n",
      "train loss:0.015358326102412882\n",
      "train loss:0.003257540709654128\n",
      "train loss:0.00010366390558499438\n",
      "train loss:0.003966101107881472\n",
      "train loss:0.000407759159750485\n",
      "train loss:0.0014811423843905263\n",
      "train loss:0.0011106097456731045\n",
      "train loss:0.002376015386417477\n",
      "train loss:0.007149298215416714\n",
      "train loss:0.009798181219941873\n",
      "train loss:0.005919022581182542\n",
      "train loss:0.015011622715920648\n",
      "train loss:0.0004144201812526344\n",
      "train loss:0.00018378628254203373\n",
      "train loss:0.0014138722259349921\n",
      "train loss:0.005726300306571905\n",
      "train loss:0.001780494605916517\n",
      "train loss:0.0030209198159546306\n",
      "train loss:0.005160080326021959\n",
      "train loss:0.001978353599697947\n",
      "train loss:0.0016778937173268537\n",
      "train loss:7.288694220744505e-05\n",
      "train loss:0.00020811804348368047\n",
      "train loss:0.014084575394038647\n",
      "train loss:0.00217040604035121\n",
      "train loss:0.0044643330076676625\n",
      "train loss:0.0010192982369113576\n",
      "train loss:0.00023386619997714715\n",
      "train loss:0.006298908380280966\n",
      "train loss:0.0045022031020615005\n",
      "train loss:0.0014316026362157655\n",
      "train loss:0.0006934436475902566\n",
      "train loss:0.002499981781206945\n",
      "train loss:0.0027585167235196057\n",
      "train loss:0.003921372536653772\n",
      "train loss:0.00017951362335944874\n",
      "train loss:0.002004316065729641\n",
      "train loss:0.0010997888762509858\n",
      "train loss:0.0025021399616673074\n",
      "train loss:0.00020596241865554168\n",
      "train loss:0.014406866310441786\n",
      "train loss:0.000569542562208872\n",
      "train loss:0.0017545859310769386\n",
      "train loss:0.00013018991293526036\n",
      "train loss:0.00042314482446666445\n",
      "train loss:0.0001278152621256694\n",
      "train loss:0.00041995435179085805\n",
      "train loss:0.0008657520767017937\n",
      "train loss:0.00378951227230207\n",
      "train loss:0.00013790656017052466\n",
      "train loss:0.00011522856675073752\n",
      "train loss:0.0002789081263255132\n",
      "train loss:0.0008262300577753895\n",
      "train loss:0.000529848056790176\n",
      "train loss:0.0006859490351455911\n",
      "train loss:0.0030391724153532485\n",
      "train loss:0.0005217727942227689\n",
      "train loss:0.0005594908720106799\n",
      "train loss:0.0001465422240639165\n",
      "train loss:0.0003647238781920397\n",
      "train loss:0.0011800338277821953\n",
      "train loss:0.0010574210063818556\n",
      "train loss:0.0014466263806744606\n",
      "train loss:0.0026335957006081477\n",
      "train loss:0.0008938775814092616\n",
      "train loss:0.0029803597327199803\n",
      "train loss:0.015799438356930484\n",
      "train loss:0.00415854952056392\n",
      "train loss:0.0008210829557362349\n",
      "train loss:0.0023179771970675144\n",
      "train loss:0.0002293477870219131\n",
      "train loss:0.001586000027220942\n",
      "train loss:0.0007936076063430812\n",
      "train loss:0.0014838569515656253\n",
      "train loss:0.000282079844952068\n",
      "train loss:0.0019054540031579721\n",
      "train loss:0.005585668021287088\n",
      "train loss:0.00018298830967191195\n",
      "train loss:0.009391657359634662\n",
      "train loss:0.003915247074628611\n",
      "train loss:0.0003229351471998824\n",
      "train loss:0.0018757991704240642\n",
      "train loss:0.0011615955456149843\n",
      "train loss:0.0018657658543546488\n",
      "train loss:0.00011415148067243939\n",
      "train loss:0.003603296729608907\n",
      "train loss:0.002901063064852955\n",
      "train loss:0.008011418505167638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0003978876640688858\n",
      "train loss:8.862684830079846e-05\n",
      "train loss:0.0013767928212465557\n",
      "train loss:0.0020038132201988987\n",
      "train loss:0.0013436252531406282\n",
      "train loss:0.00043382589651475516\n",
      "train loss:0.0011138754399372607\n",
      "train loss:0.01498137250979305\n",
      "train loss:0.001862010717249862\n",
      "train loss:0.014900818928479833\n",
      "train loss:0.005779756969709247\n",
      "train loss:0.002093883272964522\n",
      "train loss:0.0011407437355233456\n",
      "train loss:0.0011686628619791855\n",
      "train loss:0.007559125282439019\n",
      "train loss:0.015504272518092886\n",
      "train loss:0.00012600819443697473\n",
      "train loss:0.004154387974983041\n",
      "train loss:0.002242004252785893\n",
      "train loss:0.0007995211689128443\n",
      "train loss:0.0017844755171618034\n",
      "train loss:5.665140254180738e-05\n",
      "train loss:0.008773308627224584\n",
      "train loss:0.0002074680614639829\n",
      "train loss:0.0002994352725821169\n",
      "train loss:0.00023829835907038184\n",
      "train loss:0.005661544752025852\n",
      "train loss:0.0013683098449791663\n",
      "train loss:0.010846620001116948\n",
      "train loss:0.008977835303226888\n",
      "train loss:0.001041614062024276\n",
      "train loss:0.00031885580772503247\n",
      "train loss:0.003583412650966227\n",
      "train loss:0.004011858255299699\n",
      "train loss:0.000563723323332681\n",
      "train loss:0.0010468965195970402\n",
      "train loss:0.0025458426354039365\n",
      "train loss:0.001935601370683105\n",
      "train loss:4.302728105032997e-05\n",
      "train loss:0.004735252948880656\n",
      "train loss:0.0016821203781296624\n",
      "train loss:0.00012540933148956707\n",
      "train loss:0.00018658939502588792\n",
      "train loss:0.00668932785187936\n",
      "train loss:0.0036147106132490407\n",
      "train loss:0.0008614348912932002\n",
      "train loss:0.0027320649480067414\n",
      "train loss:0.0024541188507453157\n",
      "train loss:0.0015295262248506713\n",
      "train loss:0.011800012447068598\n",
      "train loss:0.0016241623052362347\n",
      "train loss:0.0014686734503973153\n",
      "train loss:0.002163178703124631\n",
      "train loss:0.0006941040248884902\n",
      "train loss:0.00029542419707997416\n",
      "train loss:0.0007093736561994722\n",
      "train loss:0.00030351115455086897\n",
      "train loss:0.0006154912939576446\n",
      "train loss:0.00040397326148999715\n",
      "train loss:0.0019801915220965614\n",
      "train loss:0.0005334604673570648\n",
      "train loss:0.0009946436155168924\n",
      "train loss:0.000587806409296451\n",
      "train loss:0.005993426550633187\n",
      "train loss:0.00020963341559693607\n",
      "train loss:0.0029085280781908653\n",
      "train loss:0.001779561664071089\n",
      "train loss:0.00030105326325263645\n",
      "train loss:0.00011705177709822286\n",
      "train loss:0.00754370863501716\n",
      "train loss:0.0019410971630382694\n",
      "train loss:0.0006601465585578438\n",
      "train loss:0.016579319645047153\n",
      "train loss:0.00030435931822473197\n",
      "train loss:0.00010145043930157958\n",
      "train loss:0.0015966171828065605\n",
      "train loss:0.0005009240989736315\n",
      "train loss:0.0012081629558344052\n",
      "train loss:0.0024486012958745947\n",
      "train loss:0.0005478386282367479\n",
      "train loss:0.005575572520917373\n",
      "train loss:0.00040853089706678787\n",
      "train loss:0.00027273916768831\n",
      "train loss:0.004688800892216685\n",
      "train loss:0.001493357928190015\n",
      "train loss:0.000999793907232691\n",
      "train loss:0.0033855714696749907\n",
      "train loss:0.0010113698170761088\n",
      "train loss:0.00443823984612632\n",
      "train loss:0.0022499446048617807\n",
      "train loss:0.0347525541177459\n",
      "train loss:0.0002616914324976526\n",
      "train loss:0.0018415167414539148\n",
      "train loss:0.03484531828614345\n",
      "train loss:0.00023000086006140256\n",
      "train loss:0.0014166257818507095\n",
      "train loss:0.0019121218111658245\n",
      "train loss:0.0007811721724688018\n",
      "train loss:0.001087597504994453\n",
      "train loss:0.001987944056099516\n",
      "train loss:0.001185534986781169\n",
      "train loss:0.00021009801792243072\n",
      "train loss:0.0023398147239265224\n",
      "train loss:0.0022877421456634324\n",
      "train loss:0.002221814523106317\n",
      "train loss:0.0007991138138603687\n",
      "train loss:0.001352553896185302\n",
      "train loss:0.00018465809075617895\n",
      "train loss:0.005183533147208427\n",
      "train loss:0.0024708068521002245\n",
      "train loss:0.002964044208106178\n",
      "train loss:0.0014657206933074455\n",
      "train loss:0.04690432639860328\n",
      "train loss:0.0007199922233671484\n",
      "train loss:0.0004773373644381976\n",
      "train loss:0.0001796354205832421\n",
      "train loss:0.0011452704635997605\n",
      "train loss:0.0014504728111971168\n",
      "train loss:0.01811521920120402\n",
      "train loss:0.0008988826556883008\n",
      "train loss:0.007092812762929255\n",
      "train loss:0.0006511079758807495\n",
      "train loss:0.0023984363993267465\n",
      "train loss:0.0006738437398094107\n",
      "train loss:0.0010920480688951205\n",
      "train loss:0.001932006118703236\n",
      "train loss:0.0002653709401871428\n",
      "train loss:0.006513291452715918\n",
      "train loss:0.022958126416180332\n",
      "train loss:0.002423365174780243\n",
      "train loss:0.0004372933047210282\n",
      "train loss:0.0020255716349074\n",
      "train loss:0.00027679637271124094\n",
      "train loss:0.0014384808493727297\n",
      "train loss:0.007549285258953761\n",
      "train loss:0.0009526588227927657\n",
      "train loss:0.0010294484039756573\n",
      "train loss:0.000627953141926828\n",
      "train loss:0.0013677306478474362\n",
      "train loss:0.0036297882444446877\n",
      "train loss:0.011508326684916912\n",
      "train loss:0.001020238428341238\n",
      "train loss:0.0021312078702333823\n",
      "train loss:0.01553886407396809\n",
      "train loss:0.0028685409587718507\n",
      "train loss:0.023619220005863636\n",
      "train loss:0.023275431835056367\n",
      "train loss:0.0005547808608809601\n",
      "train loss:0.0006055547316532981\n",
      "train loss:0.007610803605465823\n",
      "train loss:0.0051091385466276165\n",
      "train loss:0.0026755343666959126\n",
      "train loss:0.009919573959340395\n",
      "train loss:0.0060168572540041446\n",
      "train loss:0.0027518977682767253\n",
      "train loss:0.0011889635899934616\n",
      "train loss:0.0208154868409511\n",
      "train loss:0.0005789366721998769\n",
      "train loss:0.001374258359918071\n",
      "train loss:0.0030829408933990064\n",
      "train loss:0.0021623773341852455\n",
      "train loss:0.010240120063041949\n",
      "train loss:0.002329907722259902\n",
      "train loss:0.002181571470633089\n",
      "train loss:0.00011827095872691949\n",
      "train loss:0.003543197820458748\n",
      "train loss:0.001359991580879632\n",
      "train loss:0.005127559961037103\n",
      "train loss:0.0009428906257444838\n",
      "train loss:0.0012329443268234033\n",
      "train loss:0.0008082777975052372\n",
      "train loss:0.0011811578506362394\n",
      "train loss:0.0016124760892049083\n",
      "train loss:0.0016088919386216666\n",
      "train loss:0.005722483665239385\n",
      "train loss:0.01798520592152222\n",
      "train loss:0.0002978641709864938\n",
      "train loss:0.001700871806671173\n",
      "train loss:0.0006129321392556774\n",
      "train loss:0.0034711369573638936\n",
      "train loss:0.0030784393298113003\n",
      "train loss:0.0016072802301516094\n",
      "train loss:0.0001799788744498372\n",
      "train loss:0.0023858475727322696\n",
      "train loss:0.0011962629390176215\n",
      "train loss:0.0004449159602323981\n",
      "train loss:0.0007427310859615837\n",
      "train loss:0.004427613983616711\n",
      "train loss:0.0027019368769301555\n",
      "=== epoch:18, train acc:0.995, test acc:0.986 ===\n",
      "train loss:0.0025494614255334737\n",
      "train loss:0.0014730352023577251\n",
      "train loss:0.004572103435478251\n",
      "train loss:0.0021678822698006934\n",
      "train loss:0.00314912046060296\n",
      "train loss:0.006557166674962458\n",
      "train loss:0.0006158805265987112\n",
      "train loss:0.006456145550416315\n",
      "train loss:0.00033664385210667425\n",
      "train loss:0.0012271938174673663\n",
      "train loss:0.005597701076663516\n",
      "train loss:0.00024209509426739224\n",
      "train loss:0.0006790311558867574\n",
      "train loss:0.006590724852503127\n",
      "train loss:0.0008487461649944854\n",
      "train loss:0.014585736620859933\n",
      "train loss:0.000403824949102606\n",
      "train loss:0.0015484263702746056\n",
      "train loss:0.0015065375123133953\n",
      "train loss:0.0009532919133894607\n",
      "train loss:0.0005347631844705241\n",
      "train loss:0.007362192979502505\n",
      "train loss:0.00011123774639573999\n",
      "train loss:0.0015239839849156079\n",
      "train loss:0.0022497615215336664\n",
      "train loss:0.000470668988339773\n",
      "train loss:0.00480947993760294\n",
      "train loss:0.00343007402881803\n",
      "train loss:0.008188983524840078\n",
      "train loss:0.0443427404698804\n",
      "train loss:0.005315902245454298\n",
      "train loss:0.00047062156405981546\n",
      "train loss:0.004046280147688745\n",
      "train loss:0.000214061714843875\n",
      "train loss:0.00017540265892678687\n",
      "train loss:0.003398700990098436\n",
      "train loss:0.0016705062018401123\n",
      "train loss:0.0076864819999996085\n",
      "train loss:0.0034277363165092817\n",
      "train loss:0.0029519028906885535\n",
      "train loss:0.0038977142839088613\n",
      "train loss:0.0014668388882457264\n",
      "train loss:0.00035536308724500384\n",
      "train loss:0.015720539138242794\n",
      "train loss:0.0022579028707870683\n",
      "train loss:0.0019231476279701603\n",
      "train loss:0.00019380026272337143\n",
      "train loss:4.4663799489250406e-05\n",
      "train loss:0.0013487156831241314\n",
      "train loss:0.0006741739684457596\n",
      "train loss:0.000560015226109005\n",
      "train loss:0.0004654013862421885\n",
      "train loss:0.0004880677141026981\n",
      "train loss:0.0007376005173119356\n",
      "train loss:0.003500504597979508\n",
      "train loss:0.0016786788511053891\n",
      "train loss:0.0007296447329755276\n",
      "train loss:0.007160901582539831\n",
      "train loss:0.0005774590719522494\n",
      "train loss:0.0002478669119137048\n",
      "train loss:0.017999712329729682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0031339912229893085\n",
      "train loss:0.002755464380739461\n",
      "train loss:0.005506668899663141\n",
      "train loss:0.0003877918636906085\n",
      "train loss:0.007967805681630804\n",
      "train loss:0.002147075192765242\n",
      "train loss:0.001371017223310624\n",
      "train loss:0.0013190989964991916\n",
      "train loss:7.143003815394846e-05\n",
      "train loss:0.0015870676345928977\n",
      "train loss:0.0010228646472321896\n",
      "train loss:0.0030305432448965385\n",
      "train loss:0.00033794823851755263\n",
      "train loss:0.009356439768191066\n",
      "train loss:0.0002981320854774782\n",
      "train loss:0.012447396207567452\n",
      "train loss:0.001959858138618057\n",
      "train loss:0.0009279087414990876\n",
      "train loss:0.0009238347239255752\n",
      "train loss:0.0004218644111393188\n",
      "train loss:0.0016542854128092643\n",
      "train loss:0.0038659817495446647\n",
      "train loss:0.0003894771214009152\n",
      "train loss:0.002645874230813046\n",
      "train loss:0.005999738536899536\n",
      "train loss:0.0013283531074693693\n",
      "train loss:0.00045969962497630345\n",
      "train loss:0.0006126440898545839\n",
      "train loss:0.002257708844214364\n",
      "train loss:0.0012408066376056912\n",
      "train loss:0.0004764760772470135\n",
      "train loss:0.016175577998257657\n",
      "train loss:0.004948671667419772\n",
      "train loss:0.000646430386650425\n",
      "train loss:0.00037953472106721836\n",
      "train loss:0.004075472565118452\n",
      "train loss:0.0007204879808800485\n",
      "train loss:0.002982180486053303\n",
      "train loss:0.0015809317528464061\n",
      "train loss:2.7385412747359165e-05\n",
      "train loss:0.000771414543658976\n",
      "train loss:0.0019472832017537712\n",
      "train loss:0.0004350499614358281\n",
      "train loss:5.580949181121713e-05\n",
      "train loss:0.00237301553183616\n",
      "train loss:6.562627895404057e-05\n",
      "train loss:0.0021172575278560093\n",
      "train loss:0.0006072317477510749\n",
      "train loss:0.002654578084385815\n",
      "train loss:0.0001960142736165141\n",
      "train loss:0.0013302576093721346\n",
      "train loss:0.0016067374751599136\n",
      "train loss:0.0018277700619477272\n",
      "train loss:0.00011322767824963279\n",
      "train loss:0.0003885330123321503\n",
      "train loss:0.005074726218919468\n",
      "train loss:0.0033635957308126778\n",
      "train loss:0.0006215785887880155\n",
      "train loss:0.00044948283766613336\n",
      "train loss:0.0006638306553206183\n",
      "train loss:0.0006830489605963602\n",
      "train loss:0.0001423205531015679\n",
      "train loss:0.0016287144668702667\n",
      "train loss:0.0007299123853133901\n",
      "train loss:7.93421710162145e-05\n",
      "train loss:0.0005558288102121803\n",
      "train loss:0.0007265460792709323\n",
      "train loss:3.6962526426659394e-05\n",
      "train loss:0.0012272448154047905\n",
      "train loss:0.0005497556372618023\n",
      "train loss:0.001972147406625467\n",
      "train loss:0.0013889009397557413\n",
      "train loss:0.008799489706660996\n",
      "train loss:0.0009061567007182654\n",
      "train loss:0.0011130712452488908\n",
      "train loss:0.000796226540977285\n",
      "train loss:0.0007795977318563928\n",
      "train loss:0.0026941838136574255\n",
      "train loss:0.00013717417063237447\n",
      "train loss:0.0011339897541539135\n",
      "train loss:0.0030092544866947434\n",
      "train loss:0.0005799178493627404\n",
      "train loss:0.00184207994319349\n",
      "train loss:0.0013157694982592499\n",
      "train loss:0.00038556213679519094\n",
      "train loss:0.001212031352345864\n",
      "train loss:0.006320276498230087\n",
      "train loss:0.0006430780805038604\n",
      "train loss:0.0019874405213452075\n",
      "train loss:0.0012030175379814695\n",
      "train loss:0.001536318732641739\n",
      "train loss:0.0004925846641174289\n",
      "train loss:0.0004047058891804444\n",
      "train loss:0.006014398284992854\n",
      "train loss:0.007680214511986181\n",
      "train loss:4.970994909906973e-05\n",
      "train loss:0.0015656975101483836\n",
      "train loss:0.000567842364155492\n",
      "train loss:0.004548342746296007\n",
      "train loss:0.003230170921376183\n",
      "train loss:0.00046052872159866394\n",
      "train loss:0.00010171290798682117\n",
      "train loss:8.522622648664467e-05\n",
      "train loss:0.0037546480589043217\n",
      "train loss:0.0004600612533099815\n",
      "train loss:0.001641131221522561\n",
      "train loss:0.001997246820192343\n",
      "train loss:0.003824410977044857\n",
      "train loss:0.0002528464939257635\n",
      "train loss:0.002016461247814163\n",
      "train loss:0.0023622679394367015\n",
      "train loss:0.007297784198610015\n",
      "train loss:0.0014425742246370926\n",
      "train loss:0.0009464180976968239\n",
      "train loss:0.00010025116016160323\n",
      "train loss:0.0025646615453337267\n",
      "train loss:0.0034850248578725\n",
      "train loss:0.002357255595816524\n",
      "train loss:9.14828841504026e-05\n",
      "train loss:0.0012128942638033686\n",
      "train loss:0.0039054277251929508\n",
      "train loss:0.004738145352072282\n",
      "train loss:0.0035691988190541908\n",
      "train loss:0.0011708056659433305\n",
      "train loss:0.002162277189145487\n",
      "train loss:0.0006301853333770228\n",
      "train loss:5.227243105943877e-05\n",
      "train loss:0.00021214932157340036\n",
      "train loss:0.0008819264032975592\n",
      "train loss:0.004297952949537981\n",
      "train loss:0.0006565829685006114\n",
      "train loss:0.0017208507232146797\n",
      "train loss:0.0011720559953709046\n",
      "train loss:0.0002656208101624668\n",
      "train loss:0.0015206365265423352\n",
      "train loss:6.33781484890929e-05\n",
      "train loss:0.00296584492835347\n",
      "train loss:0.0014466456545915826\n",
      "train loss:0.002810232408442348\n",
      "train loss:0.0008694564827625231\n",
      "train loss:0.000261520680691978\n",
      "train loss:0.0038966862416016245\n",
      "train loss:2.9197071065331596e-05\n",
      "train loss:0.00016143725833491236\n",
      "train loss:0.00028080949606720703\n",
      "train loss:0.003670553527782174\n",
      "train loss:0.001733923329338166\n",
      "train loss:0.00027956196055396905\n",
      "train loss:0.0002522606330989624\n",
      "train loss:0.00023697925046637302\n",
      "train loss:6.547994883002894e-05\n",
      "train loss:0.0042670368508315945\n",
      "train loss:0.0009967053726746375\n",
      "train loss:0.002363376486099519\n",
      "train loss:0.011622760441408458\n",
      "train loss:0.004288284716517855\n",
      "train loss:0.0022895516754828133\n",
      "train loss:9.345316352433225e-05\n",
      "train loss:0.0002743270915733819\n",
      "train loss:0.0021974473850456203\n",
      "train loss:0.0016605813223527367\n",
      "train loss:0.000600498758447048\n",
      "train loss:0.00030990483107424236\n",
      "train loss:0.002686617583196255\n",
      "train loss:0.0029524521551155284\n",
      "train loss:0.002647994714640815\n",
      "train loss:0.0034785674042389836\n",
      "train loss:5.6804363010399484e-05\n",
      "train loss:0.007467431764330994\n",
      "train loss:0.00020219582190360747\n",
      "train loss:0.0006873153510385725\n",
      "train loss:0.004412804899615183\n",
      "train loss:0.0071949048391404665\n",
      "train loss:0.0006962617729195145\n",
      "train loss:9.049196076886666e-05\n",
      "train loss:0.0005003251017853091\n",
      "train loss:0.00011383519941109702\n",
      "train loss:0.0023585989196802256\n",
      "train loss:0.0014532430141438728\n",
      "train loss:0.0022233388335533696\n",
      "train loss:0.001960756494631012\n",
      "train loss:0.00013567501440807044\n",
      "train loss:0.0011806610203595178\n",
      "train loss:0.0001483409599996233\n",
      "train loss:0.0009248680739379675\n",
      "train loss:0.00216005309970658\n",
      "train loss:0.001102007489457835\n",
      "train loss:0.017134472722879258\n",
      "train loss:0.0001329890736046004\n",
      "train loss:0.000852551455335551\n",
      "train loss:0.0001063987772705172\n",
      "train loss:0.0011721676421081319\n",
      "train loss:0.030638500192348057\n",
      "train loss:0.00015906229850058147\n",
      "train loss:0.0008647238951930777\n",
      "train loss:0.002248596046034659\n",
      "train loss:0.002259002010112281\n",
      "train loss:0.00548394203645505\n",
      "train loss:0.001983450813196162\n",
      "train loss:0.0015536946699732806\n",
      "train loss:0.003965106202208469\n",
      "train loss:0.0021404002438183763\n",
      "train loss:0.0006888446564630615\n",
      "train loss:5.897144201139369e-05\n",
      "train loss:2.6793494427360774e-05\n",
      "train loss:0.0011157342508219892\n",
      "train loss:0.0006126155761791429\n",
      "train loss:0.006304280188921654\n",
      "train loss:0.002958443333265251\n",
      "train loss:0.0006395724811028707\n",
      "train loss:0.00031681719634250296\n",
      "train loss:0.0027690250097072615\n",
      "train loss:0.0004147875621443608\n",
      "train loss:0.0013697467931491558\n",
      "train loss:0.00234036092817545\n",
      "train loss:0.001490651690101657\n",
      "train loss:0.0011465772504562302\n",
      "train loss:2.715858440247167e-05\n",
      "train loss:0.0025880464316681167\n",
      "train loss:0.0025554336922914644\n",
      "train loss:0.006467908118181864\n",
      "train loss:0.00011867618948502991\n",
      "train loss:0.008541233672779288\n",
      "train loss:0.0003366635022117511\n",
      "train loss:0.0001487271659468196\n",
      "train loss:0.0003386029584528319\n",
      "train loss:0.0007100150955573238\n",
      "train loss:0.0005842833431929025\n",
      "train loss:0.00016615538343906338\n",
      "train loss:0.003519514422934191\n",
      "train loss:0.0014728909324636533\n",
      "train loss:0.0003398782059592781\n",
      "train loss:0.00036825191818460094\n",
      "train loss:0.0005472843765414693\n",
      "train loss:0.0005750070134275799\n",
      "train loss:0.00037311391450655886\n",
      "train loss:0.00538821196026827\n",
      "train loss:0.0066769308350201366\n",
      "train loss:7.233236685260045e-05\n",
      "train loss:0.0004979017094976955\n",
      "train loss:0.0012336386543038654\n",
      "train loss:0.0014639905294248515\n",
      "train loss:0.0001047760282022549\n",
      "train loss:0.0002460563808020884\n",
      "train loss:0.0026349215945534027\n",
      "train loss:0.0017899625179705677\n",
      "train loss:0.002142983459106213\n",
      "train loss:0.0002384893818646149\n",
      "train loss:0.0036382512175435096\n",
      "train loss:0.005239299317816372\n",
      "train loss:0.00021139053640534735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0004761053450691962\n",
      "train loss:0.008361804393150884\n",
      "train loss:0.00035705191080174706\n",
      "train loss:0.00028721404092677634\n",
      "train loss:0.0022067836005321925\n",
      "train loss:0.003855077166000928\n",
      "train loss:0.0015567844254004701\n",
      "train loss:0.0008608728125448947\n",
      "train loss:0.0028919087992705815\n",
      "train loss:0.0007433304120057631\n",
      "train loss:0.0003472663236625545\n",
      "train loss:0.003340575318288947\n",
      "train loss:0.0009373894791653311\n",
      "train loss:0.0011834311065627115\n",
      "train loss:0.0018520623719115972\n",
      "train loss:0.001003744103711725\n",
      "train loss:0.0008904980921074996\n",
      "train loss:0.004055113942965232\n",
      "train loss:0.0007078569629104618\n",
      "train loss:7.404978002128248e-05\n",
      "train loss:0.0021405282926551684\n",
      "train loss:0.0008931315207013513\n",
      "train loss:0.018012568264878924\n",
      "train loss:0.004420395485935148\n",
      "train loss:0.0005740955148296452\n",
      "train loss:0.0002193216787672417\n",
      "train loss:2.5979488336881454e-05\n",
      "train loss:0.001363195410986447\n",
      "train loss:0.005679912843886051\n",
      "train loss:0.0006742890387952979\n",
      "train loss:0.0008000026734540253\n",
      "train loss:0.0008822425191579828\n",
      "train loss:6.023836986054408e-05\n",
      "train loss:0.0015924832443771114\n",
      "train loss:0.00045963711947316426\n",
      "train loss:0.002684858826460297\n",
      "train loss:0.0002512905494839337\n",
      "train loss:0.00011011380055486114\n",
      "train loss:0.0007199956929939984\n",
      "train loss:0.0015533502519643355\n",
      "train loss:0.00024074244930014896\n",
      "train loss:0.00010658960242587002\n",
      "train loss:0.003429296564355466\n",
      "train loss:0.0012316894971501676\n",
      "train loss:0.00019451632884896913\n",
      "train loss:0.00551419215890995\n",
      "train loss:0.0011530066208508163\n",
      "train loss:0.0007466841950167117\n",
      "train loss:0.006679138111352774\n",
      "train loss:0.0007161486607767551\n",
      "train loss:0.000931604737391558\n",
      "train loss:0.00015464644201513222\n",
      "train loss:0.00031583903141957007\n",
      "train loss:0.003478409137371072\n",
      "train loss:0.002180057320871076\n",
      "train loss:0.0021162188961356097\n",
      "train loss:0.0012620148971797577\n",
      "train loss:0.002233760649008999\n",
      "train loss:0.004467024577270072\n",
      "train loss:0.0012936839587346567\n",
      "train loss:2.6921715088302783e-05\n",
      "train loss:0.003324667934528771\n",
      "train loss:0.001064922813641265\n",
      "train loss:0.0005029516877815722\n",
      "train loss:0.0008003398908824151\n",
      "train loss:4.455680924212989e-05\n",
      "train loss:0.0012183208918616932\n",
      "train loss:0.0002755393799912986\n",
      "train loss:0.00012131184173461139\n",
      "train loss:0.00010476219358219466\n",
      "train loss:0.0013038374532522714\n",
      "train loss:0.00037159570237937684\n",
      "train loss:0.0008796454889005028\n",
      "train loss:0.0011450014746997488\n",
      "train loss:0.0003124886739170447\n",
      "train loss:0.0022688815771021937\n",
      "train loss:0.00028019272731033795\n",
      "train loss:0.009369073181746778\n",
      "train loss:0.001282211939562092\n",
      "train loss:0.003741582393602069\n",
      "train loss:3.7290049456986775e-05\n",
      "train loss:0.007738713539468233\n",
      "train loss:0.0003153581374156883\n",
      "train loss:0.0004633258948023707\n",
      "train loss:0.00014767647759634408\n",
      "train loss:0.0016738611643204657\n",
      "train loss:0.0002654420186669111\n",
      "train loss:0.00061094712001108\n",
      "train loss:0.0010810142794777855\n",
      "train loss:0.0019217646921757048\n",
      "train loss:0.0006939652236326449\n",
      "train loss:7.395960417327014e-05\n",
      "train loss:0.0014990952277813753\n",
      "train loss:0.0005350844798326488\n",
      "train loss:0.0001584189618717619\n",
      "train loss:5.7400477850559834e-05\n",
      "train loss:0.0008365601851883195\n",
      "train loss:0.00026898932283055105\n",
      "train loss:0.0014570903891141\n",
      "train loss:0.0003323808265195328\n",
      "train loss:0.0030539281624321567\n",
      "train loss:0.0006864463039588309\n",
      "train loss:0.000598267239093877\n",
      "train loss:0.011934266258793878\n",
      "train loss:0.0008874909512701684\n",
      "train loss:0.02065929644072393\n",
      "train loss:0.00020137349807791748\n",
      "train loss:0.0003484817978648104\n",
      "train loss:0.00011725823896620893\n",
      "train loss:0.0008713454466381643\n",
      "train loss:0.0026624144464631066\n",
      "train loss:0.007777248812076437\n",
      "train loss:0.0037934411295908327\n",
      "train loss:0.003199205498241532\n",
      "train loss:0.00039561341401742723\n",
      "train loss:0.00012847200166022794\n",
      "train loss:0.0002412420022916229\n",
      "train loss:0.0024858488645338964\n",
      "train loss:0.0018969767985950654\n",
      "train loss:0.00031493231189375683\n",
      "train loss:0.0005824350451396667\n",
      "train loss:0.0008196854919579146\n",
      "train loss:0.0006669477879165353\n",
      "train loss:0.002455596654158344\n",
      "train loss:0.000281490653474413\n",
      "train loss:0.00039932936517635684\n",
      "train loss:0.0024917703063248543\n",
      "train loss:0.010192345297994471\n",
      "train loss:0.0018987829921459428\n",
      "train loss:0.0002122887310503752\n",
      "train loss:0.0032114185975785246\n",
      "train loss:0.0011061553139699234\n",
      "train loss:0.00041427005299238885\n",
      "train loss:0.0017076941805686697\n",
      "train loss:0.0008460786909976059\n",
      "train loss:0.0012697380136564337\n",
      "train loss:0.000745450492626858\n",
      "train loss:0.00044656321699797636\n",
      "train loss:0.0006788387867540488\n",
      "train loss:0.011135371932595218\n",
      "train loss:1.0471382840533132e-05\n",
      "train loss:0.002658570185540498\n",
      "train loss:0.000525746579786636\n",
      "train loss:0.002545470945992796\n",
      "train loss:0.0022736023285643567\n",
      "train loss:0.00047284560697797066\n",
      "train loss:0.0025294784268194105\n",
      "train loss:0.0006781247217808873\n",
      "train loss:0.0007949916332046313\n",
      "train loss:0.00034310894037477204\n",
      "train loss:0.0024278496082220445\n",
      "train loss:0.0016448479237524468\n",
      "train loss:9.899861118606238e-05\n",
      "train loss:0.0033468860283689407\n",
      "train loss:0.0022193862841320563\n",
      "train loss:0.0012220112379569556\n",
      "train loss:0.00047604493558794215\n",
      "train loss:0.0004283002812805224\n",
      "train loss:0.0003813579116094851\n",
      "train loss:0.0018056571868266408\n",
      "train loss:0.0002452566324575067\n",
      "train loss:0.00015552490294806567\n",
      "train loss:0.000436015404222736\n",
      "train loss:0.00606168584905516\n",
      "train loss:0.0002579048504947963\n",
      "train loss:0.0006671836004850663\n",
      "train loss:0.005332637578128177\n",
      "train loss:6.848401972955972e-05\n",
      "train loss:0.0024994667228132833\n",
      "train loss:0.0005606375110705298\n",
      "train loss:2.1049599432390763e-05\n",
      "train loss:0.00014486777052843014\n",
      "train loss:8.369620101815779e-05\n",
      "train loss:0.0023799527998870852\n",
      "train loss:0.0017769459136553049\n",
      "train loss:0.00037030189423179556\n",
      "train loss:0.0006930041866648026\n",
      "train loss:0.0002480963642534744\n",
      "train loss:0.0032019835375756197\n",
      "train loss:0.0007843285075131439\n",
      "train loss:0.000271804164710266\n",
      "train loss:0.00012051508326718577\n",
      "train loss:0.002830329570924604\n",
      "train loss:4.7623092780756176e-05\n",
      "train loss:0.00035622833607710765\n",
      "train loss:0.00011967133901373106\n",
      "train loss:0.0008134084603769197\n",
      "train loss:0.0008286826620524628\n",
      "train loss:0.00021299268182192608\n",
      "train loss:0.0007547426885649701\n",
      "train loss:0.0026240405416331613\n",
      "train loss:0.0009146063369132733\n",
      "train loss:0.002072140976482331\n",
      "train loss:0.0012756991392411992\n",
      "train loss:0.0004929078247551118\n",
      "train loss:0.00034280253466040375\n",
      "train loss:0.0019413243306547973\n",
      "train loss:0.00024179561417682943\n",
      "train loss:0.00013297119353074144\n",
      "train loss:0.00017245262746794741\n",
      "train loss:0.003540052203856542\n",
      "train loss:0.0001954453059059787\n",
      "train loss:0.0009298633925352009\n",
      "train loss:0.0002807172679645188\n",
      "train loss:0.0007114109436168037\n",
      "train loss:0.0001264228859942097\n",
      "train loss:0.0007857825142758801\n",
      "train loss:0.0007940092459400305\n",
      "train loss:0.01661986365424689\n",
      "train loss:0.0003788577581997532\n",
      "train loss:0.0022569471930198666\n",
      "train loss:0.002482757230714961\n",
      "train loss:0.0009623165755280794\n",
      "train loss:0.0005924029021572742\n",
      "train loss:0.0005452139862461927\n",
      "train loss:0.0008791503994227799\n",
      "train loss:0.0004146177355708856\n",
      "train loss:0.0016580208001606674\n",
      "train loss:0.001151902368482915\n",
      "train loss:0.0027221057910405246\n",
      "train loss:0.000933575972127735\n",
      "train loss:0.0014168989306902829\n",
      "train loss:0.00043753437536064394\n",
      "train loss:1.4043247563727169e-05\n",
      "train loss:0.0008528419512495302\n",
      "train loss:3.897715762710041e-05\n",
      "train loss:0.002501644564036636\n",
      "train loss:7.049340026182898e-05\n",
      "train loss:0.0004315751260492292\n",
      "train loss:0.00017475698469191528\n",
      "train loss:0.004403622585598281\n",
      "train loss:0.0029201915982855274\n",
      "train loss:0.0038439752019070856\n",
      "train loss:0.0009555815940895264\n",
      "train loss:0.0004254503637938008\n",
      "train loss:0.0003476624932321528\n",
      "train loss:0.004907466996574115\n",
      "train loss:0.0035280716239902226\n",
      "train loss:0.0012276195727481105\n",
      "train loss:0.0015304023172711318\n",
      "train loss:0.0003357207093772602\n",
      "train loss:0.0004143931064786977\n",
      "train loss:0.005458600100716709\n",
      "train loss:0.0011695136028588562\n",
      "train loss:0.0012516317184520051\n",
      "train loss:0.0006522528391605563\n",
      "train loss:0.0002510265856956179\n",
      "train loss:0.0004994229943758823\n",
      "train loss:0.001175743756751469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0021950653380512623\n",
      "train loss:3.689496865043787e-05\n",
      "train loss:0.0002696305421955667\n",
      "train loss:0.0007343779282469285\n",
      "train loss:0.0007092518505194212\n",
      "train loss:0.0009979472031766078\n",
      "train loss:0.002417980938664008\n",
      "train loss:0.00029212379456168704\n",
      "train loss:0.00011652429481367591\n",
      "train loss:0.0007636140959939334\n",
      "train loss:0.0003814214204854819\n",
      "train loss:0.0007706492062199555\n",
      "train loss:0.0006402867531149\n",
      "train loss:0.000632953289945965\n",
      "train loss:0.0018145420070143237\n",
      "train loss:0.001222238512379014\n",
      "train loss:0.001220593205215019\n",
      "train loss:0.0009283972239791006\n",
      "train loss:0.0018337719061927624\n",
      "train loss:0.0005514848663546185\n",
      "train loss:0.0001856647277418593\n",
      "train loss:0.0015409459923146976\n",
      "train loss:0.00041637419820040147\n",
      "train loss:0.0033965009246808638\n",
      "train loss:0.00031794076375582693\n",
      "train loss:0.0014181664447980255\n",
      "train loss:0.001174400544278975\n",
      "train loss:0.0015143975299835644\n",
      "train loss:0.00188558083252853\n",
      "train loss:0.006757676086667852\n",
      "train loss:0.00044875177785667306\n",
      "train loss:0.0020383838434873585\n",
      "train loss:0.00419143905783711\n",
      "train loss:0.00035765270846260507\n",
      "train loss:5.971952861983044e-05\n",
      "train loss:0.0004041082375371409\n",
      "train loss:0.0016487340024320717\n",
      "train loss:0.0007232997302172976\n",
      "train loss:0.00020775305029522702\n",
      "=== epoch:19, train acc:0.998, test acc:0.985 ===\n",
      "train loss:0.0010439995287861745\n",
      "train loss:0.0012865334103366768\n",
      "train loss:0.0007658211690873755\n",
      "train loss:0.0014666333071257274\n",
      "train loss:0.000957547685900971\n",
      "train loss:0.00035655552880437176\n",
      "train loss:0.001011746213618178\n",
      "train loss:0.001568628317321914\n",
      "train loss:0.0050757773108174005\n",
      "train loss:0.000284733808503119\n",
      "train loss:0.0005590765039843555\n",
      "train loss:7.852535617016796e-05\n",
      "train loss:0.00029237073536614056\n",
      "train loss:0.0040708563487394525\n",
      "train loss:0.000514610895375159\n",
      "train loss:0.0008858100072245183\n",
      "train loss:0.00023694998250729806\n",
      "train loss:0.0020594878742443124\n",
      "train loss:0.0010518775642996945\n",
      "train loss:0.00027073185441202076\n",
      "train loss:0.00011530826026524055\n",
      "train loss:0.0004193294247157302\n",
      "train loss:0.0006340366477661933\n",
      "train loss:0.023498363166782686\n",
      "train loss:0.0012187414843172744\n",
      "train loss:0.0018002327610337307\n",
      "train loss:0.00021843371570158372\n",
      "train loss:0.0029424458237434503\n",
      "train loss:0.0028318706116771497\n",
      "train loss:0.00033406142452380676\n",
      "train loss:0.0002744735639189779\n",
      "train loss:8.478058806106778e-05\n",
      "train loss:0.0013419841754481509\n",
      "train loss:0.00011711152105507562\n",
      "train loss:0.0009906592120683703\n",
      "train loss:0.0006960201500111877\n",
      "train loss:0.00043057052870731243\n",
      "train loss:0.00010277926398121914\n",
      "train loss:0.00011803599635572764\n",
      "train loss:0.0022876121008988466\n",
      "train loss:0.0008315504171934636\n",
      "train loss:0.001960031665033462\n",
      "train loss:0.0002628906361995509\n",
      "train loss:0.00035599109058579494\n",
      "train loss:0.0011480724462731877\n",
      "train loss:0.0006984566658757579\n",
      "train loss:0.002778005540250091\n",
      "train loss:0.0018553860142642015\n",
      "train loss:0.0009859741152807926\n",
      "train loss:0.0011448498312206599\n",
      "train loss:0.0006980469863001767\n",
      "train loss:0.002127207681608265\n",
      "train loss:0.0013054970125782614\n",
      "train loss:0.0003368848446190309\n",
      "train loss:0.00012579381484602355\n",
      "train loss:0.00029949393448299865\n",
      "train loss:0.0010830035267930997\n",
      "train loss:0.0005446740073556777\n",
      "train loss:0.0021620689489874733\n",
      "train loss:0.002865962729996574\n",
      "train loss:0.0009900725595117686\n",
      "train loss:0.001124328612662182\n",
      "train loss:0.0014745482804211904\n",
      "train loss:0.0017319985557092096\n",
      "train loss:0.0006329665380311561\n",
      "train loss:1.934680124890174e-05\n",
      "train loss:0.00014233311753609627\n",
      "train loss:0.0006254590813175374\n",
      "train loss:0.0004954380655915757\n",
      "train loss:0.00027405111093729465\n",
      "train loss:0.000298838315350959\n",
      "train loss:0.0007434697594116514\n",
      "train loss:0.0001901712819429567\n",
      "train loss:0.0023240367467401105\n",
      "train loss:0.0005963185901352476\n",
      "train loss:0.0007973748603348939\n",
      "train loss:0.001710156359609846\n",
      "train loss:0.0009336137177980756\n",
      "train loss:0.00012501316517207672\n",
      "train loss:0.0019679908065699924\n",
      "train loss:0.0010474732105735963\n",
      "train loss:0.001970548863698022\n",
      "train loss:6.543241769242005e-05\n",
      "train loss:0.0009645157614661156\n",
      "train loss:8.975249148546809e-05\n",
      "train loss:0.0008149252150883162\n",
      "train loss:0.007418829566339504\n",
      "train loss:0.0004864218477227072\n",
      "train loss:0.0008538110779219704\n",
      "train loss:0.00041394406711219126\n",
      "train loss:0.00013070635185205638\n",
      "train loss:0.00013249392415820913\n",
      "train loss:0.0009484604751303188\n",
      "train loss:0.0002928887700053064\n",
      "train loss:0.0009215011858747943\n",
      "train loss:8.263150285856139e-05\n",
      "train loss:0.0012642640289232318\n",
      "train loss:0.00012287603486360566\n",
      "train loss:0.00019908665565066235\n",
      "train loss:0.0028214724200300283\n",
      "train loss:0.0035867228398630217\n",
      "train loss:0.001251901088056749\n",
      "train loss:0.000128818799905629\n",
      "train loss:0.0018387287995823274\n",
      "train loss:0.0006855252313270967\n",
      "train loss:0.000592836830454508\n",
      "train loss:4.641642592943883e-05\n",
      "train loss:5.9005583547128637e-05\n",
      "train loss:0.0010427874197385047\n",
      "train loss:0.001821328479871349\n",
      "train loss:0.001100315893523027\n",
      "train loss:0.0018769771258715525\n",
      "train loss:0.0069174394772087925\n",
      "train loss:0.0008239908102322946\n",
      "train loss:0.0015537254420640283\n",
      "train loss:0.0001941748176506947\n",
      "train loss:0.001280310620206032\n",
      "train loss:5.4669456438999174e-05\n",
      "train loss:0.0034742051584660564\n",
      "train loss:0.0005570608547874582\n",
      "train loss:0.00017596251570573083\n",
      "train loss:0.0016139063790265521\n",
      "train loss:0.000734109949042156\n",
      "train loss:0.00028666903808538286\n",
      "train loss:0.0003985113631376315\n",
      "train loss:0.00023583533130490066\n",
      "train loss:0.0005407993054908465\n",
      "train loss:7.163734375617379e-05\n",
      "train loss:0.0008574575580578689\n",
      "train loss:0.0011003411351958945\n",
      "train loss:0.000953166893394601\n",
      "train loss:0.008815928156673291\n",
      "train loss:0.0008530457761366911\n",
      "train loss:0.0002780467630382681\n",
      "train loss:0.0014960376947479124\n",
      "train loss:0.0012760863561690253\n",
      "train loss:0.000147926997120363\n",
      "train loss:8.499728765281553e-05\n",
      "train loss:0.0002544736349191783\n",
      "train loss:0.0016357299220415176\n",
      "train loss:3.721229023057917e-05\n",
      "train loss:0.00010191417097581237\n",
      "train loss:0.0004146159942166729\n",
      "train loss:0.00011067009380151753\n",
      "train loss:0.0010480607107144172\n",
      "train loss:0.0001998231176840832\n",
      "train loss:0.0003213876420592364\n",
      "train loss:0.0002786939713525751\n",
      "train loss:0.000135152863350281\n",
      "train loss:0.0012557699978616153\n",
      "train loss:0.0006203705232402436\n",
      "train loss:0.0011445880491797822\n",
      "train loss:1.3789535034844787e-05\n",
      "train loss:0.0009956862470034498\n",
      "train loss:0.000769862301086802\n",
      "train loss:0.00014064449955835537\n",
      "train loss:0.0010423603372543154\n",
      "train loss:0.0008637310017590983\n",
      "train loss:0.0002544915868509721\n",
      "train loss:0.0008889235289007058\n",
      "train loss:9.532094627107462e-05\n",
      "train loss:0.0007045268707900101\n",
      "train loss:0.0022093221061197394\n",
      "train loss:0.0005091661667323342\n",
      "train loss:9.923151439109633e-05\n",
      "train loss:0.0016089934867838315\n",
      "train loss:7.876670799997768e-06\n",
      "train loss:0.00010427241969450718\n",
      "train loss:3.1610065469551097e-05\n",
      "train loss:0.0007039761307596602\n",
      "train loss:0.0021851357651775437\n",
      "train loss:0.000789406799701779\n",
      "train loss:0.0008856449401687293\n",
      "train loss:0.0014519251558468016\n",
      "train loss:0.0008052124271048828\n",
      "train loss:0.0030365185645836027\n",
      "train loss:0.0005207360365070198\n",
      "train loss:0.0010154122257876384\n",
      "train loss:0.0004169206440071439\n",
      "train loss:0.0011284680023918314\n",
      "train loss:0.003843740712487107\n",
      "train loss:0.004155098376846215\n",
      "train loss:7.484692747789731e-05\n",
      "train loss:0.00034762879647512625\n",
      "train loss:0.0003540150243010271\n",
      "train loss:3.8122232651042825e-05\n",
      "train loss:0.0014142156537562788\n",
      "train loss:0.0023550919630090385\n",
      "train loss:0.0015261637352984145\n",
      "train loss:0.0024700662133443872\n",
      "train loss:0.0021274134970318014\n",
      "train loss:0.0018377017412803183\n",
      "train loss:0.0008398374459082336\n",
      "train loss:0.0016222130297878679\n",
      "train loss:0.0012192201218372157\n",
      "train loss:0.0007371263740050541\n",
      "train loss:0.0007643264690434349\n",
      "train loss:0.00024026429794402976\n",
      "train loss:0.00431771451775374\n",
      "train loss:0.00019332088361315104\n",
      "train loss:0.0007219995401010099\n",
      "train loss:5.8948216346495764e-05\n",
      "train loss:0.0008170994408273184\n",
      "train loss:0.0025104388870959655\n",
      "train loss:0.00021898779076009455\n",
      "train loss:0.0014331442072957922\n",
      "train loss:0.0011439842455169818\n",
      "train loss:0.0014569471116578786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0023854043147985378\n",
      "train loss:0.00044036985725461124\n",
      "train loss:0.0011336840851579463\n",
      "train loss:6.517676755447732e-05\n",
      "train loss:0.002950103624514117\n",
      "train loss:0.00010874209849834491\n",
      "train loss:0.00019582020754600732\n",
      "train loss:0.0009402196196000242\n",
      "train loss:0.0004127463288267794\n",
      "train loss:0.0008012709133470242\n",
      "train loss:0.004902478922438311\n",
      "train loss:0.003365675040574319\n",
      "train loss:0.0016690212153753095\n",
      "train loss:0.00013882777408286782\n",
      "train loss:0.0013207186247893635\n",
      "train loss:0.00033535683329481974\n",
      "train loss:0.00015812837055851465\n",
      "train loss:0.002389781936207234\n",
      "train loss:8.618654393838635e-05\n",
      "train loss:0.00535298266274904\n",
      "train loss:0.0002640781139994319\n",
      "train loss:7.636335048585773e-05\n",
      "train loss:0.0034285282624487916\n",
      "train loss:0.003076274410857105\n",
      "train loss:0.0002019076971490771\n",
      "train loss:0.0005324339624669939\n",
      "train loss:0.0005725913508097088\n",
      "train loss:0.0019593373944719965\n",
      "train loss:0.00029382567839516475\n",
      "train loss:0.00012659120222107288\n",
      "train loss:0.0028924003234323285\n",
      "train loss:0.0004371225364704232\n",
      "train loss:0.0033243459328231996\n",
      "train loss:3.474453815317672e-05\n",
      "train loss:0.0007832785752380341\n",
      "train loss:0.006034550669701885\n",
      "train loss:0.00018703375223127718\n",
      "train loss:0.00024154539991254543\n",
      "train loss:0.00013864957996197509\n",
      "train loss:0.0013835532626600075\n",
      "train loss:0.01214526922319477\n",
      "train loss:0.000509709100845337\n",
      "train loss:0.0011960157635853893\n",
      "train loss:0.0009527987185958637\n",
      "train loss:9.341083097267216e-05\n",
      "train loss:0.0002184707406393991\n",
      "train loss:0.00013374914825667686\n",
      "train loss:0.0008820800060580933\n",
      "train loss:0.00010420753235178475\n",
      "train loss:0.002540940457663342\n",
      "train loss:0.0005412760293850696\n",
      "train loss:0.0007092374791050656\n",
      "train loss:0.0013431655930122091\n",
      "train loss:2.7332196888910495e-05\n",
      "train loss:0.00019285083607408458\n",
      "train loss:0.001939993825633858\n",
      "train loss:0.00015954612421318007\n",
      "train loss:0.0014351285187944097\n",
      "train loss:0.0018160582703080016\n",
      "train loss:0.0012083722155022752\n",
      "train loss:0.00033764004973814703\n",
      "train loss:9.847223543384928e-05\n",
      "train loss:0.00044671904711079236\n",
      "train loss:0.00017919471034510279\n",
      "train loss:0.0025451854711748836\n",
      "train loss:7.434262905610633e-05\n",
      "train loss:0.0002842149847878461\n",
      "train loss:8.758789046251824e-05\n",
      "train loss:0.0006478121027048074\n",
      "train loss:0.0003399053848376677\n",
      "train loss:0.0002439513726751724\n",
      "train loss:0.00024062304620243609\n",
      "train loss:0.0014535932335628\n",
      "train loss:9.754563542716137e-05\n",
      "train loss:0.004205012208650414\n",
      "train loss:0.00022010189288770073\n",
      "train loss:0.0011594049706386501\n",
      "train loss:0.002084284139249152\n",
      "train loss:0.0008679778372819247\n",
      "train loss:0.0015589608202033665\n",
      "train loss:0.0014451685132475787\n",
      "train loss:0.004473226675102031\n",
      "train loss:0.0003508859171024592\n",
      "train loss:0.0004855507828723924\n",
      "train loss:0.002159986239944666\n",
      "train loss:0.0011089829625511546\n",
      "train loss:9.561909117673647e-05\n",
      "train loss:0.0003530161132314975\n",
      "train loss:6.835050096744835e-05\n",
      "train loss:0.0016647546363830978\n",
      "train loss:0.002861227976753701\n",
      "train loss:0.0019219949101032638\n",
      "train loss:0.0011900314658965109\n",
      "train loss:0.0008243670460386377\n",
      "train loss:0.0006373407265048352\n",
      "train loss:4.9064894583978485e-05\n",
      "train loss:0.003793847378082449\n",
      "train loss:0.00015670840014055793\n",
      "train loss:0.013159855528648796\n",
      "train loss:0.0031612217129722044\n",
      "train loss:3.366547589374262e-06\n",
      "train loss:0.00011764383253489299\n",
      "train loss:0.0011226268871059265\n",
      "train loss:0.0009009634727829705\n",
      "train loss:0.0006894444285280854\n",
      "train loss:0.004952061428210051\n",
      "train loss:0.00038609135421309676\n",
      "train loss:3.7117484597816306e-05\n",
      "train loss:0.0025403852164172596\n",
      "train loss:0.015287552024167886\n",
      "train loss:0.00019436000418571593\n",
      "train loss:0.0001471515141071655\n",
      "train loss:0.0002768990658505385\n",
      "train loss:0.0005965188522547222\n",
      "train loss:0.0004985697728386226\n",
      "train loss:0.00018551825436667626\n",
      "train loss:0.0026858765230463287\n",
      "train loss:0.0004489265722663425\n",
      "train loss:0.0017321175134386963\n",
      "train loss:0.001084835017315693\n",
      "train loss:0.0004395830416017827\n",
      "train loss:0.002336821538057228\n",
      "train loss:0.0010961352473389167\n",
      "train loss:0.0017073423301255707\n",
      "train loss:0.00047088905502444545\n",
      "train loss:0.002652183394623231\n",
      "train loss:0.0008497870134175312\n",
      "train loss:0.001168118065682812\n",
      "train loss:0.0004194298080741781\n",
      "train loss:0.004722124771426334\n",
      "train loss:2.542139278886724e-05\n",
      "train loss:0.000841262738417467\n",
      "train loss:0.000735944379922212\n",
      "train loss:6.174381333691013e-05\n",
      "train loss:0.0008650752964589536\n",
      "train loss:8.197548833453314e-05\n",
      "train loss:0.000702174251919953\n",
      "train loss:0.011431342828302165\n",
      "train loss:0.0024073590490231024\n",
      "train loss:0.0017457265537928924\n",
      "train loss:0.0013177508592396956\n",
      "train loss:0.0037654493955511926\n",
      "train loss:0.0030312580436424254\n",
      "train loss:0.0007078202949263298\n",
      "train loss:0.00025373429040205823\n",
      "train loss:0.000855930695672781\n",
      "train loss:0.001173495840243572\n",
      "train loss:6.523375367404807e-05\n",
      "train loss:0.001478607239054623\n",
      "train loss:2.282461533694884e-05\n",
      "train loss:0.0009202428478094385\n",
      "train loss:0.00029469874914445164\n",
      "train loss:0.0014167806375560504\n",
      "train loss:0.00014724585302379337\n",
      "train loss:0.001054419436007317\n",
      "train loss:0.0015444887375748737\n",
      "train loss:0.00021670952619640496\n",
      "train loss:0.002027873939432899\n",
      "train loss:0.0006547291023987489\n",
      "train loss:0.0009517707778430043\n",
      "train loss:0.0022199018923672108\n",
      "train loss:0.0019396452153822884\n",
      "train loss:0.0007283632555229357\n",
      "train loss:1.7535000230805623e-05\n",
      "train loss:0.0033926851517814627\n",
      "train loss:0.0017500615472903905\n",
      "train loss:0.0018097610069875384\n",
      "train loss:0.003154462590043537\n",
      "train loss:0.0008474606648939975\n",
      "train loss:0.0014742515594763897\n",
      "train loss:0.0003166240783339143\n",
      "train loss:0.0002530093948847765\n",
      "train loss:0.0019521665250394368\n",
      "train loss:0.00045149008671155664\n",
      "train loss:0.0005529800530783247\n",
      "train loss:0.0028325957402808234\n",
      "train loss:0.0031216135214381085\n",
      "train loss:0.0007781856193116735\n",
      "train loss:0.0015981307295976682\n",
      "train loss:0.0018694479515515786\n",
      "train loss:0.0014705349757518124\n",
      "train loss:0.0005150609786004598\n",
      "train loss:0.008150538865971873\n",
      "train loss:5.62277085870055e-05\n",
      "train loss:0.0009207905329995524\n",
      "train loss:0.0015839756028882678\n",
      "train loss:0.0011735545013928153\n",
      "train loss:0.00269816642657317\n",
      "train loss:0.003967231313671522\n",
      "train loss:0.0002998447778418288\n",
      "train loss:0.0012676439962581343\n",
      "train loss:0.0020651839257227753\n",
      "train loss:0.0007825670662613892\n",
      "train loss:4.512867064669451e-05\n",
      "train loss:0.0019256757105185903\n",
      "train loss:0.00024652320477989113\n",
      "train loss:0.0010994980420371736\n",
      "train loss:0.001901174681899106\n",
      "train loss:0.0006791136804305373\n",
      "train loss:0.0002404124042748378\n",
      "train loss:0.0012090773647324988\n",
      "train loss:0.001154027242790694\n",
      "train loss:0.0009951887582121842\n",
      "train loss:0.014507297648827666\n",
      "train loss:0.00311937897460007\n",
      "train loss:0.0011213992723395892\n",
      "train loss:0.0018413304577579964\n",
      "train loss:0.009510795591284675\n",
      "train loss:0.0002219171183474971\n",
      "train loss:0.0016132608274896398\n",
      "train loss:0.0001452202309617688\n",
      "train loss:0.0016361107185705998\n",
      "train loss:0.0027381723617217165\n",
      "train loss:0.0002904589581602962\n",
      "train loss:0.0034274775378771477\n",
      "train loss:0.0010351093236410588\n",
      "train loss:0.005602267858912555\n",
      "train loss:0.013917297337219463\n",
      "train loss:0.00014088176195587056\n",
      "train loss:0.00458273139782819\n",
      "train loss:0.0012862873193603029\n",
      "train loss:0.0006173807677164753\n",
      "train loss:0.000673770331192538\n",
      "train loss:7.004081670094433e-05\n",
      "train loss:0.0038885752535735086\n",
      "train loss:0.0009006597687784784\n",
      "train loss:0.0004483162970205802\n",
      "train loss:0.007556557366182188\n",
      "train loss:0.0003717496782127239\n",
      "train loss:5.4692282709824014e-05\n",
      "train loss:0.004440822146545698\n",
      "train loss:0.002524027196922136\n",
      "train loss:0.0015592781667751914\n",
      "train loss:0.007205502164302075\n",
      "train loss:0.0028388499629807256\n",
      "train loss:0.017677838113705605\n",
      "train loss:0.0016634907012959333\n",
      "train loss:0.0011732931203818676\n",
      "train loss:0.0016690271928257313\n",
      "train loss:0.003314378279139875\n",
      "train loss:0.0008339614378970518\n",
      "train loss:0.0005820974222198448\n",
      "train loss:0.072011044674761\n",
      "train loss:0.0005790680188958923\n",
      "train loss:0.005177452429157632\n",
      "train loss:0.010109675998398597\n",
      "train loss:0.015859350886488432\n",
      "train loss:0.0021149637470970974\n",
      "train loss:3.058714664792439e-05\n",
      "train loss:0.0010777924851918053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.002150782404074576\n",
      "train loss:0.0018557367192922134\n",
      "train loss:0.0026011586615036116\n",
      "train loss:0.00432050485222501\n",
      "train loss:0.0022736481055257605\n",
      "train loss:0.0003128165669435432\n",
      "train loss:0.00016450516237285427\n",
      "train loss:0.0077159602279093745\n",
      "train loss:0.014690981193468657\n",
      "train loss:0.0007480650646622513\n",
      "train loss:0.00018579933821564475\n",
      "train loss:0.0015332177211069317\n",
      "train loss:0.0004469396441804276\n",
      "train loss:0.004787956700106297\n",
      "train loss:0.0026829075057912223\n",
      "train loss:0.0014995512341159567\n",
      "train loss:0.003078818566999204\n",
      "train loss:0.00025413719786006685\n",
      "train loss:0.000954938391278152\n",
      "train loss:0.00014287923386777324\n",
      "train loss:0.0006409704561845963\n",
      "train loss:0.004335132635541286\n",
      "train loss:0.0013231102417572155\n",
      "train loss:0.006219483725821818\n",
      "train loss:0.005199163348998756\n",
      "train loss:0.009383828048905009\n",
      "train loss:0.0007146168427918654\n",
      "train loss:0.007465998208360374\n",
      "train loss:0.0012061011416943726\n",
      "train loss:0.0037708225903107994\n",
      "train loss:0.001342124383419027\n",
      "train loss:0.0006191699226561372\n",
      "train loss:0.001520082892699785\n",
      "train loss:0.0019067810738433685\n",
      "train loss:0.002248536550143024\n",
      "train loss:0.00016426134129316378\n",
      "train loss:0.0092537557887213\n",
      "train loss:0.005386754916234393\n",
      "train loss:0.005012102433417884\n",
      "train loss:0.002353925591572307\n",
      "train loss:0.0005109688440729323\n",
      "train loss:0.002400315645607605\n",
      "train loss:0.0023168092997689644\n",
      "train loss:0.0005509383089972332\n",
      "train loss:0.0016381818404898438\n",
      "train loss:0.0009702949194079917\n",
      "train loss:0.001931060342302148\n",
      "train loss:0.0027717952065394193\n",
      "train loss:0.0045340476683051216\n",
      "train loss:0.0012585541632161193\n",
      "train loss:0.0026880863023574287\n",
      "train loss:0.003543357843008937\n",
      "train loss:0.00023865568437759694\n",
      "train loss:0.008258430819458715\n",
      "train loss:0.003334147167049981\n",
      "train loss:0.004199690508817916\n",
      "train loss:0.0033034592460898095\n",
      "train loss:0.0010444779407322202\n",
      "train loss:0.0008443858111933226\n",
      "train loss:0.004167374407661861\n",
      "train loss:0.005677127921918116\n",
      "train loss:0.00046637176425036756\n",
      "train loss:0.0001022747839014089\n",
      "train loss:5.974453115671543e-05\n",
      "train loss:0.00012050177376497332\n",
      "train loss:0.004219172990693316\n",
      "train loss:9.546740456083436e-05\n",
      "train loss:0.000990661639478272\n",
      "train loss:0.004575171739770961\n",
      "train loss:0.0015396913328306098\n",
      "train loss:0.00033737198913334055\n",
      "train loss:0.0006279960754929653\n",
      "train loss:0.0011064657458436998\n",
      "train loss:0.0018910452540982906\n",
      "train loss:0.00011634443134590014\n",
      "train loss:0.0043636078547221145\n",
      "train loss:0.0018522044610938546\n",
      "train loss:0.0009613910318219521\n",
      "train loss:0.0008302110401934203\n",
      "train loss:0.003983621156056327\n",
      "train loss:0.001126532451062072\n",
      "train loss:0.0005348382119132862\n",
      "train loss:0.003670164539236675\n",
      "train loss:8.122261407157686e-05\n",
      "train loss:0.0006100921214068817\n",
      "train loss:6.398476220300945e-05\n",
      "train loss:0.005934733503856786\n",
      "train loss:0.014338845145476448\n",
      "train loss:8.975281151391873e-05\n",
      "train loss:0.0010496909804591495\n",
      "train loss:0.0012131131407224264\n",
      "train loss:4.758528317821621e-05\n",
      "train loss:0.00022977741183480694\n",
      "train loss:0.0014384353091899263\n",
      "train loss:0.005492456627219108\n",
      "train loss:0.004736283194569591\n",
      "train loss:0.0038019817686832116\n",
      "train loss:0.00014948094192839136\n",
      "train loss:0.004928136710319972\n",
      "train loss:0.0001975190003305136\n",
      "train loss:0.0009068996216673367\n",
      "train loss:0.0020871708177248966\n",
      "train loss:0.0012018567949051843\n",
      "train loss:5.6472008586281795e-05\n",
      "train loss:0.004378515996550882\n",
      "train loss:0.002015947497090732\n",
      "train loss:0.0018761398197410474\n",
      "train loss:0.00023528754627227368\n",
      "train loss:0.0008440824386770154\n",
      "train loss:0.00290605154906215\n",
      "train loss:0.0004714758973101282\n",
      "train loss:0.0008308067143018773\n",
      "train loss:0.004065596419239667\n",
      "train loss:6.46000926131201e-05\n",
      "train loss:0.0017864278648531224\n",
      "train loss:0.005783319508846401\n",
      "train loss:0.00043662556116148295\n",
      "train loss:0.0035268614054972587\n",
      "train loss:0.0013863502773103214\n",
      "train loss:0.005790142768431268\n",
      "train loss:0.0006448176206248081\n",
      "train loss:0.0006077558389621155\n",
      "train loss:0.001044941904764777\n",
      "train loss:0.0010553058311285862\n",
      "train loss:0.001333453074198445\n",
      "train loss:0.0015800373919210853\n",
      "train loss:0.00195303174374482\n",
      "train loss:0.028200789513676095\n",
      "train loss:0.0018943084790541825\n",
      "train loss:0.0016932175773441379\n",
      "train loss:0.0015749602034193916\n",
      "train loss:0.0035404727102102272\n",
      "train loss:0.003447716248812527\n",
      "train loss:0.0013548354418614927\n",
      "train loss:0.005299109420330183\n",
      "train loss:0.003983246384598152\n",
      "train loss:0.0009214138607964216\n",
      "train loss:0.006543508931342771\n",
      "train loss:0.0004594477085617282\n",
      "train loss:0.01852417449065465\n",
      "train loss:0.0003565122914337693\n",
      "train loss:0.003192033699846334\n",
      "=== epoch:20, train acc:0.997, test acc:0.985 ===\n",
      "train loss:0.002966154968473962\n",
      "train loss:3.790627223853162e-05\n",
      "train loss:0.01560456376973955\n",
      "train loss:0.002841355207857072\n",
      "train loss:0.0010747307023408932\n",
      "train loss:0.004002519853182712\n",
      "train loss:0.0025316960631095856\n",
      "train loss:0.0005707224886168794\n",
      "train loss:5.7894163892085326e-05\n",
      "train loss:0.0005913114213466595\n",
      "train loss:0.00039749869102291975\n",
      "train loss:0.0005824323513354238\n",
      "train loss:0.001015799183947988\n",
      "train loss:0.001223610103874122\n",
      "train loss:0.039559913366080314\n",
      "train loss:0.002762185591314192\n",
      "train loss:8.533210291192893e-05\n",
      "train loss:0.00039313695466864284\n",
      "train loss:0.0021889162737531503\n",
      "train loss:0.007981675597381095\n",
      "train loss:0.0006162099234793591\n",
      "train loss:0.0010496747136343897\n",
      "train loss:0.0019720584068080994\n",
      "train loss:6.633056863602062e-05\n",
      "train loss:0.003511378080852128\n",
      "train loss:0.0011537550442579352\n",
      "train loss:0.0035870767551911824\n",
      "train loss:0.002234150329666009\n",
      "train loss:0.00034981819556174166\n",
      "train loss:0.003307516392254079\n",
      "train loss:0.0010198485867534456\n",
      "train loss:0.01541205684776751\n",
      "train loss:0.00046032802948376924\n",
      "train loss:0.0038823565291720495\n",
      "train loss:0.00034896732863399257\n",
      "train loss:0.0003250992030035947\n",
      "train loss:0.00013844786252838277\n",
      "train loss:3.7772005842702695e-05\n",
      "train loss:0.005062864543475462\n",
      "train loss:0.00020745644843813725\n",
      "train loss:0.0016625641557736256\n",
      "train loss:2.1867793631239952e-05\n",
      "train loss:0.00017412514944727483\n",
      "train loss:0.0003674440330620695\n",
      "train loss:0.0007120612626909831\n",
      "train loss:0.004764264220274199\n",
      "train loss:0.00023641019855817264\n",
      "train loss:0.014177647698709168\n",
      "train loss:0.0006182874204847283\n",
      "train loss:0.0007942032943437286\n",
      "train loss:0.0011458711271740327\n",
      "train loss:0.0007111573902261302\n",
      "train loss:3.953583066251952e-05\n",
      "train loss:0.004295517673170619\n",
      "train loss:0.006239393632267457\n",
      "train loss:0.001670060687209245\n",
      "train loss:0.0005861900437643904\n",
      "train loss:0.015558161978808412\n",
      "train loss:0.0001433232310165377\n",
      "train loss:0.000364340861908207\n",
      "train loss:0.00022348701629646328\n",
      "train loss:0.00020726736711539458\n",
      "train loss:0.0055596561506193656\n",
      "train loss:0.0015147351755415764\n",
      "train loss:3.1449423170482545e-05\n",
      "train loss:3.5524811064987525e-05\n",
      "train loss:0.0016169983452145056\n",
      "train loss:0.0039820726891181095\n",
      "train loss:0.0025219739109254985\n",
      "train loss:0.003781183840754406\n",
      "train loss:0.0002513762438139865\n",
      "train loss:0.0017672569828801438\n",
      "train loss:0.0008750595204659326\n",
      "train loss:0.0008117274195969516\n",
      "train loss:0.0029656121690451137\n",
      "train loss:0.0034920533398457867\n",
      "train loss:9.168426682265436e-05\n",
      "train loss:0.00247086077015654\n",
      "train loss:0.000314464441419402\n",
      "train loss:0.00813868426284522\n",
      "train loss:0.0010031894959181807\n",
      "train loss:0.0004244912197479887\n",
      "train loss:0.00022338543032720618\n",
      "train loss:0.000426583845278382\n",
      "train loss:0.0033615567699999166\n",
      "train loss:5.900432597221569e-05\n",
      "train loss:0.004433579570473589\n",
      "train loss:0.0017422082526002594\n",
      "train loss:0.00016677256810564966\n",
      "train loss:0.0076201216219457905\n",
      "train loss:0.0037304397609304123\n",
      "train loss:0.00014830522892903875\n",
      "train loss:0.0026004482180674465\n",
      "train loss:0.012631595364235106\n",
      "train loss:0.0002996856604440957\n",
      "train loss:3.4078963917468754e-05\n",
      "train loss:4.306548994906601e-05\n",
      "train loss:4.079636905239594e-05\n",
      "train loss:0.002050566551254464\n",
      "train loss:0.0001170780429456586\n",
      "train loss:7.230548241873112e-05\n",
      "train loss:0.00045543800178682363\n",
      "train loss:0.005379500355634948\n",
      "train loss:0.00080483829458842\n",
      "train loss:0.0002669676083147009\n",
      "train loss:5.27483586702413e-05\n",
      "train loss:0.00785230443832508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0006983274459876651\n",
      "train loss:0.0003278612488927503\n",
      "train loss:0.002438874217004798\n",
      "train loss:0.00015607278946037852\n",
      "train loss:0.00020493083038029006\n",
      "train loss:0.00012332764306119775\n",
      "train loss:0.00036491669719446244\n",
      "train loss:0.0010084039517532564\n",
      "train loss:0.0004525463883882865\n",
      "train loss:0.0001005976719959779\n",
      "train loss:0.007073664617042547\n",
      "train loss:0.018640222222257374\n",
      "train loss:0.006226790649685001\n",
      "train loss:0.0015844889939582818\n",
      "train loss:0.0002955066357579205\n",
      "train loss:0.0016808828587213122\n",
      "train loss:0.00041877034092613607\n",
      "train loss:0.0005979875481257324\n",
      "train loss:0.001869994061834497\n",
      "train loss:0.00029556402815678447\n",
      "train loss:0.0015172206026700754\n",
      "train loss:0.00033605724403214536\n",
      "train loss:0.002245720677128657\n",
      "train loss:0.0004991500461094401\n",
      "train loss:0.0026307171586209856\n",
      "train loss:0.001628944513626427\n",
      "train loss:0.001049871598279468\n",
      "train loss:0.00040593631755357287\n",
      "train loss:0.0010089225981470978\n",
      "train loss:0.0010245610003318693\n",
      "train loss:0.00204223195142379\n",
      "train loss:0.001119641826217307\n",
      "train loss:0.0006172304778207381\n",
      "train loss:0.002210474151965096\n",
      "train loss:0.001120556011645041\n",
      "train loss:0.00035251836115518195\n",
      "train loss:0.0003550244431058436\n",
      "train loss:0.00034625730116275976\n",
      "train loss:0.009656726084790847\n",
      "train loss:0.0002511047725488137\n",
      "train loss:0.0011673241173922409\n",
      "train loss:0.0030417272843229004\n",
      "train loss:0.0007381719461921389\n",
      "train loss:0.003328526908343367\n",
      "train loss:0.009743805790225667\n",
      "train loss:0.0011430825199897328\n",
      "train loss:0.0003808022845440748\n",
      "train loss:5.42952347830059e-05\n",
      "train loss:0.0012015878302542534\n",
      "train loss:0.00011599509588358572\n",
      "train loss:0.0006887225355678415\n",
      "train loss:0.0017320401756717493\n",
      "train loss:0.00030353381190457665\n",
      "train loss:0.00026244725458465193\n",
      "train loss:0.00026073626112581587\n",
      "train loss:0.0004671651336360487\n",
      "train loss:0.00031163362916113553\n",
      "train loss:6.35739251952065e-05\n",
      "train loss:0.0005398540211018298\n",
      "train loss:0.04385851805599523\n",
      "train loss:0.001595992302177548\n",
      "train loss:0.00037789743585381894\n",
      "train loss:0.003833810456578337\n",
      "train loss:0.00011173141321888842\n",
      "train loss:7.649460661795849e-05\n",
      "train loss:0.006575825516951767\n",
      "train loss:0.0019587374349220197\n",
      "train loss:0.011374192835877683\n",
      "train loss:0.0002590958301357292\n",
      "train loss:0.0025818750613202713\n",
      "train loss:0.0018815398049099776\n",
      "train loss:0.004395734079291083\n",
      "train loss:0.01665176947712014\n",
      "train loss:0.0008958724254248868\n",
      "train loss:0.00017241456971764033\n",
      "train loss:0.00026232347198096886\n",
      "train loss:0.0006143104892806508\n",
      "train loss:0.0014062910325121556\n",
      "train loss:0.0004855717706810674\n",
      "train loss:0.000963035568704131\n",
      "train loss:3.2208020703842355e-05\n",
      "train loss:0.00022113806249425124\n",
      "train loss:0.0012320287397347815\n",
      "train loss:0.0005354845828700395\n",
      "train loss:0.00032157943553500905\n",
      "train loss:0.000970604630658049\n",
      "train loss:0.0012365340303694546\n",
      "train loss:0.002023403758425086\n",
      "train loss:0.002160652982355128\n",
      "train loss:0.0015205090999523664\n",
      "train loss:0.0014966713838013162\n",
      "train loss:0.002769242982781857\n",
      "train loss:0.0005190567302855067\n",
      "train loss:0.00021887526126688193\n",
      "train loss:1.1895919342094603e-05\n",
      "train loss:0.00022019110723903865\n",
      "train loss:6.727279821022876e-05\n",
      "train loss:0.0018885373569255738\n",
      "train loss:0.0009236075537557513\n",
      "train loss:0.0428965235763593\n",
      "train loss:0.0009061696661062681\n",
      "train loss:0.0002547748030038087\n",
      "train loss:0.00026049743799686756\n",
      "train loss:0.0008043373774440901\n",
      "train loss:0.0017886618397147578\n",
      "train loss:0.0013584190764752066\n",
      "train loss:0.0011943260463345784\n",
      "train loss:0.0012736276636071283\n",
      "train loss:0.0003057822748418148\n",
      "train loss:0.00041443474487016325\n",
      "train loss:0.0014509744260019298\n",
      "train loss:0.0013884675751520339\n",
      "train loss:6.491663806718107e-05\n",
      "train loss:0.0010122127283501232\n",
      "train loss:0.0010867712337997814\n",
      "train loss:0.02329133180392473\n",
      "train loss:0.00431435062936103\n",
      "train loss:0.0037761116361179044\n",
      "train loss:0.0025044321423874317\n",
      "train loss:0.003993731139172498\n",
      "train loss:0.004662018177901035\n",
      "train loss:0.00010551879324612727\n",
      "train loss:0.0017313124552595896\n",
      "train loss:4.2848513843604683e-05\n",
      "train loss:0.003919290268953792\n",
      "train loss:0.000824081730826823\n",
      "train loss:0.0002039504798067458\n",
      "train loss:0.0005327111469062536\n",
      "train loss:0.002689233939308487\n",
      "train loss:0.004743730112352344\n",
      "train loss:8.911231412597532e-05\n",
      "train loss:0.02269616451016008\n",
      "train loss:0.0015743504942067224\n",
      "train loss:0.0008547347658232658\n",
      "train loss:0.0024427292832271564\n",
      "train loss:0.00013288714793127377\n",
      "train loss:0.00626849818748592\n",
      "train loss:0.00045376102530955914\n",
      "train loss:9.462348975473281e-05\n",
      "train loss:0.0003901629689820634\n",
      "train loss:0.00016718834391009715\n",
      "train loss:0.0005686037560194297\n",
      "train loss:0.0024466488030473427\n",
      "train loss:0.0012346569696705392\n",
      "train loss:0.0010509278507484115\n",
      "train loss:0.00032460934530916213\n",
      "train loss:5.9265890015155504e-05\n",
      "train loss:0.00014433804668100315\n",
      "train loss:0.005179904810204733\n",
      "train loss:0.0014393394267798057\n",
      "train loss:0.0016316917426058097\n",
      "train loss:0.002714397371054299\n",
      "train loss:8.566088752357754e-05\n",
      "train loss:0.0002674296878416365\n",
      "train loss:0.002516548383160437\n",
      "train loss:8.12867308642602e-05\n",
      "train loss:0.0013468959410726472\n",
      "train loss:0.00027224401333958046\n",
      "train loss:0.0010521111401396172\n",
      "train loss:7.378754555770058e-05\n",
      "train loss:0.0021731508509923603\n",
      "train loss:0.00017330963038188703\n",
      "train loss:2.078008049603171e-05\n",
      "train loss:0.001017243749588615\n",
      "train loss:0.00043234052621582273\n",
      "train loss:6.329514677753001e-05\n",
      "train loss:2.899995493283808e-05\n",
      "train loss:0.0015568767752008107\n",
      "train loss:0.00041636678566564446\n",
      "train loss:0.008505233044979162\n",
      "train loss:9.779014812399557e-05\n",
      "train loss:0.010339859385835743\n",
      "train loss:0.00026310890137879025\n",
      "train loss:0.0007096310873552033\n",
      "train loss:0.0036188997364317\n",
      "train loss:0.0009558585058450235\n",
      "train loss:0.003057425335634332\n",
      "train loss:0.0005668165381242176\n",
      "train loss:0.00019784111761185803\n",
      "train loss:0.0011718755888243018\n",
      "train loss:0.0037017334473753907\n",
      "train loss:6.708742264049816e-05\n",
      "train loss:7.561778403115814e-05\n",
      "train loss:0.06526129524539426\n",
      "train loss:0.000771042840220218\n",
      "train loss:0.0038085282955319494\n",
      "train loss:0.0005590380519943871\n",
      "train loss:0.0026769515150861147\n",
      "train loss:0.00035473627788713277\n",
      "train loss:0.0013089584557480257\n",
      "train loss:0.0004035635138236196\n",
      "train loss:0.0023953824448587225\n",
      "train loss:0.00031945860078642593\n",
      "train loss:0.00035451673723010566\n",
      "train loss:0.0009377300446613452\n",
      "train loss:0.0007547787244237606\n",
      "train loss:0.0010142735960351002\n",
      "train loss:0.0011220168710402909\n",
      "train loss:0.0003499251556155216\n",
      "train loss:0.0024172386085683276\n",
      "train loss:0.0006156628159142298\n",
      "train loss:0.01071729912004258\n",
      "train loss:0.00043738764035211543\n",
      "train loss:0.0038124323833367293\n",
      "train loss:0.0014285258326075247\n",
      "train loss:0.0008264924430125839\n",
      "train loss:0.00015729429049853608\n",
      "train loss:0.0007842221201701672\n",
      "train loss:0.0002917776268496418\n",
      "train loss:0.0022252906784196195\n",
      "train loss:0.00038809047413150923\n",
      "train loss:0.0005067078459626957\n",
      "train loss:0.00018666626295895557\n",
      "train loss:9.470982916614383e-05\n",
      "train loss:0.0004800081594994715\n",
      "train loss:0.0003197139706255088\n",
      "train loss:0.0005017554423066825\n",
      "train loss:7.182476737610942e-05\n",
      "train loss:0.009914386431970146\n",
      "train loss:0.0002318473065260335\n",
      "train loss:0.000836147427840338\n",
      "train loss:0.00353164712954445\n",
      "train loss:0.00021408111066089714\n",
      "train loss:0.00011245291630803456\n",
      "train loss:0.0009193054000041028\n",
      "train loss:0.0013639606384807127\n",
      "train loss:4.780841839006446e-05\n",
      "train loss:0.0023552431028596096\n",
      "train loss:0.00015718530036764992\n",
      "train loss:0.002217296276143065\n",
      "train loss:0.0006095284734565815\n",
      "train loss:0.0039083875363660585\n",
      "train loss:1.5242902358619511e-05\n",
      "train loss:0.0030536355557643268\n",
      "train loss:0.00018703753866148985\n",
      "train loss:0.0011669971094098504\n",
      "train loss:0.0031645199977052684\n",
      "train loss:0.0012826594162942487\n",
      "train loss:0.002918214531764218\n",
      "train loss:8.682131422339854e-05\n",
      "train loss:0.0004111847663250362\n",
      "train loss:6.433663418711045e-05\n",
      "train loss:0.0021054298196501557\n",
      "train loss:0.000460778582052172\n",
      "train loss:4.7982022130424306e-05\n",
      "train loss:0.0014076088306525546\n",
      "train loss:0.00021358993313764962\n",
      "train loss:0.00084013723823708\n",
      "train loss:0.0004986467928584117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.00140908099824214\n",
      "train loss:0.0017221547980783866\n",
      "train loss:0.003262767293899773\n",
      "train loss:0.0005831165692385602\n",
      "train loss:0.000637533488104984\n",
      "train loss:0.00014491052885573432\n",
      "train loss:0.00013876051283592656\n",
      "train loss:0.0012237411870885248\n",
      "train loss:4.463865749470804e-05\n",
      "train loss:0.00023865264171931322\n",
      "train loss:9.582325547628842e-05\n",
      "train loss:0.005499302293453318\n",
      "train loss:0.002967364693076095\n",
      "train loss:0.0003172844056473991\n",
      "train loss:0.00023959065791625801\n",
      "train loss:0.00043133458196154245\n",
      "train loss:0.0004953697727248051\n",
      "train loss:0.0002324334794902365\n",
      "train loss:9.836146982041825e-05\n",
      "train loss:0.0001029456708486626\n",
      "train loss:0.0002212546957326939\n",
      "train loss:0.001010746805197802\n",
      "train loss:0.0003025024781776965\n",
      "train loss:0.0007524248633201254\n",
      "train loss:0.00020447134305971132\n",
      "train loss:0.0053088759167797695\n",
      "train loss:0.00010719064358195775\n",
      "train loss:0.000695779608864045\n",
      "train loss:4.177587995159352e-05\n",
      "train loss:0.0008139522816387165\n",
      "train loss:0.0014196557528174164\n",
      "train loss:0.0051609564679796415\n",
      "train loss:0.003010084863081745\n",
      "train loss:0.000699004982764772\n",
      "train loss:0.0006434259772691385\n",
      "train loss:0.00042085269768464605\n",
      "train loss:0.0006223309304809511\n",
      "train loss:0.000673829027287913\n",
      "train loss:0.002369706325428818\n",
      "train loss:0.003244710524122025\n",
      "train loss:0.003479740107836003\n",
      "train loss:8.812353849483257e-05\n",
      "train loss:0.0006423804392761838\n",
      "train loss:0.0006474646849685573\n",
      "train loss:0.00010470852128406111\n",
      "train loss:0.0008547029931530722\n",
      "train loss:0.004461790753021604\n",
      "train loss:0.0017731362847860392\n",
      "train loss:0.00832417130365555\n",
      "train loss:0.0010057693089623653\n",
      "train loss:0.0018162030726718752\n",
      "train loss:0.0002667554599311287\n",
      "train loss:0.0010025160519999022\n",
      "train loss:0.0016270331722511918\n",
      "train loss:0.0010241300279119508\n",
      "train loss:0.0001678312977270533\n",
      "train loss:0.0012480507324446274\n",
      "train loss:0.0003940550075739834\n",
      "train loss:0.0028404571582522427\n",
      "train loss:0.0034737398227936813\n",
      "train loss:0.0009057455867806354\n",
      "train loss:0.00026904389500153887\n",
      "train loss:0.0001542709394082197\n",
      "train loss:0.004088026109235519\n",
      "train loss:0.001766206233257699\n",
      "train loss:0.0010656830904511082\n",
      "train loss:0.0007598821995275683\n",
      "train loss:0.004804917442540048\n",
      "train loss:0.00014329061548347613\n",
      "train loss:0.0013788147320209668\n",
      "train loss:5.45470705006873e-05\n",
      "train loss:0.0004806624714157294\n",
      "train loss:0.0018259344899494844\n",
      "train loss:0.0033794175370424\n",
      "train loss:0.0006758354161218034\n",
      "train loss:0.00029263151339187454\n",
      "train loss:0.0006663944334734924\n",
      "train loss:0.0002771675733083673\n",
      "train loss:0.0008873249229438265\n",
      "train loss:0.0025751057213818335\n",
      "train loss:0.001799496249466131\n",
      "train loss:0.00010557169104383278\n",
      "train loss:0.002235842299799505\n",
      "train loss:0.0004027538275257564\n",
      "train loss:0.00041386941527185426\n",
      "train loss:9.696569485155355e-05\n",
      "train loss:4.914829126968957e-05\n",
      "train loss:0.0002472476364273603\n",
      "train loss:7.387701121401013e-05\n",
      "train loss:0.000515924417256567\n",
      "train loss:0.0006395404114143994\n",
      "train loss:0.01081588712638036\n",
      "train loss:1.3396658354214669e-05\n",
      "train loss:4.1649454418811086e-05\n",
      "train loss:0.00046066935219592534\n",
      "train loss:0.00019820102351861214\n",
      "train loss:0.00020200893271452803\n",
      "train loss:0.0018785121520409097\n",
      "train loss:0.03951769691444324\n",
      "train loss:0.0014900163424441313\n",
      "train loss:0.00016648651228189646\n",
      "train loss:0.0008554981426944604\n",
      "train loss:0.0009171246615562264\n",
      "train loss:0.000417595914568545\n",
      "train loss:0.0010466464065231726\n",
      "train loss:0.0013010834024517715\n",
      "train loss:4.237174552501267e-05\n",
      "train loss:0.00014606593724576517\n",
      "train loss:0.003636410891850122\n",
      "train loss:7.64297178832918e-05\n",
      "train loss:0.0011846219425432425\n",
      "train loss:0.005549561828191507\n",
      "train loss:0.005778177705713863\n",
      "train loss:0.0004032182222356035\n",
      "train loss:0.00015548076497290864\n",
      "train loss:0.0012144878666349717\n",
      "train loss:0.0029223602210341664\n",
      "train loss:0.0023771353985484775\n",
      "train loss:0.0006373578172287486\n",
      "train loss:7.016778111856296e-05\n",
      "train loss:0.0003252834827900845\n",
      "train loss:0.002058448969736421\n",
      "train loss:0.0036785360975961578\n",
      "train loss:0.00033911477456739314\n",
      "train loss:0.0033707525508720197\n",
      "train loss:0.0014278630080196968\n",
      "train loss:0.000415798493253499\n",
      "train loss:0.009579037643147239\n",
      "train loss:0.0005613037553983199\n",
      "train loss:0.0021486947445320107\n",
      "train loss:0.00037817337832401034\n",
      "train loss:0.00010248994546424818\n",
      "train loss:0.0013885032794364593\n",
      "train loss:0.0017358734297856396\n",
      "train loss:0.00014312757350902177\n",
      "train loss:0.003224945503698316\n",
      "train loss:2.761144954045996e-05\n",
      "train loss:3.7623614269913684e-05\n",
      "train loss:0.0001256219243497464\n",
      "train loss:0.001446577880970166\n",
      "train loss:0.00010473135790533118\n",
      "train loss:0.0007424036868792102\n",
      "train loss:0.002122125609360329\n",
      "train loss:0.00015601418516965646\n",
      "train loss:0.0004943180688682732\n",
      "train loss:0.0008439927675035777\n",
      "train loss:5.9886044979533704e-05\n",
      "train loss:0.0010566805686504869\n",
      "train loss:0.0039796349205839365\n",
      "train loss:0.0003201757779709301\n",
      "train loss:0.00017365697685656263\n",
      "train loss:0.003662394662483907\n",
      "train loss:2.246081916843908e-06\n",
      "train loss:0.0010561897197641356\n",
      "train loss:0.003389483561697035\n",
      "train loss:0.00011775711576775864\n",
      "train loss:0.0005391930114658847\n",
      "train loss:0.00045291224813178473\n",
      "train loss:0.00045473098177462447\n",
      "train loss:0.0001409540453528416\n",
      "train loss:0.0008443209399943898\n",
      "train loss:0.001524276492479866\n",
      "train loss:0.0004022912959105524\n",
      "train loss:0.0001751132053581196\n",
      "train loss:0.0014212403592441753\n",
      "train loss:0.00011156689584307048\n",
      "train loss:8.949786950990344e-05\n",
      "train loss:0.00012721185288258572\n",
      "train loss:0.00135270978190465\n",
      "train loss:0.00068270537149988\n",
      "train loss:0.00020824436614353856\n",
      "train loss:0.0002916396481105472\n",
      "train loss:0.0002463166501185908\n",
      "train loss:9.383830402089089e-05\n",
      "train loss:0.005009459121050822\n",
      "train loss:0.009029539849552671\n",
      "train loss:0.0003484057374515673\n",
      "train loss:0.0003001707896518889\n",
      "train loss:0.009562807047473264\n",
      "train loss:0.0013519366584545565\n",
      "train loss:0.00016184423355393812\n",
      "train loss:0.00031395429375817796\n",
      "train loss:0.0014581450009205099\n",
      "train loss:9.983139494755043e-05\n",
      "train loss:0.00025962408882319676\n",
      "train loss:0.0004078935135619018\n",
      "train loss:0.0015779860254857023\n",
      "train loss:0.0035353706719225115\n",
      "train loss:0.0068049088705167075\n",
      "train loss:0.002603738878878151\n",
      "train loss:0.00011081522182689458\n",
      "train loss:0.0001189599884961937\n",
      "train loss:9.266126965273544e-05\n",
      "train loss:0.00019641599732836797\n",
      "train loss:0.00024515099202427216\n",
      "train loss:0.00013393288428550264\n",
      "train loss:0.0001363522913093683\n",
      "train loss:0.0016973496408094727\n",
      "train loss:0.000517605873936815\n",
      "train loss:0.0021226943503683\n",
      "train loss:0.0008909658726157782\n",
      "train loss:0.0006116613006482884\n",
      "train loss:0.0004026768443154103\n",
      "train loss:0.004037403886843164\n",
      "train loss:0.0026455589204433766\n",
      "train loss:0.00025524034439473723\n",
      "train loss:0.0012843488373641698\n",
      "train loss:0.00010062546133279929\n",
      "train loss:0.000831944486261564\n",
      "train loss:0.0035442388610949123\n",
      "train loss:0.00031516337301724034\n",
      "train loss:0.0009364758389367023\n",
      "train loss:0.0014684939326286485\n",
      "train loss:0.0003812024931941143\n",
      "train loss:0.00014259495829306018\n",
      "train loss:5.9099437447570935e-05\n",
      "train loss:0.00016180145389995234\n",
      "train loss:0.0004405286832619792\n",
      "train loss:0.00098883934207451\n",
      "train loss:0.001246630727742059\n",
      "train loss:0.0055707143258097395\n",
      "train loss:0.0005387585359763281\n",
      "train loss:0.0009236193774532477\n",
      "train loss:0.0003447898278708588\n",
      "train loss:0.0009229403549904341\n",
      "train loss:0.006914195105301426\n",
      "train loss:0.0005107046721579617\n",
      "train loss:6.106131108790555e-05\n",
      "train loss:4.773724285171822e-05\n",
      "train loss:0.00014212057320554922\n",
      "train loss:0.005539667247915013\n",
      "train loss:0.005353928219571521\n",
      "train loss:0.005630470472962334\n",
      "train loss:5.6602414411359386e-05\n",
      "train loss:0.0013736779395541615\n",
      "train loss:0.010213225664775496\n",
      "train loss:9.512416159248759e-05\n",
      "train loss:0.0002540124797425673\n",
      "train loss:0.002573925834218617\n",
      "train loss:0.0019742799628529213\n",
      "train loss:0.0002398105192035215\n",
      "train loss:0.00042135188454597047\n",
      "train loss:0.0003014491747649555\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.9872\n",
      "Saved Network Parameters!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.trainer import Trainer\n",
    "\n",
    "# 读入数据\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 处理花费时间较长的情况下减少数据 \n",
    "#x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "#x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# 保存参数\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# 绘制图形\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcEAAAEgCAYAAADMo8jPAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAb10lEQVR4nO3ce2zVd/3H8fdpT0/b05a29EJbWwaO62ADJmOIICCK7qJjIUtk0UwFljgnMSI6MxGjTiMhZluMLkOnDmEXI2OyMYczDhkIOC6DAAPWQoG1pTd6pfT6/f3Bzkk17Pd5ff15+a2f5+Ov75bX983ne873nBeH5PuJBEFgAAD4KOW/vQAAAP5bKEEAgLcoQQCAtyhBAIC3KEEAgLcoQQCAt6KhwtFokJ6e7syFeexCmffOny3PLCwsdGZqa2utpaUlYmaWm5sbjBgxwnlOLBaT16BeV01NjTyzsbFRyvX19TUGQVAUi8WCzMxMZz4jI0NeQ1ZWlpQLM/Py5ctS7vTp041BEBSZmaWlpQXKnxHmvsnPz5dyPT098sz29nZnpqury3p6eiJmVz5jaWlpznOU9zWhv79fyqWk6H8njkQiUu7ixYuNQRAUZWVlBcrrq67VzKyvr0/KdXd3yzN7e3ul3OXLl5P3Yk5OTlBUVOQ8Z/jw4fI61O9Q9bNjpr0O9fX11tbWFjEzi8ViQTwed57T1tYmr6GgoEDKhfmuVe+D+vr65Hs2WKgSTE9Pt0mTJv3LFmVmNmrUKCmnvnhmZsuWLXNmPve5zyWPR4wYYT/5yU+c55SXl8trGDNmjJT77ne/K8/8+c9/LuUuXLhQbXbli3LmzJnOvPKeJsyYMUPKjR07Vp556tQpKbdkyZLqxHFGRoZNnz7deU5eXp68jsWLF0u58+fPyzN37NjhzOzevTt5nJaWJt07EydOlNegfkkpX3gJ6pfUM888U2125S8YK1ascOabm5vlNbS0tEg59f4yM7tw4YKUO3r0aPJeLCoqsoceesh5zpIlS+R1qOV28uRJeWZlZaUzs2rVquRxPB63uXPnOs95+eWX5TUsWrRIyo0ePVqe2dDQIOUefvjh6qv9f/45FADgLUoQAOAtShAA4C1KEADgLUoQAOAtShAA4C1KEADgLUoQAOCtsDvGSA+tHz16VJ6p7tCQnZ0tz7z55pudmcE7nwwbNswWLlzoPOf48ePyGh577DEp99RTT8kz1Qd5E8aOHSs9yNrU1CTP3Lp1q5Rbv369PLOqqkrOJmRkZNj48eOdub1798oz16xZI+XU3YDMzO666y5n5o033kgejxw50h555BHnOWE+D4cOHZJyBw8elGd++MMflnLPPPOMmZkNDAxYR0eHM79nzx55DeqD4mE275gzZ46UG/wdd/HiRfvtb3/rPEd9H8zMWltbpVyYzSCU3Z66urqSxwUFBfaZz3zGeU6Y3YvUjQvCdMjdd98tZ6+GX4IAAG9RggAAb1GCAABvUYIAAG9RggAAb1GCAABvUYIAAG9RggAAb1GCAABvUYIAAG+F2jYtFovZyJEj3UOj+tjGxkYpV19fL89cuXKlM3P+/PnkcVdX199tXfVuXn31VXkNTz/9tJQLc12lpaVSrra21szMOjo6bOfOnc78Sy+9JK9B3Tatra1NnqlsxfePotGo5efnO3OzZ8+WZ1ZWVkq5EydOyDNXrFjhzDz//PPJ497eXqupqXGeU1RUJK+hpaVFynV2dsozw2pqarINGzY4c2fOnJFn5uTkSLkbbrhBnnnPPfdIuc2bNyePW1pa7LnnnnOek5Ki/+ZITU2VcuPGjZNnKt/dg7e2i8fjNm3aNOc53d3d8hoOHDgg5Xbt2iXPnDp1qpy9Gn4JAgC8RQkCALxFCQIAvEUJAgC8RQkCALxFCQIAvEUJAgC8RQkCALxFCQIAvBVqx5hoNCrt7tHQ0CDPvOaaa6TcX/7yF3lma2urMzN4Z4SmpibbtGmT85zDhw/La1B36YjFYvLMyZMnS7nEjjGVlZW2ePFiZ37wa+FSUVEh5SZMmCDPHDt2rJQ7ePBg8jgIAuvv73eek5WVJa9jypQpUi7MDjvLly93Zqqrq5PHZ8+etS9/+cvOc8aMGSOvYf78+VJO2dElYdSoUXLWzKynp8fOnj3rzIXZ3eXOO++UcmF2FJkzZ46cTaioqLBVq1Y5c319ffLM5uZmKad8BhKUXZxef/315HF6erp0n/35z3+W15CXlyfl9u3bJ888dOiQnL0afgkCALxFCQIAvEUJAgC8RQkCALxFCQIAvEUJAgC8RQkCALxFCQIAvEUJAgC8RQkCALwVatu0zs5O27NnjzOnbo1jpm9Vdfz4cXmmssbBOjs7bffu3c5cSor+d4bCwkIpl5mZKc+MRkO9XdbX1ydtYTdu3Dh55owZM6RcaWmpPDPMdlIJ5eXltnbtWmfuhRdekGfW1dVJuVtuuUWeqWwp1d7enjwuLy+3Bx988F+6hvLycimnbDf4zyooKLDbbrvNmfvWt74lz1S32wuz5eJ3vvMdOZtQXFxs9913nzMX5jvs6NGjUk7Zii5B+ZwN/o5raWmxLVu2OM8J8x2mUrfTNDPbtm3b/+nP4pcgAMBblCAAwFuUIADAW5QgAMBblCAAwFuUIADAW5QgAMBblCAAwFuUIADAW5EgCPRwJNJgZtX/vuX8R10TBEGR2ZC7LrN3rm2oXpfZkHvPhup1mXEvvtcM1esyG3Rtg4UqQQAAhhL+ORQA4C1KEADgLUoQAOAtShAA4C1KEADgLUoQAOAtShAA4C1KEADgLUoQAOAtShAA4C1KEADgLUoQAOAtShAA4C1KEADgLUoQAOAtShAA4C1KEADgLUoQAOAtShAA4K1omHAsFgvi8fi/dAEpKVoPB0Egz8zOznZmmpubrbOzM2JmNnz48KCiokKer6irq5NyTU1N8sxIJCLl+vr6GoMgKIpEIoFyjvoemJnl5uZKuaysLHmm+t6eP3++MQiCIjOznJycoKCgwHlORkaGvA7lvjEzGxgYkGd2dXU5M7W1tdbS0hIxM0tPTw+U1y49PV1eQywWk3KpqanyTPU1qK6ubgyCoCg3NzcoKSlx5tPS0uQ1XL58WcrV19fLM/v7+6XcpUuXkvei+p6F+e5UXwf1vVXV1dVZa2trxMwsIyNDuq6enh55vvqehfn+yMvLk3KJe/Ef/3+oEozH4zZ37lxnLkxhqTdGmBd6zpw5zsyPf/zj5HFFRYW99NJLznPCXNfatWul3K9//Wt5pvrFV19fX212pTSVD0lmZqa8ho9//ONSbtasWfJM9YOxatWq6sRxQUGBPfjgg85zJk6cKK9j9uzZUk4ptoQjR444M/fcc0/yOCsryxYuXOg85/3vf7+8BvUveDk5OfJM9fO4dOnSajOzkpISe+yxx5z5ESNGyGs4deqUlHv00UflmS0tLVLuwIEDyXsxKyvLPvrRjzrPmT59uryO4uJiKTd69Gh5Zl9fnzNz3333JY+zsrLs1ltvdZ5TXV3tzCScPHlSyt10003yzE996lNS7t57773qQvnnUACAtyhBAIC3KEEAgLcoQQCAtyhBAIC3KEEAgLcoQQCAtyhBAIC3Qj0sn5OTY/Pnz3fmWltb5Zn79++XcmEeVL/tttucmV/84hfJ47S0NCsrK3Oes2XLFnkN6nWFea3GjBkj5RI7ZEydOtV27NjhzKsP9puZPf7441JO2VQh4f7775dyq1atSh6//fbb9u1vf1v+MxTqg9pLly6VZyq7WQzeLKC3t9dqamqc57z55pvyGtTPzuc//3l55le+8hUpl3itYrGYjRw50plXH6Y2M3vyySel3N69e+WZDzzwgJQ7cODA3/238hpv2rRJXsfFixel3ODPhIvyORu8YUIQBNbd3e08R920wMyssLBQyt15553yzDDfNVfDL0EAgLcoQQCAtyhBAIC3KEEAgLcoQQCAtyhBAIC3KEEAgLcoQQCAtyhBAIC3KEEAgLdCbZuWkpJi8XjcmWtubpZnHjlyRMpNnTpVnjlu3DhnJiMjI3nc398vbV9WW1srr6G6ulrOqkpLS6XcW2+9ZWZmAwMD1tvb68wvWLBAXsP3v/99Kads15agbMX3j4qLi+2LX/xi6PP+N6+88oqU++UvfynPXLNmjTOTmpqaPM7MzLRJkyY5zzl48KC8BnXbsDDbAqpb+CX09vbahQsXnLk//OEP8szNmzdLudmzZ8szv/nNb0q51atXJ4+HDRtmn/jEJ5znnD9/Xl7HG2+8IeXCbFlWVVXlzAzeJq23t1f6zqurq5PXMGrUKCk3a9Yseea1114rZ6+GX4IAAG9RggAAb1GCAABvUYIAAG9RggAAb1GCAABvUYIAAG9RggAAb1GCAABvhdoxJj09XXriv6WlRZ555swZKVdYWCjP3L59uzPT1taWPFZ3RnjzzTflNbS3t0u5YcOGyTPVXTp27txpZlfeh+eee86ZHzFihLwGdb3RqH5r7du3T84mFBcX24oVK5y5S5cuyTN7enqk3GuvvSbPVO6Zy5cvJ48zMzPthhtucJ5TUFAgr6GpqUnKqbs3mZn99Kc/lbNmV96HAwcOOHOHDh2SZ5aUlEi5xYsXyzMH796jKiwstC984QvOXJjP2cqVK6Wc8l2XkJmZ6cw0NjYmjwcGBkJ9fhTq7i6RSESe2d/f/88ux8z4JQgA8BglCADwFiUIAPAWJQgA8BYlCADwFiUIAPAWJQgA8BYlCADwFiUIAPAWJQgA8FbobdOUrbuULcgS1K2PwmyNc/ToUWemq6sredzZ2Wl/+9vfnOecO3dOXoO6/dLIkSPlmcXFxXLWzCw7O9tmz57tzFVVVckzp0+fLuXC3AMDAwNyNqGrq0vaYuuvf/2rPHPKlClSTt0Sz8zsgQcekLNmZikpKdL2VtOmTZNn9vX1Sbndu3fLM8NsIWhm1traai+++KIzF2bbsrvuukvKqVt1menbOA7W39//d9swvhv1u87MbMGCBVLu8OHD8kzlPRv8vdjf32+dnZ3OcyZNmiSvYd68eVJu/Pjx8sww23ReDb8EAQDeogQBAN6iBAEA3qIEAQDeogQBAN6iBAEA3qIEAQDeogQBAN6iBAEA3ooEQaCHI5EGM6v+9y3nP+qaIAiKzIbcdZm9c21D9brMhtx7NlSvy4x78b1mqF6X2aBrGyxUCQIAMJTwz6EAAG9RggAAb1GCAABvUYIAAG9RggAAb1GCAABvUYIAAG9RggAAb1GCAABvUYIAAG9RggAAb1GCAABvUYIAAG9RggAAb1GCAABvUYIAAG9RggAAb1GCAABvUYIAAG9Fw4TT09ODrKwsZ66/v1+eOTAwEGYJ/7KZ3d3d1tfXFzEzi0QiQUqK++8DhYWF8hpyc3OlXF9fnzyzra1NyjU1NTUGQVAUjUaDWCzmzCuZhLS0NCkXjeq3Vm9vr5RLXJeZWTweD5TXWJ0dRl5enpzNzs52Zs6dO2dNTU0RM7P8/PygtLTUeU53d7e8BvW+aWlpkWcqnxczs56ensYgCIqysrKC/Px8Zz7M50EV5l5UX4POzs7kvZienh7E4/F/am3vRv1eHDZsmDxT+Zw3NDRYW1tbxMwsLS1N+v64dOmSvAb1sxMEgTwzNTVVyjU3Nyffs8FClWBWVpZ97GMfc+ba29vlmeoLGIlE5JkdHR3OzLFjx5LHKSkp0pfVkiVL5DXcfvvtUq6+vl6e+cc//lHK/epXv6o2u3LTjxkzxpkfOXKkvIaysjIpV1BQIM9UX4MnnniiOnGcm5trS5cudZ5TU1Mjr0P94lm0aJE8c9asWc7MwoULk8elpaW2adMm5zlvvfWWvAb1vtm8ebM8U/3yraqqqjYzy8/PtxUrVjjzjY2N8hrUv2yXlJTIM3/3u99Jub179ybvxXg8bh/5yEec54T5Yr98+bKUU76PE5TP+de//vXkcSwWs8mTJzvP2bdvn7yG+fPnS7kwf3lVi/U3v/lN9dX+P/8cCgDwFiUIAPAWJQgA8BYlCADwFiUIAPAWJQgA8BYlCADwVqjnBLu7u6XnkyorK+WZnZ2dUi7MA6/qMzYJ06ZNs7179zpz6kOZZmaPPPKIlNu4caM8M8zDzGZXnl+aNm2aM3fo0CF55v79+6Xc3XffLc9Un7984oknksf9/f3S63H8+HF5HepzXGEe6C4uLnZmBt/b8Xjcpk6d6jxn3bp18hoOHDgg5cI8o/fBD35QylVVVZnZlXtXeQ5R+RwmZGZmSrmvfvWr8sw1a9ZIuVtvvTV5HI1GpY00wjxPpz4DGeY7afHixc7MD3/4w+TxxIkTpffj4YcfltcwYcIEKbdt2zZ5ZphNTK6GX4IAAG9RggAAb1GCAABvUYIAAG9RggAAb1GCAABvUYIAAG9RggAAb1GCAABvUYIAAG+F2jYtHo/bBz7wAWeuq6tLnqlusZaRkSHPVLYcGrz1VVtbm73yyivOczZs2CCvQdkiykzfHsnM7Pbbb5dyp06dMjOz9PR0GzNmjDN/8uRJeQ319fVSLswWbxUVFXI2ITU11XJycpy5uro6eWZHR4eUe/vtt+WZFy5ccGZ6e3uTx42NjbZ+/fpQ57ior0GYz9ikSZOk3NatW83syr04duxYZ37mzJnyGtTP2Pjx4+WZ5eXlcjYhFotJ9/D27dvlmQ0NDVIuzOfs9OnTzkx3d3fyuLe312pra53nDBs2TF7Ds88+K+XC3N/z5s2Ts1fDL0EAgLcoQQCAtyhBAIC3KEEAgLcoQQCAtyhBAIC3KEEAgLcoQQCAtyhBAIC3Qu0YM2LECPva177mzH3605+WZyo7apiZXb58WZ555MgRZ2bjxo3J4zNnztjSpUud54TZKWTixIlSTt0Fxsxs6tSpUi6xk0ZhYaEtW7bMmQ+zw8+ePXuk3MsvvyzPDLPrRUIsFrNRo0Y5c3l5efLMM2fOSDn1NTAze9/73ufMDN6ppqury44fP+48p6enR15Dbm6ulCsuLpZntre3y1kzs5KSEvvGN77hzEWj+lfSjh07pNxDDz0kz7zuuuvkbEJmZqa0g87o0aPlmeq9qNwrCVu2bHFmBn8Wa2pqbM2aNc5z9u3bJ6/hpptuknLKZzvh2LFjcvZq+CUIAPAWJQgA8BYlCADwFiUIAPAWJQgA8BYlCADwFiUIAPAWJQgA8BYlCADwFiUIAPBWqG3TotGoDR8+3JkbN27cP72gdzMwMCBnd+7c6cy8+OKLyeNoNGpFRUXOc8Jc18KFC6XctddeK888ceKEnDUzS0tLs9LSUmdu7ty58kx1y7D6+np55u7du+VsQkpKisXjcWduwoQJ8sy+vj4pV1lZKc/cunWrM9Pa2po8HhgY+Ltt1N5NJBKR19Db2yvl1O3VzPTXKiEzM9MmT57szCmf3YSMjAwpF2ZbvtraWjk7WBAEzkyYe1G5B8zMTp8+Lc9sampyZgbfi+r3x5IlS+Q1qFvzhdnKMcx2f1fDL0EAgLcoQQCAtyhBAIC3KEEAgLcoQQCAtyhBAIC3KEEAgLcoQQCAtyhBAIC3IspOB8lwJNJgZtX/vuX8R10TBEGR2ZC7LrN3rm2oXpfZkHvPhup1mXEvvtcM1esyG3Rtg4UqQQAAhhL+ORQA4C1KEADgLUoQAOAtShAA4C1KEADgLUoQAOAtShAA4C1KEADgLUoQAOAtShAA4C1KEADgLUoQAOAtShAA4C1KEADgLUoQAOAtShAA4C1KEADgLUoQAOAtShAA4C1KEADgrWiYcH5+flBWVubMZWZmyjM7OjqkXF9fnzyzp6fHmWlubraOjo6ImVlqamqQlpbmPCc1NVVeQ39/v5Tr7e2VZw4MDKjRxiAIiiKRSKCElWtPGDFihJxVqffLqVOnGoMgKDK78p5Fo+7bN8zrq9zbZmY5OTnyzEuXLjkzTU1NyXsxGo0GsVjMeU6Y90zN5uXlyTMrKyvVaGMQBEXxeDxQ5mdlZclrUF6nMDkzs+7ubil3/Pjx5L2I97ZQJVhWVmZPP/20Mzd58mR55q5du6RcQ0ODPPPcuXPOzLp165LHaWlpVlFR4TwnPz9fXsPFixelXH19vTyzra1NjVbLQy1csa1cuVLKhShsu+6666TcLbfckryuaDRqJSUlznPq6urkdXzpS1+ScgsWLJBnvv76687Mj370o+RxLBazcePGOc8pLS2V16Bm77jjDnnmokWL1Gi12ZWCXb58uTM8ffp0eQ2jR4+WcuXl5fLM06dPS7kbb7wx1GcM/3/xz6EAAG9RggAAb1GCAABvUYIAAG9RggAAb1GCAABvUYIAAG+Fek4wMzPTrr/+emdu06ZN8swXXnhByqkPn5tpz0WpD8UO1tTUJGfVZ/qUB/sTRo4cKeXOnj1rZlee1/z973/vzP/gBz+Q16A6ceKEnN25c2fo+aNGjbKf/exnztzgZ/Bcnn32WSmnPIeaMGvWLGdm8CYMGRkZNnbsWOc56enp8hrU+/bw4cPyzM9+9rNSbsOGDWZ2ZdOA/fv3O/MhHsK3mTNnSrl7771Xnjlt2jQ5i6GBX4IAAG9RggAAb1GCAABvUYIAAG9RggAAb1GCAABvUYIAAG9RggAAb1GCAABvUYIAAG+F2jZt//79lpLi7s0gCOSZU6dOlXIzZsyQZ2ZnZzszg68jOzvbPvShDznP6erqktfQ0tIi5UpKSuSZc+bMkXLLly83syvbceXk5DjzU6ZMkddQV1cn5ZYtWybPVLdY27JlS/I4EolYLBZznqPcCwnbt2+XcseOHZNnDgwMODPt7e3J47KyMvve977nPOfJJ5+U15CXlyfllC3eElavXi3lEtumpaWlSdsZ/ulPf5LXsHHjRim3fv16eabyPYChhV+CAABvUYIAAG9RggAAb1GCAABvUYIAAG9RggAAb1GCAABvUYIAAG9RggAAb4XaMSYWi1lZWZkzd/PNN8sz77jjDinX09Mjz+zt7XVm0tPTk8c5OTk2b9485zmNjY3yGpSdWszCvVbq7jqJHWOam5ulXTXC7NJRXl4u5ZSdhRK2bdsmZxOi0agVFRU5cwsWLJBnzp49W8o9+uij8szq6mpnZvC93draKr0eBw4ckNegXtf9998vzwyzE4+ZWTwetxtvvNGZa2trk2fm5uZKuUuXLskzq6qq5CyGBn4JAgC8RQkCALxFCQIAvEUJAgC8RQkCALxFCQIAvEUJAgC8RQkCALxFCQIAvEUJAgC8FWrbtIqKClu7dq0zl5+fL888c+aMlDty5Ig8U9HV1ZU8DoLAgiBwnpOXlyfPv/7666VcbW2tPPPxxx+Xs2ZXtpX65Cc/6cwtXrxYnrlu3Top9+qrr8ozN23aJOWeeuqp5HFGRoaNHz/eeU57e7u8joyMDClXU1Mjz3z++eedme7u7uRxSkqKtCVZa2urvAZ1K7DVq1fLM1977TUpt2vXLjMzS01Nlb4Xwmxzp265eOrUKXnmoUOH5CyGBn4JAgC8RQkCALxFCQIAvEUJAgC8RQkCALxFCQIAvEUJAgC8RQkCALxFCQIAvBVRdkpJhiORBjOr/vct5z/qmiAIisyG3HWZvXNtQ/W6zIbcezZUr8vMg3sR722hShAAgKGEfw4FAHiLEgQAeIsSBAB4ixIEAHiLEgQAeIsSBAB4ixIEAHiLEgQAeIsSBAB4638Axg0RoFgAaA0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 7.6 CNN的可视化\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from simple_convnet import SimpleConvNet\n",
    "\n",
    "def filter_show(filters, nx=8, margin=3, scale=10):\n",
    "    \"\"\"\n",
    "    c.f. https://gist.github.com/aidiary/07d530d5e08011832b12#file-draw_weight-py\n",
    "    \"\"\"\n",
    "    FN, C, FH, FW = filters.shape\n",
    "    ny = int(np.ceil(FN / nx))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "    for i in range(FN):\n",
    "        ax = fig.add_subplot(ny, nx, i+1, xticks=[], yticks=[])\n",
    "        ax.imshow(filters[i, 0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "network = SimpleConvNet()\n",
    "# 随机进行初始化后的权重\n",
    "filter_show(network.params['W1'])\n",
    "\n",
    "# 学习后的权重\n",
    "network.load_params(\"params.pkl\")\n",
    "filter_show(network.params['W1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于分层结构的信息提取\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.7 具有代表性的CNN\n",
    "# LeNet\n",
    "# AlexNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第8章 深度学习\n",
    "# 8.1 加深网络\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 为了导入父目录的文件而进行的设定\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "\n",
    "\n",
    "class DeepConvNet:\n",
    "    \"\"\"识别率为99%以上的高精度的ConvNet\n",
    "\n",
    "    网络结构如下所示\n",
    "        conv - relu - conv- relu - pool -\n",
    "        conv - relu - conv- relu - pool -\n",
    "        conv - relu - conv- relu - pool -\n",
    "        affine - relu - dropout - affine - dropout - softmax\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28),\n",
    "                 conv_param_1 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_2 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_3 = {'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_4 = {'filter_num':32, 'filter_size':3, 'pad':2, 'stride':1},\n",
    "                 conv_param_5 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_6 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 hidden_size=50, output_size=10):\n",
    "        # 初始化权重===========\n",
    "        # 各层的神经元平均与前一层的几个神经元有连接（TODO:自动计算）\n",
    "        pre_node_nums = np.array([1*3*3, 16*3*3, 16*3*3, 32*3*3, 32*3*3, 64*3*3, 64*4*4, hidden_size])\n",
    "        wight_init_scales = np.sqrt(2.0 / pre_node_nums)  # 使用ReLU的情况下推荐的初始值\n",
    "        \n",
    "        self.params = {}\n",
    "        pre_channel_num = input_dim[0]\n",
    "        for idx, conv_param in enumerate([conv_param_1, conv_param_2, conv_param_3, conv_param_4, conv_param_5, conv_param_6]):\n",
    "            self.params['W' + str(idx+1)] = wight_init_scales[idx] * np.random.randn(conv_param['filter_num'], pre_channel_num, conv_param['filter_size'], conv_param['filter_size'])\n",
    "            self.params['b' + str(idx+1)] = np.zeros(conv_param['filter_num'])\n",
    "            pre_channel_num = conv_param['filter_num']\n",
    "        self.params['W7'] = wight_init_scales[6] * np.random.randn(64*4*4, hidden_size)\n",
    "        self.params['b7'] = np.zeros(hidden_size)\n",
    "        self.params['W8'] = wight_init_scales[7] * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b8'] = np.zeros(output_size)\n",
    "\n",
    "        # 生成层===========\n",
    "        self.layers = []\n",
    "        self.layers.append(Convolution(self.params['W1'], self.params['b1'], \n",
    "                           conv_param_1['stride'], conv_param_1['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W2'], self.params['b2'], \n",
    "                           conv_param_2['stride'], conv_param_2['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Convolution(self.params['W3'], self.params['b3'], \n",
    "                           conv_param_3['stride'], conv_param_3['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W4'], self.params['b4'],\n",
    "                           conv_param_4['stride'], conv_param_4['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Convolution(self.params['W5'], self.params['b5'],\n",
    "                           conv_param_5['stride'], conv_param_5['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W6'], self.params['b6'],\n",
    "                           conv_param_6['stride'], conv_param_6['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Affine(self.params['W7'], self.params['b7']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Dropout(0.5))\n",
    "        self.layers.append(Affine(self.params['W8'], self.params['b8']))\n",
    "        self.layers.append(Dropout(0.5))\n",
    "        \n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x, train_flg=False):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Dropout):\n",
    "                x = layer.forward(x, train_flg)\n",
    "            else:\n",
    "                x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x, train_flg=True)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "\n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx, train_flg=False)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        tmp_layers = self.layers.copy()\n",
    "        tmp_layers.reverse()\n",
    "        for layer in tmp_layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 设定\n",
    "        grads = {}\n",
    "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
    "            grads['W' + str(i+1)] = self.layers[layer_idx].dW\n",
    "            grads['b' + str(i+1)] = self.layers[layer_idx].db\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
    "            self.layers[layer_idx].W = self.params['W' + str(i+1)]\n",
    "            self.layers[layer_idx].b = self.params['b' + str(i+1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.322156886353913\n",
      "=== epoch:1, train acc:0.127, test acc:0.142 ===\n",
      "train loss:2.300906055579133\n",
      "train loss:2.279349120568929\n",
      "train loss:2.273920013811468\n",
      "train loss:2.260374906262916\n",
      "train loss:2.273950765652722\n",
      "train loss:2.2553510042179004\n",
      "train loss:2.2515685853447094\n",
      "train loss:2.241791308609802\n",
      "train loss:2.2690657947304875\n",
      "train loss:2.2546024938487177\n",
      "train loss:2.228363424122237\n",
      "train loss:2.1768480332774494\n",
      "train loss:2.204725696329932\n",
      "train loss:2.203103769396089\n",
      "train loss:2.1317897648911552\n",
      "train loss:2.218855265966909\n",
      "train loss:2.1573115989194798\n",
      "train loss:2.0658688798282525\n",
      "train loss:2.145956441237658\n",
      "train loss:2.112340064439502\n",
      "train loss:2.099898520282849\n",
      "train loss:2.0758529277807707\n",
      "train loss:1.9573745535064346\n",
      "train loss:2.0467801906732497\n",
      "train loss:2.054521707541405\n",
      "train loss:2.09412312802795\n",
      "train loss:1.9855166480011623\n",
      "train loss:2.0386864009763515\n",
      "train loss:2.0611484583948196\n",
      "train loss:1.916858188152461\n",
      "train loss:1.9698702705054192\n",
      "train loss:1.974530867942045\n",
      "train loss:1.8721999863148724\n",
      "train loss:1.8689603727235748\n",
      "train loss:1.7925480734618569\n",
      "train loss:1.8607980467277332\n",
      "train loss:1.8224828976943812\n",
      "train loss:1.8381041767433657\n",
      "train loss:1.875120286483404\n",
      "train loss:1.853124147062972\n",
      "train loss:1.9262616732863933\n",
      "train loss:1.917233528822384\n",
      "train loss:2.0341162313743637\n",
      "train loss:1.6771805218793643\n",
      "train loss:1.8796462339987758\n",
      "train loss:1.640268505796535\n",
      "train loss:1.7946623555968992\n",
      "train loss:1.7456119569051298\n",
      "train loss:1.8463622967688773\n",
      "train loss:1.7259896367815413\n",
      "train loss:1.5994327973988336\n",
      "train loss:1.826686753125872\n",
      "train loss:1.948508828574158\n",
      "train loss:1.8716719696543003\n",
      "train loss:1.6331687071744054\n",
      "train loss:1.7546151844329159\n",
      "train loss:1.6854097772365835\n",
      "train loss:1.715714408971833\n",
      "train loss:1.8072127085864826\n",
      "train loss:1.7520887891970585\n",
      "train loss:1.631384618698042\n",
      "train loss:1.7788305760705023\n",
      "train loss:1.8182732333212772\n",
      "train loss:1.8404347453538845\n",
      "train loss:1.740001905279417\n",
      "train loss:1.767699831831368\n",
      "train loss:1.5531728683139152\n",
      "train loss:1.7691469517241445\n",
      "train loss:1.7128735323210653\n",
      "train loss:1.7016445331878651\n",
      "train loss:1.91497536102237\n",
      "train loss:1.888093195024907\n",
      "train loss:1.684098793825523\n",
      "train loss:1.6695395362923902\n",
      "train loss:1.7632793142377818\n",
      "train loss:1.6160196118727557\n",
      "train loss:1.654859017636535\n",
      "train loss:1.6717275034421775\n",
      "train loss:1.5958056680440493\n",
      "train loss:1.5917269210461547\n",
      "train loss:1.6060094206015074\n",
      "train loss:1.6114294534370357\n",
      "train loss:1.7483654288934225\n",
      "train loss:1.5308653569759485\n",
      "train loss:1.6219434893863847\n",
      "train loss:1.67305797387547\n",
      "train loss:1.5951469936363305\n",
      "train loss:1.8215958614499976\n",
      "train loss:1.5998633655100636\n",
      "train loss:1.5674992318356291\n",
      "train loss:1.5830567255473602\n",
      "train loss:1.5787383453400312\n",
      "train loss:1.4712307357041197\n",
      "train loss:1.5735121580408469\n",
      "train loss:1.5290931787596798\n",
      "train loss:1.4142065185588453\n",
      "train loss:1.5487246950980254\n",
      "train loss:1.5576227422393683\n",
      "train loss:1.5315908565480185\n",
      "train loss:1.7294182316075573\n",
      "train loss:1.4849807501422179\n",
      "train loss:1.3847866287900206\n",
      "train loss:1.5633548313803993\n",
      "train loss:1.5446741962119632\n",
      "train loss:1.4384070335743662\n",
      "train loss:1.5561733580946866\n",
      "train loss:1.5053795589707337\n",
      "train loss:1.5326512517647088\n",
      "train loss:1.542026552992019\n",
      "train loss:1.6550765784330932\n",
      "train loss:1.5021433335745942\n",
      "train loss:1.58078775578837\n",
      "train loss:1.3650782212687897\n",
      "train loss:1.4969539019509777\n",
      "train loss:1.4065176034904987\n",
      "train loss:1.3942389880157717\n",
      "train loss:1.3885957056144465\n",
      "train loss:1.3478592035482244\n",
      "train loss:1.4585126698528983\n",
      "train loss:1.6412159945682574\n",
      "train loss:1.406464457364788\n",
      "train loss:1.4694565136367095\n",
      "train loss:1.4510425364357962\n",
      "train loss:1.7452126582063086\n",
      "train loss:1.4890053486059072\n",
      "train loss:1.565866583065734\n",
      "train loss:1.513060562632249\n",
      "train loss:1.2939316436664268\n",
      "train loss:1.4103725164468361\n",
      "train loss:1.4984115509882334\n",
      "train loss:1.5491805924171893\n",
      "train loss:1.2875283158659492\n",
      "train loss:1.4740643651192502\n",
      "train loss:1.4868908634693987\n",
      "train loss:1.4591251906159206\n",
      "train loss:1.5329523652933392\n",
      "train loss:1.430805793116227\n",
      "train loss:1.4837484585163367\n",
      "train loss:1.4417556554228588\n",
      "train loss:1.506746505145887\n",
      "train loss:1.3360035413134426\n",
      "train loss:1.3218308161798589\n",
      "train loss:1.4412843425981592\n",
      "train loss:1.534680743535091\n",
      "train loss:1.4357292602459149\n",
      "train loss:1.37633048071292\n",
      "train loss:1.35610916582059\n",
      "train loss:1.2799050470697801\n",
      "train loss:1.4629493569315613\n",
      "train loss:1.3580018187533343\n",
      "train loss:1.4266107809019715\n",
      "train loss:1.2835052728781002\n",
      "train loss:1.4229378665600962\n",
      "train loss:1.3589528727344453\n",
      "train loss:1.2149311548527582\n",
      "train loss:1.303410934856829\n",
      "train loss:1.2452189896595838\n",
      "train loss:1.4624077083919877\n",
      "train loss:1.4336865250172357\n",
      "train loss:1.4591448908667195\n",
      "train loss:1.5877427364480983\n",
      "train loss:1.2643488175069166\n",
      "train loss:1.4568511357523446\n",
      "train loss:1.405150724397497\n",
      "train loss:1.4073788106677665\n",
      "train loss:1.2761216879740764\n",
      "train loss:1.6049908421818893\n",
      "train loss:1.2266764482125316\n",
      "train loss:1.488871980428707\n",
      "train loss:1.4062980715577773\n",
      "train loss:1.6297633915215757\n",
      "train loss:1.3446723705169876\n",
      "train loss:1.3569508713943501\n",
      "train loss:1.293374192429184\n",
      "train loss:1.2242385805615745\n",
      "train loss:1.4052478548678815\n",
      "train loss:1.2531857451527442\n",
      "train loss:1.2775165198407974\n",
      "train loss:1.3237736368388182\n",
      "train loss:1.4630538226945706\n",
      "train loss:1.5305213165236566\n",
      "train loss:1.209920131233142\n",
      "train loss:1.5074629887503799\n",
      "train loss:1.158689197964772\n",
      "train loss:1.3307092074207634\n",
      "train loss:1.4069793908634025\n",
      "train loss:1.4215064529615764\n",
      "train loss:1.4279628628395469\n",
      "train loss:1.267520070453481\n",
      "train loss:1.3067312751875968\n",
      "train loss:1.3664057518824813\n",
      "train loss:1.2966241232141271\n",
      "train loss:1.413220140297692\n",
      "train loss:1.3298135063876477\n",
      "train loss:1.2906147091261633\n",
      "train loss:1.297184987821323\n",
      "train loss:1.4004709155760267\n",
      "train loss:1.4202616527503216\n",
      "train loss:1.1295603427066145\n",
      "train loss:1.3902752672141425\n",
      "train loss:1.253580971158825\n",
      "train loss:1.3252307709263287\n",
      "train loss:1.3605685631113418\n",
      "train loss:1.2521023050894344\n",
      "train loss:1.3892751239749794\n",
      "train loss:1.409459990299458\n",
      "train loss:1.3287323494483503\n",
      "train loss:1.2230839994579725\n",
      "train loss:1.3373239345763477\n",
      "train loss:1.342177645094518\n",
      "train loss:1.3913845958162656\n",
      "train loss:1.3214312227421956\n",
      "train loss:1.4018016857855093\n",
      "train loss:1.349815473872482\n",
      "train loss:1.4319126517908143\n",
      "train loss:1.2105432686755875\n",
      "train loss:1.2543631060204234\n",
      "train loss:1.2152611898042096\n",
      "train loss:1.334560466927004\n",
      "train loss:1.0896414973749657\n",
      "train loss:1.3269045256769363\n",
      "train loss:1.2178091263474153\n",
      "train loss:1.0705171383593313\n",
      "train loss:1.4610774943824123\n",
      "train loss:1.196008759014534\n",
      "train loss:1.2230756634455913\n",
      "train loss:1.1921486884872343\n",
      "train loss:1.2510103634206629\n",
      "train loss:1.360027227805725\n",
      "train loss:1.3232154232267437\n",
      "train loss:1.4015595762997188\n",
      "train loss:1.22350857709637\n",
      "train loss:1.1165617768808125\n",
      "train loss:1.2855958147094981\n",
      "train loss:1.3536440798281284\n",
      "train loss:1.1765704935112409\n",
      "train loss:1.2597688925903447\n",
      "train loss:1.219233598476433\n",
      "train loss:1.3944419944703217\n",
      "train loss:1.476712331288451\n",
      "train loss:1.2232464190730103\n",
      "train loss:1.1619259250394678\n",
      "train loss:1.246862340340879\n",
      "train loss:1.0439480781463737\n",
      "train loss:1.354142695273928\n",
      "train loss:1.2326430800635222\n",
      "train loss:1.252699591017409\n",
      "train loss:1.2568749210633843\n",
      "train loss:1.2276494470073132\n",
      "train loss:1.3244368444177477\n",
      "train loss:1.1804538015678354\n",
      "train loss:1.0719831065492147\n",
      "train loss:1.409239476855962\n",
      "train loss:1.264915433813244\n",
      "train loss:1.156639632757489\n",
      "train loss:1.23773440529368\n",
      "train loss:1.316990666840416\n",
      "train loss:1.3487648447041098\n",
      "train loss:1.2940047691664487\n",
      "train loss:1.1932536358120216\n",
      "train loss:1.3169978416036707\n",
      "train loss:1.0260455882490511\n",
      "train loss:1.2823845773278089\n",
      "train loss:1.372634157248748\n",
      "train loss:1.3202864099459872\n",
      "train loss:1.1776991785700155\n",
      "train loss:1.2801291928098422\n",
      "train loss:1.2879574017220912\n",
      "train loss:1.4919244910936336\n",
      "train loss:1.3540583709107192\n",
      "train loss:1.4027633514005087\n",
      "train loss:1.2526042153027386\n",
      "train loss:1.21804176050418\n",
      "train loss:1.213222523631927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.3851802383466716\n",
      "train loss:1.3750156060707663\n",
      "train loss:1.4560404030219738\n",
      "train loss:1.248289240332672\n",
      "train loss:1.248804092798191\n",
      "train loss:1.1506293129469425\n",
      "train loss:1.3476446854269417\n",
      "train loss:1.2434738320952314\n",
      "train loss:1.165116909310695\n",
      "train loss:1.2329053833281014\n",
      "train loss:1.4669460800224365\n",
      "train loss:1.4804027597528056\n",
      "train loss:1.162581692332465\n",
      "train loss:1.4049598954163307\n",
      "train loss:1.1809034866282082\n",
      "train loss:1.1366979348869863\n",
      "train loss:1.2207406550450046\n",
      "train loss:1.1978776099601545\n",
      "train loss:1.1664665967004633\n",
      "train loss:1.247147534888372\n",
      "train loss:1.1519744100385934\n",
      "train loss:1.4381832247425013\n",
      "train loss:1.119995299054236\n",
      "train loss:1.168094561061739\n",
      "train loss:1.0945185086308915\n",
      "train loss:1.2843911780696196\n",
      "train loss:1.2826012907867281\n",
      "train loss:1.0594764315462895\n",
      "train loss:1.2259903850709006\n",
      "train loss:1.2727975852149855\n",
      "train loss:1.2471402008102692\n",
      "train loss:1.2382411638392\n",
      "train loss:1.433797596110712\n",
      "train loss:1.2051493968320481\n",
      "train loss:1.079023147443242\n",
      "train loss:1.0412812810136622\n",
      "train loss:1.3654089831706915\n",
      "train loss:1.211450083871946\n",
      "train loss:1.2213735196528308\n",
      "train loss:1.0787787128125288\n",
      "train loss:1.3759281621551267\n",
      "train loss:1.1310811575588031\n",
      "train loss:1.1505734244027281\n",
      "train loss:1.2706611207676493\n",
      "train loss:1.0472295906341165\n",
      "train loss:0.9967678616486617\n",
      "train loss:1.2111283273617284\n",
      "train loss:1.2419406191897735\n",
      "train loss:1.1961316791903713\n",
      "train loss:1.2926707791872136\n",
      "train loss:1.0352206893684541\n",
      "train loss:1.0512773979763577\n",
      "train loss:1.246664546114864\n",
      "train loss:1.0387374484447132\n",
      "train loss:1.2544140517272246\n",
      "train loss:1.0900942984590734\n",
      "train loss:1.037628925690822\n",
      "train loss:1.0842638454245204\n",
      "train loss:1.3828678651048392\n",
      "train loss:1.0936931060462585\n",
      "train loss:1.3400061092700453\n",
      "train loss:1.2234246833081093\n",
      "train loss:1.2128572592753177\n",
      "train loss:1.1849213321017087\n",
      "train loss:1.1816582457496425\n",
      "train loss:1.1424510479771424\n",
      "train loss:1.3730441191743799\n",
      "train loss:1.1666921055254393\n",
      "train loss:1.2655255302056865\n",
      "train loss:1.2247888797689759\n",
      "train loss:1.1052475318893453\n",
      "train loss:1.1842086106345098\n",
      "train loss:1.1542210907130417\n",
      "train loss:1.2423119541575958\n",
      "train loss:1.290580642676857\n",
      "train loss:1.2341171790252075\n",
      "train loss:1.1880977792006406\n",
      "train loss:1.3690692021786217\n",
      "train loss:1.1631023477977822\n",
      "train loss:1.1398851659057598\n",
      "train loss:1.0685858335862877\n",
      "train loss:1.126632278895312\n",
      "train loss:1.5007075317515939\n",
      "train loss:1.292145588936699\n",
      "train loss:1.1277159623855493\n",
      "train loss:1.0880536417866338\n",
      "train loss:1.3508456940273872\n",
      "train loss:1.2677308452663099\n",
      "train loss:1.2108842724457434\n",
      "train loss:1.3466228356271108\n",
      "train loss:1.1640201618394712\n",
      "train loss:1.0853077000006506\n",
      "train loss:1.1551046993022243\n",
      "train loss:1.2181252182303828\n",
      "train loss:1.2939937965459976\n",
      "train loss:1.266492259366944\n",
      "train loss:1.2690058649867857\n",
      "train loss:1.0209513457338328\n",
      "train loss:1.1914133877046285\n",
      "train loss:1.429694159730813\n",
      "train loss:1.0668161084635726\n",
      "train loss:1.1630550835731563\n",
      "train loss:1.1621867675999378\n",
      "train loss:0.9941774607664253\n",
      "train loss:1.3903086744064612\n",
      "train loss:1.170273737518825\n",
      "train loss:1.2198146893963682\n",
      "train loss:1.2327145795186336\n",
      "train loss:1.1717649127443863\n",
      "train loss:1.1703048735635757\n",
      "train loss:1.2226791972425126\n",
      "train loss:1.1118428394501476\n",
      "train loss:1.2325487168234937\n",
      "train loss:1.269540364623795\n",
      "train loss:1.0942551442420685\n",
      "train loss:1.088400052374865\n",
      "train loss:1.2190739976061977\n",
      "train loss:1.1451804054332215\n",
      "train loss:1.242428525160372\n",
      "train loss:1.1567687185662683\n",
      "train loss:1.1926078984205262\n",
      "train loss:1.4299994627241406\n",
      "train loss:1.1639486044615936\n",
      "train loss:1.1734041134277873\n",
      "train loss:1.1390868169576776\n",
      "train loss:0.9267431255241906\n",
      "train loss:1.1600921266162296\n",
      "train loss:1.3068179327182392\n",
      "train loss:1.0208303019961427\n",
      "train loss:1.079508337973098\n",
      "train loss:1.1346808330618612\n",
      "train loss:0.9159990263355641\n",
      "train loss:1.0558108548022473\n",
      "train loss:1.3174030682669597\n",
      "train loss:1.1654075886769377\n",
      "train loss:1.2366929548075625\n",
      "train loss:1.1018901062503184\n",
      "train loss:1.284324397308476\n",
      "train loss:1.1853177464831497\n",
      "train loss:1.1301574437497264\n",
      "train loss:1.0072564548071972\n",
      "train loss:1.0750549958288262\n",
      "train loss:1.1283265608411983\n",
      "train loss:1.1411749114675416\n",
      "train loss:1.0930212410984326\n",
      "train loss:1.3374241297863907\n",
      "train loss:1.1640531061735229\n",
      "train loss:1.152381132400415\n",
      "train loss:1.1488932415365467\n",
      "train loss:1.0299370212596033\n",
      "train loss:1.0490979103950542\n",
      "train loss:1.2199537969912948\n",
      "train loss:1.1365634391652295\n",
      "train loss:1.2602766466527493\n",
      "train loss:1.1596948827367282\n",
      "train loss:1.0652380619503823\n",
      "train loss:1.0716134233617738\n",
      "train loss:1.0968945844426157\n",
      "train loss:1.2880037639867488\n",
      "train loss:1.333632652484617\n",
      "train loss:1.2670441170157927\n",
      "train loss:1.2712722249905277\n",
      "train loss:1.1213610469838482\n",
      "train loss:1.2340244500114352\n",
      "train loss:1.0535630425986704\n",
      "train loss:1.0088545331133714\n",
      "train loss:1.0704465318057845\n",
      "train loss:1.037762974262784\n",
      "train loss:0.9361116947902102\n",
      "train loss:1.0031991410484233\n",
      "train loss:0.9988089856982887\n",
      "train loss:1.2755913166350716\n",
      "train loss:1.0493084080323174\n",
      "train loss:1.116974322282942\n",
      "train loss:1.0171108776510769\n",
      "train loss:1.2611204972559469\n",
      "train loss:1.0334118853564904\n",
      "train loss:1.3174644638684103\n",
      "train loss:1.158229338000071\n",
      "train loss:0.9933761892677269\n",
      "train loss:0.9893104545151982\n",
      "train loss:1.2419850234268597\n",
      "train loss:1.1147210633079778\n",
      "train loss:1.310299439796586\n",
      "train loss:0.9829307412442286\n",
      "train loss:1.0888884407758315\n",
      "train loss:1.114187805029922\n",
      "train loss:1.191687299066277\n",
      "train loss:1.125219619231495\n",
      "train loss:1.1437561807844174\n",
      "train loss:1.1518878463979754\n",
      "train loss:1.0631941592337477\n",
      "train loss:1.2818177134851396\n",
      "train loss:1.0775520036623625\n",
      "train loss:1.1341408285978096\n",
      "train loss:1.1683654045324037\n",
      "train loss:1.0424525440605412\n",
      "train loss:1.2480460104839133\n",
      "train loss:1.0006647488955023\n",
      "train loss:1.1133930943873644\n",
      "train loss:1.1909176390263854\n",
      "train loss:1.0807495494891952\n",
      "train loss:1.3515222480328988\n",
      "train loss:1.1660588841305637\n",
      "train loss:1.0945064455226836\n",
      "train loss:1.1000285030443968\n",
      "train loss:1.1965645401146259\n",
      "train loss:1.0589023833229243\n",
      "train loss:1.0996186293924253\n",
      "train loss:1.1997479408745728\n",
      "train loss:1.0305752889613569\n",
      "train loss:1.1482419322599777\n",
      "train loss:1.14616780913709\n",
      "train loss:1.0414581341171845\n",
      "train loss:1.2793965450146114\n",
      "train loss:1.1334580526719154\n",
      "train loss:0.9736720545180881\n",
      "train loss:1.2259645007286293\n",
      "train loss:1.269083398193198\n",
      "train loss:1.223413473748809\n",
      "train loss:1.1768476447604208\n",
      "train loss:1.1451248788436958\n",
      "train loss:1.1695122747388356\n",
      "train loss:1.2170872914933684\n",
      "train loss:1.261263468360091\n",
      "train loss:0.9510958969903561\n",
      "train loss:0.9770728182090829\n",
      "train loss:1.0000788727475294\n",
      "train loss:1.1755731758587724\n",
      "train loss:1.089900086414937\n",
      "train loss:1.1729936627364932\n",
      "train loss:1.1377679293464917\n",
      "train loss:1.1396403413614575\n",
      "train loss:1.1248588521504261\n",
      "train loss:1.0315093678650393\n",
      "train loss:1.000796936773105\n",
      "train loss:1.042172351488923\n",
      "train loss:1.2178067024501473\n",
      "train loss:0.9832896460234288\n",
      "train loss:1.0177331584601372\n",
      "train loss:1.0410570675077804\n",
      "train loss:1.1941163890381539\n",
      "train loss:1.2418501894889382\n",
      "train loss:1.3394369811640416\n",
      "train loss:1.2483641062954771\n",
      "train loss:1.2394779556884326\n",
      "train loss:0.9808180844394373\n",
      "train loss:1.0672145623324294\n",
      "train loss:1.3257993789280844\n",
      "train loss:1.0634640001082714\n",
      "train loss:1.255582587922129\n",
      "train loss:1.1339085570213057\n",
      "train loss:1.0896729592695262\n",
      "train loss:1.0304625525172981\n",
      "train loss:1.050090235938436\n",
      "train loss:1.4528550430563647\n",
      "train loss:0.9058712193653805\n",
      "train loss:1.0280399235330164\n",
      "train loss:1.1666301264342003\n",
      "train loss:1.019604120499548\n",
      "train loss:1.1504713846583734\n",
      "train loss:1.0491332755179825\n",
      "train loss:1.1545108140214682\n",
      "train loss:1.0767450105673904\n",
      "train loss:1.1113187084495115\n",
      "train loss:0.9804526027217855\n",
      "train loss:1.2996498236979113\n",
      "train loss:1.1572898713716944\n",
      "train loss:1.048096175266965\n",
      "train loss:1.0647485620892005\n",
      "train loss:0.9903690324843767\n",
      "train loss:1.1658677337899348\n",
      "train loss:1.1073098771069514\n",
      "train loss:1.06507050678388\n",
      "train loss:1.2311026197659671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.302524814549991\n",
      "train loss:0.8652615501606518\n",
      "train loss:0.9426570083444712\n",
      "train loss:1.0192557023440667\n",
      "train loss:1.1660022108600556\n",
      "train loss:1.1836336215787373\n",
      "train loss:1.3337490402074794\n",
      "train loss:1.1466897259997404\n",
      "train loss:1.282534282885325\n",
      "train loss:0.9817289043449658\n",
      "train loss:1.1043547931485123\n",
      "train loss:0.9639464720088102\n",
      "train loss:1.021722143772599\n",
      "train loss:1.037965926400939\n",
      "train loss:1.1704049140196857\n",
      "train loss:1.1252196098899814\n",
      "train loss:1.0572360980222368\n",
      "train loss:1.0303731136148706\n",
      "train loss:1.0861735220286781\n",
      "train loss:1.2723964083249013\n",
      "train loss:1.1448968449207393\n",
      "train loss:1.0940257883617368\n",
      "train loss:1.329830457763344\n",
      "train loss:1.1696188194960477\n",
      "train loss:1.1045251050582074\n",
      "train loss:1.2528875624736733\n",
      "train loss:1.149159093268875\n",
      "train loss:1.0710875534897493\n",
      "train loss:1.2643226148403806\n",
      "train loss:1.1391624043282311\n",
      "train loss:1.27162271123657\n",
      "train loss:1.116446181755823\n",
      "train loss:1.019482662412458\n",
      "train loss:1.1100042009616862\n",
      "train loss:0.9757657871474424\n",
      "train loss:1.1255390741867224\n",
      "train loss:0.8881970883966129\n",
      "train loss:1.2324259634439003\n",
      "train loss:1.1608388401602239\n",
      "train loss:1.0758498537980141\n",
      "train loss:1.0986173573908131\n",
      "train loss:1.0120065407199388\n",
      "train loss:1.2732764456635655\n",
      "train loss:1.1116094521925028\n",
      "train loss:1.2657503931412306\n",
      "train loss:1.3373635450183168\n",
      "train loss:1.1580828897430615\n",
      "train loss:1.0934921951621315\n",
      "train loss:1.0212094203760538\n",
      "train loss:1.206278358513626\n",
      "train loss:0.9600272711570983\n",
      "=== epoch:2, train acc:0.978, test acc:0.973 ===\n",
      "train loss:0.9840207222205042\n",
      "train loss:1.1510805165283886\n",
      "train loss:1.1170534820473372\n",
      "train loss:1.268688582823885\n",
      "train loss:1.1945920825497531\n",
      "train loss:1.1289791637482745\n",
      "train loss:1.0544197132878381\n",
      "train loss:1.1243985076656562\n",
      "train loss:1.1684985232371774\n",
      "train loss:1.030739850006351\n",
      "train loss:1.0738259056121648\n",
      "train loss:1.1376654815284153\n",
      "train loss:1.0133751641093083\n",
      "train loss:1.1324286543623223\n",
      "train loss:1.157509232125722\n",
      "train loss:1.2971979740648016\n",
      "train loss:1.075245290537959\n",
      "train loss:1.255567915481724\n",
      "train loss:1.2684422245192533\n",
      "train loss:1.0223597916417209\n",
      "train loss:1.1819932044863897\n",
      "train loss:0.870623845903035\n",
      "train loss:0.9767154496949569\n",
      "train loss:1.0301779494965329\n",
      "train loss:1.138062845490429\n",
      "train loss:1.196139702478672\n",
      "train loss:0.9270293048322148\n",
      "train loss:1.0511654466725104\n",
      "train loss:0.8521368958463913\n",
      "train loss:0.9810267736903241\n",
      "train loss:0.9564270765108056\n",
      "train loss:0.984527051559049\n",
      "train loss:0.9073760701899196\n",
      "train loss:1.0652485467804766\n",
      "train loss:1.038971471444438\n",
      "train loss:1.0840893509526983\n",
      "train loss:1.0901746050391798\n",
      "train loss:1.15770158126542\n",
      "train loss:1.0913955880841555\n",
      "train loss:0.8808083174584246\n",
      "train loss:1.0198664591866398\n",
      "train loss:1.0395178207463562\n",
      "train loss:1.0827861985283662\n",
      "train loss:1.1920237725238279\n",
      "train loss:1.2113901839921275\n",
      "train loss:0.9773436505856777\n",
      "train loss:1.0366619595615871\n",
      "train loss:1.0808773502764744\n",
      "train loss:1.1422287625644854\n",
      "train loss:1.3970000436044803\n",
      "train loss:0.978780190488562\n",
      "train loss:1.2612999366515052\n",
      "train loss:1.1038376467193363\n",
      "train loss:0.8909868664693005\n",
      "train loss:0.9457623953527998\n",
      "train loss:0.9464293456496294\n",
      "train loss:1.1712306732608835\n",
      "train loss:1.129124749396605\n",
      "train loss:0.9765982645046527\n",
      "train loss:1.0313673099534844\n",
      "train loss:0.9357524155400508\n",
      "train loss:1.1348956923240765\n",
      "train loss:1.0126667190239436\n",
      "train loss:1.1516719671980276\n",
      "train loss:1.0084165308849953\n",
      "train loss:1.0455984220860473\n",
      "train loss:1.0805577686668177\n",
      "train loss:1.1217002393312714\n",
      "train loss:1.0587982045156321\n",
      "train loss:1.035965466765733\n",
      "train loss:1.1113519671981016\n",
      "train loss:1.2219775078314634\n",
      "train loss:1.0428100950484351\n",
      "train loss:1.1196034077236698\n",
      "train loss:1.015729150020355\n",
      "train loss:1.1630577170948326\n",
      "train loss:1.0522646638899629\n",
      "train loss:1.0226232397625596\n",
      "train loss:1.0806959312800202\n",
      "train loss:1.0059180246991604\n",
      "train loss:0.9778504787855168\n",
      "train loss:1.004769836877766\n",
      "train loss:1.089111232910914\n",
      "train loss:0.9925343164501131\n",
      "train loss:1.1424389556091392\n",
      "train loss:1.1646423908366221\n",
      "train loss:1.064369117611986\n",
      "train loss:1.022336294226586\n",
      "train loss:1.1212985152783832\n",
      "train loss:0.918509114871323\n",
      "train loss:0.9720200859452658\n",
      "train loss:1.0236423128057328\n",
      "train loss:1.107015575111268\n",
      "train loss:1.1837979015273332\n",
      "train loss:1.28961782686024\n",
      "train loss:1.050396370180249\n",
      "train loss:0.9361590308838342\n",
      "train loss:1.153951534467013\n",
      "train loss:0.9260449360078858\n",
      "train loss:1.0354104371521644\n",
      "train loss:1.0373809557682232\n",
      "train loss:0.9411263568691807\n",
      "train loss:0.931422506475981\n",
      "train loss:1.1033548606829975\n",
      "train loss:1.0994024245273382\n",
      "train loss:1.1624374134629152\n",
      "train loss:1.1391234103423802\n",
      "train loss:1.0511666324007225\n",
      "train loss:1.2483472420582828\n",
      "train loss:1.2045676007623762\n",
      "train loss:1.0481099495047572\n",
      "train loss:1.190094290886719\n",
      "train loss:0.9271218043037476\n",
      "train loss:1.0938861170261953\n",
      "train loss:1.0855938852140137\n",
      "train loss:1.1594294749684217\n",
      "train loss:1.122000526048113\n",
      "train loss:1.0784853548704254\n",
      "train loss:1.1533694103781789\n",
      "train loss:0.9024867922396624\n",
      "train loss:0.8759673346432063\n",
      "train loss:1.0927588880086518\n",
      "train loss:1.1310810937057745\n",
      "train loss:1.084343928425384\n",
      "train loss:1.0128355546426764\n",
      "train loss:1.0565408158442209\n",
      "train loss:1.0576080361090718\n",
      "train loss:1.0857676481622371\n",
      "train loss:1.0688710521795202\n",
      "train loss:1.0812201018495038\n",
      "train loss:1.1355732063406838\n",
      "train loss:1.171481023592\n",
      "train loss:1.1465463239512097\n",
      "train loss:1.0195520641500893\n",
      "train loss:1.148819028662798\n",
      "train loss:1.0846629766210796\n",
      "train loss:1.0003918256520508\n",
      "train loss:0.904963488366347\n",
      "train loss:1.2151895838771825\n",
      "train loss:0.9979268726843671\n",
      "train loss:1.181343520445971\n",
      "train loss:0.9956028829527487\n",
      "train loss:1.1031561193408328\n",
      "train loss:1.0416698067626393\n",
      "train loss:1.0452278475382741\n",
      "train loss:1.0345681362974788\n",
      "train loss:1.0848403412405252\n",
      "train loss:1.2590296484497159\n",
      "train loss:1.149435258992287\n",
      "train loss:1.0474083404562768\n",
      "train loss:1.1957054152598783\n",
      "train loss:1.0107458209208684\n",
      "train loss:1.1246426673719063\n",
      "train loss:1.0088639807491373\n",
      "train loss:1.0009322644137586\n",
      "train loss:1.0852207348848781\n",
      "train loss:0.8945521757675461\n",
      "train loss:0.8946360744892402\n",
      "train loss:1.0240040056700677\n",
      "train loss:0.8284319774314549\n",
      "train loss:0.8658104692229053\n",
      "train loss:0.9405168802934587\n",
      "train loss:1.1965500783916907\n",
      "train loss:1.2903492997526262\n",
      "train loss:1.1415168637014133\n",
      "train loss:1.002507790134501\n",
      "train loss:1.0436221937556096\n",
      "train loss:0.9745479031886064\n",
      "train loss:1.1470230718761143\n",
      "train loss:1.1048766538583092\n",
      "train loss:1.109475846369665\n",
      "train loss:1.050423997972694\n",
      "train loss:1.2355299100363173\n",
      "train loss:0.9761255790352934\n",
      "train loss:1.0908142452892888\n",
      "train loss:0.9832110937116921\n",
      "train loss:1.1322680401583411\n",
      "train loss:1.1163240423027763\n",
      "train loss:0.883643463971086\n",
      "train loss:1.1988428693984579\n",
      "train loss:1.2144077831641595\n",
      "train loss:0.9017696296536518\n",
      "train loss:0.921045497859688\n",
      "train loss:1.042075834153326\n",
      "train loss:0.9928977060180924\n",
      "train loss:0.9118895493888134\n",
      "train loss:0.9574227582401369\n",
      "train loss:1.3159949349989393\n",
      "train loss:1.1727585594077836\n",
      "train loss:1.0264022926492027\n",
      "train loss:0.9996244776719078\n",
      "train loss:1.2208215177720025\n",
      "train loss:1.1090297134889489\n",
      "train loss:1.1518416086082235\n",
      "train loss:1.053295632798653\n",
      "train loss:1.1069580415936984\n",
      "train loss:1.1011603785498327\n",
      "train loss:1.0692394428082421\n",
      "train loss:1.049087127155716\n",
      "train loss:1.1102928129855734\n",
      "train loss:1.0593094530877178\n",
      "train loss:1.0091955467923888\n",
      "train loss:1.3537148995817785\n",
      "train loss:1.0276053385941186\n",
      "train loss:1.11573410381065\n",
      "train loss:1.0190396624660731\n",
      "train loss:0.9586121590520793\n",
      "train loss:0.8928846301809557\n",
      "train loss:1.0257815918933517\n",
      "train loss:1.0180116903520713\n",
      "train loss:0.9313850445422321\n",
      "train loss:1.132987107696377\n",
      "train loss:0.9354667974987346\n",
      "train loss:1.024071199317021\n",
      "train loss:0.9854301299915962\n",
      "train loss:0.9957480250165888\n",
      "train loss:1.1313603850702882\n",
      "train loss:0.9741755841451798\n",
      "train loss:1.1331511796188156\n",
      "train loss:1.0339081783279962\n",
      "train loss:0.9475422968951296\n",
      "train loss:1.0328658504166632\n",
      "train loss:1.0503834508084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.0144706147963174\n",
      "train loss:1.037387597043324\n",
      "train loss:1.0987826359860602\n",
      "train loss:1.1633532208081185\n",
      "train loss:0.8845879021771207\n",
      "train loss:1.2658720518408286\n",
      "train loss:0.979067136292126\n",
      "train loss:1.109176427984227\n",
      "train loss:0.9846129956695252\n",
      "train loss:1.1321195176729995\n",
      "train loss:1.014651599833903\n",
      "train loss:1.064186938333358\n",
      "train loss:1.095494811220455\n",
      "train loss:1.040076677494374\n",
      "train loss:0.9400142290382888\n",
      "train loss:1.0293642988464842\n",
      "train loss:1.1145611077579367\n",
      "train loss:1.0236416007942881\n",
      "train loss:0.9967931907058986\n",
      "train loss:1.0962370834524378\n",
      "train loss:1.1522596995906411\n",
      "train loss:0.9044491745643558\n",
      "train loss:0.9303680430615993\n",
      "train loss:1.13081259443769\n",
      "train loss:1.0733581572887971\n",
      "train loss:1.0188552507873183\n",
      "train loss:1.004658014658092\n",
      "train loss:1.0322706749679411\n",
      "train loss:1.2005593048540955\n",
      "train loss:1.0507581525885974\n",
      "train loss:0.908486192979034\n",
      "train loss:0.8564504164056003\n",
      "train loss:1.1545824952045174\n",
      "train loss:1.0651401351443672\n",
      "train loss:1.0286789613533778\n",
      "train loss:0.9722994584356077\n",
      "train loss:0.8929726864589292\n",
      "train loss:0.9735345113589747\n",
      "train loss:1.2153157280158775\n",
      "train loss:0.94027220985469\n",
      "train loss:1.099264745723407\n",
      "train loss:0.912723018587405\n",
      "train loss:1.0906344294242318\n",
      "train loss:1.1109166803205273\n",
      "train loss:0.9666507403453096\n",
      "train loss:0.9071284270056553\n",
      "train loss:1.107141573637163\n",
      "train loss:1.0580079785530945\n",
      "train loss:1.1226104528564904\n",
      "train loss:0.905164926596997\n",
      "train loss:1.17789960368886\n",
      "train loss:1.0586228756796274\n",
      "train loss:0.9660826529233362\n",
      "train loss:0.8728430738001314\n",
      "train loss:1.0524331969190177\n",
      "train loss:1.0638738591955565\n",
      "train loss:0.9612022968404758\n",
      "train loss:1.0888563002479215\n",
      "train loss:0.9763983674237121\n",
      "train loss:0.9544756239694145\n",
      "train loss:1.0520792477432026\n",
      "train loss:1.082434425747908\n",
      "train loss:1.0529915624318522\n",
      "train loss:1.0182078750948054\n",
      "train loss:1.2254091652242873\n",
      "train loss:1.2281623114122617\n",
      "train loss:1.04262778310152\n",
      "train loss:1.352849447900298\n",
      "train loss:1.120564906362098\n",
      "train loss:1.0081299954274165\n",
      "train loss:0.959829311673817\n",
      "train loss:1.2697371039119734\n",
      "train loss:0.9514178312168166\n",
      "train loss:1.0040963755136652\n",
      "train loss:1.0434528406641854\n",
      "train loss:0.9584433348989044\n",
      "train loss:1.1216153989249613\n",
      "train loss:0.9302658702055795\n",
      "train loss:1.1497118538759603\n",
      "train loss:0.9776542350006655\n",
      "train loss:1.0107669179868037\n",
      "train loss:0.900183422942446\n",
      "train loss:0.9314156802970268\n",
      "train loss:0.9697687959092621\n",
      "train loss:0.9996687910685064\n",
      "train loss:0.8431941778128372\n",
      "train loss:1.0116737545070196\n",
      "train loss:1.1058355872738097\n",
      "train loss:0.8548570245134859\n",
      "train loss:1.0863813147887442\n",
      "train loss:1.0654878605202114\n",
      "train loss:1.0645151766185945\n",
      "train loss:1.0409919476367633\n",
      "train loss:1.0302549282362652\n",
      "train loss:1.0275712578863005\n",
      "train loss:1.0079632410381578\n",
      "train loss:1.0349402071558926\n",
      "train loss:1.0263226501361116\n",
      "train loss:0.8446344658438258\n",
      "train loss:0.8787432215611665\n",
      "train loss:1.1566650808094054\n",
      "train loss:1.0898106392800346\n",
      "train loss:1.2122347346450293\n",
      "train loss:1.0534778792230306\n",
      "train loss:1.0217107651276318\n",
      "train loss:1.0968100634208258\n",
      "train loss:1.0045934308782207\n",
      "train loss:1.042087097693574\n",
      "train loss:0.9568816282893233\n",
      "train loss:0.9316480613451303\n",
      "train loss:1.170667597444878\n",
      "train loss:0.9615374124022875\n",
      "train loss:1.3012086023306078\n",
      "train loss:0.9161341506908074\n",
      "train loss:0.9129604456425874\n",
      "train loss:1.2426783643185484\n",
      "train loss:1.0449226631181103\n",
      "train loss:1.0540971093785052\n",
      "train loss:1.043098845447002\n",
      "train loss:1.013544043603611\n",
      "train loss:0.8521370584721211\n",
      "train loss:0.9313449259193841\n",
      "train loss:0.8943428807582688\n",
      "train loss:1.0031934563523706\n",
      "train loss:1.0543847734368017\n",
      "train loss:1.1188553302300737\n",
      "train loss:1.0740670527146274\n",
      "train loss:1.0225816592032546\n",
      "train loss:1.1472698998955217\n",
      "train loss:0.9915091229355681\n",
      "train loss:0.995731684962829\n",
      "train loss:1.0676021938753857\n",
      "train loss:0.9493584582626953\n",
      "train loss:1.236128264323811\n",
      "train loss:1.0640981156363305\n",
      "train loss:1.0155590410582627\n",
      "train loss:0.9115722973597957\n",
      "train loss:1.0565650613694082\n",
      "train loss:1.1303807844796225\n",
      "train loss:1.1179695773461347\n",
      "train loss:1.01145269301644\n",
      "train loss:0.9942903502542151\n",
      "train loss:1.0513857080051956\n",
      "train loss:1.0211643132166448\n",
      "train loss:0.9572354162729438\n",
      "train loss:1.125505951649097\n",
      "train loss:1.0764494288482178\n",
      "train loss:1.0586616238502282\n",
      "train loss:1.0529640540593146\n",
      "train loss:1.06275427065411\n",
      "train loss:1.1116392956345746\n",
      "train loss:1.0759271616174464\n",
      "train loss:0.9422838676034356\n",
      "train loss:0.9721941283590714\n",
      "train loss:1.1287171741661164\n",
      "train loss:1.0593568432470912\n",
      "train loss:1.0739986436642213\n",
      "train loss:0.9580782649224686\n",
      "train loss:1.0052537477319816\n",
      "train loss:0.926637050740072\n",
      "train loss:1.039347989641521\n",
      "train loss:1.0012972115397643\n",
      "train loss:0.8398058811128943\n",
      "train loss:1.1763837635226198\n",
      "train loss:0.9542386184109245\n",
      "train loss:0.9438826245677937\n",
      "train loss:1.2449112516686052\n",
      "train loss:1.0509836934212669\n",
      "train loss:0.9781950534188488\n",
      "train loss:0.9473123673740069\n",
      "train loss:0.8475137846085279\n",
      "train loss:0.9171289363704931\n",
      "train loss:0.9689745221016751\n",
      "train loss:0.9727561424823078\n",
      "train loss:0.9877433081086668\n",
      "train loss:1.0578221907940872\n",
      "train loss:0.9013401014434286\n",
      "train loss:1.1284795135511576\n",
      "train loss:1.116731928126721\n",
      "train loss:0.9669966242732674\n",
      "train loss:0.9673572350959752\n",
      "train loss:1.0883966451743723\n",
      "train loss:0.9484970268192611\n",
      "train loss:1.1043054394638077\n",
      "train loss:1.0251767610716112\n",
      "train loss:1.1560346487730344\n",
      "train loss:0.995448215801466\n",
      "train loss:1.129695334233005\n",
      "train loss:1.035731826984881\n",
      "train loss:0.9065142046802118\n",
      "train loss:0.8276613428231393\n",
      "train loss:1.1334187120774137\n",
      "train loss:1.0504523603947566\n",
      "train loss:1.0683970548600292\n",
      "train loss:1.1128160042551345\n",
      "train loss:0.8673253466368128\n",
      "train loss:1.0354908992707479\n",
      "train loss:1.0111236804685118\n",
      "train loss:0.9015584093126503\n",
      "train loss:0.9349442936198971\n",
      "train loss:0.8295675508159374\n",
      "train loss:1.0088763185297678\n",
      "train loss:0.8754409874787794\n",
      "train loss:0.9737276468409027\n",
      "train loss:1.011684339140257\n",
      "train loss:0.9856203954039541\n",
      "train loss:0.9550914254915258\n",
      "train loss:1.0043462387971067\n",
      "train loss:0.992802112343831\n",
      "train loss:0.7703458176293642\n",
      "train loss:1.0597679941362146\n",
      "train loss:1.0467910004589083\n",
      "train loss:1.0672512643223544\n",
      "train loss:1.1915935799953667\n",
      "train loss:0.9885501249588765\n",
      "train loss:0.9917282439352051\n",
      "train loss:1.0260027515832897\n",
      "train loss:1.038125860758317\n",
      "train loss:1.0805671668780952\n",
      "train loss:0.7970018178594266\n",
      "train loss:1.0231669519340019\n",
      "train loss:1.117408031448377\n",
      "train loss:1.018435099476196\n",
      "train loss:0.9550862763912294\n",
      "train loss:1.0684867815239973\n",
      "train loss:1.0595438631700762\n",
      "train loss:0.942996893982608\n",
      "train loss:1.0600353036479873\n",
      "train loss:1.1246170545163252\n",
      "train loss:0.9466888561716885\n",
      "train loss:1.0414500995653537\n",
      "train loss:0.860252414218237\n",
      "train loss:1.0246106966643909\n",
      "train loss:1.1622380821728542\n",
      "train loss:0.8917061356761953\n",
      "train loss:0.9675482453371859\n",
      "train loss:1.0286516672021577\n",
      "train loss:1.0000980911499915\n",
      "train loss:0.9758083750160741\n",
      "train loss:0.9960433883767132\n",
      "train loss:0.9769531451934854\n",
      "train loss:1.0992840978438965\n",
      "train loss:1.070897926996462\n",
      "train loss:0.9323659544885807\n",
      "train loss:0.9386418914085359\n",
      "train loss:1.062106259730831\n",
      "train loss:1.0293552704135605\n",
      "train loss:0.9006335799062356\n",
      "train loss:0.9237058310970113\n",
      "train loss:0.9297606966940157\n",
      "train loss:1.2276046705778854\n",
      "train loss:1.1965144946411443\n",
      "train loss:1.0486483422115493\n",
      "train loss:0.877876489547301\n",
      "train loss:0.9147568928484578\n",
      "train loss:0.7899564139568066\n",
      "train loss:0.9578860195520511\n",
      "train loss:0.9476711478548296\n",
      "train loss:0.9060093676212879\n",
      "train loss:0.971806706859133\n",
      "train loss:1.1007803129863398\n",
      "train loss:1.0365282387999901\n",
      "train loss:0.8936620251108452\n",
      "train loss:0.8854775082955547\n",
      "train loss:0.9937485281591634\n",
      "train loss:0.8553084240337122\n",
      "train loss:0.8557181688568035\n",
      "train loss:0.7659290146566538\n",
      "train loss:1.0656479166997688\n",
      "train loss:0.8244271798125989\n",
      "train loss:0.9515508615997288\n",
      "train loss:1.055820039254173\n",
      "train loss:1.0519953131303275\n",
      "train loss:1.0354318829244604\n",
      "train loss:1.0387485860336994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.043206525659677\n",
      "train loss:1.0875044896899653\n",
      "train loss:1.0621256410576871\n",
      "train loss:1.0075010321934808\n",
      "train loss:1.0788609723288294\n",
      "train loss:0.8895848326152028\n",
      "train loss:0.9095898730810267\n",
      "train loss:1.0985891549337539\n",
      "train loss:0.8869788209063535\n",
      "train loss:0.9607103955120432\n",
      "train loss:0.8953853448712927\n",
      "train loss:0.9861696110655488\n",
      "train loss:1.1478963895292047\n",
      "train loss:1.0925459440032137\n",
      "train loss:0.9639381780547802\n",
      "train loss:1.1767287371056916\n",
      "train loss:1.0974489307898951\n",
      "train loss:1.016725093617097\n",
      "train loss:0.8921841601781695\n",
      "train loss:0.9968641873192453\n",
      "train loss:0.9693729481733854\n",
      "train loss:1.0989598095933382\n",
      "train loss:1.0098427597758521\n",
      "train loss:0.8334470449172684\n",
      "train loss:0.9423634355538362\n",
      "train loss:1.0956408892876526\n",
      "train loss:0.8131283101469087\n",
      "train loss:1.0273313974834424\n",
      "train loss:0.8910620646681653\n",
      "train loss:1.0358077421423173\n",
      "train loss:1.0860296433371384\n",
      "train loss:0.8090628276746373\n",
      "train loss:1.021379272931802\n",
      "train loss:1.058655784127192\n",
      "train loss:1.0465518134772838\n",
      "train loss:1.0192471401734446\n",
      "train loss:0.9254027859864188\n",
      "train loss:1.0016766566670132\n",
      "train loss:1.047315860938013\n",
      "train loss:1.1317266016638499\n",
      "train loss:1.00796262772735\n",
      "train loss:0.9808749205483888\n",
      "train loss:1.0492170509786571\n",
      "train loss:0.9642754699808195\n",
      "train loss:1.0448061378788172\n",
      "train loss:1.0793945518411667\n",
      "train loss:0.9095365769162699\n",
      "train loss:1.0175304314021714\n",
      "train loss:1.1012141682235117\n",
      "train loss:0.7923540167220041\n",
      "train loss:0.9229921889635915\n",
      "train loss:1.2918924095594297\n",
      "train loss:1.1469315620333909\n",
      "train loss:1.0074937374766921\n",
      "train loss:0.8198400817125829\n",
      "train loss:1.0102264134539123\n",
      "train loss:0.9952580274031206\n",
      "train loss:0.8950970054716705\n",
      "train loss:1.0150636657993544\n",
      "train loss:0.9524190258987203\n",
      "train loss:1.042692943164716\n",
      "train loss:1.1567922800566415\n",
      "train loss:0.9952351651226925\n",
      "train loss:1.0998773536690267\n",
      "train loss:0.9873361200885918\n",
      "train loss:1.0139481473528824\n",
      "train loss:1.1059539211253588\n",
      "train loss:0.8610518605201876\n",
      "train loss:1.023906218618158\n",
      "train loss:0.854352614770892\n",
      "train loss:0.9322057817530544\n",
      "train loss:1.0938371131342384\n",
      "train loss:0.9888945294523991\n",
      "train loss:1.020696541452662\n",
      "train loss:1.1580337838032118\n",
      "train loss:1.0249856324120494\n",
      "train loss:1.1177584223637271\n",
      "train loss:0.9052798282017165\n",
      "train loss:1.0639923859835627\n",
      "train loss:0.8728611447027603\n",
      "train loss:0.9982685957984817\n",
      "train loss:1.1691851564311404\n",
      "train loss:0.9842006886479494\n",
      "train loss:0.907354357640687\n",
      "train loss:1.0511516031823553\n",
      "train loss:1.0916774196035257\n",
      "train loss:1.1003377453082643\n",
      "train loss:0.9873464130841009\n",
      "train loss:1.0866291392369136\n",
      "train loss:0.9984177574511716\n",
      "train loss:0.8804718866682651\n",
      "train loss:0.9282239270611147\n",
      "train loss:0.9410102287265704\n",
      "train loss:0.9607226826941649\n",
      "train loss:1.1464036833356575\n",
      "train loss:1.0703572050569412\n",
      "train loss:0.9498715270247519\n",
      "train loss:1.0080101567505215\n",
      "train loss:0.8853651231481336\n",
      "train loss:1.0599823488779874\n",
      "train loss:0.9860842388872969\n",
      "train loss:1.0787495635911701\n",
      "=== epoch:3, train acc:0.981, test acc:0.986 ===\n",
      "train loss:1.093660817120196\n",
      "train loss:1.1077850884303821\n",
      "train loss:0.8502514480904412\n",
      "train loss:0.8804833601414442\n",
      "train loss:1.0733916670677761\n",
      "train loss:1.130197848358654\n",
      "train loss:0.9591657884312785\n",
      "train loss:1.0579364153057462\n",
      "train loss:1.0541037445132957\n",
      "train loss:1.0739995997998704\n",
      "train loss:1.173758541873839\n",
      "train loss:1.0325852709930663\n",
      "train loss:0.9539610342430238\n",
      "train loss:0.9552402381151995\n",
      "train loss:0.9670243802491963\n",
      "train loss:1.0292202637136725\n",
      "train loss:0.938237374945346\n",
      "train loss:1.02147505102511\n",
      "train loss:1.0514557904350086\n",
      "train loss:0.9760530306660159\n",
      "train loss:0.8725909362873239\n",
      "train loss:1.0387856474951898\n",
      "train loss:0.8474395963017166\n",
      "train loss:0.8964514236772022\n",
      "train loss:1.0431944192567693\n",
      "train loss:0.9249727201605057\n",
      "train loss:0.9359694896408496\n",
      "train loss:0.7787710038507716\n",
      "train loss:1.038408173645263\n",
      "train loss:1.2392887513556265\n",
      "train loss:1.0867107336901076\n",
      "train loss:0.8327485944283249\n",
      "train loss:0.9734314485887035\n",
      "train loss:0.97923940635236\n",
      "train loss:1.0528941494703958\n",
      "train loss:0.8334087987115721\n",
      "train loss:0.9000835823714087\n",
      "train loss:0.9921611032922982\n",
      "train loss:0.9412048713601961\n",
      "train loss:1.0418194440733006\n",
      "train loss:1.0900345349571179\n",
      "train loss:1.1817667944556687\n",
      "train loss:1.0133126034072206\n",
      "train loss:0.9606783178183613\n",
      "train loss:0.91203810897914\n",
      "train loss:0.8570103470178629\n",
      "train loss:1.0502763921108949\n",
      "train loss:1.0097221669614211\n",
      "train loss:1.064029951984316\n",
      "train loss:1.1555766595429662\n",
      "train loss:0.9719742867980427\n",
      "train loss:0.8811864533728663\n",
      "train loss:1.1076547503812808\n",
      "train loss:1.0125733977535307\n",
      "train loss:0.942476663307819\n",
      "train loss:0.9910178694771328\n",
      "train loss:1.091220021658001\n",
      "train loss:1.1523843932847482\n",
      "train loss:0.8504558427753769\n",
      "train loss:0.9281520452933417\n",
      "train loss:0.7866605698576364\n",
      "train loss:1.0050997872650222\n",
      "train loss:1.0933094478370433\n",
      "train loss:0.9156103459025263\n",
      "train loss:0.8440214112578056\n",
      "train loss:0.9954338640711532\n",
      "train loss:0.9510764548316817\n",
      "train loss:1.2138658004966902\n",
      "train loss:0.9146525343334223\n",
      "train loss:1.1035904570305792\n",
      "train loss:0.8812934882034347\n",
      "train loss:1.1519559257166778\n",
      "train loss:1.0612136292661127\n",
      "train loss:0.804242987011815\n",
      "train loss:1.1362634212028035\n",
      "train loss:1.2140238827185725\n",
      "train loss:0.9344239179115642\n",
      "train loss:1.0305922558063916\n",
      "train loss:0.8416589340292296\n",
      "train loss:1.0566896688552012\n",
      "train loss:0.8800460027289216\n",
      "train loss:1.023229163048488\n",
      "train loss:1.0641677216492298\n",
      "train loss:0.8838026220288362\n",
      "train loss:0.9695941564358685\n",
      "train loss:1.061616623965444\n",
      "train loss:1.0313574584777638\n",
      "train loss:0.9972340081281648\n",
      "train loss:0.9099273890736416\n",
      "train loss:0.9634260576309395\n",
      "train loss:0.8242222601815561\n",
      "train loss:0.9939322248049258\n",
      "train loss:0.9364227172248163\n",
      "train loss:0.906042231092063\n",
      "train loss:1.122227063555561\n",
      "train loss:0.9699125380529408\n",
      "train loss:0.9141553149090454\n",
      "train loss:1.0057352311543846\n",
      "train loss:0.938465044376357\n",
      "train loss:1.1643953559004578\n",
      "train loss:0.9332506022827518\n",
      "train loss:0.9884659922681728\n",
      "train loss:1.0885890682879782\n",
      "train loss:1.049820587245661\n",
      "train loss:1.0861928251577644\n",
      "train loss:0.8751481662761749\n",
      "train loss:1.0451509100406808\n",
      "train loss:0.9573282333912604\n",
      "train loss:1.021700787854745\n",
      "train loss:0.7848130899540264\n",
      "train loss:0.9634064735265698\n",
      "train loss:0.8898348484086085\n",
      "train loss:1.0663264238010897\n",
      "train loss:1.072830052555334\n",
      "train loss:0.993801890888696\n",
      "train loss:0.8673170075811635\n",
      "train loss:0.9783809788259101\n",
      "train loss:1.0340728091676343\n",
      "train loss:1.1290205392438408\n",
      "train loss:0.9357861575479929\n",
      "train loss:1.0288886718254218\n",
      "train loss:0.8252704806953371\n",
      "train loss:0.8736505724679751\n",
      "train loss:0.9378326352626287\n",
      "train loss:0.9312579395537607\n",
      "train loss:0.9255045041472526\n",
      "train loss:0.9465201155666949\n",
      "train loss:0.9187874861830905\n",
      "train loss:0.8217196457767117\n",
      "train loss:0.8363844731316157\n",
      "train loss:0.946103792627908\n",
      "train loss:1.0291608678861326\n",
      "train loss:0.8078375023279333\n",
      "train loss:0.860759569114586\n",
      "train loss:0.9257194703463973\n",
      "train loss:0.9262292798184585\n",
      "train loss:1.1031556188679574\n",
      "train loss:1.1057441363484575\n",
      "train loss:1.1716918021676508\n",
      "train loss:1.0304250257459235\n",
      "train loss:1.0311213221204512\n",
      "train loss:1.0171238909614064\n",
      "train loss:0.9704498310573776\n",
      "train loss:0.9107648048971196\n",
      "train loss:0.9571624513607669\n",
      "train loss:0.9063957001928814\n",
      "train loss:0.8941689511133923\n",
      "train loss:0.8796567999643785\n",
      "train loss:1.0276099367234184\n",
      "train loss:1.0125479950098704\n",
      "train loss:0.8525349603366409\n",
      "train loss:1.184848220966675\n",
      "train loss:0.7835545321283296\n",
      "train loss:0.9156639186255103\n",
      "train loss:1.0952975487335335\n",
      "train loss:0.96632468034758\n",
      "train loss:0.9907629215191196\n",
      "train loss:0.8375675217247511\n",
      "train loss:0.9246028532330243\n",
      "train loss:0.9881964447842588\n",
      "train loss:1.1628627205186823\n",
      "train loss:0.8646411881218315\n",
      "train loss:0.9711181527255622\n",
      "train loss:0.9480335235465983\n",
      "train loss:1.109423390066\n",
      "train loss:0.8965282089888527\n",
      "train loss:0.7798112983009721\n",
      "train loss:1.0305218136230525\n",
      "train loss:0.9523915693915561\n",
      "train loss:1.1785365104831191\n",
      "train loss:0.873148308190664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.097902644702257\n",
      "train loss:0.9807845318275319\n",
      "train loss:0.846816396709271\n",
      "train loss:0.9477051538450881\n",
      "train loss:0.9642970142205409\n",
      "train loss:0.8856322816288467\n",
      "train loss:1.0301333193293183\n",
      "train loss:0.9938939215819734\n",
      "train loss:1.046317459888772\n",
      "train loss:1.000272022349399\n",
      "train loss:1.0559401409687244\n",
      "train loss:0.9348025177914643\n",
      "train loss:0.9858294463666044\n",
      "train loss:0.901985701804006\n",
      "train loss:0.8278186714276405\n",
      "train loss:0.8627670511979002\n",
      "train loss:0.8160920490883511\n",
      "train loss:1.0916106556620802\n",
      "train loss:1.0462347512070747\n",
      "train loss:0.941919459949477\n",
      "train loss:1.0119841091797561\n",
      "train loss:0.8681455076925182\n",
      "train loss:1.060681807146697\n",
      "train loss:1.0671568032811785\n",
      "train loss:0.9914847029486695\n",
      "train loss:0.8344704635398104\n",
      "train loss:0.889106672044733\n",
      "train loss:0.9693705066380042\n",
      "train loss:0.949163213623424\n",
      "train loss:1.0329136523149411\n",
      "train loss:0.9694536777281741\n",
      "train loss:1.0567334435855746\n",
      "train loss:1.157385454855945\n",
      "train loss:0.6836731896511137\n",
      "train loss:0.9419251015458279\n",
      "train loss:1.0820193384282375\n",
      "train loss:0.9006044671360417\n",
      "train loss:0.9223666707674867\n",
      "train loss:0.9952564823540615\n",
      "train loss:0.9796232930490021\n",
      "train loss:0.9921971928956274\n",
      "train loss:0.9853836188471955\n",
      "train loss:1.0188046435816198\n",
      "train loss:0.9935525288134006\n",
      "train loss:0.9383798899160783\n",
      "train loss:0.880807922093086\n",
      "train loss:0.9651187937400385\n",
      "train loss:0.9650675359392276\n",
      "train loss:0.9264997938043706\n",
      "train loss:0.8276758048437958\n",
      "train loss:1.1114124641178207\n",
      "train loss:0.9982329850803091\n",
      "train loss:1.292717143061305\n",
      "train loss:0.9142135076483222\n",
      "train loss:1.023250192326992\n",
      "train loss:1.058696095042862\n",
      "train loss:0.9215607044152918\n",
      "train loss:0.8585075466858417\n",
      "train loss:1.1224375918922544\n",
      "train loss:0.7843917786901292\n",
      "train loss:1.0898485017857906\n",
      "train loss:0.9770344314270514\n",
      "train loss:1.0167353512423665\n",
      "train loss:0.9701306062929048\n",
      "train loss:1.0286173635716498\n",
      "train loss:0.8045776050170721\n",
      "train loss:1.0184007554332852\n",
      "train loss:0.9674936221139282\n",
      "train loss:0.8664235702888458\n",
      "train loss:1.0989765814505827\n",
      "train loss:1.2307714978121072\n",
      "train loss:0.8916992128929799\n",
      "train loss:0.9563488857588018\n",
      "train loss:1.1007108592065256\n",
      "train loss:1.0346213569701135\n",
      "train loss:0.9340048787075494\n",
      "train loss:1.0026211415395883\n",
      "train loss:0.970963180537234\n",
      "train loss:0.7564446709234071\n",
      "train loss:1.1276767942795518\n",
      "train loss:0.8150638699710508\n",
      "train loss:0.8910082928796179\n",
      "train loss:0.967076943552608\n",
      "train loss:0.942083500844266\n",
      "train loss:1.0813299836117045\n",
      "train loss:0.8961225687063898\n",
      "train loss:0.9643092933028207\n",
      "train loss:1.0332596658066897\n",
      "train loss:1.1073765577063304\n",
      "train loss:1.0045118240452549\n",
      "train loss:0.96503293758867\n",
      "train loss:0.9146396747193772\n",
      "train loss:0.8685723003730732\n",
      "train loss:0.9224564391558178\n",
      "train loss:1.0000201726030757\n",
      "train loss:0.8798177950462671\n",
      "train loss:1.0105638066972995\n",
      "train loss:1.065992466317875\n",
      "train loss:0.8586994081630533\n",
      "train loss:1.1647460434000758\n",
      "train loss:1.0478735628906475\n",
      "train loss:0.990940136543616\n",
      "train loss:0.92005361884803\n",
      "train loss:0.9087781683193008\n",
      "train loss:0.9971293903511134\n",
      "train loss:0.9825620133360538\n",
      "train loss:0.9605414150293693\n",
      "train loss:0.9080271267573424\n",
      "train loss:1.0219502658355155\n",
      "train loss:1.0439414950066912\n",
      "train loss:0.9463806777340816\n",
      "train loss:0.9421790285350545\n",
      "train loss:0.9022959718500985\n",
      "train loss:0.9094017276786666\n",
      "train loss:0.8681166299544715\n",
      "train loss:0.85258721242069\n",
      "train loss:0.8627480535739785\n",
      "train loss:0.942807798696067\n",
      "train loss:0.9939022050811782\n",
      "train loss:0.9967061611481886\n",
      "train loss:0.7680058832074382\n",
      "train loss:1.0342197317528778\n",
      "train loss:0.9461616106443703\n",
      "train loss:1.138077260401983\n",
      "train loss:1.0308815789800532\n",
      "train loss:0.9352033293776942\n",
      "train loss:0.9018456132325855\n",
      "train loss:0.8838830637277998\n",
      "train loss:0.8448731855807088\n",
      "train loss:0.9145870384264987\n",
      "train loss:0.8530452107624628\n",
      "train loss:1.1486220290376523\n",
      "train loss:1.1037130511779845\n",
      "train loss:0.9631049402207984\n",
      "train loss:1.0046612197296172\n",
      "train loss:0.8962818467019764\n",
      "train loss:0.8771139666544342\n",
      "train loss:1.0274704659535667\n",
      "train loss:1.1756444990901513\n",
      "train loss:1.1058391127588811\n",
      "train loss:0.894207324833744\n",
      "train loss:1.0335325359647631\n",
      "train loss:0.9144584586907\n",
      "train loss:0.9309051742709183\n",
      "train loss:1.0019918987588696\n",
      "train loss:1.043171022598772\n",
      "train loss:0.8788246882012115\n",
      "train loss:1.0077389125247034\n",
      "train loss:0.9848776051081262\n",
      "train loss:1.0056818784385053\n",
      "train loss:0.9995421382756571\n",
      "train loss:1.0482532989283544\n",
      "train loss:0.9616006964038837\n",
      "train loss:0.8339221233615592\n",
      "train loss:0.8780100571135909\n",
      "train loss:0.9378739731465611\n",
      "train loss:0.9731467423558897\n",
      "train loss:0.9141875412326409\n",
      "train loss:1.0245505817180973\n",
      "train loss:0.8142041730226529\n",
      "train loss:1.054815362501719\n",
      "train loss:0.9959030897154048\n",
      "train loss:1.054209434684298\n",
      "train loss:0.9551495078610946\n",
      "train loss:0.814117598600621\n",
      "train loss:0.9388261721864581\n",
      "train loss:0.9038578173409946\n",
      "train loss:0.9687433105252016\n",
      "train loss:0.9314748848225937\n",
      "train loss:0.9839043364499834\n",
      "train loss:0.9325165321441022\n",
      "train loss:0.9346412671758546\n",
      "train loss:0.9257457970719376\n",
      "train loss:0.9397432511858531\n",
      "train loss:1.1110601756818312\n",
      "train loss:0.9806149020515352\n",
      "train loss:1.0030143007860075\n",
      "train loss:1.0496225229827152\n",
      "train loss:0.8643165622145871\n",
      "train loss:0.8801153454341479\n",
      "train loss:1.069738866338016\n",
      "train loss:0.9843557149626031\n",
      "train loss:0.9623801694118518\n",
      "train loss:1.1003416272092128\n",
      "train loss:1.0450542852291724\n",
      "train loss:1.0204051271113588\n",
      "train loss:0.8860384035327761\n",
      "train loss:1.0920331015318099\n",
      "train loss:1.0383373863758032\n",
      "train loss:0.9804868613856698\n",
      "train loss:0.9644213672927124\n",
      "train loss:1.0681799888853165\n",
      "train loss:0.9524747235455646\n",
      "train loss:0.9510779377708636\n",
      "train loss:1.0396668716138284\n",
      "train loss:1.0047078565013283\n",
      "train loss:0.96424570310792\n",
      "train loss:0.9749145418702702\n",
      "train loss:0.9189576771598831\n",
      "train loss:1.105965539328788\n",
      "train loss:0.9020930518242881\n",
      "train loss:0.9292646485265608\n",
      "train loss:1.1069974993839864\n",
      "train loss:0.7941542978122799\n",
      "train loss:1.1427367583579262\n",
      "train loss:0.8760547284343054\n",
      "train loss:1.0446916875358967\n",
      "train loss:0.9966519305509735\n",
      "train loss:1.0059374565061656\n",
      "train loss:0.8968725694067606\n",
      "train loss:0.968519295304654\n",
      "train loss:0.8468646101927143\n",
      "train loss:0.895901672904641\n",
      "train loss:1.0353509256808726\n",
      "train loss:0.9592533834058325\n",
      "train loss:0.9239302767051746\n",
      "train loss:0.9678346012727214\n",
      "train loss:1.075596421744664\n",
      "train loss:0.8786839492221901\n",
      "train loss:0.7718934920031825\n",
      "train loss:0.9874399674654569\n",
      "train loss:1.0072678369192727\n",
      "train loss:1.116622062098795\n",
      "train loss:0.9844769143771904\n",
      "train loss:1.1413149205664468\n",
      "train loss:0.879439266502737\n",
      "train loss:1.007010668870831\n",
      "train loss:1.1422379592231375\n",
      "train loss:0.942006131999256\n",
      "train loss:1.0008276167649417\n",
      "train loss:1.0784172386744288\n",
      "train loss:0.8292620428411165\n",
      "train loss:0.9478078581951253\n",
      "train loss:1.0145170670333845\n",
      "train loss:0.9688987657917069\n",
      "train loss:0.9923645254924991\n",
      "train loss:0.8498424654217561\n",
      "train loss:1.0344029055171666\n",
      "train loss:1.0585601304967305\n",
      "train loss:0.9512534019559691\n",
      "train loss:1.1357096262174635\n",
      "train loss:0.9897809131257628\n",
      "train loss:1.1523249029473634\n",
      "train loss:1.084004140398922\n",
      "train loss:1.0599990527185612\n",
      "train loss:1.1013998057086631\n",
      "train loss:0.905599222392792\n",
      "train loss:1.028342603223086\n",
      "train loss:0.9102505637402762\n",
      "train loss:0.9790352124806706\n",
      "train loss:0.9789605007862877\n",
      "train loss:1.0299515558692292\n",
      "train loss:0.887900025270338\n",
      "train loss:1.0187809040640188\n",
      "train loss:1.2955356961506563\n",
      "train loss:1.010178422217498\n",
      "train loss:0.9513249070432789\n",
      "train loss:0.7812840266449471\n",
      "train loss:1.013662516643015\n",
      "train loss:1.207476180441767\n",
      "train loss:0.8956025961641132\n",
      "train loss:1.0160483753586829\n",
      "train loss:1.0612992986914702\n",
      "train loss:0.9094712413803349\n",
      "train loss:0.9751637457123289\n",
      "train loss:1.051262547778129\n",
      "train loss:0.9077787746929527\n",
      "train loss:0.9072553377433298\n",
      "train loss:0.9445646102652034\n",
      "train loss:0.8663853226363796\n",
      "train loss:0.8689691070948956\n",
      "train loss:0.9603769216479834\n",
      "train loss:0.8915425728075423\n",
      "train loss:1.025209315557732\n",
      "train loss:0.9334232254207592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.9613249513065341\n",
      "train loss:1.0579890983036884\n",
      "train loss:0.9364534915972791\n",
      "train loss:1.0534871971024335\n",
      "train loss:1.1027143337886107\n",
      "train loss:1.2122824777474464\n",
      "train loss:0.9315794540219459\n",
      "train loss:1.0559045463603305\n",
      "train loss:1.0400086281591563\n",
      "train loss:0.8709829202721113\n",
      "train loss:0.8425630611262105\n",
      "train loss:0.9614478175412207\n",
      "train loss:0.9827994469529326\n",
      "train loss:1.0781850504546897\n",
      "train loss:1.0343152472096897\n",
      "train loss:0.9790763207649253\n",
      "train loss:0.9845948448265616\n",
      "train loss:0.7905075984417604\n",
      "train loss:1.0202124783200726\n",
      "train loss:1.0647050513301937\n",
      "train loss:1.047362484660902\n",
      "train loss:1.0311417893460066\n",
      "train loss:0.9243105081032233\n",
      "train loss:0.9415425464897734\n",
      "train loss:1.1353697776421188\n",
      "train loss:1.0074962491170496\n",
      "train loss:0.923789389180484\n",
      "train loss:0.9186251699494364\n",
      "train loss:1.001736335114351\n",
      "train loss:1.0332703340659974\n",
      "train loss:0.8686518594660069\n",
      "train loss:0.8888358125542837\n",
      "train loss:0.9376434189628512\n",
      "train loss:0.7310025214207269\n",
      "train loss:0.8948489022429342\n",
      "train loss:0.9346653002292277\n",
      "train loss:0.8346740936601995\n",
      "train loss:1.0612647798912347\n",
      "train loss:0.9608057198701887\n",
      "train loss:0.8464463750464251\n",
      "train loss:1.1315760225082667\n",
      "train loss:0.7369456617191378\n",
      "train loss:0.9221851702379977\n",
      "train loss:1.0329574724081163\n",
      "train loss:1.1665956995550935\n",
      "train loss:0.9559907131353513\n",
      "train loss:0.9703162839120187\n",
      "train loss:1.1768098575490313\n",
      "train loss:1.084779340038147\n",
      "train loss:0.8566399055901209\n",
      "train loss:0.871699200750615\n",
      "train loss:0.8938346996290916\n",
      "train loss:1.0244603832738017\n",
      "train loss:0.9422858841219864\n",
      "train loss:1.063129637661961\n",
      "train loss:0.9255073016983111\n",
      "train loss:0.961602540214257\n",
      "train loss:0.9636404101508086\n",
      "train loss:0.9172212085058539\n",
      "train loss:0.8384633731567303\n",
      "train loss:1.04853393129372\n",
      "train loss:0.940644746688803\n",
      "train loss:0.9146343436510099\n",
      "train loss:0.9918548205768919\n",
      "train loss:0.9832547550400597\n",
      "train loss:0.8366542488001606\n",
      "train loss:0.901984127530575\n",
      "train loss:1.0595878897098392\n",
      "train loss:1.0522150071249925\n",
      "train loss:1.1041210052114623\n",
      "train loss:1.0591575477418538\n",
      "train loss:0.9076296141823869\n",
      "train loss:0.9854428611989736\n",
      "train loss:1.0872791446312216\n",
      "train loss:1.0699063312649262\n",
      "train loss:1.0348481384137782\n",
      "train loss:0.8367897196481251\n",
      "train loss:0.9419039578159196\n",
      "train loss:0.9617043923559676\n",
      "train loss:1.0833707829613253\n",
      "train loss:0.9043443774573682\n",
      "train loss:1.0896395990886016\n",
      "train loss:0.8912583895660974\n",
      "train loss:0.9559468392442922\n",
      "train loss:0.9568886961960019\n",
      "train loss:0.9134212622177045\n",
      "train loss:1.0820289005821726\n",
      "train loss:1.101486651156789\n",
      "train loss:0.9415079383054977\n",
      "train loss:1.0759333290375326\n",
      "train loss:0.9017124203425116\n",
      "train loss:0.9801146647571004\n",
      "train loss:0.8130226471422575\n",
      "train loss:0.9876956492784095\n",
      "train loss:0.9511326380894886\n",
      "train loss:0.9733161553918772\n",
      "train loss:0.865110046118508\n",
      "train loss:0.9393204761999435\n",
      "train loss:0.8570088105641702\n",
      "train loss:1.0369819012830943\n",
      "train loss:0.9693839955299821\n",
      "train loss:0.960810787004887\n",
      "train loss:0.8696064910255188\n",
      "train loss:0.9752264713157714\n",
      "train loss:1.0234570522340078\n",
      "train loss:1.0047826377946327\n",
      "train loss:0.9433878392861179\n",
      "train loss:0.8841693762021758\n",
      "train loss:0.8371484315007872\n",
      "train loss:0.9320028543601786\n",
      "train loss:0.8651510531336802\n",
      "train loss:0.8900673603663042\n",
      "train loss:0.939893436289374\n",
      "train loss:0.998107233921142\n",
      "train loss:1.0387801587811258\n",
      "train loss:0.9658398601018428\n",
      "train loss:0.9711775238288379\n",
      "train loss:1.033067558305824\n",
      "train loss:1.0095547708506765\n",
      "train loss:0.9082259239826722\n",
      "train loss:1.1010731834385679\n",
      "train loss:0.9718787440564938\n",
      "train loss:0.8209341133780392\n",
      "train loss:1.0787374501885343\n",
      "train loss:1.0866867356263819\n",
      "train loss:0.9609627098552527\n",
      "train loss:1.0598742504685845\n",
      "train loss:0.935883456102411\n",
      "train loss:1.0334714334987354\n",
      "train loss:0.9057173822999159\n",
      "train loss:0.8924067450964996\n",
      "train loss:0.9118592817297569\n",
      "train loss:0.9849352271247553\n",
      "train loss:0.8995174780095558\n",
      "train loss:1.0242076471181656\n",
      "train loss:1.0402427855254388\n",
      "train loss:0.8917695858685936\n",
      "train loss:0.9211338122895596\n",
      "train loss:1.0760007284540076\n",
      "train loss:0.9146237246411962\n",
      "train loss:0.9647246636304904\n",
      "train loss:0.8904880779567114\n",
      "train loss:0.8431664735951478\n",
      "train loss:1.084812010577449\n",
      "train loss:0.7806979956177266\n",
      "train loss:0.907563252526011\n",
      "train loss:1.049971397763105\n",
      "train loss:0.877311719665817\n",
      "train loss:0.9783282020306523\n",
      "train loss:0.9936383498399666\n",
      "train loss:0.8789014488410632\n",
      "train loss:0.9880477751026118\n",
      "train loss:1.129760681028871\n",
      "train loss:1.1522851147452782\n",
      "=== epoch:4, train acc:0.986, test acc:0.987 ===\n",
      "train loss:1.139594941624705\n",
      "train loss:1.1291512001563204\n",
      "train loss:0.86136082137407\n",
      "train loss:0.9150960478127027\n",
      "train loss:0.9890503747377744\n",
      "train loss:1.0613622213643743\n",
      "train loss:0.834807122993058\n",
      "train loss:1.0088270846657508\n",
      "train loss:0.9410218486434411\n",
      "train loss:0.8538033516806758\n",
      "train loss:1.1131838640575609\n",
      "train loss:0.8586147526878796\n",
      "train loss:0.8321171621440449\n",
      "train loss:1.0026070810190588\n",
      "train loss:0.9036817025600925\n",
      "train loss:0.9828764871126647\n",
      "train loss:1.004614865828338\n",
      "train loss:0.8375531130650197\n",
      "train loss:0.7892296208369052\n",
      "train loss:0.931868006920196\n",
      "train loss:0.8182565297602473\n",
      "train loss:0.9763371891903095\n",
      "train loss:1.035759162894838\n",
      "train loss:0.9771230334083896\n",
      "train loss:0.9115006001610605\n",
      "train loss:0.7587374588685689\n",
      "train loss:0.8215805195243909\n",
      "train loss:1.0608464202643964\n",
      "train loss:0.9473261570922372\n",
      "train loss:0.9377675760981172\n",
      "train loss:0.9384135900614532\n",
      "train loss:0.951714804157492\n",
      "train loss:0.8746960342060948\n",
      "train loss:0.8377985672779218\n",
      "train loss:0.946156223279204\n",
      "train loss:1.0488747200760509\n",
      "train loss:0.8090323411241446\n",
      "train loss:0.8679287693655574\n",
      "train loss:1.0707812735040587\n",
      "train loss:1.0256297159615564\n",
      "train loss:0.8242286480204909\n",
      "train loss:1.0049060858150498\n",
      "train loss:0.8118029218126952\n",
      "train loss:0.8815462923014235\n",
      "train loss:0.8706870239228006\n",
      "train loss:0.909847583771177\n",
      "train loss:0.8876139324574979\n",
      "train loss:0.9961261801682249\n",
      "train loss:0.7927717135260884\n",
      "train loss:1.0184500421751377\n",
      "train loss:0.9330236925427425\n",
      "train loss:0.852809191406022\n",
      "train loss:0.9577276122707716\n",
      "train loss:0.935680796597817\n",
      "train loss:0.8824543705703046\n",
      "train loss:0.9847195750110364\n",
      "train loss:0.9705586199935\n",
      "train loss:1.0613275996592348\n",
      "train loss:1.0819606753307554\n",
      "train loss:0.9746289827897194\n",
      "train loss:1.0479707493046813\n",
      "train loss:0.9586761020555339\n",
      "train loss:0.9518536560253874\n",
      "train loss:1.1063114373444412\n",
      "train loss:0.9068126790011384\n",
      "train loss:0.9167724731039973\n",
      "train loss:1.1113701575537012\n",
      "train loss:0.9563162295981497\n",
      "train loss:0.8803939289795415\n",
      "train loss:0.9325150594521733\n",
      "train loss:0.9075409655241506\n",
      "train loss:0.8386278904269018\n",
      "train loss:0.9195672555571235\n",
      "train loss:0.8619024467853162\n",
      "train loss:0.9736542155470296\n",
      "train loss:0.8330140024234097\n",
      "train loss:0.8955702751569834\n",
      "train loss:0.8524884747615187\n",
      "train loss:0.9109546859147636\n",
      "train loss:0.8970224865807076\n",
      "train loss:0.8054097304531197\n",
      "train loss:1.0397580420113042\n",
      "train loss:1.0278885318070632\n",
      "train loss:0.970053277681398\n",
      "train loss:1.0034970843422895\n",
      "train loss:0.8263541804000586\n",
      "train loss:1.0450533023660207\n",
      "train loss:1.0602791357362376\n",
      "train loss:0.9344446873757173\n",
      "train loss:0.8430180041973049\n",
      "train loss:0.9708408865590467\n",
      "train loss:0.9320646807261924\n",
      "train loss:1.1178019020853993\n",
      "train loss:0.9179401379489401\n",
      "train loss:1.0232191800809085\n",
      "train loss:1.0022120378417714\n",
      "train loss:0.877438722035859\n",
      "train loss:0.9710393715057726\n",
      "train loss:0.8815315260227874\n",
      "train loss:0.8649177648583156\n",
      "train loss:0.8429052069718882\n",
      "train loss:1.0655888536943925\n",
      "train loss:0.9319887882619371\n",
      "train loss:0.8632356469556403\n",
      "train loss:0.8305324781459852\n",
      "train loss:0.7979529332615001\n",
      "train loss:0.6813014422219669\n",
      "train loss:0.9294287715670032\n",
      "train loss:0.9848491791576148\n",
      "train loss:0.9314985060590866\n",
      "train loss:1.047623838325039\n",
      "train loss:0.9588214495533185\n",
      "train loss:0.9480253455629456\n",
      "train loss:1.0225046790245715\n",
      "train loss:0.6296498388868539\n",
      "train loss:1.0456544086197035\n",
      "train loss:0.8913147308177962\n",
      "train loss:1.097617839298004\n",
      "train loss:0.9053127194684343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.8585483812200685\n",
      "train loss:1.020227972197298\n",
      "train loss:1.0169688179371277\n",
      "train loss:0.9479411661554935\n",
      "train loss:0.829146474900184\n",
      "train loss:0.9866358685906442\n",
      "train loss:0.828247764271482\n",
      "train loss:0.8627506357344149\n",
      "train loss:1.0239761458993624\n",
      "train loss:0.9067637790190963\n",
      "train loss:0.8678014468394936\n",
      "train loss:1.1321866790356754\n",
      "train loss:1.0817039638781443\n",
      "train loss:0.9946299795702725\n",
      "train loss:0.9775993745555627\n",
      "train loss:0.920737522359833\n",
      "train loss:0.9181085297343253\n",
      "train loss:0.9047529145072802\n",
      "train loss:0.865476743801063\n",
      "train loss:0.9276853946995264\n",
      "train loss:0.9610670820866295\n",
      "train loss:1.0071281727340757\n",
      "train loss:1.0608224930220091\n",
      "train loss:1.1112310042400158\n",
      "train loss:0.8933560976427845\n",
      "train loss:0.8859179442712803\n",
      "train loss:1.0574380071999738\n",
      "train loss:0.848735276519715\n",
      "train loss:0.835270879081901\n",
      "train loss:0.9842492290694941\n",
      "train loss:0.9002683639713608\n",
      "train loss:0.8897888523404679\n",
      "train loss:0.9246749506124555\n",
      "train loss:1.007951972187938\n",
      "train loss:0.9757178510100275\n",
      "train loss:1.1128628155164055\n",
      "train loss:0.9412461612183682\n",
      "train loss:0.969521969199104\n",
      "train loss:0.959843039535556\n",
      "train loss:1.0198517522538963\n",
      "train loss:0.9512369554057853\n",
      "train loss:0.813738084718815\n",
      "train loss:0.9682604815231435\n",
      "train loss:0.8842520886781061\n",
      "train loss:1.059308724904286\n",
      "train loss:1.0467363814805455\n",
      "train loss:0.8780375756812663\n",
      "train loss:1.0163328150406472\n",
      "train loss:0.9235638564329277\n",
      "train loss:1.0447830021890432\n",
      "train loss:0.964326759380421\n",
      "train loss:0.831991391753954\n",
      "train loss:0.9748042341910839\n",
      "train loss:0.8882071279285687\n",
      "train loss:0.9383287223045962\n",
      "train loss:1.1043965165510101\n",
      "train loss:0.8355113247905315\n",
      "train loss:1.0833933249138268\n",
      "train loss:1.0206008500393409\n",
      "train loss:0.8263819096265599\n",
      "train loss:1.0754004024963415\n",
      "train loss:0.945997460796559\n",
      "train loss:1.0111569333329011\n",
      "train loss:0.9178482245151289\n",
      "train loss:0.88677819699644\n",
      "train loss:1.038347295743927\n",
      "train loss:1.0101881632012373\n",
      "train loss:0.9140022141537567\n",
      "train loss:0.9697710388751137\n",
      "train loss:0.9254909303941118\n",
      "train loss:0.994545327635431\n",
      "train loss:0.8940124597209277\n",
      "train loss:0.9622631352270572\n",
      "train loss:0.8720735749839956\n",
      "train loss:1.0025582376372266\n",
      "train loss:0.8839968870223504\n",
      "train loss:0.9366491422876352\n",
      "train loss:0.9619415758519945\n",
      "train loss:0.9185745805834419\n",
      "train loss:1.0405347941870666\n",
      "train loss:0.8881974531579224\n",
      "train loss:0.7918449303995849\n",
      "train loss:0.8316590597763455\n",
      "train loss:0.83241285715489\n",
      "train loss:0.8251672128311195\n",
      "train loss:0.8704552887858823\n",
      "train loss:1.0017313820327625\n",
      "train loss:0.919520091475618\n",
      "train loss:0.9622812398751037\n",
      "train loss:0.9739283537671145\n",
      "train loss:0.6791807565422724\n",
      "train loss:0.9556150143593511\n",
      "train loss:0.9355943653692508\n",
      "train loss:0.9263003636210161\n",
      "train loss:0.9266743039138055\n",
      "train loss:1.0800574711828475\n",
      "train loss:0.9249710954784054\n",
      "train loss:0.8988657329529717\n",
      "train loss:0.8529472516615918\n",
      "train loss:0.8767324787168504\n",
      "train loss:0.9358982055361562\n",
      "train loss:0.9663168756750007\n",
      "train loss:0.9266193360019305\n",
      "train loss:0.9218912490956847\n",
      "train loss:1.1278236438164777\n",
      "train loss:0.788451632534557\n",
      "train loss:0.9806422938031932\n",
      "train loss:0.8578837851668001\n",
      "train loss:1.0246813466064355\n",
      "train loss:1.0341628021798683\n",
      "train loss:0.9310789377501031\n",
      "train loss:0.9191372505951004\n",
      "train loss:0.9872463318473347\n",
      "train loss:0.7648024782963962\n",
      "train loss:0.9052729598005446\n",
      "train loss:0.8940224356858597\n",
      "train loss:0.800848331086796\n",
      "train loss:1.0868036579024853\n",
      "train loss:0.962330331950558\n",
      "train loss:0.9159707771379467\n",
      "train loss:0.9257832165588487\n",
      "train loss:0.8932721104472836\n",
      "train loss:0.8238027637138345\n",
      "train loss:0.8244384833859278\n",
      "train loss:0.9017909483260322\n",
      "train loss:0.8249014853360953\n",
      "train loss:1.09365674153236\n",
      "train loss:0.8823394369944378\n",
      "train loss:0.9179659555669599\n",
      "train loss:0.9734413557198947\n",
      "train loss:0.9375771778979065\n",
      "train loss:0.9653679687556946\n",
      "train loss:0.9292043219911463\n",
      "train loss:0.9150858232309338\n",
      "train loss:0.6789294317517732\n",
      "train loss:0.7716492422187593\n",
      "train loss:0.8331631676622032\n",
      "train loss:1.2144808975932861\n",
      "train loss:0.8940729649597422\n",
      "train loss:0.9047408392408823\n",
      "train loss:0.9552212922560643\n",
      "train loss:0.8878084894814648\n",
      "train loss:0.9964074797883385\n",
      "train loss:0.9863654794902637\n",
      "train loss:1.0247196468337894\n",
      "train loss:1.0239550961605999\n",
      "train loss:0.9272682119016273\n",
      "train loss:1.018144404343062\n",
      "train loss:0.8133023474728344\n",
      "train loss:0.6677286000841197\n",
      "train loss:0.9384712464854278\n",
      "train loss:0.9552304734810804\n",
      "train loss:0.9550015602612061\n",
      "train loss:1.002635812476646\n",
      "train loss:1.0487396570199545\n",
      "train loss:0.8366474460117973\n",
      "train loss:0.8241857590475722\n",
      "train loss:1.088181757016146\n",
      "train loss:0.958234411572377\n",
      "train loss:0.9242296303902131\n",
      "train loss:0.9115607372547366\n",
      "train loss:1.0308934312143536\n",
      "train loss:0.9104036730197756\n",
      "train loss:0.8388226840389069\n",
      "train loss:0.9068078843294319\n",
      "train loss:1.0068308446394174\n",
      "train loss:0.8503775881525355\n",
      "train loss:1.025722474627195\n",
      "train loss:1.072899517560238\n",
      "train loss:0.9925979230781707\n",
      "train loss:0.6587812711010819\n",
      "train loss:0.8713958115641784\n",
      "train loss:0.9353330448363613\n",
      "train loss:0.9053901063530796\n",
      "train loss:0.9881678753907573\n",
      "train loss:0.9563494804845231\n",
      "train loss:0.9072949199279009\n",
      "train loss:0.8625113069649903\n",
      "train loss:0.826041466999165\n",
      "train loss:0.9951249324708368\n",
      "train loss:0.9097622099878676\n",
      "train loss:0.9997597132644923\n",
      "train loss:0.9796615349111929\n",
      "train loss:0.8822152885822021\n",
      "train loss:0.9670364441807705\n",
      "train loss:0.886971535450325\n",
      "train loss:0.9582172344212476\n",
      "train loss:0.9338498429820049\n",
      "train loss:0.9727294209650189\n",
      "train loss:1.0543607248691915\n",
      "train loss:0.8639005242091227\n",
      "train loss:0.8675311934117589\n",
      "train loss:0.9116576653891292\n",
      "train loss:0.9400236190979434\n",
      "train loss:0.8358603529311515\n",
      "train loss:0.8257172777844471\n",
      "train loss:0.99235319512251\n",
      "train loss:0.968095130994664\n",
      "train loss:0.7872900389194843\n",
      "train loss:0.814712180184678\n",
      "train loss:0.9988835486281944\n",
      "train loss:0.9702258406759021\n",
      "train loss:0.9315942154131572\n",
      "train loss:0.8895878190851083\n",
      "train loss:0.9665104435972297\n",
      "train loss:0.8544091665777301\n",
      "train loss:1.0134044666247233\n",
      "train loss:0.8402561728467487\n",
      "train loss:0.8824344094250769\n",
      "train loss:0.8955850279703877\n",
      "train loss:1.087295791914463\n",
      "train loss:0.9176238878779422\n",
      "train loss:0.9492524963812908\n",
      "train loss:0.826831263785766\n",
      "train loss:0.977894324752275\n",
      "train loss:0.7919383742089933\n",
      "train loss:0.847262368567479\n",
      "train loss:0.947283072418962\n",
      "train loss:0.9684340343797286\n",
      "train loss:0.8821803667322426\n",
      "train loss:1.0334741421456055\n",
      "train loss:1.053065417098471\n",
      "train loss:0.8309508200068416\n",
      "train loss:0.7699881910859836\n",
      "train loss:1.1352640961790714\n",
      "train loss:0.7527521805299182\n",
      "train loss:0.8983440460565487\n",
      "train loss:1.114327701999084\n",
      "train loss:0.9791712710605023\n",
      "train loss:0.8597293799049749\n",
      "train loss:0.8522307541293812\n",
      "train loss:0.8670146822954093\n",
      "train loss:1.0120965428066935\n",
      "train loss:0.9519267108995428\n",
      "train loss:0.8083775090061515\n",
      "train loss:1.0100688982978854\n",
      "train loss:1.0078593555006539\n",
      "train loss:0.8369970956803094\n",
      "train loss:0.9989451660598432\n",
      "train loss:0.8869914664494409\n",
      "train loss:0.8376133524623942\n",
      "train loss:1.0260282836418033\n",
      "train loss:0.9004089791248394\n",
      "train loss:0.9956514547819799\n",
      "train loss:0.9846275047377644\n",
      "train loss:0.9624642990785788\n",
      "train loss:0.8646880160958099\n",
      "train loss:0.8700365147486916\n",
      "train loss:0.8754055316390489\n",
      "train loss:1.0068249594967096\n",
      "train loss:0.9208528093914922\n",
      "train loss:1.0126237616697127\n",
      "train loss:0.9833958650074578\n",
      "train loss:0.8566728685831962\n",
      "train loss:0.930891974041685\n",
      "train loss:0.9738417616001324\n",
      "train loss:0.8231073542180343\n",
      "train loss:0.8844300121304852\n",
      "train loss:0.9718582501891274\n",
      "train loss:0.8925868659206486\n",
      "train loss:0.8935129000857864\n",
      "train loss:1.0538729755105671\n",
      "train loss:0.7549290096484452\n",
      "train loss:0.8873093748912787\n",
      "train loss:1.022709516543291\n",
      "train loss:0.8361872420004302\n",
      "train loss:0.8337131635883029\n",
      "train loss:0.8550310184246225\n",
      "train loss:0.9018916945343417\n",
      "train loss:0.9091056086232301\n",
      "train loss:1.0378654907237739\n",
      "train loss:0.8649731143227324\n",
      "train loss:0.8314657598720326\n",
      "train loss:0.9656402374912758\n",
      "train loss:0.8880735935105561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.8433728244136677\n",
      "train loss:1.1103099577097368\n",
      "train loss:0.7999123711039076\n",
      "train loss:0.9236557311481154\n",
      "train loss:0.9844048113064388\n",
      "train loss:0.9195904871677911\n",
      "train loss:0.8660193142640528\n",
      "train loss:0.9687831808564372\n",
      "train loss:0.8366906807915855\n",
      "train loss:1.067893399465572\n",
      "train loss:1.0426471465795724\n",
      "train loss:1.0989559889724168\n",
      "train loss:0.8591021383257327\n",
      "train loss:0.9191285531913351\n",
      "train loss:0.8311482625624907\n",
      "train loss:0.9120219672078547\n",
      "train loss:0.9794181144828478\n",
      "train loss:0.783083270131099\n",
      "train loss:0.8934543343236266\n",
      "train loss:0.7939807684040916\n",
      "train loss:0.9903918406789763\n",
      "train loss:1.155066884685395\n",
      "train loss:1.0100215914616195\n",
      "train loss:0.9373726031661781\n",
      "train loss:0.7866114169662717\n",
      "train loss:1.1091221696005849\n",
      "train loss:0.9227568804743123\n",
      "train loss:0.9692054649898726\n",
      "train loss:1.0378717708597343\n",
      "train loss:0.9890609859160961\n",
      "train loss:0.8577557412314709\n",
      "train loss:0.7639906956366821\n",
      "train loss:0.959187915328005\n",
      "train loss:0.9465817527423216\n",
      "train loss:0.9101804381474503\n",
      "train loss:0.9009569602121489\n",
      "train loss:1.084679458557533\n",
      "train loss:1.0173234108952705\n",
      "train loss:1.0235178409671968\n",
      "train loss:0.8197040593948915\n",
      "train loss:0.7174785173092861\n",
      "train loss:1.1190943056809535\n",
      "train loss:0.9774997136231538\n",
      "train loss:0.9848122312258651\n",
      "train loss:0.8455479783434887\n",
      "train loss:0.9562256001465156\n",
      "train loss:0.898663843055285\n",
      "train loss:0.9551041256809952\n",
      "train loss:0.8787841440406635\n",
      "train loss:0.970048956974368\n",
      "train loss:0.8799026596464072\n",
      "train loss:1.0789437231797896\n",
      "train loss:0.8586025857313961\n",
      "train loss:1.0865370615520515\n",
      "train loss:1.1450559345110944\n",
      "train loss:0.9413706358998196\n",
      "train loss:0.9756927717229594\n",
      "train loss:0.9665720166859783\n",
      "train loss:0.9402141733349956\n",
      "train loss:0.8333534786920929\n",
      "train loss:0.848350580082095\n",
      "train loss:1.006584676880603\n",
      "train loss:0.9095341191346448\n",
      "train loss:0.8520300492401103\n",
      "train loss:0.9156834852368422\n",
      "train loss:0.8138535280277881\n",
      "train loss:0.9378260861994602\n",
      "train loss:0.9759742158488812\n",
      "train loss:0.8503469096163332\n",
      "train loss:0.9265088264856801\n",
      "train loss:0.8347585628610833\n",
      "train loss:1.0524760929210095\n",
      "train loss:0.9258658543448927\n",
      "train loss:0.8591632293626607\n",
      "train loss:0.9097383974823444\n",
      "train loss:0.9313705817132907\n",
      "train loss:0.9137839101384252\n",
      "train loss:1.1297139095876592\n",
      "train loss:0.7715687861146234\n",
      "train loss:0.781666590259716\n",
      "train loss:0.8088025835745148\n",
      "train loss:0.9841029663127117\n",
      "train loss:1.022917230292267\n",
      "train loss:0.9349371430663421\n",
      "train loss:0.9288904046462093\n",
      "train loss:1.0100533573570367\n",
      "train loss:0.926890368275476\n",
      "train loss:0.8601319556172335\n",
      "train loss:0.9542786581675632\n",
      "train loss:1.0420739280073281\n",
      "train loss:1.1608650329801946\n",
      "train loss:0.9097469529085527\n",
      "train loss:0.8087660997740451\n",
      "train loss:0.8340220629015951\n",
      "train loss:0.9198914626263585\n",
      "train loss:0.869457708120664\n",
      "train loss:0.9335030896604201\n",
      "train loss:0.9086319645356993\n",
      "train loss:0.9923631181270174\n",
      "train loss:0.9560634319845162\n",
      "train loss:1.0805456134279567\n",
      "train loss:1.0203443613583183\n",
      "train loss:0.9853870922460668\n",
      "train loss:0.9846141171215868\n",
      "train loss:0.8582108749579851\n",
      "train loss:0.9645066882627147\n",
      "train loss:1.0166944947640417\n",
      "train loss:1.026267247062066\n",
      "train loss:1.0131052080482792\n",
      "train loss:1.0587885774991133\n",
      "train loss:0.8693183944774325\n",
      "train loss:1.0466395589856095\n",
      "train loss:1.0288318948743502\n",
      "train loss:0.8345335918022326\n",
      "train loss:1.1056663979709216\n",
      "train loss:0.7918419658350752\n",
      "train loss:0.8562328179248003\n",
      "train loss:0.9522788692121703\n",
      "train loss:0.8214639150632332\n",
      "train loss:0.9784343568613265\n",
      "train loss:0.8699158028122188\n",
      "train loss:0.869192278421195\n",
      "train loss:0.8629578199722512\n",
      "train loss:0.9016061141455272\n",
      "train loss:0.9943790122078755\n",
      "train loss:0.8645252210603354\n",
      "train loss:1.1522898308641023\n",
      "train loss:1.0009378280354613\n",
      "train loss:1.0944398714655441\n",
      "train loss:0.8464814598531695\n",
      "train loss:0.874398412072249\n",
      "train loss:0.881862798052408\n",
      "train loss:0.9004772541588205\n",
      "train loss:0.7712252905252526\n",
      "train loss:0.9778471325575443\n",
      "train loss:0.95852027311937\n",
      "train loss:0.8824076897560472\n",
      "train loss:1.1802642947702866\n",
      "train loss:0.8966418629040674\n",
      "train loss:1.1230309717478884\n",
      "train loss:0.8430297330920473\n",
      "train loss:1.0076276600575274\n",
      "train loss:0.978267422896788\n",
      "train loss:0.9813430623425918\n",
      "train loss:0.942709345536654\n",
      "train loss:0.6984761478981788\n",
      "train loss:0.9115872186067934\n",
      "train loss:0.8010708559711963\n",
      "train loss:1.1786563525167526\n",
      "train loss:1.1018121972085615\n",
      "train loss:1.0584472262847866\n",
      "train loss:0.7750502845853251\n",
      "train loss:0.8581780102480466\n",
      "train loss:0.9195159892550221\n",
      "train loss:0.8825530434240304\n",
      "train loss:0.9576225784900678\n",
      "train loss:0.8511945741346011\n",
      "train loss:0.6790032006481772\n",
      "train loss:0.9667988792907214\n",
      "train loss:1.0122201736386465\n",
      "train loss:0.8702232823792682\n",
      "train loss:1.0153271165233642\n",
      "train loss:0.8241327002476754\n",
      "train loss:0.9702061622546342\n",
      "train loss:1.1254934573871958\n",
      "train loss:1.0212101129968285\n",
      "train loss:0.8413260341265701\n",
      "train loss:0.9629886093326528\n",
      "train loss:1.2553177905502184\n",
      "train loss:0.8819308642448557\n",
      "train loss:0.8595429415821307\n",
      "train loss:0.8862082071087215\n",
      "train loss:1.0468329271813155\n",
      "train loss:1.0627583299755212\n",
      "train loss:0.7962873003256123\n",
      "train loss:0.9573277005495965\n",
      "train loss:0.9396618785812137\n",
      "train loss:0.8897948278311181\n",
      "train loss:0.9755433919077755\n",
      "train loss:0.9217094177852486\n",
      "train loss:0.9132791540912696\n",
      "train loss:0.8895627783880335\n",
      "train loss:0.9566593688981453\n",
      "train loss:1.0009806471927571\n",
      "train loss:0.8495745685570367\n",
      "train loss:0.884275692092926\n",
      "train loss:0.9465125416139777\n",
      "train loss:0.8953363455633508\n",
      "train loss:0.9188612667972228\n",
      "train loss:0.93517639218821\n",
      "train loss:0.6845710516655766\n",
      "train loss:0.8839783186286972\n",
      "train loss:0.9911235575346418\n",
      "train loss:0.935067112733529\n",
      "train loss:0.934611917444283\n",
      "train loss:0.9132639702688051\n",
      "train loss:0.8745096489531554\n",
      "train loss:1.0491077515285974\n",
      "train loss:0.8817307238016491\n",
      "train loss:0.918871439450364\n",
      "train loss:0.8400724358599829\n",
      "train loss:0.7996948051153728\n",
      "train loss:0.9471379796132751\n",
      "train loss:0.8933032419788209\n",
      "train loss:0.8367969340301268\n",
      "train loss:0.9842649923681189\n",
      "=== epoch:5, train acc:0.992, test acc:0.986 ===\n",
      "train loss:0.8978673828433504\n",
      "train loss:1.0355765495911282\n",
      "train loss:0.9081257111245334\n",
      "train loss:1.0999030365555482\n",
      "train loss:1.0011636663376333\n",
      "train loss:0.8732105092132116\n",
      "train loss:0.8745753256782498\n",
      "train loss:0.9210068914752162\n",
      "train loss:0.9015519906664352\n",
      "train loss:0.9102816249660057\n",
      "train loss:1.0350539666508933\n",
      "train loss:1.029831079791459\n",
      "train loss:0.8612176861120276\n",
      "train loss:0.9706414522012949\n",
      "train loss:0.9615579964643655\n",
      "train loss:0.8706110523586278\n",
      "train loss:0.9391009442483862\n",
      "train loss:0.9149185048963759\n",
      "train loss:1.110278471024247\n",
      "train loss:0.9813091934617433\n",
      "train loss:1.0365744793192242\n",
      "train loss:0.7874840341154855\n",
      "train loss:0.6964153927285046\n",
      "train loss:0.8838596440333596\n",
      "train loss:0.9719716889169223\n",
      "train loss:0.882288730309794\n",
      "train loss:0.9769981542243887\n",
      "train loss:0.9340999185333446\n",
      "train loss:0.7657548246506952\n",
      "train loss:1.058843494287895\n",
      "train loss:1.0046187726017584\n",
      "train loss:1.0358691798542523\n",
      "train loss:0.9324580461721126\n",
      "train loss:0.9350693472168892\n",
      "train loss:0.827928688387937\n",
      "train loss:0.7854558511677346\n",
      "train loss:0.9927363909023663\n",
      "train loss:0.9065213964301734\n",
      "train loss:0.9415100322758317\n",
      "train loss:0.990164021658861\n",
      "train loss:0.9282937006515409\n",
      "train loss:0.9579881243593061\n",
      "train loss:0.8252962137111436\n",
      "train loss:0.8813765154390343\n",
      "train loss:0.8640125829379839\n",
      "train loss:0.9039787962902741\n",
      "train loss:0.90525801195536\n",
      "train loss:1.0479918829365957\n",
      "train loss:0.9165749799912102\n",
      "train loss:0.8994661514062624\n",
      "train loss:1.0285812563977281\n",
      "train loss:0.942504265165234\n",
      "train loss:0.8077092362911636\n",
      "train loss:0.8412629184470317\n",
      "train loss:1.1153061968406515\n",
      "train loss:0.827202149630017\n",
      "train loss:0.9977755426425824\n",
      "train loss:0.7385960108772386\n",
      "train loss:0.9640853300716836\n",
      "train loss:0.8216350803786652\n",
      "train loss:1.0104506934414588\n",
      "train loss:1.0821106796545152\n",
      "train loss:0.9663904200251711\n",
      "train loss:0.858771671339233\n",
      "train loss:1.0056492797574041\n",
      "train loss:1.0099588518224996\n",
      "train loss:0.9805899506461111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.0921876909932797\n",
      "train loss:1.1704911235239635\n",
      "train loss:0.8886780032689758\n",
      "train loss:1.1421882205501206\n",
      "train loss:1.0493364135216787\n",
      "train loss:1.216255875995958\n",
      "train loss:0.9646416880189309\n",
      "train loss:0.9886497541572394\n",
      "train loss:0.9919913033192269\n",
      "train loss:1.0020994406734767\n",
      "train loss:0.9027382662223203\n",
      "train loss:0.9044220866596624\n",
      "train loss:0.9900722788201348\n",
      "train loss:0.7990351338347154\n",
      "train loss:0.8665692910293593\n",
      "train loss:1.1696438603360082\n",
      "train loss:1.0005093488986871\n",
      "train loss:0.9145939660833159\n",
      "train loss:0.8646562267328106\n",
      "train loss:0.7305289428569735\n",
      "train loss:0.957016850469532\n",
      "train loss:0.9226698797174592\n",
      "train loss:1.0086091915256936\n",
      "train loss:0.969025273312665\n",
      "train loss:0.9436585127793573\n",
      "train loss:0.952042395852446\n",
      "train loss:1.0030096732422067\n",
      "train loss:0.8006134739127145\n",
      "train loss:1.0765441914890617\n",
      "train loss:0.8569864633477619\n",
      "train loss:0.9818031390896244\n",
      "train loss:0.8990781857219836\n",
      "train loss:0.9776286306290985\n",
      "train loss:0.9554760167914297\n",
      "train loss:0.9772293868720101\n",
      "train loss:0.9038416603293988\n",
      "train loss:1.0744861265687888\n",
      "train loss:0.9329441454432572\n",
      "train loss:0.9371417405445203\n",
      "train loss:0.9326009143125534\n",
      "train loss:0.7746789132009827\n",
      "train loss:0.8957376389767546\n",
      "train loss:0.8578471384712989\n",
      "train loss:0.9042464610697257\n",
      "train loss:1.0293362206426544\n",
      "train loss:1.0938001859195001\n",
      "train loss:1.0164359913553178\n",
      "train loss:0.8998394925994324\n",
      "train loss:0.9308234571132288\n",
      "train loss:0.9593442449682203\n",
      "train loss:1.0989414036223943\n",
      "train loss:1.0219234097458687\n",
      "train loss:0.9150674497045044\n",
      "train loss:0.6759517947810619\n",
      "train loss:0.9180456386835226\n",
      "train loss:0.9933601672871891\n",
      "train loss:1.0252652738599501\n",
      "train loss:0.76777592116985\n",
      "train loss:1.1095427045295754\n",
      "train loss:0.832206619241425\n",
      "train loss:0.9265859968174436\n",
      "train loss:0.6955014375912344\n",
      "train loss:0.7876918154966183\n",
      "train loss:0.7668399289813934\n",
      "train loss:0.9752952246452185\n",
      "train loss:0.8353061216039158\n",
      "train loss:0.8771184821094393\n",
      "train loss:0.9906034018001836\n",
      "train loss:1.0001038308169796\n",
      "train loss:1.0466780731751373\n",
      "train loss:0.9764297402988624\n",
      "train loss:0.9272469446524565\n",
      "train loss:0.7098431822363541\n",
      "train loss:0.9405298527790271\n",
      "train loss:0.9172047465850849\n",
      "train loss:0.8269421397510169\n",
      "train loss:0.7237349990939019\n",
      "train loss:0.9260956421535996\n",
      "train loss:0.934790981302685\n",
      "train loss:1.0098300376189973\n",
      "train loss:1.0182590792050659\n",
      "train loss:1.0085717169255437\n",
      "train loss:0.8276991784738017\n",
      "train loss:1.0596485245401077\n",
      "train loss:0.9147294120638706\n",
      "train loss:0.9356759793598929\n",
      "train loss:0.9854305005410166\n",
      "train loss:0.956449060469302\n",
      "train loss:0.8674943804704176\n",
      "train loss:0.9169340595354591\n",
      "train loss:0.944354682980633\n",
      "train loss:0.9675380941754967\n",
      "train loss:0.9765795079638567\n",
      "train loss:0.8967625631021847\n",
      "train loss:0.8945974312570698\n",
      "train loss:1.0796490998053425\n",
      "train loss:0.9325238011262473\n",
      "train loss:0.9847633434694584\n",
      "train loss:0.9797945084211303\n",
      "train loss:1.0738937846009822\n",
      "train loss:1.184527946553125\n",
      "train loss:0.7736610544112591\n",
      "train loss:0.9823847009871071\n",
      "train loss:0.8993675704435832\n",
      "train loss:1.132289971276313\n",
      "train loss:0.9216000071874327\n",
      "train loss:0.9153068287292249\n",
      "train loss:1.007134031725504\n",
      "train loss:0.9023279440155157\n",
      "train loss:0.9502790292662543\n",
      "train loss:0.9942636885991092\n",
      "train loss:0.8884403935800775\n",
      "train loss:1.0654744090514219\n",
      "train loss:0.9863293223728885\n",
      "train loss:0.860628849920258\n",
      "train loss:0.9185797079390079\n",
      "train loss:0.8575457107455061\n",
      "train loss:0.8355502470511139\n",
      "train loss:0.897863581421233\n",
      "train loss:0.9050790530046533\n",
      "train loss:0.8684123196829774\n",
      "train loss:0.966596001089413\n",
      "train loss:0.88274885026734\n",
      "train loss:0.9076504865991306\n",
      "train loss:1.0298768813970158\n",
      "train loss:0.9015601658049716\n",
      "train loss:0.7047490099141024\n",
      "train loss:0.9069627834180894\n",
      "train loss:0.8687628762138498\n",
      "train loss:0.853040745710809\n",
      "train loss:1.082712533393542\n",
      "train loss:0.9877154367468519\n",
      "train loss:0.8745068116983945\n",
      "train loss:0.9131577338158574\n",
      "train loss:0.963987489912881\n",
      "train loss:0.9221820122590139\n",
      "train loss:0.9459632234138527\n",
      "train loss:0.8299628046560747\n",
      "train loss:1.054497588734061\n",
      "train loss:1.023491674494358\n",
      "train loss:0.8253992424302831\n",
      "train loss:0.9431855862332971\n",
      "train loss:0.9538923810102078\n",
      "train loss:0.9252996056476668\n",
      "train loss:1.1209158974557953\n",
      "train loss:0.8694459102695692\n",
      "train loss:0.9763182802458157\n",
      "train loss:1.0032872170629321\n",
      "train loss:1.0414771911198752\n",
      "train loss:0.9844319058337011\n",
      "train loss:0.9877855045988625\n",
      "train loss:1.0118354435198673\n",
      "train loss:1.079443509216605\n",
      "train loss:0.9027072270465051\n",
      "train loss:0.8826855331835587\n",
      "train loss:0.9054832728629013\n",
      "train loss:0.8888396506711436\n",
      "train loss:0.9823456840721891\n",
      "train loss:0.967661316752684\n",
      "train loss:0.8661032143692767\n",
      "train loss:1.0249613750371196\n",
      "train loss:1.023480220340201\n",
      "train loss:0.9063144649140219\n",
      "train loss:0.92834366332611\n",
      "train loss:0.9693881763815854\n",
      "train loss:1.0397862178412078\n",
      "train loss:0.8598133920348896\n",
      "train loss:0.8132654451019239\n",
      "train loss:0.8804439972458644\n",
      "train loss:0.7608678976975398\n",
      "train loss:0.8751887774685081\n",
      "train loss:1.0820622217778377\n",
      "train loss:1.1010964991415582\n",
      "train loss:1.0036465317013212\n",
      "train loss:0.796102841799772\n",
      "train loss:0.9017819583056337\n",
      "train loss:0.9221423713352928\n",
      "train loss:0.7959404012658956\n",
      "train loss:0.8568445103959417\n",
      "train loss:0.7415071241658443\n",
      "train loss:1.008474971129643\n",
      "train loss:0.9122295398269539\n",
      "train loss:0.8468934415535074\n",
      "train loss:0.7551914231943802\n",
      "train loss:0.8301607700492534\n",
      "train loss:0.9610138200954572\n",
      "train loss:0.9757217851546746\n",
      "train loss:0.9272629786056561\n",
      "train loss:0.9040450133491669\n",
      "train loss:0.9980473081727281\n",
      "train loss:0.7505431905179701\n",
      "train loss:0.9151841924934244\n",
      "train loss:0.7692898372874329\n",
      "train loss:1.024523070739242\n",
      "train loss:0.8991792452029813\n",
      "train loss:1.0491330665643468\n",
      "train loss:0.894656659909911\n",
      "train loss:0.8940688778704139\n",
      "train loss:1.1424594086260045\n",
      "train loss:0.9808972289822772\n",
      "train loss:1.0178468548648607\n",
      "train loss:0.8619745989297732\n",
      "train loss:0.8222102874319736\n",
      "train loss:1.1388383287705066\n",
      "train loss:0.9304077710645118\n",
      "train loss:0.923388540175177\n",
      "train loss:0.8003855901524688\n",
      "train loss:0.9639177154289997\n",
      "train loss:0.918158143341505\n",
      "train loss:0.8681037242342815\n",
      "train loss:0.7849484369655444\n",
      "train loss:0.9653462433999889\n",
      "train loss:0.9605367598671387\n",
      "train loss:0.8224532225320567\n",
      "train loss:0.925394347856299\n",
      "train loss:0.8762551332532081\n",
      "train loss:0.9933824305956288\n",
      "train loss:1.0661270707728954\n",
      "train loss:0.867730849110529\n",
      "train loss:0.9039519155662235\n",
      "train loss:0.9572106136803222\n",
      "train loss:0.9334098247078779\n",
      "train loss:0.8306044022169178\n",
      "train loss:1.0532831668476492\n",
      "train loss:0.8530315874722696\n",
      "train loss:0.8270485663771009\n",
      "train loss:0.895509939513887\n",
      "train loss:0.6613694032781717\n",
      "train loss:0.7997388222143408\n",
      "train loss:0.9663997121084208\n",
      "train loss:1.2149900015076804\n",
      "train loss:0.928862362216844\n",
      "train loss:1.009174629699536\n",
      "train loss:0.840743191146697\n",
      "train loss:0.9229525884699471\n",
      "train loss:0.8733812068476959\n",
      "train loss:0.8301594510688954\n",
      "train loss:0.8208667667721522\n",
      "train loss:0.9704108645876779\n",
      "train loss:1.0846110696760576\n",
      "train loss:0.9272871564988112\n",
      "train loss:0.8811280444249846\n",
      "train loss:0.8914740759838388\n",
      "train loss:0.8219170190119095\n",
      "train loss:1.0589313569228793\n",
      "train loss:0.7087364517457968\n",
      "train loss:0.9641443459606741\n",
      "train loss:0.9548030344559946\n",
      "train loss:0.8624361237172199\n",
      "train loss:0.8370476351416809\n",
      "train loss:1.1234216331228395\n",
      "train loss:0.8848833253023565\n",
      "train loss:0.7090945905630355\n",
      "train loss:1.0925241186125745\n",
      "train loss:0.9635890627458401\n",
      "train loss:0.9357285534986522\n",
      "train loss:0.9992383543057908\n",
      "train loss:0.9921203851290364\n",
      "train loss:0.8150377604582012\n",
      "train loss:1.038818031579462\n",
      "train loss:0.6967126444767512\n",
      "train loss:1.0290293521231557\n",
      "train loss:0.8981641046890461\n",
      "train loss:1.047925272775408\n",
      "train loss:0.8494498799265771\n",
      "train loss:0.9141417971786661\n",
      "train loss:0.9242981811026165\n",
      "train loss:0.8928502393996156\n",
      "train loss:0.9122567765486786\n",
      "train loss:0.8064916327814383\n",
      "train loss:0.851070157780039\n",
      "train loss:0.9725823452600842\n",
      "train loss:0.8957149437811017\n",
      "train loss:0.931455683154383\n",
      "train loss:0.910216469515954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.9371889930917895\n",
      "train loss:0.9092108094832787\n",
      "train loss:0.9558934355194281\n",
      "train loss:0.8830328398539481\n",
      "train loss:0.9602395242356064\n",
      "train loss:0.9017088677098449\n",
      "train loss:0.8702475958627243\n",
      "train loss:0.8866585552883472\n",
      "train loss:0.9424129161785209\n",
      "train loss:0.6766450756067782\n",
      "train loss:0.7962578838639341\n",
      "train loss:0.9093807072315543\n",
      "train loss:0.807021760524659\n",
      "train loss:0.8815441771250103\n",
      "train loss:0.9444914253187952\n",
      "train loss:0.8624580278133754\n",
      "train loss:0.9213879749416531\n",
      "train loss:0.7987394467625444\n",
      "train loss:0.9162242616252178\n",
      "train loss:0.9456548822589573\n",
      "train loss:0.9498463086900295\n",
      "train loss:0.8332939467358839\n",
      "train loss:0.7769954789654355\n",
      "train loss:0.7088256204361353\n",
      "train loss:0.8731996569122726\n",
      "train loss:0.938629463065099\n",
      "train loss:1.0321347177504456\n",
      "train loss:0.8649651540697816\n",
      "train loss:0.9160827443360613\n",
      "train loss:0.9032867197269803\n",
      "train loss:0.8687972472252583\n",
      "train loss:0.7263591811663234\n",
      "train loss:1.0585306085591257\n",
      "train loss:0.9816554913795269\n",
      "train loss:0.9516851402890708\n",
      "train loss:0.9309995746529013\n",
      "train loss:0.8506356287634793\n",
      "train loss:0.933919715774067\n",
      "train loss:1.0197557301702371\n",
      "train loss:0.9165658456415461\n",
      "train loss:0.947047360678978\n",
      "train loss:0.8762360281131397\n",
      "train loss:1.0266271120041663\n",
      "train loss:0.9591129659087535\n",
      "train loss:0.9107151678507209\n",
      "train loss:0.6264442307815398\n",
      "train loss:0.9366378796931071\n",
      "train loss:0.927788375872684\n",
      "train loss:1.0306227668358947\n",
      "train loss:1.0359874379540703\n",
      "train loss:0.9874785257707537\n",
      "train loss:0.8150214091614727\n",
      "train loss:0.9412564680528037\n",
      "train loss:0.9481889392920431\n",
      "train loss:1.0597973182180653\n",
      "train loss:0.8029974022789587\n",
      "train loss:0.7079352250959782\n",
      "train loss:0.9405674643141411\n",
      "train loss:1.170833496839662\n",
      "train loss:0.8626453481803563\n",
      "train loss:0.8952299363827197\n",
      "train loss:0.8409013264181253\n",
      "train loss:0.9611986488953178\n",
      "train loss:0.7992293417533699\n",
      "train loss:1.03712242424572\n",
      "train loss:0.9799580523180232\n",
      "train loss:1.0777932740885996\n",
      "train loss:1.0933564064148609\n",
      "train loss:0.9887244703608462\n",
      "train loss:1.0007216806967427\n",
      "train loss:0.9809792568519462\n",
      "train loss:0.879078246006446\n",
      "train loss:0.7726070013462949\n",
      "train loss:0.8619237446948986\n",
      "train loss:0.9695295055944694\n",
      "train loss:0.7991376205971841\n",
      "train loss:0.9503305058879483\n",
      "train loss:0.9144863601749953\n",
      "train loss:0.9493974327843543\n",
      "train loss:0.8770878508612587\n",
      "train loss:0.9086583905718111\n",
      "train loss:1.1336442962926232\n",
      "train loss:1.225288933571388\n",
      "train loss:0.9723564204022407\n",
      "train loss:0.8390657222446475\n",
      "train loss:0.9556612463272711\n",
      "train loss:0.954786264484878\n",
      "train loss:0.6778643152598351\n",
      "train loss:0.7243736914078847\n",
      "train loss:0.9731292191103406\n",
      "train loss:1.1012108283244721\n",
      "train loss:0.9372237458244753\n",
      "train loss:0.9064266931228202\n",
      "train loss:0.8081341669610211\n",
      "train loss:0.8795238566047672\n",
      "train loss:0.8540525321811201\n",
      "train loss:0.85436059052543\n",
      "train loss:1.008293292580996\n",
      "train loss:0.8648921644433138\n",
      "train loss:0.8658476250756584\n",
      "train loss:0.8389456576129146\n",
      "train loss:0.8370586304445493\n",
      "train loss:0.8050427180767465\n",
      "train loss:1.0158771671489149\n",
      "train loss:0.8545118791014552\n",
      "train loss:0.9249254955400774\n",
      "train loss:1.0237856613684584\n",
      "train loss:0.9124973741949332\n",
      "train loss:0.7245972168067243\n",
      "train loss:1.0461250725002658\n",
      "train loss:1.0783672719742727\n",
      "train loss:0.9233417466678624\n",
      "train loss:1.0385315100084194\n",
      "train loss:0.8406250107896835\n",
      "train loss:0.9050026595987228\n",
      "train loss:0.9565380339356312\n",
      "train loss:0.9075185568267442\n",
      "train loss:0.8569821665015283\n",
      "train loss:0.8247392643398155\n",
      "train loss:1.008110559807643\n",
      "train loss:0.8827037077730324\n",
      "train loss:0.78497940436753\n",
      "train loss:0.9652618118254936\n",
      "train loss:1.0367421481307824\n",
      "train loss:0.8566087663298605\n",
      "train loss:1.0436287701893503\n",
      "train loss:0.8746474458301203\n",
      "train loss:0.953292118247939\n",
      "train loss:1.0471907079183789\n",
      "train loss:0.9857146662252682\n",
      "train loss:1.0392573480116465\n",
      "train loss:0.9152109802614439\n",
      "train loss:0.9060347286385809\n",
      "train loss:0.7452797971333244\n",
      "train loss:0.7015268819429029\n",
      "train loss:0.8302372180625771\n",
      "train loss:1.027128943638929\n",
      "train loss:0.8677033356326176\n",
      "train loss:1.0870829605895562\n",
      "train loss:0.8128654632029793\n",
      "train loss:0.8780446977640551\n",
      "train loss:0.8684842754335251\n",
      "train loss:0.8850315606605933\n",
      "train loss:1.0056826084324346\n",
      "train loss:1.1476905713282168\n",
      "train loss:0.9799152486579459\n",
      "train loss:0.799270386870526\n",
      "train loss:0.8944494899178163\n",
      "train loss:0.8770995164360477\n",
      "train loss:0.9554283424712547\n",
      "train loss:0.9305425354656899\n",
      "train loss:0.8188826517201978\n",
      "train loss:0.8622001424285304\n",
      "train loss:1.0572213752802961\n",
      "train loss:0.8513087225499899\n",
      "train loss:0.9050786317698446\n",
      "train loss:0.8842111281063636\n",
      "train loss:0.7924011521807579\n",
      "train loss:0.9400195512337725\n",
      "train loss:0.7935671833391639\n",
      "train loss:0.9551942037522655\n",
      "train loss:0.9476567212424613\n",
      "train loss:1.0090504180185982\n",
      "train loss:1.0886311428187287\n",
      "train loss:1.0493705713940735\n",
      "train loss:0.9425662097498642\n",
      "train loss:0.8978871356539504\n",
      "train loss:1.0596955165136448\n",
      "train loss:0.9303725747756084\n",
      "train loss:1.0615048042826174\n",
      "train loss:0.9714574265022249\n",
      "train loss:1.035393335514174\n",
      "train loss:0.910909408655006\n",
      "train loss:0.9443240512958309\n",
      "train loss:0.9232246483142346\n",
      "train loss:0.9713086577305414\n",
      "train loss:0.8615410184385602\n",
      "train loss:0.9004593876222035\n",
      "train loss:1.108177795755855\n",
      "train loss:0.9121529237135344\n",
      "train loss:0.8138405149326816\n",
      "train loss:0.7979368762857129\n",
      "train loss:0.8404473071079107\n",
      "train loss:1.0515646188859353\n",
      "train loss:1.002054240146091\n",
      "train loss:0.8495978884572725\n",
      "train loss:0.8389681910784915\n",
      "train loss:0.9092138043453124\n",
      "train loss:0.9322602200852883\n",
      "train loss:0.9413714482017561\n",
      "train loss:0.739882502636102\n",
      "train loss:0.9393340362949654\n",
      "train loss:0.9722315431145381\n",
      "train loss:1.0370327436816402\n",
      "train loss:0.8811011262644147\n",
      "train loss:1.0903992150126651\n",
      "train loss:1.1088033486535154\n",
      "train loss:1.0109782726101653\n",
      "train loss:0.9681145844401493\n",
      "train loss:0.8737660026169206\n",
      "train loss:0.9253436441482265\n",
      "train loss:1.1519213573701033\n",
      "train loss:1.0257957763387908\n",
      "train loss:0.8397885647487882\n",
      "train loss:0.9239109761637502\n",
      "train loss:0.9307083626125185\n",
      "train loss:0.9369570006172154\n",
      "train loss:1.0390133830097812\n",
      "train loss:0.902106949230621\n",
      "train loss:1.1013146898035018\n",
      "train loss:1.0143561127915688\n",
      "train loss:0.8172571099040619\n",
      "train loss:0.8863843101153396\n",
      "train loss:0.9955022135506987\n",
      "train loss:0.9449749021785909\n",
      "train loss:0.871240211269816\n",
      "train loss:0.8819824601846749\n",
      "train loss:0.9353699613147143\n",
      "train loss:0.9525734354524219\n",
      "train loss:0.9050975423067551\n",
      "train loss:0.8922422959612412\n",
      "train loss:0.991622940461087\n",
      "train loss:0.9501460301316379\n",
      "train loss:0.9513636068649874\n",
      "train loss:0.9101409566731321\n",
      "train loss:0.9771062237041582\n",
      "train loss:0.9304378592315969\n",
      "train loss:0.9978701839449415\n",
      "train loss:1.0116389428691819\n",
      "train loss:0.9380682677324301\n",
      "train loss:0.858907356417935\n",
      "train loss:1.0046947438342015\n",
      "train loss:1.0174965130721396\n",
      "train loss:0.8850279044059168\n",
      "train loss:0.8136383348029775\n",
      "train loss:1.0780611428337956\n",
      "train loss:0.9542000656105425\n",
      "train loss:0.8794499586053361\n",
      "train loss:1.0850474664657324\n",
      "train loss:0.9010849579736664\n",
      "train loss:0.9487169127293751\n",
      "train loss:1.0257716254075147\n",
      "train loss:0.9522842351434064\n",
      "train loss:1.0881031388642606\n",
      "train loss:0.9153030020942462\n",
      "train loss:1.0641661029030192\n",
      "train loss:0.7650114019711072\n",
      "train loss:0.9570464964603594\n",
      "train loss:0.8371509993735106\n",
      "train loss:0.945055793552303\n",
      "train loss:0.9159411512439058\n",
      "train loss:0.91654229203948\n",
      "train loss:0.8957745622366947\n",
      "train loss:0.8351437608429576\n",
      "train loss:0.9260732078591049\n",
      "train loss:0.848948403825111\n",
      "train loss:0.9127980943525499\n",
      "train loss:0.9968047868996799\n",
      "=== epoch:6, train acc:0.993, test acc:0.99 ===\n",
      "train loss:0.7288921541706435\n",
      "train loss:1.0294538773218074\n",
      "train loss:0.7871263772854307\n",
      "train loss:1.0253835167804628\n",
      "train loss:0.8951704153899679\n",
      "train loss:0.9104243401824124\n",
      "train loss:0.7536103922532665\n",
      "train loss:1.0940024493868499\n",
      "train loss:0.9057521728496835\n",
      "train loss:0.8742167008729489\n",
      "train loss:0.9277971847029355\n",
      "train loss:0.9797334783388219\n",
      "train loss:0.9683764708636928\n",
      "train loss:0.9742614484051384\n",
      "train loss:0.8499581577723889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.0483880823818574\n",
      "train loss:0.8719999627125139\n",
      "train loss:0.9462010944231137\n",
      "train loss:1.0357530076959005\n",
      "train loss:0.8541699757244752\n",
      "train loss:0.9908009711208264\n",
      "train loss:0.8947202985953827\n",
      "train loss:0.9807454429008439\n",
      "train loss:1.0305297309742418\n",
      "train loss:1.0960095350116246\n",
      "train loss:0.9603909646475058\n",
      "train loss:0.9692936782998597\n",
      "train loss:0.9213291518715048\n",
      "train loss:0.9476990270241168\n",
      "train loss:0.8281317953713261\n",
      "train loss:0.8314192531554674\n",
      "train loss:0.8301312971079328\n",
      "train loss:1.0737190795752678\n",
      "train loss:0.6585867801457848\n",
      "train loss:0.9151927485422877\n",
      "train loss:0.8034883778391351\n",
      "train loss:0.9820395959520214\n",
      "train loss:0.9054765196708897\n",
      "train loss:0.9691582826203295\n",
      "train loss:0.9515796255280152\n",
      "train loss:0.8177364987449093\n",
      "train loss:0.9186909651677072\n",
      "train loss:0.9340403374264212\n",
      "train loss:0.9146612978161063\n",
      "train loss:0.9196374955277643\n",
      "train loss:1.0127249790458697\n",
      "train loss:0.9749256751587799\n",
      "train loss:0.9807672437326984\n",
      "train loss:0.8005066925281732\n",
      "train loss:0.8808355280362652\n",
      "train loss:0.9824128577450202\n",
      "train loss:0.8303292869355967\n",
      "train loss:0.8059927814402837\n",
      "train loss:0.965503373482085\n",
      "train loss:0.9159328635706541\n",
      "train loss:0.9485668547511152\n",
      "train loss:0.9278659004333941\n",
      "train loss:0.9694690358710067\n",
      "train loss:1.1054866489730795\n",
      "train loss:1.0584616513643046\n",
      "train loss:0.9296133246804524\n",
      "train loss:0.8532963990665797\n",
      "train loss:0.8899577684060916\n",
      "train loss:0.853244449224618\n",
      "train loss:0.9520604371013168\n",
      "train loss:0.8509597620897712\n",
      "train loss:0.8936822972356389\n",
      "train loss:0.7624572621992914\n",
      "train loss:0.9661203119845243\n",
      "train loss:0.92723661761955\n",
      "train loss:0.8765605151625777\n",
      "train loss:0.9345352034699519\n",
      "train loss:0.8831806968442476\n",
      "train loss:0.8413603555183573\n",
      "train loss:0.9917694817321248\n",
      "train loss:1.0018485283991603\n",
      "train loss:0.8805490870002854\n",
      "train loss:0.9439237127167833\n",
      "train loss:1.048537159569881\n",
      "train loss:0.8970315559132106\n",
      "train loss:1.0037368274858902\n",
      "train loss:0.9691067601411845\n",
      "train loss:0.976535914729\n",
      "train loss:0.7587406769182343\n",
      "train loss:0.8593312426407561\n",
      "train loss:0.8695164783707517\n",
      "train loss:0.9432316847192562\n",
      "train loss:1.1046364695207458\n",
      "train loss:1.0484609606171866\n",
      "train loss:0.8837858309720893\n",
      "train loss:0.7586862585866982\n",
      "train loss:0.9401109640438261\n",
      "train loss:1.0039547326620666\n",
      "train loss:0.6963304901895364\n",
      "train loss:0.863074052333827\n",
      "train loss:0.8407744917903609\n",
      "train loss:1.0578304721361633\n",
      "train loss:1.0123632762961323\n",
      "train loss:0.7477028212993724\n",
      "train loss:0.9010638037572667\n",
      "train loss:0.7971353526328846\n",
      "train loss:0.8224523569798354\n",
      "train loss:0.9188854467757868\n",
      "train loss:1.0836645958871178\n",
      "train loss:0.891082440414049\n",
      "train loss:1.016126837301708\n",
      "train loss:0.9559739385379572\n",
      "train loss:0.9445278512390416\n",
      "train loss:0.9825101227727214\n",
      "train loss:0.7193611818577618\n",
      "train loss:0.8271874418215803\n",
      "train loss:0.8979189578146162\n",
      "train loss:0.9066735706402569\n",
      "train loss:0.9440478987356344\n",
      "train loss:0.9129985462191982\n",
      "train loss:0.6610670556677724\n",
      "train loss:0.8379598687118949\n",
      "train loss:0.7205713028736142\n",
      "train loss:0.8844977975119328\n",
      "train loss:0.9602499775741242\n",
      "train loss:0.8877526520254654\n",
      "train loss:0.9686588140031545\n",
      "train loss:0.9121468157475092\n",
      "train loss:0.9054382689462368\n",
      "train loss:0.9210060569069902\n",
      "train loss:0.9968493114718088\n",
      "train loss:0.8805348014942694\n",
      "train loss:0.894372645528824\n",
      "train loss:0.8002171366232891\n",
      "train loss:0.8813448612035992\n",
      "train loss:1.0868187732797687\n",
      "train loss:0.8859260600783333\n",
      "train loss:0.9832441832711897\n",
      "train loss:1.0948999070510643\n",
      "train loss:1.1271061319692364\n",
      "train loss:1.0360744944161635\n",
      "train loss:0.797025395655258\n",
      "train loss:0.8466130183250768\n",
      "train loss:1.0522249263788364\n",
      "train loss:1.0551107516586329\n",
      "train loss:0.9960685035976244\n",
      "train loss:0.8882518768573184\n",
      "train loss:0.9117882692494835\n",
      "train loss:0.9123812464156833\n",
      "train loss:1.0127272337806856\n",
      "train loss:0.8399190480369998\n",
      "train loss:0.8806795912689321\n",
      "train loss:0.8628127234580074\n",
      "train loss:0.891485063964537\n",
      "train loss:0.9303054285399588\n",
      "train loss:0.8635962958482699\n",
      "train loss:0.9605395513198866\n",
      "train loss:0.8683248014628593\n",
      "train loss:1.137547163583788\n",
      "train loss:0.8578724872493507\n",
      "train loss:1.1645395872579456\n",
      "train loss:0.7245923931665403\n",
      "train loss:0.965673665893267\n",
      "train loss:0.8496371168232423\n",
      "train loss:0.8637355719057027\n",
      "train loss:0.8396657696336574\n",
      "train loss:1.0997648635364619\n",
      "train loss:0.8637412741932251\n",
      "train loss:1.0531029307069495\n",
      "train loss:0.8632805352641699\n",
      "train loss:0.8564181842999274\n",
      "train loss:1.001827942267476\n",
      "train loss:0.8005955505877473\n",
      "train loss:0.8328235569929248\n",
      "train loss:0.827583916776692\n",
      "train loss:0.7732951477237235\n",
      "train loss:1.0239153369461937\n",
      "train loss:0.9225161127995579\n",
      "train loss:0.8103771453900389\n",
      "train loss:0.8104586628778182\n",
      "train loss:0.8738111221607672\n",
      "train loss:0.8112895155901163\n",
      "train loss:1.0305852889955174\n",
      "train loss:0.8736436485466111\n",
      "train loss:0.9301780936860929\n",
      "train loss:0.9794629105221959\n",
      "train loss:0.7872615866718163\n",
      "train loss:1.1153737961133425\n",
      "train loss:0.9804621695299164\n",
      "train loss:0.9312763484422435\n",
      "train loss:1.0037740069638479\n",
      "train loss:0.6929178583356884\n",
      "train loss:0.7823653743394031\n",
      "train loss:0.9818501277251056\n",
      "train loss:0.9072099127211672\n",
      "train loss:0.8719084402325629\n",
      "train loss:0.8145828036629407\n",
      "train loss:1.0636194231988343\n",
      "train loss:0.8196661675825115\n",
      "train loss:0.7413379197103499\n",
      "train loss:0.8973854663538977\n",
      "train loss:0.9115180095209447\n",
      "train loss:0.8486030223736754\n",
      "train loss:0.9812134994103674\n",
      "train loss:0.9059705000905118\n",
      "train loss:0.8931063215117508\n",
      "train loss:0.9935273363048066\n",
      "train loss:1.0934647335795216\n",
      "train loss:0.8783136651302125\n",
      "train loss:1.0290876865110479\n",
      "train loss:0.922585595277644\n",
      "train loss:0.8099123447085748\n",
      "train loss:0.8645436352329868\n",
      "train loss:0.8196458367016178\n",
      "train loss:0.7299107908881338\n",
      "train loss:1.0453069503003747\n",
      "train loss:0.8384206728825245\n",
      "train loss:1.0242171641920443\n",
      "train loss:0.8396763781958889\n",
      "train loss:1.1396185127894376\n",
      "train loss:0.9886174610270458\n",
      "train loss:0.8868781201630286\n",
      "train loss:0.9942393674127119\n",
      "train loss:0.9721414556320576\n",
      "train loss:1.0524817893804075\n",
      "train loss:0.9515350550910182\n",
      "train loss:0.9738745105680126\n",
      "train loss:0.8909847918439447\n",
      "train loss:0.8865913304678615\n",
      "train loss:0.9445851637438176\n",
      "train loss:0.8417275056333557\n",
      "train loss:0.8011610212219286\n",
      "train loss:0.8229579144846909\n",
      "train loss:1.0819251462975734\n",
      "train loss:0.7875634929415811\n",
      "train loss:1.0311393484738192\n",
      "train loss:0.9236163977609911\n",
      "train loss:1.0327433740787513\n",
      "train loss:0.8710687514355377\n",
      "train loss:1.0703897506619864\n",
      "train loss:0.9313570291054083\n",
      "train loss:0.9074325156221272\n",
      "train loss:0.8488252415436512\n",
      "train loss:0.8813953187424443\n",
      "train loss:0.8299251132829996\n",
      "train loss:0.9451531861308169\n",
      "train loss:0.9749081157416797\n",
      "train loss:0.8640275776683878\n",
      "train loss:0.9276167515243776\n",
      "train loss:0.9752558596864033\n",
      "train loss:0.9742852074380266\n",
      "train loss:1.0091787246128332\n",
      "train loss:1.0361709575922704\n",
      "train loss:0.9504090484638916\n",
      "train loss:0.948246756002789\n",
      "train loss:0.9339914959413921\n",
      "train loss:0.8924192069467687\n",
      "train loss:0.9801100464401082\n",
      "train loss:0.9371811920256207\n",
      "train loss:1.0374901246575552\n",
      "train loss:0.9864762036767992\n",
      "train loss:0.9307840000898198\n",
      "train loss:0.8397290292324828\n",
      "train loss:0.8528490714950955\n",
      "train loss:0.9220884900498948\n",
      "train loss:0.9104136365194829\n",
      "train loss:0.9904790108131241\n",
      "train loss:0.9906075935160698\n",
      "train loss:0.9302003582533562\n",
      "train loss:0.9471131971599277\n",
      "train loss:0.876476668292726\n",
      "train loss:0.9738234739177666\n",
      "train loss:0.947308784616102\n",
      "train loss:0.9211288761209212\n",
      "train loss:0.7751376550040235\n",
      "train loss:0.8604334446455288\n",
      "train loss:0.9236580067552531\n",
      "train loss:0.9535182501600267\n",
      "train loss:0.9740819918841914\n",
      "train loss:0.8259203712282612\n",
      "train loss:0.9781491358264389\n",
      "train loss:0.8984124789655639\n",
      "train loss:0.9779842396773905\n",
      "train loss:0.9363856084309703\n",
      "train loss:0.9503295839774764\n",
      "train loss:0.8792958381623817\n",
      "train loss:0.8140366803590163\n",
      "train loss:0.9411386773346965\n",
      "train loss:0.9591884045917267\n",
      "train loss:0.7424342980295557\n",
      "train loss:0.9514232819922048\n",
      "train loss:0.8445370640571089\n",
      "train loss:0.9027476579542353\n",
      "train loss:0.7813240150377146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.9175695066146012\n",
      "train loss:0.9101095732006607\n",
      "train loss:0.802283784838302\n",
      "train loss:0.7019663231418933\n",
      "train loss:0.9530083106994792\n",
      "train loss:0.8172253903679259\n",
      "train loss:0.9000487338712183\n",
      "train loss:1.0041031069915283\n",
      "train loss:0.986640850621083\n",
      "train loss:0.8769874439265731\n",
      "train loss:0.9983697842224004\n",
      "train loss:0.9328619480658135\n",
      "train loss:0.9776535819033724\n",
      "train loss:0.8203821542167549\n",
      "train loss:0.8261442729755522\n",
      "train loss:0.9565753765388785\n",
      "train loss:0.7331351684334299\n",
      "train loss:0.8395376342455559\n",
      "train loss:0.8682861460195318\n",
      "train loss:0.8763579873828502\n",
      "train loss:0.7876750088257048\n",
      "train loss:0.7702552995267321\n",
      "train loss:0.9056361475911724\n",
      "train loss:0.7623570128643721\n",
      "train loss:0.9423087212646856\n",
      "train loss:0.863292225021012\n",
      "train loss:0.7142070155325103\n",
      "train loss:0.891085862732006\n",
      "train loss:0.9268838355929649\n",
      "train loss:0.9445621592159142\n",
      "train loss:0.8688603369477131\n",
      "train loss:0.6755558888303278\n",
      "train loss:0.8403767858286132\n",
      "train loss:0.8863627467237021\n",
      "train loss:1.0254642498311264\n",
      "train loss:1.0165892129662062\n",
      "train loss:0.7923728789562908\n",
      "train loss:0.8371403102275873\n",
      "train loss:1.0897386014303307\n",
      "train loss:0.9394525949499268\n",
      "train loss:0.8113556255787239\n",
      "train loss:0.9315159798383683\n",
      "train loss:0.9283747631166231\n",
      "train loss:0.8461825683924619\n",
      "train loss:0.7171332068089208\n",
      "train loss:0.9208850347888845\n",
      "train loss:0.9082430350528564\n",
      "train loss:0.8992693368433915\n",
      "train loss:0.7985706175623629\n",
      "train loss:0.8310729162492093\n",
      "train loss:0.9771289413160282\n",
      "train loss:0.9266996514684211\n",
      "train loss:0.8482197771195237\n",
      "train loss:0.9458499235554909\n",
      "train loss:0.8462738957595181\n",
      "train loss:1.0072158611482418\n",
      "train loss:0.9432565441300959\n",
      "train loss:1.0335063906595894\n",
      "train loss:0.7979495290866535\n",
      "train loss:0.9774647080786303\n",
      "train loss:0.9088099255012562\n",
      "train loss:0.901317435706433\n",
      "train loss:1.0554490599223008\n",
      "train loss:0.8215517849562444\n",
      "train loss:0.9060394556928459\n",
      "train loss:0.8126672162436909\n",
      "train loss:0.9087038172361254\n",
      "train loss:0.8504685946623202\n",
      "train loss:0.8146949683669676\n",
      "train loss:0.7674808443332605\n",
      "train loss:0.9449419128947498\n",
      "train loss:0.8725567498112419\n",
      "train loss:1.1627565847563452\n",
      "train loss:1.0123452336958814\n",
      "train loss:0.9596660951432388\n",
      "train loss:1.0768212675300701\n",
      "train loss:0.7482796584807196\n",
      "train loss:0.9542934587803777\n",
      "train loss:0.980788682679577\n",
      "train loss:0.8440077663086627\n",
      "train loss:0.9854350861508349\n",
      "train loss:1.0408265515057114\n",
      "train loss:1.0430841297708933\n",
      "train loss:0.8119947652580316\n",
      "train loss:0.912475757958687\n",
      "train loss:0.9885889380657032\n",
      "train loss:0.8763560196839635\n",
      "train loss:0.9061967188271338\n",
      "train loss:0.9710015776247211\n",
      "train loss:0.7574040709831662\n",
      "train loss:0.8691873670345474\n",
      "train loss:0.8739797901237522\n",
      "train loss:1.0888912481313928\n",
      "train loss:0.9620757095360167\n",
      "train loss:1.0774674338984236\n",
      "train loss:0.6807715126674124\n",
      "train loss:0.9282468933942606\n",
      "train loss:0.9024556104924898\n",
      "train loss:0.9300811357845317\n",
      "train loss:0.7242561642554858\n",
      "train loss:1.0285211162309067\n",
      "train loss:0.9468296833419445\n",
      "train loss:0.7662414041981243\n",
      "train loss:0.9690123955211581\n",
      "train loss:0.8995708530907679\n",
      "train loss:0.8440070619200892\n",
      "train loss:0.9846336797181082\n",
      "train loss:1.0096742076500873\n",
      "train loss:0.9688372656078235\n",
      "train loss:0.7012326929124906\n",
      "train loss:0.9691908800268743\n",
      "train loss:0.8869955090452906\n",
      "train loss:0.9636974749629649\n",
      "train loss:0.9948709084659374\n",
      "train loss:0.8683440672739142\n",
      "train loss:0.834479896239309\n",
      "train loss:1.0285409495920521\n",
      "train loss:0.9229112619816262\n",
      "train loss:0.9366027765336513\n",
      "train loss:0.9359857710651391\n",
      "train loss:0.9245887557311063\n",
      "train loss:0.7329175898370918\n",
      "train loss:0.901124599477784\n",
      "train loss:0.8396721285693522\n",
      "train loss:1.0548895227876587\n",
      "train loss:0.8598897113590073\n",
      "train loss:0.9631004800324828\n",
      "train loss:0.7838282123118426\n",
      "train loss:0.9174453757550509\n",
      "train loss:0.972760795494164\n",
      "train loss:0.8902210385000683\n",
      "train loss:0.7812650068146603\n",
      "train loss:0.7881198089604986\n",
      "train loss:0.75479538509885\n",
      "train loss:1.0116608174949986\n",
      "train loss:1.025910848478603\n",
      "train loss:0.9075167375653962\n",
      "train loss:0.9514069362672418\n",
      "train loss:0.7611519346406651\n",
      "train loss:0.902599595334094\n",
      "train loss:0.853352558184331\n",
      "train loss:0.9366954953653955\n",
      "train loss:0.9684906705238393\n",
      "train loss:0.8320380262618388\n",
      "train loss:0.8727326727260553\n",
      "train loss:1.0049671571943855\n",
      "train loss:0.7257432941455949\n",
      "train loss:1.1197253144425894\n",
      "train loss:0.9212125279393898\n",
      "train loss:0.8880914699111417\n",
      "train loss:1.0749390859653505\n",
      "train loss:0.7954014721989712\n",
      "train loss:0.7590699058282331\n",
      "train loss:1.056243675455475\n",
      "train loss:0.949987842810348\n",
      "train loss:1.0160165148972897\n",
      "train loss:0.8079193424896713\n",
      "train loss:0.8117411268080219\n",
      "train loss:0.9580642762547436\n",
      "train loss:0.7355156001496028\n",
      "train loss:0.9663527132541602\n",
      "train loss:1.1055041019957894\n",
      "train loss:1.1703034105834522\n",
      "train loss:1.0515744445598971\n",
      "train loss:0.8665920250714199\n",
      "train loss:0.7813933149531647\n",
      "train loss:0.8969954350517867\n",
      "train loss:0.9693804552428995\n",
      "train loss:1.0585412462182924\n",
      "train loss:0.9391602605863687\n",
      "train loss:0.8551438454260877\n",
      "train loss:0.9256372152584345\n",
      "train loss:0.8931319704514378\n",
      "train loss:0.9150120241209106\n",
      "train loss:1.0731036716864852\n",
      "train loss:0.8141707734346292\n",
      "train loss:0.920484079646315\n",
      "train loss:0.8400361130481151\n",
      "train loss:1.0148978393503574\n",
      "train loss:1.0604011747376525\n",
      "train loss:1.07982338474503\n",
      "train loss:0.9273207550466224\n",
      "train loss:1.0676640326880975\n",
      "train loss:0.9292589651711887\n",
      "train loss:0.8213254348564657\n",
      "train loss:0.8773935854138628\n",
      "train loss:0.7320846667537809\n",
      "train loss:0.9954530525912553\n",
      "train loss:0.9048000098415335\n",
      "train loss:0.834632861612026\n",
      "train loss:0.8151096937332585\n",
      "train loss:0.9097276076577914\n",
      "train loss:1.0697435450975858\n",
      "train loss:1.0134191155397745\n",
      "train loss:1.042630320115536\n",
      "train loss:0.9344300878612973\n",
      "train loss:0.6984294007158408\n",
      "train loss:0.8478688552507921\n",
      "train loss:0.9255207716830369\n",
      "train loss:0.8979804950878131\n",
      "train loss:0.8392796864299861\n",
      "train loss:0.8565558439597984\n",
      "train loss:0.8692079559148054\n",
      "train loss:0.8501285614239598\n",
      "train loss:0.9486261289696768\n",
      "train loss:0.9175053106993434\n",
      "train loss:0.9031898111152198\n",
      "train loss:0.7214813768104034\n",
      "train loss:0.8114299493369951\n",
      "train loss:0.9889971743662251\n",
      "train loss:1.06360360246794\n",
      "train loss:0.8918007408154892\n",
      "train loss:0.8763521289149204\n",
      "train loss:0.9201061221692184\n",
      "train loss:0.918927219872394\n",
      "train loss:0.9470721064961153\n",
      "train loss:1.1383335275949427\n",
      "train loss:0.7668968069036921\n",
      "train loss:0.7787465251782456\n",
      "train loss:0.9883067996670611\n",
      "train loss:0.9191000886150743\n",
      "train loss:0.7105709668637319\n",
      "train loss:0.827397931406568\n",
      "train loss:0.8695255252080255\n",
      "train loss:0.8245045681591927\n",
      "train loss:0.9833236541810215\n",
      "train loss:0.9451287638274295\n",
      "train loss:1.0203561411194886\n",
      "train loss:0.9307523220741284\n",
      "train loss:0.7864103700489021\n",
      "train loss:0.9051013541244344\n",
      "train loss:0.8922536355856744\n",
      "train loss:0.8712071040731014\n",
      "train loss:0.8753678414047772\n",
      "train loss:0.9096319170007061\n",
      "train loss:0.7761621812692983\n",
      "train loss:1.0012047163807165\n",
      "train loss:0.9302577118112556\n",
      "train loss:1.039249727193966\n",
      "train loss:0.9145742525691616\n",
      "train loss:0.774765590912126\n",
      "train loss:0.9664356264431604\n",
      "train loss:1.0312029071157227\n",
      "train loss:1.1037782461232097\n",
      "train loss:0.9832761266380095\n",
      "train loss:1.0531235749311127\n",
      "train loss:0.8408539581058586\n",
      "train loss:0.9361290424566708\n",
      "train loss:0.6836764710710191\n",
      "train loss:0.8963295614211116\n",
      "train loss:0.9439414246272215\n",
      "train loss:0.8108790872211941\n",
      "train loss:0.8783123072414435\n",
      "train loss:0.8688047572516343\n",
      "train loss:0.9870925633584006\n",
      "train loss:0.9226945964321801\n",
      "train loss:0.920772324067712\n",
      "train loss:0.940122183675389\n",
      "train loss:1.031307366725515\n",
      "train loss:0.9489831374203693\n",
      "train loss:0.819686355257333\n",
      "train loss:0.9901145132077851\n",
      "train loss:0.7942112348716228\n",
      "train loss:1.0087349545317263\n",
      "train loss:0.9712701742210575\n",
      "train loss:0.8240813664993474\n",
      "train loss:0.7684006279604821\n",
      "train loss:0.9079055394551446\n",
      "train loss:0.8403014228129828\n",
      "train loss:0.9557948449544706\n",
      "train loss:1.0419681867315789\n",
      "train loss:0.9167585994851208\n",
      "train loss:0.9580038974213773\n",
      "train loss:0.9319169147440783\n",
      "train loss:0.9941928036734295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.9104763451947352\n",
      "train loss:0.7963827437842703\n",
      "train loss:0.808484266513213\n",
      "train loss:0.7504229784539453\n",
      "train loss:0.7838610982213801\n",
      "train loss:0.8661940441292184\n",
      "train loss:1.05105946753842\n",
      "train loss:0.9193318932545625\n",
      "train loss:0.8492559819340151\n",
      "train loss:1.0808702148162765\n",
      "train loss:0.9841847360214565\n",
      "train loss:0.9814635195352949\n",
      "train loss:0.8767772990996783\n",
      "train loss:0.8379904886752939\n",
      "train loss:0.9408457347665817\n",
      "train loss:0.8379659554588739\n",
      "train loss:1.0890497249821194\n",
      "train loss:0.8278944141792627\n",
      "train loss:0.8105282475126036\n",
      "train loss:0.825619239126591\n",
      "train loss:0.6486761034547973\n",
      "train loss:1.0094770192455242\n",
      "train loss:0.750611894671772\n",
      "train loss:0.9756085806575452\n",
      "train loss:0.8880483172167843\n",
      "train loss:0.7942490697310741\n",
      "train loss:0.7397943754882103\n",
      "train loss:0.7834336865032338\n",
      "train loss:1.0233874614735474\n",
      "train loss:1.0879913816161986\n",
      "train loss:0.8594754052146514\n",
      "train loss:0.9030336745087201\n",
      "train loss:0.9118021647686934\n",
      "train loss:0.7627997337922028\n",
      "train loss:0.9729835719375062\n",
      "train loss:1.0310147594245682\n",
      "=== epoch:7, train acc:0.994, test acc:0.991 ===\n",
      "train loss:0.9943183711152221\n",
      "train loss:0.9331662679805928\n",
      "train loss:0.9470133633761957\n",
      "train loss:0.9756325639236306\n",
      "train loss:0.8569721896285504\n",
      "train loss:0.9938439769544729\n",
      "train loss:0.6923889548854367\n",
      "train loss:0.8665750374109136\n",
      "train loss:0.921040202578919\n",
      "train loss:1.1006762724246426\n",
      "train loss:0.9560772800059874\n",
      "train loss:0.9064866422127581\n",
      "train loss:0.9010456790756891\n",
      "train loss:1.0342038831709501\n",
      "train loss:0.8999592521974862\n",
      "train loss:0.7969349104064757\n",
      "train loss:0.8241037532660812\n",
      "train loss:0.8480970041115576\n",
      "train loss:0.8119900160645802\n",
      "train loss:0.9241605733064374\n",
      "train loss:0.9287593788995924\n",
      "train loss:0.9508809057908906\n",
      "train loss:0.7862540261055222\n",
      "train loss:0.9432008926513048\n",
      "train loss:1.0066709720443425\n",
      "train loss:0.8105734974120165\n",
      "train loss:0.9042747951606581\n",
      "train loss:0.8032357521448283\n",
      "train loss:0.9457334394696505\n",
      "train loss:0.7724288983824852\n",
      "train loss:0.8807017679830745\n",
      "train loss:0.8092789595965894\n",
      "train loss:0.9484645837603464\n",
      "train loss:0.9860057627375423\n",
      "train loss:1.0502603391014569\n",
      "train loss:0.8874963465439977\n",
      "train loss:0.9197123429854913\n",
      "train loss:0.7680215756651095\n",
      "train loss:0.917085511711111\n",
      "train loss:0.7716868334436962\n",
      "train loss:0.874016655075461\n",
      "train loss:0.9135934002773587\n",
      "train loss:0.8346906732245327\n",
      "train loss:0.9905027315122994\n",
      "train loss:0.920522981251302\n",
      "train loss:0.7469070046178204\n",
      "train loss:0.7457831510984945\n",
      "train loss:0.8284763929942378\n",
      "train loss:0.8831461284639512\n",
      "train loss:0.8179618147929874\n",
      "train loss:0.9357453110644625\n",
      "train loss:0.9299382664505537\n",
      "train loss:0.9540418126756676\n",
      "train loss:0.963369863331089\n",
      "train loss:0.8108271527373758\n",
      "train loss:0.9058429796079759\n",
      "train loss:0.9870855831300915\n",
      "train loss:0.8848800650943415\n",
      "train loss:0.9617867937815413\n",
      "train loss:0.8406264352342268\n",
      "train loss:0.7869979240357101\n",
      "train loss:0.9094754154191205\n",
      "train loss:0.9822564158605364\n",
      "train loss:0.788060989558732\n",
      "train loss:0.9034501295941326\n",
      "train loss:1.016845811228197\n",
      "train loss:1.0668283240553413\n",
      "train loss:0.8283373402954175\n",
      "train loss:0.768044941833805\n",
      "train loss:0.8185124032380289\n",
      "train loss:0.8049817606698305\n",
      "train loss:0.8731640639258785\n",
      "train loss:0.797288320605169\n",
      "train loss:0.7410359267830599\n",
      "train loss:0.9701131752348556\n",
      "train loss:0.9611966634832936\n",
      "train loss:1.0625107753222651\n",
      "train loss:0.8560598398342263\n",
      "train loss:0.8351989981842123\n",
      "train loss:0.9232404743280275\n",
      "train loss:0.9253272262307128\n",
      "train loss:0.9688717263839313\n",
      "train loss:1.0081595808849202\n",
      "train loss:0.8144232145606612\n",
      "train loss:0.8194078730201138\n",
      "train loss:0.9803661567574622\n",
      "train loss:0.9574759676533553\n",
      "train loss:0.9108039075736262\n",
      "train loss:0.9384293720950858\n",
      "train loss:0.8632084591343915\n",
      "train loss:0.782691875972989\n",
      "train loss:1.0269404332166852\n",
      "train loss:0.8699970194937704\n",
      "train loss:0.8814903335070522\n",
      "train loss:1.0061133273709952\n",
      "train loss:1.2559186426432025\n",
      "train loss:0.8804875702079972\n",
      "train loss:0.8529483253259994\n",
      "train loss:0.8534277258694073\n",
      "train loss:0.9796249644065448\n",
      "train loss:0.8973209532337936\n",
      "train loss:0.8819211016819515\n",
      "train loss:0.9581507763435119\n",
      "train loss:0.8685687804179568\n",
      "train loss:1.0423819723283823\n",
      "train loss:0.8911069059238186\n",
      "train loss:0.9461497633930598\n",
      "train loss:1.0114551900811328\n",
      "train loss:1.0384586419533723\n",
      "train loss:0.9141434469400109\n",
      "train loss:1.1099467724989942\n",
      "train loss:0.789015416402207\n",
      "train loss:0.9062445805365177\n",
      "train loss:0.9258146167287629\n",
      "train loss:0.9563732983456106\n",
      "train loss:0.8457216393296325\n",
      "train loss:1.0408653827560763\n",
      "train loss:0.8224323177519763\n",
      "train loss:0.937069934795124\n",
      "train loss:1.011625046163879\n",
      "train loss:0.8735098654907825\n",
      "train loss:0.9260301798890178\n",
      "train loss:0.9362917832334489\n",
      "train loss:0.9862060700845225\n",
      "train loss:0.833277481038796\n",
      "train loss:0.8798159083610692\n",
      "train loss:0.9214117072834291\n",
      "train loss:1.0925832250560723\n",
      "train loss:0.8067305061210667\n",
      "train loss:0.8325087065863253\n",
      "train loss:0.8186915162971402\n",
      "train loss:0.7500074305411434\n",
      "train loss:0.8295972069913646\n",
      "train loss:0.9913610275377717\n",
      "train loss:0.9817931740464978\n",
      "train loss:0.795722589716302\n",
      "train loss:0.9886179251599748\n",
      "train loss:0.9459265352782897\n",
      "train loss:0.7426525347622638\n",
      "train loss:0.9873318885150928\n",
      "train loss:0.7366544375863991\n",
      "train loss:0.9585122195910691\n",
      "train loss:0.9691738813528669\n",
      "train loss:0.8607091733149858\n",
      "train loss:0.7336652576775688\n",
      "train loss:0.8540172166794615\n",
      "train loss:0.9444391785029292\n",
      "train loss:0.9049397478034504\n",
      "train loss:0.839718435383473\n",
      "train loss:0.8083764653329741\n",
      "train loss:0.9066133911234852\n",
      "train loss:1.1465919219670413\n",
      "train loss:0.8292130302300349\n",
      "train loss:0.7994981724734407\n",
      "train loss:0.8082550737970854\n",
      "train loss:0.960384173071375\n",
      "train loss:0.8391084473687483\n",
      "train loss:0.8084009877147074\n",
      "train loss:0.7375617337740189\n",
      "train loss:0.953916815756576\n",
      "train loss:1.023529967194616\n",
      "train loss:0.8905433959018595\n",
      "train loss:1.0489195140964354\n",
      "train loss:0.9930565277818532\n",
      "train loss:0.8072563765064236\n",
      "train loss:0.9249223884808285\n",
      "train loss:0.8740214441836432\n",
      "train loss:0.9606888216778607\n",
      "train loss:0.771515196507395\n",
      "train loss:0.7907599276656048\n",
      "train loss:1.0113116602265562\n",
      "train loss:0.9284941340132445\n",
      "train loss:0.9820651065223407\n",
      "train loss:0.7724490269769738\n",
      "train loss:0.7555075014539319\n",
      "train loss:0.9672882253034387\n",
      "train loss:0.887073261284256\n",
      "train loss:0.827993231935511\n",
      "train loss:0.9566501506703418\n",
      "train loss:0.8816805905454115\n",
      "train loss:0.9106614076097995\n",
      "train loss:0.7921979783558188\n",
      "train loss:0.8795852674484972\n",
      "train loss:0.8510860865875095\n",
      "train loss:0.7735424132247011\n",
      "train loss:1.0502806507764206\n",
      "train loss:0.8087341934945649\n",
      "train loss:0.658758043446203\n",
      "train loss:0.8730497752325214\n",
      "train loss:1.0303267177073203\n",
      "train loss:0.8344310405570379\n",
      "train loss:0.7974045659747055\n",
      "train loss:0.959252644738604\n",
      "train loss:0.9235751118761332\n",
      "train loss:0.8576693671903408\n",
      "train loss:0.9917983738253663\n",
      "train loss:1.0130965760645636\n",
      "train loss:1.0771995506556855\n",
      "train loss:1.0584284908335904\n",
      "train loss:0.932120268403374\n",
      "train loss:0.8973425566769555\n",
      "train loss:1.0369989650959188\n",
      "train loss:0.7083467242911717\n",
      "train loss:0.7400223580543817\n",
      "train loss:0.9992632140439602\n",
      "train loss:0.9324539060409568\n",
      "train loss:0.9575189980931317\n",
      "train loss:1.0298170981840138\n",
      "train loss:1.0693133917693816\n",
      "train loss:0.8581360768386329\n",
      "train loss:0.9518467080789379\n",
      "train loss:1.0998633572385965\n",
      "train loss:0.7652183442988161\n",
      "train loss:1.009704320680802\n",
      "train loss:0.7654156333247829\n",
      "train loss:0.9795068993194139\n",
      "train loss:0.9844683187618809\n",
      "train loss:0.884211559192166\n",
      "train loss:1.1101749618934746\n",
      "train loss:0.7379232088453388\n",
      "train loss:0.8237939088834151\n",
      "train loss:0.929258850210867\n",
      "train loss:0.8214802567876182\n",
      "train loss:1.077310267161024\n",
      "train loss:0.9894058223490121\n",
      "train loss:0.9172421993540714\n",
      "train loss:0.9345943080256148\n",
      "train loss:0.7870022442077371\n",
      "train loss:1.0353323634997604\n",
      "train loss:1.0009261470620983\n",
      "train loss:0.7595057366581389\n",
      "train loss:0.8992086945213541\n",
      "train loss:0.6219718763748885\n",
      "train loss:0.8975516884148038\n",
      "train loss:0.7510869944142955\n",
      "train loss:0.9014352794453583\n",
      "train loss:0.799996345337843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.0677461268071449\n",
      "train loss:1.0771653054244148\n",
      "train loss:0.8422997529696665\n",
      "train loss:0.9809752780848437\n",
      "train loss:0.9509831592230387\n",
      "train loss:0.804898247056264\n",
      "train loss:0.8305432115442803\n",
      "train loss:0.7847773041987611\n",
      "train loss:0.9897556002039394\n",
      "train loss:0.8465097730421631\n",
      "train loss:0.9006942723554985\n",
      "train loss:0.857871669171201\n",
      "train loss:0.8320764382824004\n",
      "train loss:0.9894598660503489\n",
      "train loss:0.8642477433989102\n",
      "train loss:1.0059634144437444\n",
      "train loss:0.8986712807754504\n",
      "train loss:1.1517428232771203\n",
      "train loss:0.7766351078272506\n",
      "train loss:0.956460799844636\n",
      "train loss:0.993750071389702\n",
      "train loss:0.9170360592627957\n",
      "train loss:0.8266218416325417\n",
      "train loss:0.9456774373055623\n",
      "train loss:0.9027805641235205\n",
      "train loss:0.8434036415051017\n",
      "train loss:0.8068787808039979\n",
      "train loss:0.7983529580329854\n",
      "train loss:0.8142284360368128\n",
      "train loss:0.8876465876749006\n",
      "train loss:1.0054518256959277\n",
      "train loss:1.1384020847383765\n",
      "train loss:0.6819059185617614\n",
      "train loss:0.8274956700307861\n",
      "train loss:0.9813127735310175\n",
      "train loss:0.876155579350381\n",
      "train loss:1.0107205522702563\n",
      "train loss:0.7704409707760004\n",
      "train loss:0.7381325830486257\n",
      "train loss:0.9243896989404989\n",
      "train loss:0.8753138821058023\n",
      "train loss:0.9039394946726266\n",
      "train loss:0.8544382496686888\n",
      "train loss:0.9340473161072732\n",
      "train loss:0.869864935584331\n",
      "train loss:1.0178563161112992\n",
      "train loss:0.8765052199433692\n",
      "train loss:0.8653718302254961\n",
      "train loss:0.847094281927022\n",
      "train loss:0.7242201621783014\n",
      "train loss:0.8118441506598814\n",
      "train loss:0.7731321108877978\n",
      "train loss:0.7751555557010659\n",
      "train loss:0.9422377130340455\n",
      "train loss:0.9829273476855933\n",
      "train loss:0.8348414200115123\n",
      "train loss:0.8903196904870048\n",
      "train loss:0.7866058987594303\n",
      "train loss:1.006679728239383\n",
      "train loss:0.8032749181492553\n",
      "train loss:0.9139944600115016\n",
      "train loss:0.8832682390790542\n",
      "train loss:0.9106561809865198\n",
      "train loss:0.8826885167376386\n",
      "train loss:0.9174347063036972\n",
      "train loss:0.7938144448238086\n",
      "train loss:0.8319850987421067\n",
      "train loss:0.9613569549451384\n",
      "train loss:0.9584061064636473\n",
      "train loss:0.9774715178784187\n",
      "train loss:0.7634175650109727\n",
      "train loss:0.7772922231421597\n",
      "train loss:0.9287520788773292\n",
      "train loss:0.9181844265929725\n",
      "train loss:0.9456270589753139\n",
      "train loss:1.0208805532637002\n",
      "train loss:0.8829539761267549\n",
      "train loss:0.8901470582326483\n",
      "train loss:0.8979427411341709\n",
      "train loss:0.9511957935545143\n",
      "train loss:0.8198275745527265\n",
      "train loss:0.76050330363867\n",
      "train loss:0.9079120951583066\n",
      "train loss:0.9016131733233367\n",
      "train loss:0.8762020892557157\n",
      "train loss:0.8731256702699506\n",
      "train loss:0.9745425644375345\n",
      "train loss:0.963333792695446\n",
      "train loss:1.0246011624245255\n",
      "train loss:0.9584186252073437\n",
      "train loss:0.7445581024332047\n",
      "train loss:0.8496760578127079\n",
      "train loss:1.044281092155026\n",
      "train loss:0.9314551642447111\n",
      "train loss:0.9532972128547017\n",
      "train loss:0.979284608886739\n",
      "train loss:0.904968096159793\n",
      "train loss:1.0336504360075487\n",
      "train loss:0.8706772310927059\n",
      "train loss:0.9593354097125563\n",
      "train loss:0.9629839145974975\n",
      "train loss:0.9222013142624991\n",
      "train loss:0.9426586230498545\n",
      "train loss:0.7728066445711882\n",
      "train loss:0.7735279355626791\n",
      "train loss:0.7444514410511761\n",
      "train loss:0.8619294742360514\n",
      "train loss:0.8857988814269069\n",
      "train loss:0.8753484418723293\n",
      "train loss:0.9047511471635988\n",
      "train loss:0.9207751620905236\n",
      "train loss:0.8679523339863014\n",
      "train loss:1.0045041518348914\n",
      "train loss:0.8931922103247478\n",
      "train loss:0.8804287459500173\n",
      "train loss:0.8472817169676926\n",
      "train loss:0.9251683557277544\n",
      "train loss:0.8193774760157789\n",
      "train loss:0.8230222653298783\n",
      "train loss:0.8505489630580607\n",
      "train loss:0.9466228328674763\n",
      "train loss:0.87907454863145\n",
      "train loss:0.6953389741430029\n",
      "train loss:0.780789267555356\n",
      "train loss:0.7965455645234109\n",
      "train loss:0.7902332483809991\n",
      "train loss:0.8167954188293125\n",
      "train loss:0.7838037160936484\n",
      "train loss:0.8491349085804184\n",
      "train loss:0.8916191616086085\n",
      "train loss:0.8728898499703293\n",
      "train loss:0.941458373974239\n",
      "train loss:0.7697261614662768\n",
      "train loss:0.9557272539004417\n",
      "train loss:0.733993665411722\n",
      "train loss:0.8197443688518673\n",
      "train loss:0.9050971841700373\n",
      "train loss:1.0031255559802283\n",
      "train loss:0.9624764801890536\n",
      "train loss:0.9033918837617488\n",
      "train loss:0.8440479271685942\n",
      "train loss:0.9328030319351658\n",
      "train loss:1.043870261710982\n",
      "train loss:0.8901211815546184\n",
      "train loss:0.9062716267003056\n",
      "train loss:1.020915164349628\n",
      "train loss:0.9572329642771621\n",
      "train loss:0.8793165093161965\n",
      "train loss:0.9923598313434603\n",
      "train loss:0.7591143904324447\n",
      "train loss:0.8302347591233208\n",
      "train loss:0.8022306168757312\n",
      "train loss:0.7621454135533243\n",
      "train loss:0.8003426026247483\n",
      "train loss:0.9954642764105549\n",
      "train loss:0.9218961890876386\n",
      "train loss:0.7611444221751743\n",
      "train loss:0.8022282140280332\n",
      "train loss:0.885054560340134\n",
      "train loss:0.9586044924595859\n",
      "train loss:0.9560386448611174\n",
      "train loss:0.9279118826561492\n",
      "train loss:1.0934049770930756\n",
      "train loss:0.9253884759108699\n",
      "train loss:1.1654198301658487\n",
      "train loss:0.8338029588758105\n",
      "train loss:0.9261936973863795\n",
      "train loss:0.7818883826273934\n",
      "train loss:0.8977058284703255\n",
      "train loss:0.7796258407226984\n",
      "train loss:1.044892029038761\n",
      "train loss:0.9618685563455328\n",
      "train loss:0.9058948797153829\n",
      "train loss:0.8742143047778125\n",
      "train loss:0.9289642645765571\n",
      "train loss:1.0134056823867412\n",
      "train loss:0.9491516200808625\n",
      "train loss:1.0127662730881373\n",
      "train loss:0.9429644247916772\n",
      "train loss:0.885213901050268\n",
      "train loss:1.0294717266434583\n",
      "train loss:0.9105794212737197\n",
      "train loss:0.7979468464807724\n",
      "train loss:0.8127862420970356\n",
      "train loss:1.0383657742640173\n",
      "train loss:0.9397599002287729\n",
      "train loss:1.0791923248619275\n",
      "train loss:0.8268448265425379\n",
      "train loss:0.9069927168141075\n",
      "train loss:0.9135337506681901\n",
      "train loss:0.85103785907427\n",
      "train loss:1.0231338109145864\n",
      "train loss:0.885501037812852\n",
      "train loss:0.737710479277152\n",
      "train loss:0.9751317278975722\n",
      "train loss:0.8498983592631076\n",
      "train loss:1.025881891307558\n",
      "train loss:0.9817994218936187\n",
      "train loss:0.9531510255782982\n",
      "train loss:0.9123276183007734\n",
      "train loss:0.782252335426371\n",
      "train loss:0.9327012462684913\n",
      "train loss:0.9360029583119224\n",
      "train loss:0.8910841919913137\n",
      "train loss:0.9212442018286912\n",
      "train loss:1.041179902778933\n",
      "train loss:0.9230901172347009\n",
      "train loss:1.0353574741027156\n",
      "train loss:0.843828237929006\n",
      "train loss:0.9113483807244908\n",
      "train loss:0.8316704692863373\n",
      "train loss:0.8808048075137019\n",
      "train loss:0.916221557136572\n",
      "train loss:0.7196980957532978\n",
      "train loss:0.8940941051755233\n",
      "train loss:0.8740602772000462\n",
      "train loss:0.9212323292960537\n",
      "train loss:0.9297022691743787\n",
      "train loss:0.8167855775655597\n",
      "train loss:1.017824794284236\n",
      "train loss:1.0811249681459136\n",
      "train loss:0.9191809133124639\n",
      "train loss:0.8642739105579951\n",
      "train loss:0.8827532592453734\n",
      "train loss:0.939222158240761\n",
      "train loss:0.9640024259947544\n",
      "train loss:0.9726231470711255\n",
      "train loss:1.033315857602083\n",
      "train loss:0.8785188547813227\n",
      "train loss:0.886615297648533\n",
      "train loss:0.954036216957549\n",
      "train loss:0.9122950044939028\n",
      "train loss:0.9917779892682294\n",
      "train loss:0.9395983960434467\n",
      "train loss:0.8772777714297314\n",
      "train loss:1.005131430392816\n",
      "train loss:0.9558095357716776\n",
      "train loss:0.8732230123169603\n",
      "train loss:0.8609223034209172\n",
      "train loss:0.88767784461717\n",
      "train loss:0.8500665932014705\n",
      "train loss:0.8180701770874532\n",
      "train loss:1.0126422178535548\n",
      "train loss:0.8870859097055772\n",
      "train loss:1.0156759523287713\n",
      "train loss:0.8504847477046669\n",
      "train loss:1.0704976688401988\n",
      "train loss:0.9607948897978033\n",
      "train loss:1.1052273337071947\n",
      "train loss:0.897653130424309\n",
      "train loss:0.9778351177138194\n",
      "train loss:0.8920144644929761\n",
      "train loss:0.9373132460810655\n",
      "train loss:0.8846999695134398\n",
      "train loss:0.9302176530376128\n",
      "train loss:0.9886684021017644\n",
      "train loss:0.923456865965134\n",
      "train loss:1.0027479510535549\n",
      "train loss:0.8706464821238176\n",
      "train loss:0.9276252378001696\n",
      "train loss:0.7664397437697608\n",
      "train loss:1.009557086714965\n",
      "train loss:0.8772628384606981\n",
      "train loss:0.8699707969858512\n",
      "train loss:1.001151352659721\n",
      "train loss:0.8576039325185318\n",
      "train loss:0.7245935146626479\n",
      "train loss:0.9357757577556833\n",
      "train loss:0.9549035318049925\n",
      "train loss:0.8142515827902838\n",
      "train loss:0.9761163085260915\n",
      "train loss:0.8723754463031128\n",
      "train loss:0.9420629912489424\n",
      "train loss:0.905348854579162\n",
      "train loss:0.7421474300978821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.8640411208015019\n",
      "train loss:0.8996281518910142\n",
      "train loss:1.0667341467866203\n",
      "train loss:0.7444809036010444\n",
      "train loss:0.9333552195367755\n",
      "train loss:0.8321127930968184\n",
      "train loss:0.8935176365420682\n",
      "train loss:0.895389944846939\n",
      "train loss:0.9037817651365561\n",
      "train loss:0.8563052138585603\n",
      "train loss:0.7913061204835228\n",
      "train loss:0.8916277886462929\n",
      "train loss:0.8716759460406219\n",
      "train loss:0.8233801238372638\n",
      "train loss:0.8901803047712917\n",
      "train loss:0.8893315638235946\n",
      "train loss:0.9770314409588997\n",
      "train loss:0.8931090439305374\n",
      "train loss:0.9312263722915206\n",
      "train loss:0.82185436541566\n",
      "train loss:0.8754426350392519\n",
      "train loss:0.8890101028957952\n",
      "train loss:0.9435265273391928\n",
      "train loss:1.0286758385345605\n",
      "train loss:0.9376420375790936\n",
      "train loss:1.0223794879396337\n",
      "train loss:0.8984428036088348\n",
      "train loss:0.8969995909705125\n",
      "train loss:0.7732176109758073\n",
      "train loss:1.1171777427890746\n",
      "train loss:0.9201734285145358\n",
      "train loss:0.9471328556668912\n",
      "train loss:0.7590558098779071\n",
      "train loss:0.723874602377773\n",
      "train loss:0.9022732606046227\n",
      "train loss:0.92711123364569\n",
      "train loss:0.9070514728157639\n",
      "train loss:0.8963298629150122\n",
      "train loss:1.0372179748103383\n",
      "train loss:1.0625057370352429\n",
      "train loss:0.9770840618113432\n",
      "train loss:0.7462728132696133\n",
      "train loss:0.8645949995286898\n",
      "train loss:0.6742849332941263\n",
      "train loss:0.9139413276285521\n",
      "train loss:0.9558349867196629\n",
      "train loss:1.0076166229709584\n",
      "train loss:0.8660666669347505\n",
      "train loss:0.9774692134830032\n",
      "train loss:0.897152955356719\n",
      "train loss:1.0564373217559146\n",
      "train loss:0.7077174972209189\n",
      "train loss:0.9333801560743935\n",
      "train loss:0.9838857679355836\n",
      "train loss:0.813862826613557\n",
      "train loss:0.9427398036234677\n",
      "train loss:0.7924764890920831\n",
      "train loss:1.004539209314651\n",
      "train loss:0.8748632576988542\n",
      "train loss:0.8617910783704614\n",
      "train loss:0.9337873686259754\n",
      "train loss:0.8834842570027223\n",
      "train loss:0.957817628412608\n",
      "train loss:0.7846333591286393\n",
      "train loss:0.8798496276950111\n",
      "train loss:0.8055731035842687\n",
      "train loss:0.7829335168841337\n",
      "train loss:0.8512211405313934\n",
      "train loss:0.8871388739754471\n",
      "train loss:1.0825846950226647\n",
      "train loss:0.9593468987241316\n",
      "train loss:1.0447918341423603\n",
      "train loss:1.023581575428035\n",
      "train loss:0.6741764337444939\n",
      "train loss:0.8678856178887361\n",
      "train loss:0.7732139612151151\n",
      "train loss:0.9890341508624158\n",
      "train loss:0.8741512431536179\n",
      "train loss:0.8349865692825993\n",
      "train loss:0.751216731518538\n",
      "train loss:0.858457318823065\n",
      "train loss:1.0692477206079811\n",
      "train loss:0.8965237670623882\n",
      "train loss:0.8506609048430813\n",
      "train loss:1.016236983350554\n",
      "train loss:0.7620290120166793\n",
      "train loss:0.9462412619272855\n",
      "train loss:0.8229042790572325\n",
      "=== epoch:8, train acc:0.993, test acc:0.989 ===\n",
      "train loss:0.8616075913197159\n",
      "train loss:0.9677890769997956\n",
      "train loss:1.062932120945358\n",
      "train loss:0.8652004712375307\n",
      "train loss:0.7893025936967695\n",
      "train loss:0.9635982571558945\n",
      "train loss:0.9364979068836135\n",
      "train loss:0.8203813829318354\n",
      "train loss:0.8616046580767639\n",
      "train loss:0.8982642641936012\n",
      "train loss:0.9919017608125706\n",
      "train loss:1.0297732843893885\n",
      "train loss:0.8888224669934084\n",
      "train loss:0.941807541583606\n",
      "train loss:0.9000170365770285\n",
      "train loss:0.9393293256441381\n",
      "train loss:0.8454354805119303\n",
      "train loss:0.8392373121016106\n",
      "train loss:0.8730316646556894\n",
      "train loss:0.9541541752553944\n",
      "train loss:0.8084190956393462\n",
      "train loss:1.032589324844029\n",
      "train loss:0.9069877887480079\n",
      "train loss:1.1123890872472593\n",
      "train loss:0.9608650214388257\n",
      "train loss:0.838256367035447\n",
      "train loss:0.868722284128757\n",
      "train loss:0.8590446936019126\n",
      "train loss:0.9521618818253574\n",
      "train loss:0.823357105916973\n",
      "train loss:0.9603467693136326\n",
      "train loss:0.95255423755725\n",
      "train loss:0.7812296893702296\n",
      "train loss:0.7389136122472703\n",
      "train loss:0.9571602402121008\n",
      "train loss:0.9039043512762325\n",
      "train loss:0.7747883560620983\n",
      "train loss:0.7605659733568764\n",
      "train loss:0.9290632244205128\n",
      "train loss:0.9104763914823611\n",
      "train loss:0.8918860253726719\n",
      "train loss:0.9096794355412405\n",
      "train loss:0.8175290809883813\n",
      "train loss:0.9132904357378274\n",
      "train loss:0.8902938058187476\n",
      "train loss:0.9775178607426233\n",
      "train loss:0.7814571467773324\n",
      "train loss:0.9337229021557627\n",
      "train loss:0.91226930339421\n",
      "train loss:0.9971289910756151\n",
      "train loss:0.9150873187798706\n",
      "train loss:0.8978706642009366\n",
      "train loss:0.8782957328074071\n",
      "train loss:1.0625648773812326\n",
      "train loss:0.8512891701400561\n",
      "train loss:0.8423916937030641\n",
      "train loss:0.8964283410942648\n",
      "train loss:0.693605624413711\n",
      "train loss:0.7623116043646402\n",
      "train loss:0.9335264383300934\n",
      "train loss:1.0171737810849952\n",
      "train loss:0.8981540673632733\n",
      "train loss:0.9365930145390187\n",
      "train loss:0.8306023046515476\n",
      "train loss:0.8333812641217883\n",
      "train loss:0.8874954337146361\n",
      "train loss:0.8914695239365077\n",
      "train loss:0.9723870243795697\n",
      "train loss:0.7781867462946789\n",
      "train loss:0.9235504612157631\n",
      "train loss:0.9472678259765915\n",
      "train loss:0.9267716373403754\n",
      "train loss:0.8300308552405953\n",
      "train loss:0.9227076608195852\n",
      "train loss:1.0359998307892972\n",
      "train loss:0.9068096922662612\n",
      "train loss:1.021287528575248\n",
      "train loss:0.8987677396671649\n",
      "train loss:1.0117967237959877\n",
      "train loss:0.8553319490692727\n",
      "train loss:0.8392461968405605\n",
      "train loss:0.8887378769671677\n",
      "train loss:1.0297718514324357\n",
      "train loss:0.988824280380711\n",
      "train loss:0.9818948488005226\n",
      "train loss:0.895433932059506\n",
      "train loss:0.89304336396025\n",
      "train loss:0.9255301963283266\n",
      "train loss:0.9176612962007243\n",
      "train loss:0.7554286973904047\n",
      "train loss:0.9594662496870412\n",
      "train loss:0.7869561740525072\n",
      "train loss:0.8952709735528102\n",
      "train loss:0.9626817166648413\n",
      "train loss:1.0041304996598859\n",
      "train loss:0.9458809114963244\n",
      "train loss:0.8319001868948349\n",
      "train loss:0.8357377990537457\n",
      "train loss:0.6694510858143218\n",
      "train loss:0.9393215042731282\n",
      "train loss:0.9356759588025889\n",
      "train loss:0.9902288346061124\n",
      "train loss:0.9330244898957888\n",
      "train loss:0.7120726573999182\n",
      "train loss:0.9751789902477901\n",
      "train loss:1.0373296820066333\n",
      "train loss:0.9859748984624184\n",
      "train loss:0.9416132206829556\n",
      "train loss:0.9444150027819042\n",
      "train loss:0.876908783532214\n",
      "train loss:0.7462803332510448\n",
      "train loss:0.7527862031051713\n",
      "train loss:0.9039456390003606\n",
      "train loss:0.9546572361687942\n",
      "train loss:1.0130567232598882\n",
      "train loss:0.8023950144665535\n",
      "train loss:0.9620637126293937\n",
      "train loss:0.9436039961372907\n",
      "train loss:1.0024125975444016\n",
      "train loss:0.9397855423003985\n",
      "train loss:0.8849398062798496\n",
      "train loss:0.9747618223923062\n",
      "train loss:0.9494449730350876\n",
      "train loss:0.7270942398637801\n",
      "train loss:0.8732329437333314\n",
      "train loss:0.8686576958371118\n",
      "train loss:1.045619979974125\n",
      "train loss:0.8993643050203339\n",
      "train loss:0.921362795323991\n",
      "train loss:0.899564881706354\n",
      "train loss:0.9047069403305088\n",
      "train loss:0.8358324511719732\n",
      "train loss:0.8619245830669751\n",
      "train loss:0.8491749917788509\n",
      "train loss:1.0943452081326965\n",
      "train loss:0.9246008857971957\n",
      "train loss:1.0330891308589578\n",
      "train loss:0.8606789191186647\n",
      "train loss:0.9666810900779484\n",
      "train loss:1.0133488556809294\n",
      "train loss:0.9219605459049709\n",
      "train loss:0.9094204661456087\n",
      "train loss:0.7931562243506136\n",
      "train loss:0.8410524940237714\n",
      "train loss:0.7149067600105358\n",
      "train loss:0.8674112043812099\n",
      "train loss:0.880750318991098\n",
      "train loss:0.8812258116465753\n",
      "train loss:0.9491509013565395\n",
      "train loss:0.9156645112848524\n",
      "train loss:0.8778024835517784\n",
      "train loss:0.8653994312469273\n",
      "train loss:0.8618853660670147\n",
      "train loss:0.8996139261128652\n",
      "train loss:0.9550693926033929\n",
      "train loss:0.8946390366561843\n",
      "train loss:0.9855140604511184\n",
      "train loss:1.0261079086846503\n",
      "train loss:1.0256910792229132\n",
      "train loss:0.7249418772332332\n",
      "train loss:0.9946261620565369\n",
      "train loss:0.9635329488725641\n",
      "train loss:0.874752152575155\n",
      "train loss:1.123511209187587\n",
      "train loss:0.8000123886271586\n",
      "train loss:0.9381083778679827\n",
      "train loss:0.9017492715587211\n",
      "train loss:0.9276686732153218\n",
      "train loss:0.978012774835738\n",
      "train loss:0.9177604911809374\n",
      "train loss:0.8359738145538365\n",
      "train loss:1.036198414325746\n",
      "train loss:0.9148732383207269\n",
      "train loss:0.8966649742865404\n",
      "train loss:0.8755943371608036\n",
      "train loss:0.9878669894268538\n",
      "train loss:0.873108860666469\n",
      "train loss:1.0889669860739357\n",
      "train loss:0.9552709607988352\n",
      "train loss:0.9296064066218088\n",
      "train loss:0.8386385793231576\n",
      "train loss:0.9681646929450138\n",
      "train loss:0.8477711646612599\n",
      "train loss:0.8851231857936797\n",
      "train loss:0.9202299906796351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.9643464619213262\n",
      "train loss:0.7459178980686704\n",
      "train loss:0.890598176907516\n",
      "train loss:0.8562269802836959\n",
      "train loss:0.8538365635550222\n",
      "train loss:1.0213241067666587\n",
      "train loss:1.0775535423903313\n",
      "train loss:0.8837740915372179\n",
      "train loss:0.89410438151735\n",
      "train loss:0.7381608166131879\n",
      "train loss:0.8570797207592611\n",
      "train loss:0.9228674811236526\n",
      "train loss:0.8539988324078144\n",
      "train loss:0.8636986948050843\n",
      "train loss:1.050343037763639\n",
      "train loss:0.9812631329853124\n",
      "train loss:1.13153671595751\n",
      "train loss:0.8439032883109222\n",
      "train loss:0.8868827543630637\n",
      "train loss:0.9378748284087846\n",
      "train loss:0.6614667288846935\n",
      "train loss:0.9791777230573854\n",
      "train loss:0.73509493392935\n",
      "train loss:0.7688228053839257\n",
      "train loss:0.8201489029088175\n",
      "train loss:0.9095722103214241\n",
      "train loss:0.9733577818919684\n",
      "train loss:0.9665390198394129\n",
      "train loss:0.9223182605788329\n",
      "train loss:0.9734166452973265\n",
      "train loss:0.9064335967927655\n",
      "train loss:0.8938385312784953\n",
      "train loss:0.9300551714222673\n",
      "train loss:0.9256323291522652\n",
      "train loss:0.9006817167153507\n",
      "train loss:0.8993139737203306\n",
      "train loss:0.9660377319681409\n",
      "train loss:1.0197915482838145\n",
      "train loss:0.8863173978444252\n",
      "train loss:0.8435821857097191\n",
      "train loss:0.8361764830580759\n",
      "train loss:0.938009458417917\n",
      "train loss:0.7725242211003527\n",
      "train loss:0.837109008111782\n",
      "train loss:0.8696612689081284\n",
      "train loss:0.8909102412437835\n",
      "train loss:0.9541913617100566\n",
      "train loss:0.7883819115950313\n",
      "train loss:0.7611759074096699\n",
      "train loss:0.9114399894675159\n",
      "train loss:0.7414346820096276\n",
      "train loss:0.9550629501981212\n",
      "train loss:1.1399108127872202\n",
      "train loss:0.7721602474844056\n",
      "train loss:0.9435877385744027\n",
      "train loss:0.9012937094661372\n",
      "train loss:0.991112834760948\n",
      "train loss:0.9525419630737331\n",
      "train loss:0.6465792417135703\n",
      "train loss:0.9483335202680031\n",
      "train loss:0.9694581719473264\n",
      "train loss:1.0282859297767883\n",
      "train loss:0.9710574190395443\n",
      "train loss:1.0145471276884963\n",
      "train loss:0.7243705032666053\n",
      "train loss:0.8618952418720557\n",
      "train loss:0.902386498355293\n",
      "train loss:0.9572265387269637\n",
      "train loss:0.8963033947238482\n",
      "train loss:0.7552326610088786\n",
      "train loss:0.9074630973049155\n",
      "train loss:0.8959710682807602\n",
      "train loss:0.969737679193804\n",
      "train loss:0.9616014412697357\n",
      "train loss:1.0363391100911086\n",
      "train loss:0.9154310115529145\n",
      "train loss:0.754587184561722\n",
      "train loss:0.9421744076599541\n",
      "train loss:0.8846254626739403\n",
      "train loss:0.8789611363325076\n",
      "train loss:1.1599727786097336\n",
      "train loss:0.868790801152977\n",
      "train loss:0.9189096857747923\n",
      "train loss:0.8421412249911968\n",
      "train loss:1.0019195723433363\n",
      "train loss:1.0508979505360627\n",
      "train loss:0.7598027664482763\n",
      "train loss:0.7366519083897427\n",
      "train loss:0.8421136562392232\n",
      "train loss:0.9713474475721458\n",
      "train loss:0.9959983709205551\n",
      "train loss:0.7344838659077854\n",
      "train loss:0.963414735372029\n",
      "train loss:0.8335273093000185\n",
      "train loss:1.0513264962050162\n",
      "train loss:0.8414993164082568\n",
      "train loss:0.9065335989391408\n",
      "train loss:0.880467962869786\n",
      "train loss:0.8978845949796255\n",
      "train loss:1.0702217139316157\n",
      "train loss:0.8633092585755623\n",
      "train loss:0.9314938612461655\n",
      "train loss:0.7733677325658583\n",
      "train loss:0.7923782271169533\n",
      "train loss:0.8318683782203437\n",
      "train loss:0.8496962813556589\n",
      "train loss:0.8993129209959853\n",
      "train loss:0.8617627455356509\n",
      "train loss:1.0101603297020103\n",
      "train loss:1.1807009861517743\n",
      "train loss:0.909922565444856\n",
      "train loss:0.7597108863029669\n",
      "train loss:0.8520831641873685\n",
      "train loss:0.9861608849507022\n",
      "train loss:0.8317860140368535\n",
      "train loss:0.941477696802322\n",
      "train loss:0.9811449681275473\n",
      "train loss:0.7936836363716766\n",
      "train loss:0.8881623175024174\n",
      "train loss:1.0018774299955848\n",
      "train loss:1.0152159155691607\n",
      "train loss:1.1762062988939885\n",
      "train loss:1.0745659906285916\n",
      "train loss:0.8354478102378724\n",
      "train loss:0.9426741268249379\n",
      "train loss:0.7767360161082489\n",
      "train loss:0.8300277420134161\n",
      "train loss:0.8644830303065639\n",
      "train loss:0.8804436847371349\n",
      "train loss:0.9636898095926948\n",
      "train loss:0.7686702666300457\n",
      "train loss:0.8118173376703883\n",
      "train loss:0.964274027009824\n",
      "train loss:0.8475719586189158\n",
      "train loss:0.7272969515194326\n",
      "train loss:0.8958543983502105\n",
      "train loss:0.835349766420633\n",
      "train loss:0.9589429448400522\n",
      "train loss:0.7560158963768586\n",
      "train loss:0.8405230086919713\n",
      "train loss:0.7093722785058911\n",
      "train loss:0.9987639444434505\n",
      "train loss:1.0214215167718983\n",
      "train loss:0.9468086806160891\n",
      "train loss:0.9872110091994053\n",
      "train loss:1.0610873707647241\n",
      "train loss:0.8847364884745836\n",
      "train loss:1.0014985942274357\n",
      "train loss:0.8304606191617249\n",
      "train loss:0.7731784309157556\n",
      "train loss:0.9117361920854592\n",
      "train loss:0.9343766202899796\n",
      "train loss:0.7698663459044262\n",
      "train loss:0.8130692010975069\n",
      "train loss:0.9040289089740269\n",
      "train loss:0.8340408371978966\n",
      "train loss:1.071944825606107\n",
      "train loss:0.9885590685092065\n",
      "train loss:0.8231974187607602\n",
      "train loss:0.91371868122664\n",
      "train loss:1.0562493106744306\n",
      "train loss:0.8915118768715182\n",
      "train loss:0.8661661736782286\n",
      "train loss:1.0123862940048611\n",
      "train loss:0.9468031016906303\n",
      "train loss:0.8900530970565697\n",
      "train loss:0.8530622988976387\n",
      "train loss:0.7418533957151092\n",
      "train loss:0.8302507510342791\n",
      "train loss:1.0604716212164367\n",
      "train loss:1.0243452537876345\n",
      "train loss:0.9911533777868052\n",
      "train loss:0.8358213785814982\n",
      "train loss:1.0311500915235465\n",
      "train loss:1.0805468261256974\n",
      "train loss:1.0548249048387712\n",
      "train loss:0.8956606207886224\n",
      "train loss:0.7529253824834428\n",
      "train loss:0.8423451165113299\n",
      "train loss:0.9735370734708164\n",
      "train loss:0.9182643979451736\n",
      "train loss:0.9809009947801087\n",
      "train loss:0.8978595152238228\n",
      "train loss:0.7692622112054256\n",
      "train loss:0.9233155878585064\n",
      "train loss:0.9078426092868905\n",
      "train loss:0.829634419558069\n",
      "train loss:0.9327306764131137\n",
      "train loss:0.9682050863234728\n",
      "train loss:0.8445857889759134\n",
      "train loss:0.7943374040167506\n",
      "train loss:1.1433134567614103\n",
      "train loss:0.933401014746217\n",
      "train loss:0.953392150742443\n",
      "train loss:0.89819669754271\n",
      "train loss:0.8206332042033349\n",
      "train loss:0.9907588600093261\n",
      "train loss:0.7488993510357425\n",
      "train loss:0.78352607730542\n",
      "train loss:1.0824213624208623\n",
      "train loss:0.9235786331324589\n",
      "train loss:0.9737772203870838\n",
      "train loss:0.9016654555102609\n",
      "train loss:0.913493872384239\n",
      "train loss:1.0642960388847784\n",
      "train loss:0.8029746443328356\n",
      "train loss:0.7708929897720997\n",
      "train loss:0.914062335692661\n",
      "train loss:0.865531044545377\n",
      "train loss:1.019315559200627\n",
      "train loss:0.7570821076338138\n",
      "train loss:0.95219504007445\n",
      "train loss:0.9844757353707526\n",
      "train loss:0.9316724532322661\n",
      "train loss:0.8082147633344742\n",
      "train loss:1.0441783110764689\n",
      "train loss:0.8070084895534859\n",
      "train loss:0.8999703707672941\n",
      "train loss:0.8416873232799292\n",
      "train loss:0.9436079887119527\n",
      "train loss:0.9697820989842926\n",
      "train loss:0.8151620947736243\n",
      "train loss:0.7988381876812989\n",
      "train loss:0.8013313630316121\n",
      "train loss:0.8699956435505557\n",
      "train loss:0.9815608464401422\n",
      "train loss:0.7484976869076856\n",
      "train loss:0.7909108364804502\n",
      "train loss:0.9315415405209884\n",
      "train loss:0.9962311217952976\n",
      "train loss:0.931924963377955\n",
      "train loss:0.8884333363038073\n",
      "train loss:0.8390508552011096\n",
      "train loss:0.8146606545604382\n",
      "train loss:0.8850696276198132\n",
      "train loss:0.9119665483310031\n",
      "train loss:0.9838947224767952\n",
      "train loss:0.9051905643551906\n",
      "train loss:0.9868546330264444\n",
      "train loss:1.004854369295848\n",
      "train loss:0.9403882647284755\n",
      "train loss:1.0353903319476112\n",
      "train loss:1.0555968237833417\n",
      "train loss:0.8904789412728641\n",
      "train loss:0.9531248678368276\n",
      "train loss:1.1581719485746036\n",
      "train loss:0.9210006299057392\n",
      "train loss:0.8558279255909824\n",
      "train loss:1.0600041219307852\n",
      "train loss:0.9014317678198975\n",
      "train loss:0.8229134596385925\n",
      "train loss:0.9343216686459095\n",
      "train loss:0.816962349114057\n",
      "train loss:1.0606182086061957\n",
      "train loss:0.7016634378537394\n",
      "train loss:0.9548076548525841\n",
      "train loss:0.8383489120775803\n",
      "train loss:1.0459747242323518\n",
      "train loss:0.8694950863439204\n",
      "train loss:0.8100081260190669\n",
      "train loss:0.7859127719514479\n",
      "train loss:1.0142856631605837\n",
      "train loss:1.0435293178811136\n",
      "train loss:0.8751327060125195\n",
      "train loss:0.8001209555850852\n",
      "train loss:0.9638881007809654\n",
      "train loss:0.9933971612185788\n",
      "train loss:0.896255766056298\n",
      "train loss:0.8874546577935252\n",
      "train loss:0.8967081180347337\n",
      "train loss:0.9208823528030782\n",
      "train loss:0.9238621104950969\n",
      "train loss:0.9295466978415236\n",
      "train loss:0.962082275814079\n",
      "train loss:0.8116503564486892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.9828380971550466\n",
      "train loss:0.8171854775147491\n",
      "train loss:1.05260834502286\n",
      "train loss:0.8891081295725101\n",
      "train loss:0.9148923670510072\n",
      "train loss:0.6283106832400202\n",
      "train loss:0.908923480179283\n",
      "train loss:0.9294407320807848\n",
      "train loss:0.8863254967161255\n",
      "train loss:0.913785846949069\n",
      "train loss:1.0392484180394792\n",
      "train loss:0.8176225390505322\n",
      "train loss:0.9436673245694382\n",
      "train loss:0.8105028992740348\n",
      "train loss:0.8996520757885439\n",
      "train loss:0.866704470249958\n",
      "train loss:0.7901106621913055\n",
      "train loss:0.9424920888023793\n",
      "train loss:0.9665315384547418\n",
      "train loss:1.0473051297568636\n",
      "train loss:0.99451287525886\n",
      "train loss:0.8657605869981105\n",
      "train loss:0.8838627936861646\n",
      "train loss:0.8664456417069752\n",
      "train loss:0.7275469887453505\n",
      "train loss:1.2196668410963876\n",
      "train loss:0.9001007819927973\n",
      "train loss:0.7183465969875269\n",
      "train loss:0.86955630267719\n",
      "train loss:0.7481235844080638\n",
      "train loss:0.8241527137657141\n",
      "train loss:0.9144630027916891\n",
      "train loss:0.969917939148553\n",
      "train loss:1.026533544967516\n",
      "train loss:0.8700407905322298\n",
      "train loss:0.950473196482517\n",
      "train loss:0.8379656827510061\n",
      "train loss:0.7896349063799054\n",
      "train loss:0.899282238426662\n",
      "train loss:0.8327336260886824\n",
      "train loss:0.8796660599286824\n",
      "train loss:0.9095616353392612\n",
      "train loss:0.9341741958602783\n",
      "train loss:0.8090419100520989\n",
      "train loss:0.9228421422372144\n",
      "train loss:0.846654669479558\n",
      "train loss:0.8646524518813733\n",
      "train loss:0.9548473139073276\n",
      "train loss:0.8490131241937813\n",
      "train loss:0.8232166744867319\n",
      "train loss:0.9200034307686374\n",
      "train loss:0.8333471932089698\n",
      "train loss:0.7211418726334152\n",
      "train loss:0.804109579814626\n",
      "train loss:0.8813725906508448\n",
      "train loss:0.9901813644850048\n",
      "train loss:0.6750163123475957\n",
      "train loss:1.0522900831554487\n",
      "train loss:0.8741266752183511\n",
      "train loss:0.8925660365597865\n",
      "train loss:1.0995559001149111\n",
      "train loss:0.8209865520065633\n",
      "train loss:0.7240638194915457\n",
      "train loss:1.0555112381391363\n",
      "train loss:0.8744621146573909\n",
      "train loss:0.8807550484788692\n",
      "train loss:0.9354092700610885\n",
      "train loss:0.8465808923066129\n",
      "train loss:0.9203496794108724\n",
      "train loss:0.847480785530961\n",
      "train loss:0.9124576341994074\n",
      "train loss:0.9445981429824417\n",
      "train loss:0.8879166619982741\n",
      "train loss:1.0064988926934546\n",
      "train loss:0.8442721774564922\n",
      "train loss:1.0069793423572253\n",
      "train loss:0.9089669679650525\n",
      "train loss:0.9878706899281146\n",
      "train loss:0.9752173240705531\n",
      "train loss:0.9629107612437422\n",
      "train loss:0.9965319323571564\n",
      "train loss:0.8262994624249502\n",
      "train loss:0.948530716206662\n",
      "train loss:0.8556728786965084\n",
      "train loss:0.7933755623522794\n",
      "train loss:0.8421326685684061\n",
      "train loss:0.8674504936805664\n",
      "train loss:0.6378606780181748\n",
      "train loss:0.7581769152073121\n",
      "train loss:0.9632362631828753\n",
      "train loss:1.001629069545541\n",
      "train loss:0.9744388399138141\n",
      "train loss:0.7262570850961579\n",
      "train loss:0.9035636826961947\n",
      "train loss:0.8463619537318547\n",
      "train loss:0.8959608507839393\n",
      "train loss:0.9233887720735254\n",
      "train loss:0.7740755354189062\n",
      "train loss:0.8733720140481479\n",
      "train loss:1.0163399045419963\n",
      "train loss:0.8115231783587233\n",
      "train loss:1.0068614383999834\n",
      "train loss:0.9579933069446384\n",
      "train loss:0.9712975136731987\n",
      "train loss:1.0907865834158397\n",
      "train loss:0.9042980778770862\n",
      "train loss:1.0361222683191658\n",
      "train loss:0.7702407437074125\n",
      "train loss:0.8791880226910364\n",
      "train loss:0.9466367382632024\n",
      "train loss:0.7237188299568115\n",
      "train loss:0.7726904383368546\n",
      "train loss:0.9287412410429131\n",
      "train loss:0.9382759609426136\n",
      "train loss:0.9051643276658105\n",
      "train loss:0.9542149960958497\n",
      "train loss:0.8383647669558546\n",
      "train loss:0.8783823405721999\n",
      "train loss:0.9974423124278482\n",
      "train loss:0.8769224373649918\n",
      "train loss:1.0360386587608794\n",
      "train loss:0.8563615481357395\n",
      "train loss:0.9315078927746963\n",
      "train loss:0.8557341400928471\n",
      "train loss:0.7732956279292987\n",
      "train loss:0.8911714800440924\n",
      "train loss:1.0349797581238307\n",
      "train loss:0.9838254947095205\n",
      "train loss:0.9118482174121388\n",
      "train loss:0.933791794181459\n",
      "train loss:0.8583957177033763\n",
      "train loss:1.0144330060254823\n",
      "train loss:1.0312482191899985\n",
      "train loss:0.8592524580704425\n",
      "train loss:0.8871173054212371\n",
      "train loss:0.7969107314733728\n",
      "train loss:0.8954491478526181\n",
      "train loss:0.9822478830898116\n",
      "train loss:0.8490921069089077\n",
      "train loss:0.8174363475367776\n",
      "=== epoch:9, train acc:0.992, test acc:0.992 ===\n",
      "train loss:0.7911602741072286\n",
      "train loss:0.9530621459434393\n",
      "train loss:0.719464305140425\n",
      "train loss:0.8952543482706841\n",
      "train loss:0.9890827490618129\n",
      "train loss:0.939183769624368\n",
      "train loss:0.8620752459643691\n",
      "train loss:1.026492846330998\n",
      "train loss:0.98498140286696\n",
      "train loss:0.9421344982715846\n",
      "train loss:0.9180525206860828\n",
      "train loss:1.033631254762047\n",
      "train loss:1.0060174228732697\n",
      "train loss:1.0024515628402373\n",
      "train loss:0.693439363805883\n",
      "train loss:0.7533987421256519\n",
      "train loss:0.8870224677771367\n",
      "train loss:1.0199330915375369\n",
      "train loss:0.8270823595578998\n",
      "train loss:1.0064278482319127\n",
      "train loss:0.7871438822763046\n",
      "train loss:0.8525949176392735\n",
      "train loss:0.9896931154568311\n",
      "train loss:0.8126552329996979\n",
      "train loss:0.8639962832847892\n",
      "train loss:0.9938838462919238\n",
      "train loss:0.7104636982526518\n",
      "train loss:0.8306279735916369\n",
      "train loss:0.8041502306757367\n",
      "train loss:0.941639316521028\n",
      "train loss:0.8409854932329819\n",
      "train loss:0.9650263362754545\n",
      "train loss:0.9393724189672983\n",
      "train loss:0.8757715633747091\n",
      "train loss:0.9206221650080518\n",
      "train loss:0.8664299980988724\n",
      "train loss:0.9930055676398897\n",
      "train loss:0.8448057379729415\n",
      "train loss:0.9779706696207517\n",
      "train loss:0.7672048610890339\n",
      "train loss:0.8355419841291328\n",
      "train loss:0.780603729573846\n",
      "train loss:0.9575576113547569\n",
      "train loss:0.794124530674324\n",
      "train loss:0.6621647099848654\n",
      "train loss:1.0538826988247652\n",
      "train loss:0.777338576969332\n",
      "train loss:0.9152330165109344\n",
      "train loss:0.8136064722692035\n",
      "train loss:0.900361629167175\n",
      "train loss:0.9219247114638938\n",
      "train loss:0.9376013384199712\n",
      "train loss:0.9827518842941635\n",
      "train loss:0.7297863589974085\n",
      "train loss:0.801502824496053\n",
      "train loss:0.9312624655410944\n",
      "train loss:0.7537221532116266\n",
      "train loss:0.7429912999791622\n",
      "train loss:0.8236060318887846\n",
      "train loss:0.8384168229838217\n",
      "train loss:0.7632169329128415\n",
      "train loss:0.9726552897457005\n",
      "train loss:0.9229579648418382\n",
      "train loss:1.212930450062519\n",
      "train loss:0.8923771673068337\n",
      "train loss:0.7180462267504049\n",
      "train loss:0.9433035476719818\n",
      "train loss:1.1027491743061832\n",
      "train loss:0.8182782948827106\n",
      "train loss:0.8609743914204906\n",
      "train loss:0.9483262602004057\n",
      "train loss:0.8235671579599984\n",
      "train loss:0.8330353849953828\n",
      "train loss:1.007902132032268\n",
      "train loss:0.8662551340323991\n",
      "train loss:1.0406514441443628\n",
      "train loss:1.0026507092578607\n",
      "train loss:0.9266305248784039\n",
      "train loss:1.0916421709692008\n",
      "train loss:1.0605230680128415\n",
      "train loss:1.021204609903054\n",
      "train loss:0.748792366329367\n",
      "train loss:0.7796065021062202\n",
      "train loss:0.8846447018316168\n",
      "train loss:1.003479073906457\n",
      "train loss:0.758197612013136\n",
      "train loss:0.9668225046802834\n",
      "train loss:0.8261732602694513\n",
      "train loss:0.9705812228908987\n",
      "train loss:0.6833597860631424\n",
      "train loss:0.8622736913133832\n",
      "train loss:0.709094539724331\n",
      "train loss:0.875643617566795\n",
      "train loss:0.9343421085513183\n",
      "train loss:1.0045220254394724\n",
      "train loss:0.8735011461728603\n",
      "train loss:0.8595593955592108\n",
      "train loss:0.8730199262887666\n",
      "train loss:0.883193789571805\n",
      "train loss:0.8902477036141262\n",
      "train loss:0.8581135188126366\n",
      "train loss:0.8773750814555923\n",
      "train loss:0.9984913981233434\n",
      "train loss:0.7137336246423991\n",
      "train loss:0.874431709890378\n",
      "train loss:0.833914736148034\n",
      "train loss:1.024905448459838\n",
      "train loss:0.8525487611199222\n",
      "train loss:0.9759293335974857\n",
      "train loss:0.9112011329992007\n",
      "train loss:0.9728791178080773\n",
      "train loss:0.8610722256030735\n",
      "train loss:0.9518088513462206\n",
      "train loss:0.8085280779602477\n",
      "train loss:0.8841754841320784\n",
      "train loss:0.9529640883725782\n",
      "train loss:1.0133485762153471\n",
      "train loss:0.8125968910509672\n",
      "train loss:0.9198499118839212\n",
      "train loss:1.0663759292599777\n",
      "train loss:0.9599199442339754\n",
      "train loss:0.7820243520835564\n",
      "train loss:0.9319584358628324\n",
      "train loss:0.9130389688645648\n",
      "train loss:0.8275400933978695\n",
      "train loss:0.8589010398264574\n",
      "train loss:1.0380768361817103\n",
      "train loss:0.9192939483896584\n",
      "train loss:0.8160563449245413\n",
      "train loss:1.0281260539556116\n",
      "train loss:0.9632353381269085\n",
      "train loss:1.1322535815340344\n",
      "train loss:1.0038661500017907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.0305181752683448\n",
      "train loss:0.9532741556746768\n",
      "train loss:0.8766059535121283\n",
      "train loss:0.9329059027414995\n",
      "train loss:0.93726470016725\n",
      "train loss:0.9628137188268755\n",
      "train loss:0.9127603502882932\n",
      "train loss:0.95201071641145\n",
      "train loss:0.821996969576373\n",
      "train loss:1.017742728464964\n",
      "train loss:0.9145607301377597\n",
      "train loss:0.9183311411027002\n",
      "train loss:0.9386846686033457\n",
      "train loss:0.9514532104446958\n",
      "train loss:0.7436763275426442\n",
      "train loss:0.7870697530994839\n",
      "train loss:0.7922899822859283\n",
      "train loss:0.8514480264357879\n",
      "train loss:0.9274618165214135\n",
      "train loss:0.9518658317634912\n",
      "train loss:1.0359023628888528\n",
      "train loss:0.8668089950852159\n",
      "train loss:1.049079376526017\n",
      "train loss:0.7291221105098745\n",
      "train loss:0.9440687845868951\n",
      "train loss:0.894570942178776\n",
      "train loss:0.9562115119011891\n",
      "train loss:0.9915479190859412\n",
      "train loss:0.9471628976608528\n",
      "train loss:0.9638930268298407\n",
      "train loss:0.9965855116088985\n",
      "train loss:0.8660073441929219\n",
      "train loss:0.7365237187084277\n",
      "train loss:1.0625318176755716\n",
      "train loss:0.8489035043950299\n",
      "train loss:0.6841705515225456\n",
      "train loss:0.9828545911129322\n",
      "train loss:0.7730442108485187\n",
      "train loss:0.814166303680418\n",
      "train loss:0.8028396911574407\n",
      "train loss:0.7930027428327131\n",
      "train loss:0.8268644235447113\n",
      "train loss:1.0118503978943894\n",
      "train loss:0.871713620806893\n",
      "train loss:0.8309009169603455\n",
      "train loss:0.8729609839452256\n",
      "train loss:1.004962066204148\n",
      "train loss:0.9193080129839853\n",
      "train loss:0.7737850903299061\n",
      "train loss:0.8942603379469237\n",
      "train loss:0.7339454010510782\n",
      "train loss:0.8133969534903762\n",
      "train loss:1.040863953866249\n",
      "train loss:0.9795488422903776\n",
      "train loss:0.7766870529372835\n",
      "train loss:0.8944107570918399\n",
      "train loss:0.8300318587452502\n",
      "train loss:0.7960754789809478\n",
      "train loss:1.0656959718874415\n",
      "train loss:0.8348963023707274\n",
      "train loss:0.893979858924978\n",
      "train loss:0.8965317238288611\n",
      "train loss:0.8891248698757527\n",
      "train loss:0.9396858856232714\n",
      "train loss:0.8695251963306005\n",
      "train loss:0.9479803480836143\n",
      "train loss:0.7105403267985164\n",
      "train loss:0.7447622178687843\n",
      "train loss:0.8546299162706353\n",
      "train loss:0.9003196935762395\n",
      "train loss:0.820887757032188\n",
      "train loss:0.9007298975623371\n",
      "train loss:1.000937489786348\n",
      "train loss:0.946520952701314\n",
      "train loss:0.8505387563620794\n",
      "train loss:1.0226427550850687\n",
      "train loss:0.7242541325745557\n",
      "train loss:0.7984207758869333\n",
      "train loss:1.0408759978689945\n",
      "train loss:0.7951499556271787\n",
      "train loss:0.8791381308667163\n",
      "train loss:0.7600650335533448\n",
      "train loss:0.8507532990647023\n",
      "train loss:0.7635503786434643\n",
      "train loss:0.9630964835706374\n",
      "train loss:0.8301974925613175\n",
      "train loss:0.9047192439976885\n",
      "train loss:0.9205884509175151\n",
      "train loss:0.8804043112971488\n",
      "train loss:0.9507688456958724\n",
      "train loss:0.854525084081108\n",
      "train loss:0.9143777400457851\n",
      "train loss:0.8227519625483651\n",
      "train loss:0.9416855252662019\n",
      "train loss:0.9464719682392584\n",
      "train loss:0.8707881872016007\n",
      "train loss:0.9204155631815344\n",
      "train loss:0.8300584435255252\n",
      "train loss:0.9304262935849781\n",
      "train loss:0.8901423895124253\n",
      "train loss:0.9355349904927427\n",
      "train loss:0.8205552698047778\n",
      "train loss:0.8792168498294974\n",
      "train loss:0.8414178010169435\n",
      "train loss:0.8965802212457561\n",
      "train loss:0.9646261293802677\n",
      "train loss:0.8511967399721287\n",
      "train loss:0.9137467447687251\n",
      "train loss:0.8771796016165232\n",
      "train loss:0.9855931161099261\n",
      "train loss:0.9671410884594219\n",
      "train loss:0.9917954122095515\n",
      "train loss:0.8970510617910239\n",
      "train loss:0.827212083966344\n",
      "train loss:0.7529805562791668\n",
      "train loss:1.067479226481695\n",
      "train loss:0.8190331037938631\n",
      "train loss:0.9162184393386518\n",
      "train loss:0.7634019931325539\n",
      "train loss:0.8527428785775194\n",
      "train loss:0.9631277344281896\n",
      "train loss:0.8934238442509898\n",
      "train loss:0.910912931827918\n",
      "train loss:0.8475539207401335\n",
      "train loss:0.9167521863320901\n",
      "train loss:0.9870018164754782\n",
      "train loss:0.9823949083293606\n",
      "train loss:0.7836568342899932\n",
      "train loss:0.9313457024710518\n",
      "train loss:0.8935197754122658\n",
      "train loss:0.8818351438365449\n",
      "train loss:0.9938839886924873\n",
      "train loss:1.0220257760124487\n",
      "train loss:0.7733032132881047\n",
      "train loss:0.9264073824272596\n",
      "train loss:0.9919195163404823\n",
      "train loss:1.0066757125692747\n",
      "train loss:1.0050754392163397\n",
      "train loss:0.8884689140416023\n",
      "train loss:0.7576254617558418\n",
      "train loss:0.7651773103353938\n",
      "train loss:0.9701904860286408\n",
      "train loss:0.6412937836655556\n",
      "train loss:0.8586867229509225\n",
      "train loss:0.8359644423399893\n",
      "train loss:0.7639544345895024\n",
      "train loss:0.9329406043185291\n",
      "train loss:0.9786523999873011\n",
      "train loss:0.8611468993581358\n",
      "train loss:0.9791554735745572\n",
      "train loss:0.7998079207623383\n",
      "train loss:0.8347648562657878\n",
      "train loss:0.8286586838989731\n",
      "train loss:1.0127780260554125\n",
      "train loss:0.9986696980480739\n",
      "train loss:0.8570899439208045\n",
      "train loss:0.7877913875933272\n",
      "train loss:0.8135869858240602\n",
      "train loss:0.9211640511290656\n",
      "train loss:1.002744307537107\n",
      "train loss:0.8654126856864489\n",
      "train loss:0.8596056348683407\n",
      "train loss:0.7815762278745896\n",
      "train loss:0.8706599026808871\n",
      "train loss:0.9208142209572518\n",
      "train loss:0.9111164171625172\n",
      "train loss:0.9378436491712437\n",
      "train loss:0.9529938352600239\n",
      "train loss:0.7902628626038376\n",
      "train loss:1.017747963949292\n",
      "train loss:0.9216644411230546\n",
      "train loss:0.892936073791205\n",
      "train loss:0.8020529888986008\n",
      "train loss:0.6922377866547018\n",
      "train loss:0.9576902186068763\n",
      "train loss:0.8652482551459554\n",
      "train loss:0.9270391961748848\n",
      "train loss:0.9654282289010109\n",
      "train loss:0.9019433306622159\n",
      "train loss:1.0186786736746307\n",
      "train loss:1.049817993672287\n",
      "train loss:0.8135224988017058\n",
      "train loss:0.8790019341870844\n",
      "train loss:0.9126111730126165\n",
      "train loss:0.9158391394229862\n",
      "train loss:0.8253100699548717\n",
      "train loss:0.8349225643340773\n",
      "train loss:0.9218360361656408\n",
      "train loss:0.7365116746454268\n",
      "train loss:0.7850597205618761\n",
      "train loss:0.9367361261298045\n",
      "train loss:0.8469402501170467\n",
      "train loss:1.015915487297853\n",
      "train loss:0.9449815363820676\n",
      "train loss:0.8984344796961294\n",
      "train loss:0.8628869035190618\n",
      "train loss:0.8797119035589938\n",
      "train loss:0.8396502732778609\n",
      "train loss:0.9736110881653494\n",
      "train loss:0.8482856454740562\n",
      "train loss:0.8678308484344265\n",
      "train loss:0.8866229693589871\n",
      "train loss:0.8734461688109291\n",
      "train loss:0.800335031999321\n",
      "train loss:0.8955677080260795\n",
      "train loss:1.054091664963402\n",
      "train loss:0.9633483974170663\n",
      "train loss:0.8366701679094303\n",
      "train loss:0.8785000892248815\n",
      "train loss:0.6169625337416853\n",
      "train loss:0.7933924660371001\n",
      "train loss:0.90664509805598\n",
      "train loss:1.0098006915260591\n",
      "train loss:1.051690297647146\n",
      "train loss:0.9224288437600522\n",
      "train loss:0.8768132756007262\n",
      "train loss:0.9280332673161901\n",
      "train loss:0.7708203528563086\n",
      "train loss:1.0251381177887113\n",
      "train loss:0.9127232759267381\n",
      "train loss:0.8462429093545327\n",
      "train loss:0.9414025332527654\n",
      "train loss:0.9076434763520288\n",
      "train loss:0.8124773200126103\n",
      "train loss:0.7777093969242351\n",
      "train loss:0.8636761566809116\n",
      "train loss:0.8451092528918476\n",
      "train loss:0.8976419554836069\n",
      "train loss:0.9279557405496389\n",
      "train loss:0.7920352647331861\n",
      "train loss:0.8915707406190857\n",
      "train loss:0.8705756908720396\n",
      "train loss:0.9085890752820035\n",
      "train loss:0.8176962973039906\n",
      "train loss:0.7932755062842409\n",
      "train loss:1.0069304071804448\n",
      "train loss:0.9476991471501691\n",
      "train loss:1.049390845796357\n",
      "train loss:0.8659687653522612\n",
      "train loss:0.8869013331726492\n",
      "train loss:0.9073531079747669\n",
      "train loss:0.8806136309050079\n",
      "train loss:0.9015606153335517\n",
      "train loss:0.9369869467032313\n",
      "train loss:0.8235014717621231\n",
      "train loss:0.992581788192526\n",
      "train loss:0.8745713758684056\n",
      "train loss:0.6708077621782589\n",
      "train loss:0.774572305422471\n",
      "train loss:0.944493623423497\n",
      "train loss:0.8483635711708662\n",
      "train loss:0.9315972030455739\n",
      "train loss:0.9064435677361228\n",
      "train loss:0.9036678130375974\n",
      "train loss:1.0418298958395964\n",
      "train loss:0.9753392175495251\n",
      "train loss:1.0111235626178183\n",
      "train loss:0.7681951303706178\n",
      "train loss:1.0874432425913074\n",
      "train loss:0.753760133652265\n",
      "train loss:0.8061084785806313\n",
      "train loss:0.927860434162566\n",
      "train loss:0.8968482497660607\n",
      "train loss:0.9690954233052388\n",
      "train loss:0.9344027791009942\n",
      "train loss:0.9625550251734051\n",
      "train loss:0.9013944942988203\n",
      "train loss:0.9826919696111497\n",
      "train loss:1.0328679385101707\n",
      "train loss:0.8483346793803443\n",
      "train loss:0.9030560546675995\n",
      "train loss:0.8710154089593243\n",
      "train loss:0.8213208754541671\n",
      "train loss:0.9357728251455465\n",
      "train loss:0.8197671973871665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.03118442744332\n",
      "train loss:0.8450313673280617\n",
      "train loss:0.8111152766547343\n",
      "train loss:1.0373880950861196\n",
      "train loss:0.939138796337909\n",
      "train loss:0.720018989103783\n",
      "train loss:0.773321100892196\n",
      "train loss:0.7728771342813733\n",
      "train loss:0.8637753657165613\n",
      "train loss:0.8472612062844596\n",
      "train loss:0.8453287166165118\n",
      "train loss:0.7288457953243102\n",
      "train loss:0.8670008493133889\n",
      "train loss:0.9230307787390195\n",
      "train loss:0.8713755556545906\n",
      "train loss:0.8171196044700789\n",
      "train loss:1.024713812160163\n",
      "train loss:0.8308373472658799\n",
      "train loss:0.7793219090192589\n",
      "train loss:0.8744181470543295\n",
      "train loss:0.9022265068190438\n",
      "train loss:0.7689292855124866\n",
      "train loss:0.9106597717458262\n",
      "train loss:0.8877142033074006\n",
      "train loss:1.0757761089218862\n",
      "train loss:0.9328496796757965\n",
      "train loss:0.8576247909895645\n",
      "train loss:0.8307101519572256\n",
      "train loss:0.8178605067296374\n",
      "train loss:0.9197955851078672\n",
      "train loss:0.8623316117540939\n",
      "train loss:0.8940663430603598\n",
      "train loss:0.8032284162243886\n",
      "train loss:0.8746882171562034\n",
      "train loss:1.0162040253607252\n",
      "train loss:0.8531656919152543\n",
      "train loss:0.8737658150482239\n",
      "train loss:1.015401395341992\n",
      "train loss:0.9455938975057357\n",
      "train loss:0.967469990438954\n",
      "train loss:0.8641129215029558\n",
      "train loss:1.0482601384208199\n",
      "train loss:1.0049619251950568\n",
      "train loss:0.9574872329098478\n",
      "train loss:0.9073563160232462\n",
      "train loss:0.9393248825816708\n",
      "train loss:0.8768418582487879\n",
      "train loss:0.9250759777915535\n",
      "train loss:1.0879495272679867\n",
      "train loss:0.8815328604163756\n",
      "train loss:0.9568436538041227\n",
      "train loss:0.9993972600436996\n",
      "train loss:0.7217508643887046\n",
      "train loss:0.8187572096825051\n",
      "train loss:0.9243819413300131\n",
      "train loss:0.9225350980791011\n",
      "train loss:0.8340858333807879\n",
      "train loss:0.8728683025338863\n",
      "train loss:0.901744855791766\n",
      "train loss:0.8521945716336193\n",
      "train loss:0.8010241844931225\n",
      "train loss:0.9537845157093056\n",
      "train loss:1.0004935912596618\n",
      "train loss:0.9462929699882905\n",
      "train loss:0.9708033176901116\n",
      "train loss:0.8087113935783257\n",
      "train loss:0.798307426102216\n",
      "train loss:0.8369443398030915\n",
      "train loss:0.8813677297925451\n",
      "train loss:0.9173959920363282\n",
      "train loss:0.9604652377879431\n",
      "train loss:0.8780418304207998\n",
      "train loss:0.9880084591642887\n",
      "train loss:0.9780595892576288\n",
      "train loss:0.8757862928914694\n",
      "train loss:0.8461317805604032\n",
      "train loss:0.8873720462417843\n",
      "train loss:0.8260997979321135\n",
      "train loss:0.8767726697211974\n",
      "train loss:0.9603881555594738\n",
      "train loss:1.056729136812803\n",
      "train loss:0.9248647256673814\n",
      "train loss:0.8336916853435965\n",
      "train loss:0.8183722250551884\n",
      "train loss:0.8826962186986737\n",
      "train loss:0.979683670641712\n",
      "train loss:0.7760873944443317\n",
      "train loss:0.9891839963209768\n",
      "train loss:0.8354753209320575\n",
      "train loss:0.9408473487747441\n",
      "train loss:0.7252135068950919\n",
      "train loss:0.9792017708992905\n",
      "train loss:0.907260079569519\n",
      "train loss:0.9528497929669613\n",
      "train loss:0.6680898160395026\n",
      "train loss:0.812091974189262\n",
      "train loss:0.9653383808955631\n",
      "train loss:0.9521425811974582\n",
      "train loss:0.9712529996912161\n",
      "train loss:0.9502989278468918\n",
      "train loss:0.863325599833522\n",
      "train loss:0.8948878598842183\n",
      "train loss:0.7602159907730511\n",
      "train loss:0.9598965690119964\n",
      "train loss:1.0612669909617356\n",
      "train loss:0.8605284621694451\n",
      "train loss:0.9368360012957843\n",
      "train loss:0.8764016522766166\n",
      "train loss:1.0653144237277459\n",
      "train loss:1.053931048207553\n",
      "train loss:0.9678550995776669\n",
      "train loss:0.8492883280031669\n",
      "train loss:0.8229168105805428\n",
      "train loss:0.9850727930167396\n",
      "train loss:0.8041289367431596\n",
      "train loss:0.9478853804785388\n",
      "train loss:0.9741316037512301\n",
      "train loss:0.8816765585832652\n",
      "train loss:0.8040140674821447\n",
      "train loss:0.8165617759102471\n",
      "train loss:0.8885057584219147\n",
      "train loss:0.9436742175897268\n",
      "train loss:0.9895332515216552\n",
      "train loss:0.9405789983502468\n",
      "train loss:0.7316040797308068\n",
      "train loss:0.8806118481468981\n",
      "train loss:0.8446133849783608\n",
      "train loss:0.9125875164098837\n",
      "train loss:0.7774586929583278\n",
      "train loss:0.634189998422594\n",
      "train loss:0.9553279769763191\n",
      "train loss:0.8918737588853136\n",
      "train loss:0.8926429592690088\n",
      "train loss:0.953562206640498\n",
      "train loss:0.7957535659965158\n",
      "train loss:0.8669977258215685\n",
      "train loss:0.8926316759145763\n",
      "train loss:0.8854507186192951\n",
      "train loss:0.9339131992786541\n",
      "train loss:0.9565202306722353\n",
      "train loss:0.7969946520442224\n",
      "train loss:0.7385848854874637\n",
      "train loss:0.8079502829833978\n",
      "train loss:0.8950010904182095\n",
      "train loss:0.7952450210102814\n",
      "train loss:0.9191442814618732\n",
      "train loss:0.7989778342415959\n",
      "train loss:0.8122812951797884\n",
      "train loss:0.9805888606087655\n",
      "train loss:1.1001061178632667\n",
      "train loss:1.027546694453223\n",
      "train loss:0.8775962168848495\n",
      "train loss:0.8319221099003603\n",
      "train loss:0.8325491157915467\n",
      "train loss:0.8546657219966448\n",
      "train loss:0.7118854591373815\n",
      "train loss:0.806336061485679\n",
      "train loss:0.9776040827299453\n",
      "train loss:0.8672324714967293\n",
      "train loss:0.7303367714193082\n",
      "train loss:0.8471458470997645\n",
      "train loss:0.84101247237663\n",
      "train loss:0.9799454643853166\n",
      "train loss:0.764866666034156\n",
      "train loss:0.7763091289608517\n",
      "train loss:0.8605503849156545\n",
      "train loss:0.8975094434571242\n",
      "train loss:1.005486777461203\n",
      "train loss:0.8314766869803641\n",
      "train loss:0.9169721664026521\n",
      "train loss:0.7781670205867628\n",
      "train loss:0.929072212962952\n",
      "train loss:0.7299347557369537\n",
      "train loss:0.8650575952205688\n",
      "train loss:0.9798723402167879\n",
      "train loss:0.7584075624768739\n",
      "train loss:0.9178824923278495\n",
      "train loss:0.5864569373356646\n",
      "train loss:0.7872512477771031\n",
      "train loss:0.8431258195460849\n",
      "train loss:1.1882249238621305\n",
      "train loss:0.823380774596738\n",
      "train loss:0.7732472816186737\n",
      "train loss:0.8582165854324324\n",
      "train loss:0.9102938249844477\n",
      "train loss:1.0080425555678176\n",
      "train loss:0.9137269045992517\n",
      "train loss:0.8993441669623661\n",
      "train loss:0.8084402918573539\n",
      "train loss:0.7770338087203221\n",
      "train loss:0.8154868761702417\n",
      "train loss:0.8351929212460747\n",
      "=== epoch:10, train acc:0.997, test acc:0.991 ===\n",
      "train loss:0.7657927912197781\n",
      "train loss:0.7513704808336006\n",
      "train loss:0.8673293798699239\n",
      "train loss:0.8670946766547623\n",
      "train loss:0.9123373529169547\n",
      "train loss:0.8598924312497778\n",
      "train loss:0.8891534972213697\n",
      "train loss:0.8036450523553246\n",
      "train loss:0.9947492720172164\n",
      "train loss:0.8533606434182456\n",
      "train loss:0.9092932034319879\n",
      "train loss:0.901436897048947\n",
      "train loss:0.8382362308466308\n",
      "train loss:0.8502934041100729\n",
      "train loss:0.8389146736536398\n",
      "train loss:0.9320008491736919\n",
      "train loss:0.8419949960255444\n",
      "train loss:0.8773748458550007\n",
      "train loss:0.9953876165211933\n",
      "train loss:0.8359825062811678\n",
      "train loss:0.8812559339619253\n",
      "train loss:0.897880136188291\n",
      "train loss:0.9313729318421451\n",
      "train loss:0.9395896177800656\n",
      "train loss:0.7887264201153024\n",
      "train loss:0.819949679236097\n",
      "train loss:0.8850138606207981\n",
      "train loss:0.8596677393789001\n",
      "train loss:0.90304455142325\n",
      "train loss:0.9364758026105282\n",
      "train loss:0.9188433025603296\n",
      "train loss:0.9583612284018974\n",
      "train loss:0.9093428951509266\n",
      "train loss:0.7502310407895504\n",
      "train loss:0.8934640353523758\n",
      "train loss:0.789704614564014\n",
      "train loss:0.6429162823991885\n",
      "train loss:1.0040527711522749\n",
      "train loss:0.8766130901493737\n",
      "train loss:1.0363263178055413\n",
      "train loss:0.8382603266548503\n",
      "train loss:0.9026215496682813\n",
      "train loss:0.8773680646949285\n",
      "train loss:0.9725739806080697\n",
      "train loss:0.8444713601037211\n",
      "train loss:0.9555445857958123\n",
      "train loss:0.9471280716618431\n",
      "train loss:0.867769153151443\n",
      "train loss:0.7572370605248784\n",
      "train loss:0.9680681737212127\n",
      "train loss:0.8214512659767164\n",
      "train loss:0.923622839590834\n",
      "train loss:0.9075969645417095\n",
      "train loss:0.958875074876095\n",
      "train loss:0.9489328408390985\n",
      "train loss:0.8120806739724028\n",
      "train loss:0.744165154598312\n",
      "train loss:0.7811502431496687\n",
      "train loss:0.8850162636957228\n",
      "train loss:0.8406687277044373\n",
      "train loss:0.8179080490338067\n",
      "train loss:0.9448256550668924\n",
      "train loss:0.9267725430946965\n",
      "train loss:0.9463402629274471\n",
      "train loss:0.8717712230603173\n",
      "train loss:0.8797913740104903\n",
      "train loss:0.9228750084829715\n",
      "train loss:0.8570871470447743\n",
      "train loss:0.9134042682660076\n",
      "train loss:0.8786016545244577\n",
      "train loss:1.0955653291376968\n",
      "train loss:0.7959299480221149\n",
      "train loss:0.9613810342807138\n",
      "train loss:0.7897826659173917\n",
      "train loss:0.8637645043982201\n",
      "train loss:0.7729974913531321\n",
      "train loss:0.8732807278802666\n",
      "train loss:0.9241670830003561\n",
      "train loss:1.0434476926240714\n",
      "train loss:0.8935763450389169\n",
      "train loss:0.8526050243717471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.8654633923266062\n",
      "train loss:0.8731592062112274\n",
      "train loss:0.7803300290342879\n",
      "train loss:1.0095503142853142\n",
      "train loss:0.7684151540021662\n",
      "train loss:0.9180335017474144\n",
      "train loss:0.8168232784493771\n",
      "train loss:0.8931546252522554\n",
      "train loss:0.7970879017364476\n",
      "train loss:0.7942721967529277\n",
      "train loss:0.9034070957762688\n",
      "train loss:0.8845139806951472\n",
      "train loss:0.8731243959152808\n",
      "train loss:0.9492212008095271\n",
      "train loss:0.782329236982704\n",
      "train loss:0.9366984034122106\n",
      "train loss:0.9571818830353073\n",
      "train loss:0.8856307101129485\n",
      "train loss:0.7947430183490793\n",
      "train loss:1.090635104575921\n",
      "train loss:0.8449731945376061\n",
      "train loss:0.9316866996243484\n",
      "train loss:0.8207639914047513\n",
      "train loss:0.8314291608710878\n",
      "train loss:0.9117217409944005\n",
      "train loss:1.0280070954070628\n",
      "train loss:0.9371073875312045\n",
      "train loss:0.7224844013901179\n",
      "train loss:0.828739922218203\n",
      "train loss:0.8251984317566042\n",
      "train loss:1.0718010696427243\n",
      "train loss:0.8117747802595694\n",
      "train loss:0.8906222554416141\n",
      "train loss:0.7349240462706164\n",
      "train loss:0.961100605520586\n",
      "train loss:0.9100187794353949\n",
      "train loss:0.9467274957338703\n",
      "train loss:0.8279502584007747\n",
      "train loss:0.6425049644051505\n",
      "train loss:1.0230301467389296\n",
      "train loss:0.9154778553012245\n",
      "train loss:0.916567950391358\n",
      "train loss:0.8210906703237633\n",
      "train loss:0.9104258986584634\n",
      "train loss:0.8849066943515929\n",
      "train loss:0.8516589070180699\n",
      "train loss:0.9287716788130683\n",
      "train loss:0.8524070819104361\n",
      "train loss:0.9683870111883901\n",
      "train loss:0.8688046005052089\n",
      "train loss:0.8757112464028631\n",
      "train loss:0.8831932006064926\n",
      "train loss:0.8007436674202747\n",
      "train loss:0.8777793547692968\n",
      "train loss:1.0149497183577392\n",
      "train loss:0.8711656242584848\n",
      "train loss:0.9105498391182949\n",
      "train loss:0.9531915990870324\n",
      "train loss:0.7581433176937744\n",
      "train loss:0.826894796747204\n",
      "train loss:0.8881558158571246\n",
      "train loss:0.840935362487632\n",
      "train loss:0.9329152371322047\n",
      "train loss:0.789020342943181\n",
      "train loss:0.9378835094179186\n",
      "train loss:0.8408693226249452\n",
      "train loss:0.960350842338611\n",
      "train loss:0.9221611112203363\n",
      "train loss:0.9668726334117226\n",
      "train loss:1.0537158687449402\n",
      "train loss:1.0176568882879058\n",
      "train loss:0.7991019742324983\n",
      "train loss:1.0243739326290422\n",
      "train loss:0.8445063468818809\n",
      "train loss:0.8337426463545614\n",
      "train loss:0.9786768899057363\n",
      "train loss:0.8738552034275924\n",
      "train loss:0.94238130239221\n",
      "train loss:0.8139861986609078\n",
      "train loss:0.9355007653060468\n",
      "train loss:0.988666983710005\n",
      "train loss:0.9743797488203433\n",
      "train loss:1.0289872757261178\n",
      "train loss:0.861455491276802\n",
      "train loss:0.9889179460894133\n",
      "train loss:0.975776645168986\n",
      "train loss:0.9543643785478377\n",
      "train loss:0.986894306344932\n",
      "train loss:0.8976942058090646\n",
      "train loss:0.8786391770602968\n",
      "train loss:0.9512229412864808\n",
      "train loss:0.9442040511664789\n",
      "train loss:0.8866893351229505\n",
      "train loss:0.8564384877703312\n",
      "train loss:0.7551119789481595\n",
      "train loss:0.9484892777088899\n",
      "train loss:0.8809986689906601\n",
      "train loss:0.9121558305872918\n",
      "train loss:0.852627233509521\n",
      "train loss:0.7408367429440559\n",
      "train loss:0.8370570077223843\n",
      "train loss:0.8650249714695174\n",
      "train loss:0.8649195253224378\n",
      "train loss:0.7992408667073247\n",
      "train loss:0.7904833078283997\n",
      "train loss:0.7564717980338966\n",
      "train loss:0.9338463592070131\n",
      "train loss:0.8942899505763566\n",
      "train loss:0.9780867318319248\n",
      "train loss:0.989219060651357\n",
      "train loss:0.7545257612506177\n",
      "train loss:0.8223069861788029\n",
      "train loss:0.8678873664364007\n",
      "train loss:0.8652503610271502\n",
      "train loss:0.8093843614770025\n",
      "train loss:0.9590883629601636\n",
      "train loss:0.8285311324478873\n",
      "train loss:1.079985608517176\n",
      "train loss:0.8301135854203738\n",
      "train loss:1.0245307766728449\n",
      "train loss:0.859861523876338\n",
      "train loss:0.8402551365413451\n",
      "train loss:0.8088103881147272\n",
      "train loss:0.8968150485102627\n",
      "train loss:0.9642627605712927\n",
      "train loss:1.0855491945495501\n",
      "train loss:0.8927445824349892\n",
      "train loss:0.8789394644647871\n",
      "train loss:1.0424699948780625\n",
      "train loss:0.9672805665720513\n",
      "train loss:0.967284119365938\n",
      "train loss:0.9006514465119534\n",
      "train loss:0.944999215210489\n",
      "train loss:0.8445517333659234\n",
      "train loss:0.9183869631006898\n",
      "train loss:0.9152650725136776\n",
      "train loss:0.8794259398982757\n",
      "train loss:0.6560269971052547\n",
      "train loss:0.8231732186968074\n",
      "train loss:1.005119976729754\n",
      "train loss:1.0327954091116132\n",
      "train loss:0.863784037174618\n",
      "train loss:0.7882432552002469\n",
      "train loss:0.8678998247807557\n",
      "train loss:1.031927613492234\n",
      "train loss:0.8209823440360101\n",
      "train loss:0.9498682356924548\n",
      "train loss:0.8659163145619163\n",
      "train loss:0.7878455898913687\n",
      "train loss:0.7798689732562135\n",
      "train loss:0.7800533295461163\n",
      "train loss:0.8162930970841306\n",
      "train loss:0.8338680718041535\n",
      "train loss:1.0335106004619474\n",
      "train loss:1.0463822379173575\n",
      "train loss:0.8520511134056649\n",
      "train loss:0.8361782393113237\n",
      "train loss:1.0059438211459146\n",
      "train loss:0.9029920431767876\n",
      "train loss:0.7698117372488055\n",
      "train loss:0.7463808256166896\n",
      "train loss:0.7970486011448084\n",
      "train loss:0.8397728653609355\n",
      "train loss:0.7466201769057252\n",
      "train loss:0.8873210702720751\n",
      "train loss:0.897712771400772\n",
      "train loss:0.963587992443281\n",
      "train loss:0.9461381216747283\n",
      "train loss:1.0194649861241332\n",
      "train loss:0.8176979104922563\n",
      "train loss:1.0071413339893172\n",
      "train loss:0.8030093639084426\n",
      "train loss:0.8285924413624344\n",
      "train loss:0.7661909590804512\n",
      "train loss:0.7443145537539899\n",
      "train loss:0.8617063283952723\n",
      "train loss:0.8907735501870513\n",
      "train loss:1.0358259170009474\n",
      "train loss:0.8416603916136108\n",
      "train loss:0.8147209032071276\n",
      "train loss:1.062232115359375\n",
      "train loss:0.770877782927953\n",
      "train loss:0.7276298424753455\n",
      "train loss:0.780268533201791\n",
      "train loss:0.9548232514718533\n",
      "train loss:0.976645724793523\n",
      "train loss:0.7705330584488198\n",
      "train loss:0.7937365314472438\n",
      "train loss:0.8707127356410215\n",
      "train loss:0.9049079140563382\n",
      "train loss:0.9008742194476924\n",
      "train loss:0.9158143102052554\n",
      "train loss:0.836928749658817\n",
      "train loss:0.9790999763819105\n",
      "train loss:0.8910273389815662\n",
      "train loss:0.8051900279746635\n",
      "train loss:0.860985265344486\n",
      "train loss:0.7870180997488899\n",
      "train loss:0.8847834715472981\n",
      "train loss:1.0070974197704048\n",
      "train loss:0.9944502385043176\n",
      "train loss:0.8239232671865488\n",
      "train loss:0.8271795764362787\n",
      "train loss:0.7168207246663763\n",
      "train loss:0.9244324245895367\n",
      "train loss:0.8630763984360026\n",
      "train loss:0.8325002316520086\n",
      "train loss:0.7932556480148888\n",
      "train loss:0.9540623560843058\n",
      "train loss:0.8797933446748768\n",
      "train loss:0.9092950655684585\n",
      "train loss:0.8927443751837306\n",
      "train loss:0.9928504905598381\n",
      "train loss:0.8542425743245576\n",
      "train loss:0.7890777310641087\n",
      "train loss:0.9002356475920184\n",
      "train loss:0.7609901739743207\n",
      "train loss:0.8495725685055325\n",
      "train loss:1.0568430496764116\n",
      "train loss:0.5590181271565386\n",
      "train loss:0.7355577542574983\n",
      "train loss:0.9458194816054679\n",
      "train loss:0.8426207722493346\n",
      "train loss:1.0142255624005134\n",
      "train loss:0.9480356282477047\n",
      "train loss:0.8275501263387735\n",
      "train loss:1.0525079005977158\n",
      "train loss:0.9541759765633452\n",
      "train loss:0.7961804910542887\n",
      "train loss:0.9968142975544754\n",
      "train loss:1.02976147659854\n",
      "train loss:1.0521289430762613\n",
      "train loss:1.0918134178681478\n",
      "train loss:1.00863410254127\n",
      "train loss:0.8752449557191286\n",
      "train loss:0.7484644594551995\n",
      "train loss:0.9274911247346421\n",
      "train loss:0.8646725110322303\n",
      "train loss:0.8563598641525693\n",
      "train loss:0.8551343456131003\n",
      "train loss:0.877825246796202\n",
      "train loss:0.9060046922882812\n",
      "train loss:1.036369523522958\n",
      "train loss:0.9834441270452775\n",
      "train loss:0.8978483557938374\n",
      "train loss:0.8962873117302301\n",
      "train loss:0.8415152081949531\n",
      "train loss:0.9964690667263076\n",
      "train loss:1.1399276391335713\n",
      "train loss:0.9287649218082386\n",
      "train loss:0.8460303248018615\n",
      "train loss:1.1295648672257328\n",
      "train loss:0.6924420590558834\n",
      "train loss:0.8178152409226906\n",
      "train loss:0.8692073155060099\n",
      "train loss:0.8852827343257543\n",
      "train loss:0.7530557398542839\n",
      "train loss:0.9900828968096491\n",
      "train loss:0.9961284515022701\n",
      "train loss:1.0600870233325665\n",
      "train loss:1.0236034472927575\n",
      "train loss:0.9795380846568862\n",
      "train loss:0.7897745366964402\n",
      "train loss:0.8607056240110641\n",
      "train loss:0.9593800665537019\n",
      "train loss:0.963317815344174\n",
      "train loss:0.8132973738122294\n",
      "train loss:0.8593699223601018\n",
      "train loss:0.8892967683329778\n",
      "train loss:1.0772382648406371\n",
      "train loss:0.9736067690767741\n",
      "train loss:1.029902455506402\n",
      "train loss:0.7979712547933857\n",
      "train loss:0.769930938653731\n",
      "train loss:0.8155179584947418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.775978446291965\n",
      "train loss:0.8778581873848583\n",
      "train loss:0.8099481097615712\n",
      "train loss:0.8081954089241797\n",
      "train loss:0.9424618620650397\n",
      "train loss:0.8898493738391486\n",
      "train loss:0.8528099351825369\n",
      "train loss:0.9417163849656295\n",
      "train loss:1.0236574627057475\n",
      "train loss:1.1360251799746666\n",
      "train loss:0.7474806952554196\n",
      "train loss:0.9315688301319658\n",
      "train loss:0.9580932633947475\n",
      "train loss:0.9681427341286128\n",
      "train loss:0.8166454900967635\n",
      "train loss:1.0129371198100134\n",
      "train loss:0.6742487006475085\n",
      "train loss:0.8272684346361538\n",
      "train loss:1.0136559860485117\n",
      "train loss:0.9176591499026092\n",
      "train loss:0.8330079922940066\n",
      "train loss:0.9437658551126011\n",
      "train loss:0.7633249316190046\n",
      "train loss:0.7882890143739328\n",
      "train loss:1.0107110600075213\n",
      "train loss:0.8003352334511188\n",
      "train loss:0.7215749859143304\n",
      "train loss:0.8301972392683085\n",
      "train loss:0.9923949360691005\n",
      "train loss:1.0093964810905103\n",
      "train loss:0.8567255754945946\n",
      "train loss:0.845771884381108\n",
      "train loss:0.8854010841079326\n",
      "train loss:0.9844480540880567\n",
      "train loss:0.9304403280054274\n",
      "train loss:0.9435050712896036\n",
      "train loss:0.9421006156754799\n",
      "train loss:0.9811760716769851\n",
      "train loss:0.8222441949777144\n",
      "train loss:0.8847694722927844\n",
      "train loss:0.9123812926024697\n",
      "train loss:0.8876816593794556\n",
      "train loss:0.9124220759758542\n",
      "train loss:0.874760749125298\n",
      "train loss:0.8318285617907405\n",
      "train loss:0.8256984454808247\n",
      "train loss:0.8617696214991929\n",
      "train loss:0.8323587049505815\n",
      "train loss:0.9806664551133144\n",
      "train loss:0.7728109007696338\n",
      "train loss:1.0205541895414725\n",
      "train loss:0.8504838409733128\n",
      "train loss:0.8488276538637229\n",
      "train loss:0.8414087139961823\n",
      "train loss:0.8407596998282351\n",
      "train loss:0.9508628271209474\n",
      "train loss:1.0363362671510088\n",
      "train loss:0.8399770898066451\n",
      "train loss:0.7659632174716806\n",
      "train loss:0.9322539252767507\n",
      "train loss:0.802125887371053\n",
      "train loss:1.0348309886516125\n",
      "train loss:0.922985513189803\n",
      "train loss:0.7993410464773821\n",
      "train loss:0.8918292527468985\n",
      "train loss:0.8386055278510598\n",
      "train loss:0.8697791104164194\n",
      "train loss:0.8287196023998165\n",
      "train loss:1.101278182936221\n",
      "train loss:0.9291267524627497\n",
      "train loss:0.9336811664165887\n",
      "train loss:1.0073760945452919\n",
      "train loss:0.922175332172475\n",
      "train loss:0.8411900984395198\n",
      "train loss:0.7166017267750303\n",
      "train loss:0.7371611934166756\n",
      "train loss:1.0072217632378924\n",
      "train loss:0.990248437048402\n",
      "train loss:0.9502912793633099\n",
      "train loss:0.8139868527257018\n",
      "train loss:0.9958005709497931\n",
      "train loss:0.8810534827030209\n",
      "train loss:0.7545276912498382\n",
      "train loss:0.7653780714397296\n",
      "train loss:0.8862858177494901\n",
      "train loss:0.9850489077230533\n",
      "train loss:0.8790219890686601\n",
      "train loss:0.9571684932536259\n",
      "train loss:1.0033562898966577\n",
      "train loss:0.7906314507880735\n",
      "train loss:0.974416314994962\n",
      "train loss:0.9257947101877317\n",
      "train loss:0.9609487590617083\n",
      "train loss:0.9049011356999285\n",
      "train loss:0.9278009645103344\n",
      "train loss:1.0291162909057345\n",
      "train loss:0.841346187140998\n",
      "train loss:0.9957499123994584\n",
      "train loss:0.876431058309225\n",
      "train loss:0.8471254142427198\n",
      "train loss:0.9845094922313424\n",
      "train loss:1.0442847751776803\n",
      "train loss:0.8448730749396175\n",
      "train loss:0.8482619167159146\n",
      "train loss:0.9076250253133975\n",
      "train loss:0.7626934762145464\n",
      "train loss:0.7628624435011458\n",
      "train loss:0.9476882084889447\n",
      "train loss:0.6930065871142845\n",
      "train loss:0.7962841651243209\n",
      "train loss:0.8479531946714458\n",
      "train loss:1.0701004506787766\n",
      "train loss:0.896736836360938\n",
      "train loss:0.6954464397944108\n",
      "train loss:0.9577982038665165\n",
      "train loss:0.8258859486523695\n",
      "train loss:0.9439714919997858\n",
      "train loss:0.8114552794129222\n",
      "train loss:0.7686508684493313\n",
      "train loss:0.7746448453636225\n",
      "train loss:0.885692848233345\n",
      "train loss:0.7994596969788569\n",
      "train loss:1.0034944700947082\n",
      "train loss:0.9713477888832454\n",
      "train loss:0.9535289743658574\n",
      "train loss:0.7387708965881657\n",
      "train loss:0.7901664017363806\n",
      "train loss:0.8568368372569118\n",
      "train loss:0.765485384530164\n",
      "train loss:0.9106664626703254\n",
      "train loss:0.9630244494534281\n",
      "train loss:0.8764529587322744\n",
      "train loss:0.8580111489871007\n",
      "train loss:0.6091178343106616\n",
      "train loss:0.9110659370086047\n",
      "train loss:0.9012355724918288\n",
      "train loss:0.866058678193463\n",
      "train loss:0.9234098968894467\n",
      "train loss:0.6821992959882135\n",
      "train loss:0.8048825618623333\n",
      "train loss:0.8766589327464804\n",
      "train loss:0.8201532286455547\n",
      "train loss:0.9727926305051738\n",
      "train loss:0.9193695208033623\n",
      "train loss:0.9077902372430559\n",
      "train loss:0.8638810253227449\n",
      "train loss:0.9065146683924794\n",
      "train loss:0.9514572113763451\n",
      "train loss:0.9257834473520393\n",
      "train loss:0.7950434317441697\n",
      "train loss:0.9644165084309093\n",
      "train loss:0.9915379281997577\n",
      "train loss:0.9442614074926103\n",
      "train loss:1.0445188469002609\n",
      "train loss:1.0012576948458323\n",
      "train loss:0.9377781797569769\n",
      "train loss:0.7653018745383637\n",
      "train loss:0.8948835936498205\n",
      "train loss:0.8660915918063011\n",
      "train loss:0.8670504590303064\n",
      "train loss:0.8659338587165412\n",
      "train loss:0.9067710067959471\n",
      "train loss:0.8994536248144618\n",
      "train loss:0.9064000141628716\n",
      "train loss:1.0479187369930631\n",
      "train loss:0.783509596148139\n",
      "train loss:0.8355316222457592\n",
      "train loss:0.824798453762692\n",
      "train loss:0.8992948700168489\n",
      "train loss:0.7520941504102285\n",
      "train loss:0.9175731691759432\n",
      "train loss:1.0041940822687385\n",
      "train loss:0.8571062182643945\n",
      "train loss:0.8525383301321363\n",
      "train loss:0.8693798208859539\n",
      "train loss:0.941570327085476\n",
      "train loss:1.0155018647746865\n",
      "train loss:0.821871708301655\n",
      "train loss:0.927645925392978\n",
      "train loss:0.8165003109285651\n",
      "train loss:0.886158806826746\n",
      "train loss:1.0347286529281592\n",
      "train loss:0.7216566889021634\n",
      "train loss:0.7525456503573633\n",
      "train loss:1.0274939163755124\n",
      "train loss:0.8753455335879178\n",
      "train loss:0.8268876323306416\n",
      "train loss:0.7491318276720491\n",
      "train loss:0.7981363396503874\n",
      "train loss:1.0008694732304257\n",
      "train loss:0.8014207471430109\n",
      "train loss:0.7919483274057866\n",
      "train loss:0.9373037648114436\n",
      "train loss:0.8668223555510625\n",
      "train loss:0.922663793137006\n",
      "train loss:1.0554154258913244\n",
      "train loss:0.8524837564592899\n",
      "train loss:0.8731585506708509\n",
      "train loss:0.8653384341134712\n",
      "train loss:0.887590270457894\n",
      "train loss:0.9371293638411069\n",
      "train loss:0.9225268314027225\n",
      "train loss:0.863318647541295\n",
      "train loss:0.9905564643271353\n",
      "train loss:0.7241067923309579\n",
      "train loss:0.9348609514692595\n",
      "train loss:0.6922850856359544\n",
      "train loss:0.9390282384791453\n",
      "train loss:0.8646080120914873\n",
      "train loss:1.03833217528573\n",
      "train loss:0.8707519244629842\n",
      "train loss:0.8037071366481756\n",
      "train loss:0.802439051106334\n",
      "train loss:0.8340723200926732\n",
      "train loss:0.9810720821364377\n",
      "train loss:0.8241824665464883\n",
      "train loss:0.7525313751230941\n",
      "train loss:0.8597128975045923\n",
      "train loss:0.8036804159241097\n",
      "train loss:0.6818354292690668\n",
      "train loss:0.7432917375166095\n",
      "train loss:0.9391797038092637\n",
      "train loss:0.7555462182120659\n",
      "train loss:0.8531896830361957\n",
      "train loss:0.9408017399572638\n",
      "train loss:0.7935212764450401\n",
      "train loss:0.7990492330843899\n",
      "train loss:0.8717479881371072\n",
      "train loss:1.0162643048679159\n",
      "train loss:0.8225763950897492\n",
      "train loss:0.8674353167339411\n",
      "train loss:1.0345501313680363\n",
      "train loss:0.9588319060822432\n",
      "train loss:0.843429846287814\n",
      "train loss:0.892233255724218\n",
      "train loss:0.9412707622227379\n",
      "train loss:0.7714667477005981\n",
      "train loss:0.9129187325736308\n",
      "train loss:0.8199941687613719\n",
      "train loss:0.7828526092932704\n",
      "train loss:1.0561225094332982\n",
      "train loss:1.1040453530152279\n",
      "train loss:0.8799059737626534\n",
      "train loss:0.8796800315014217\n",
      "=== epoch:11, train acc:0.995, test acc:0.987 ===\n",
      "train loss:0.8135739600782409\n",
      "train loss:0.8702997916629063\n",
      "train loss:0.7321652759489135\n",
      "train loss:0.870665562690468\n",
      "train loss:0.7685177737274764\n",
      "train loss:0.9570029625554086\n",
      "train loss:0.9303757601145923\n",
      "train loss:0.8593709994151798\n",
      "train loss:0.7349625764186986\n",
      "train loss:0.8120292960416637\n",
      "train loss:0.7957708504470056\n",
      "train loss:0.8175338816950655\n",
      "train loss:0.745712720996013\n",
      "train loss:0.8339051272984597\n",
      "train loss:0.9342293381352621\n",
      "train loss:0.8020607282025399\n",
      "train loss:0.861502045177842\n",
      "train loss:0.9039436468265061\n",
      "train loss:0.6715005403484021\n",
      "train loss:0.8501307343552638\n",
      "train loss:0.9377836420134029\n",
      "train loss:0.9026555248696188\n",
      "train loss:0.8770473583934105\n",
      "train loss:0.9943883293958193\n",
      "train loss:1.0162158368110037\n",
      "train loss:0.8174982679819878\n",
      "train loss:0.7046560489908623\n",
      "train loss:0.8556414224406077\n",
      "train loss:0.872046668799403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.043541187921918\n",
      "train loss:0.8786476552008955\n",
      "train loss:0.8477028612548021\n",
      "train loss:0.7471063373917501\n",
      "train loss:1.0241931995507139\n",
      "train loss:0.9504362942519657\n",
      "train loss:0.8814655995534912\n",
      "train loss:0.9771028956347155\n",
      "train loss:0.8810686759957267\n",
      "train loss:0.828721858838849\n",
      "train loss:0.8434956200992579\n",
      "train loss:0.9079549704872792\n",
      "train loss:0.7893399028721413\n",
      "train loss:0.7695774426732733\n",
      "train loss:0.9743856028743473\n",
      "train loss:0.8740572935885649\n",
      "train loss:0.9608868491081884\n",
      "train loss:0.7244284677739017\n",
      "train loss:0.8716221845032248\n",
      "train loss:0.9252030044422501\n",
      "train loss:0.9114572910879167\n",
      "train loss:0.9090056733649324\n",
      "train loss:0.945759284857239\n",
      "train loss:1.0019326744446402\n",
      "train loss:0.8492670196516879\n",
      "train loss:0.863679509044461\n",
      "train loss:0.9852476140291698\n",
      "train loss:0.8495457789115288\n",
      "train loss:0.8621326716673411\n",
      "train loss:0.8723801481738965\n",
      "train loss:1.0585706664499608\n",
      "train loss:0.9468816817615042\n",
      "train loss:0.9396088059256731\n",
      "train loss:0.8108855194508571\n",
      "train loss:1.1150004426201954\n",
      "train loss:0.940903478952523\n",
      "train loss:0.8572706418074716\n",
      "train loss:0.8881440195344166\n",
      "train loss:0.704513628983785\n",
      "train loss:0.9759515444634637\n",
      "train loss:0.9442960522071612\n",
      "train loss:0.7488303247180338\n",
      "train loss:0.939576293707075\n",
      "train loss:0.8390652295702428\n",
      "train loss:0.7951172367063599\n",
      "train loss:0.9576601246973786\n",
      "train loss:0.8332076290921823\n",
      "train loss:0.7977234672674662\n",
      "train loss:0.8800315305984102\n",
      "train loss:0.9571979222281226\n",
      "train loss:0.9994298890186455\n",
      "train loss:0.9732842794803989\n",
      "train loss:0.9952559058389725\n",
      "train loss:0.7560878804536222\n",
      "train loss:0.9118174045180095\n",
      "train loss:0.828936512170023\n",
      "train loss:0.8743630138972782\n",
      "train loss:0.7492428763561508\n",
      "train loss:0.8230203744832923\n",
      "train loss:0.8377309196731842\n",
      "train loss:0.6834728964470158\n",
      "train loss:0.791955743321181\n",
      "train loss:0.8804145654407526\n",
      "train loss:1.0790822820001518\n",
      "train loss:0.7567195415691778\n",
      "train loss:0.8823835842569002\n",
      "train loss:0.8078107367042836\n",
      "train loss:0.884931645930682\n",
      "train loss:1.0242134758470223\n",
      "train loss:1.0965109266730406\n",
      "train loss:0.8968200607931379\n",
      "train loss:0.794015335918975\n",
      "train loss:0.9425651130251606\n",
      "train loss:0.9592363017994399\n",
      "train loss:0.8676216754835947\n",
      "train loss:1.043034193346565\n",
      "train loss:0.9362570234794805\n",
      "train loss:0.8467411818082887\n",
      "train loss:0.7458557336380206\n",
      "train loss:0.8055239473088182\n",
      "train loss:0.9548043840782099\n",
      "train loss:1.0007062359940333\n",
      "train loss:0.762673826765603\n",
      "train loss:0.8771647552361643\n",
      "train loss:0.8808152626962805\n",
      "train loss:0.8341190425482501\n",
      "train loss:0.9075489447987589\n",
      "train loss:0.8886212636972916\n",
      "train loss:0.7785116451615893\n",
      "train loss:0.8039674021455586\n",
      "train loss:0.8921764509505137\n",
      "train loss:0.9098717560601876\n",
      "train loss:1.038695559933491\n",
      "train loss:0.8828664427794043\n",
      "train loss:0.8655944528967379\n",
      "train loss:0.863174821571971\n",
      "train loss:0.898646263070179\n",
      "train loss:0.9334018619878041\n",
      "train loss:0.8607785869087425\n",
      "train loss:0.7800217854781382\n",
      "train loss:0.7905017952061378\n",
      "train loss:0.7822786646376225\n",
      "train loss:0.7692852465754263\n",
      "train loss:0.9167808343778201\n",
      "train loss:0.9399944113274968\n",
      "train loss:0.9314744389725975\n",
      "train loss:0.892115415466185\n",
      "train loss:0.8836661189174607\n",
      "train loss:0.8833895234193528\n",
      "train loss:0.8037248732574237\n",
      "train loss:1.0218574259816153\n",
      "train loss:0.9432018967833864\n",
      "train loss:0.8615751639414606\n",
      "train loss:0.8345482420396205\n",
      "train loss:1.0033920456908803\n",
      "train loss:1.0584830875766187\n",
      "train loss:0.8998465282815644\n",
      "train loss:1.010130018623457\n",
      "train loss:0.8812953289590238\n",
      "train loss:0.7704205868010803\n",
      "train loss:0.7922750111941899\n",
      "train loss:0.9167409662399134\n",
      "train loss:0.74646411112409\n",
      "train loss:0.8351662966194299\n",
      "train loss:0.9581264579719999\n",
      "train loss:0.8121447411144745\n",
      "train loss:0.8838408307108788\n",
      "train loss:0.7310736671923505\n",
      "train loss:0.8320865548528132\n",
      "train loss:0.902483739044529\n",
      "train loss:0.9726285951446877\n",
      "train loss:0.9591287737313074\n",
      "train loss:0.948996070234713\n",
      "train loss:0.8282467451909794\n",
      "train loss:0.9095133115008481\n",
      "train loss:1.0621911265368826\n",
      "train loss:0.901085258359464\n",
      "train loss:0.8886602864281008\n",
      "train loss:0.8298050540672678\n",
      "train loss:0.9500413803243\n",
      "train loss:0.9583775017439394\n",
      "train loss:0.9568876758087135\n",
      "train loss:0.8843035211486534\n",
      "train loss:0.7880728799289538\n",
      "train loss:0.9228945587445994\n",
      "train loss:0.6861367835667238\n",
      "train loss:0.9426023561360918\n",
      "train loss:0.9119024119954047\n",
      "train loss:1.0551624632768561\n",
      "train loss:0.8750246587874696\n",
      "train loss:0.9163916529714348\n",
      "train loss:1.0578035108386332\n",
      "train loss:0.656332868023037\n",
      "train loss:0.6750076025221428\n",
      "train loss:0.9137221096139477\n",
      "train loss:0.8654921898097059\n",
      "train loss:0.7809857687555709\n",
      "train loss:0.8445079207323569\n",
      "train loss:0.83587689076016\n",
      "train loss:0.7656771048264299\n",
      "train loss:0.9095034945635997\n",
      "train loss:0.804065489039919\n",
      "train loss:0.9732345925245659\n",
      "train loss:0.7487094598997663\n",
      "train loss:0.8989135469634193\n",
      "train loss:0.9516406788169225\n",
      "train loss:0.8398047458725153\n",
      "train loss:0.9810759874491058\n",
      "train loss:0.8296693638068653\n",
      "train loss:0.7082256745278618\n",
      "train loss:0.9258053910792313\n",
      "train loss:0.8078905371172044\n",
      "train loss:0.9529562293913675\n",
      "train loss:0.7803830638678696\n",
      "train loss:0.9436220266940694\n",
      "train loss:0.9104666767468831\n",
      "train loss:0.8100975986615598\n",
      "train loss:0.7893046454753629\n",
      "train loss:0.9231609974194549\n",
      "train loss:0.8251210052877629\n",
      "train loss:0.9194568841641477\n",
      "train loss:0.9729585247674839\n",
      "train loss:0.9600104563324708\n",
      "train loss:0.9415867093599207\n",
      "train loss:0.9272221471961898\n",
      "train loss:0.9269269196488972\n",
      "train loss:0.9547207529852985\n",
      "train loss:0.8911249631454657\n",
      "train loss:0.8090756514216069\n",
      "train loss:0.8658168056585628\n",
      "train loss:0.8864748499515707\n",
      "train loss:0.9414844130674221\n",
      "train loss:1.1693636749130674\n",
      "train loss:0.9112726182087567\n",
      "train loss:0.8235999769355301\n",
      "train loss:0.9033768310316528\n",
      "train loss:0.7294998603526852\n",
      "train loss:0.8331121557124408\n",
      "train loss:0.7901428593019793\n",
      "train loss:0.8057186319170319\n",
      "train loss:0.857257506969955\n",
      "train loss:0.8939899791575173\n",
      "train loss:0.890948976825316\n",
      "train loss:1.012628554216787\n",
      "train loss:1.0028989892446447\n",
      "train loss:0.9469627110635733\n",
      "train loss:0.9335194672168932\n",
      "train loss:0.9353472616772462\n",
      "train loss:1.0582540115056849\n",
      "train loss:0.7765691796871398\n",
      "train loss:0.8643287693215295\n",
      "train loss:1.0663605307972979\n",
      "train loss:0.8733841219995101\n",
      "train loss:0.8991393759774149\n",
      "train loss:1.019473618847306\n",
      "train loss:0.7953310776297466\n",
      "train loss:0.8514363450287565\n",
      "train loss:0.8972887263401038\n",
      "train loss:0.823400902574094\n",
      "train loss:0.9478044698815081\n",
      "train loss:0.6410657086849147\n",
      "train loss:0.796802069263466\n",
      "train loss:0.991071389913433\n",
      "train loss:0.8306981098907904\n",
      "train loss:0.966644430700552\n",
      "train loss:0.8769060947048505\n",
      "train loss:0.9988423839115235\n",
      "train loss:0.8473163369006707\n",
      "train loss:0.915992266197672\n",
      "train loss:0.8129150795937776\n",
      "train loss:0.8294516882560863\n",
      "train loss:0.7820848997037639\n",
      "train loss:0.8889109635354595\n",
      "train loss:0.9481267863129357\n",
      "train loss:0.9500481471561821\n",
      "train loss:0.8811811934504813\n",
      "train loss:0.8812996017636227\n",
      "train loss:1.056690309658089\n",
      "train loss:0.9600451402580705\n",
      "train loss:0.965653930407582\n",
      "train loss:0.733057306145841\n",
      "train loss:0.8418934749461121\n",
      "train loss:0.7513306809477958\n",
      "train loss:0.9193023071959816\n",
      "train loss:0.9660616049586175\n",
      "train loss:0.8924048353924446\n",
      "train loss:0.7978024408716813\n",
      "train loss:0.8924204290689665\n",
      "train loss:0.9345814189777445\n",
      "train loss:0.7497335538969152\n",
      "train loss:0.7494246956423816\n",
      "train loss:0.8553663588096554\n",
      "train loss:0.7717287686965494\n",
      "train loss:0.7435907255586102\n",
      "train loss:0.8167538254393817\n",
      "train loss:0.8801103493271099\n",
      "train loss:0.832331094422672\n",
      "train loss:1.0299334652445848\n",
      "train loss:0.8673407793845324\n",
      "train loss:0.8561696339537407\n",
      "train loss:0.7509289430376873\n",
      "train loss:0.7948044003702743\n",
      "train loss:0.947530536978376\n",
      "train loss:0.9338199516949911\n",
      "train loss:0.8791097603948136\n",
      "train loss:0.8727529051635308\n",
      "train loss:0.8301979224770021\n",
      "train loss:0.6861037404445512\n",
      "train loss:0.9020798335851044\n",
      "train loss:1.0439804306253437\n",
      "train loss:0.8453167987159244\n",
      "train loss:0.7331581013059828\n",
      "train loss:0.7385746218557228\n",
      "train loss:0.9341455770327015\n",
      "train loss:1.0167408113017493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.8121602546671726\n",
      "train loss:0.7769742917913764\n",
      "train loss:0.995454798864142\n",
      "train loss:0.856891089135881\n",
      "train loss:0.6899188813812277\n",
      "train loss:0.8724901951102049\n",
      "train loss:0.8856082465959412\n",
      "train loss:0.9472602522748379\n",
      "train loss:0.9378173973158027\n",
      "train loss:0.8735648405538398\n",
      "train loss:1.04860818090588\n",
      "train loss:0.7938017477675977\n",
      "train loss:0.8321152470420514\n",
      "train loss:0.81540227104405\n",
      "train loss:0.880678985467154\n",
      "train loss:0.8454656191120677\n",
      "train loss:0.8714083808162537\n",
      "train loss:0.9936742485202567\n",
      "train loss:0.8088199766559859\n",
      "train loss:0.8717732172100828\n",
      "train loss:0.9326908561310219\n",
      "train loss:0.9628178180982971\n",
      "train loss:1.015147167693027\n",
      "train loss:0.9061646371936949\n",
      "train loss:0.881689360634089\n",
      "train loss:0.8249829842371369\n",
      "train loss:0.8405504994088864\n",
      "train loss:0.861985215312379\n",
      "train loss:0.791131985286748\n",
      "train loss:0.9347576620863847\n",
      "train loss:0.85591617316526\n",
      "train loss:0.8416907809004802\n",
      "train loss:0.9190796406126408\n",
      "train loss:0.9543306971561317\n",
      "train loss:0.9541165480821776\n",
      "train loss:1.0499908172840648\n",
      "train loss:0.9283871129596336\n",
      "train loss:0.6927583496431359\n",
      "train loss:0.7350168245174684\n",
      "train loss:0.7642241407435563\n",
      "train loss:0.9610435638333565\n",
      "train loss:0.9504687129545737\n",
      "train loss:0.8619014484596808\n",
      "train loss:0.8748940912095966\n",
      "train loss:0.8368188299298656\n",
      "train loss:0.8117167625117625\n",
      "train loss:0.9495852005809283\n",
      "train loss:0.9177324805194297\n",
      "train loss:0.8993314114168993\n",
      "train loss:0.8394229674375435\n",
      "train loss:0.9021806412593615\n",
      "train loss:1.0201827102932737\n",
      "train loss:1.0072747670275617\n",
      "train loss:0.7604084711174306\n",
      "train loss:0.895351320311113\n",
      "train loss:0.8303517423710006\n",
      "train loss:0.7988022725163942\n",
      "train loss:0.9195720671577347\n",
      "train loss:0.974823338698597\n",
      "train loss:0.8802224006444771\n",
      "train loss:1.0297575097574143\n",
      "train loss:1.0021508755207946\n",
      "train loss:0.8947761365493523\n",
      "train loss:0.8555063323306068\n",
      "train loss:0.9795597827251156\n",
      "train loss:0.7671532131300473\n",
      "train loss:0.6953764955308201\n",
      "train loss:0.8515767084750341\n",
      "train loss:0.9061994237830016\n",
      "train loss:0.8312035804681948\n",
      "train loss:0.7408192645259838\n",
      "train loss:0.7676661762937804\n",
      "train loss:0.840797720534768\n",
      "train loss:1.0151057201046378\n",
      "train loss:0.8151455841158511\n",
      "train loss:0.8391589291339326\n",
      "train loss:0.7998008994989432\n",
      "train loss:0.8424362103724943\n",
      "train loss:0.9239644329425752\n",
      "train loss:0.9281465477814144\n",
      "train loss:0.8699399594296507\n",
      "train loss:0.9308430948332846\n",
      "train loss:0.94128727427273\n",
      "train loss:0.8997095779331022\n",
      "train loss:0.8282139954594685\n",
      "train loss:0.8734774119428286\n",
      "train loss:0.9091769640510674\n",
      "train loss:0.8134731591819923\n",
      "train loss:0.8454522894312413\n",
      "train loss:0.9055552942876597\n",
      "train loss:0.8035227655268572\n",
      "train loss:0.7023482309132425\n",
      "train loss:0.8626139957348831\n",
      "train loss:0.9293433569660869\n",
      "train loss:0.7960132773844731\n",
      "train loss:0.9366398727427802\n",
      "train loss:0.7876253669670744\n",
      "train loss:0.8773543273105421\n",
      "train loss:0.946501814741948\n",
      "train loss:0.8755206092839799\n",
      "train loss:0.7204756523918069\n",
      "train loss:0.9872112844566563\n",
      "train loss:0.7882405496559809\n",
      "train loss:0.7726411137860036\n",
      "train loss:0.9113187956224289\n",
      "train loss:0.7644454600237391\n",
      "train loss:0.8648127597900825\n",
      "train loss:0.8947057485654569\n",
      "train loss:0.9334791651886601\n",
      "train loss:0.9060053655949869\n",
      "train loss:0.8550899339506169\n",
      "train loss:0.6924484648802708\n",
      "train loss:0.8996564267963368\n",
      "train loss:0.8507385763389\n",
      "train loss:0.7925912852125978\n",
      "train loss:0.8558993553960218\n",
      "train loss:0.8048151209674284\n",
      "train loss:1.0498426979190603\n",
      "train loss:0.8386218037736225\n",
      "train loss:0.8985981368312304\n",
      "train loss:0.9240531757196794\n",
      "train loss:1.1947511084287594\n",
      "train loss:0.7736860636268219\n",
      "train loss:0.9626278355888922\n",
      "train loss:0.963026530917411\n",
      "train loss:0.7555940903857793\n",
      "train loss:0.7243009148111892\n",
      "train loss:0.9154446859305303\n",
      "train loss:0.8040454699853055\n",
      "train loss:0.9619126288440634\n",
      "train loss:0.8814585372413819\n",
      "train loss:0.8669493896662993\n",
      "train loss:0.7820854464226885\n",
      "train loss:0.7341000968728166\n",
      "train loss:0.9370848818314468\n",
      "train loss:0.9502159711592448\n",
      "train loss:0.7021375247806599\n",
      "train loss:0.8280228904283707\n",
      "train loss:0.7902005194682815\n",
      "train loss:0.7740745598642889\n",
      "train loss:0.9516088417666144\n",
      "train loss:0.7454605758620036\n",
      "train loss:1.1053893914921322\n",
      "train loss:0.8239383954505989\n",
      "train loss:0.9629877251803586\n",
      "train loss:0.8819928393817504\n",
      "train loss:0.9896339998834663\n",
      "train loss:0.7949627338794013\n",
      "train loss:1.0696225429158632\n",
      "train loss:0.7529141004697371\n",
      "train loss:0.9310681249873461\n",
      "train loss:0.9274863839859033\n",
      "train loss:0.7608875568849645\n",
      "train loss:1.0150835499786866\n",
      "train loss:0.8926472195194632\n",
      "train loss:0.7701814735389081\n",
      "train loss:0.8799220686244205\n",
      "train loss:0.8791967951576476\n",
      "train loss:0.8135702007169289\n",
      "train loss:0.9424072182903397\n",
      "train loss:0.8459614014222694\n",
      "train loss:0.9452695076417177\n",
      "train loss:0.8971589801094604\n",
      "train loss:0.7048064374355701\n",
      "train loss:0.9386875131925604\n",
      "train loss:0.826623682662035\n",
      "train loss:0.8079425719498644\n",
      "train loss:0.8706704464859852\n",
      "train loss:0.7918308031958956\n",
      "train loss:0.8886639104011248\n",
      "train loss:0.8148464009193362\n",
      "train loss:1.1189451394645382\n",
      "train loss:0.8382125073668729\n",
      "train loss:0.8637248262564904\n",
      "train loss:0.8576056001961819\n",
      "train loss:0.7715529193528732\n",
      "train loss:0.7451104574509843\n",
      "train loss:0.8576179826270538\n",
      "train loss:0.9069401074169012\n",
      "train loss:0.9465539254728017\n",
      "train loss:0.9769649156530229\n",
      "train loss:0.873318425445989\n",
      "train loss:0.9630264482844444\n",
      "train loss:0.920123018099688\n",
      "train loss:0.8629249416911636\n",
      "train loss:1.042527596568298\n",
      "train loss:1.0331256139977738\n",
      "train loss:1.0115207459159672\n",
      "train loss:0.8686275441287239\n",
      "train loss:0.9099443873783736\n",
      "train loss:0.7591379002533257\n",
      "train loss:0.7607304005781544\n",
      "train loss:0.9016743851752177\n",
      "train loss:0.8646803795006517\n",
      "train loss:0.831473585196681\n",
      "train loss:0.8200544369554486\n",
      "train loss:0.8143287500210523\n",
      "train loss:0.9764245822688389\n",
      "train loss:0.8517031614862327\n",
      "train loss:0.9069510600433055\n",
      "train loss:0.8918355543059029\n",
      "train loss:1.1109358025351865\n",
      "train loss:0.9005997331402196\n",
      "train loss:0.8290261846259959\n",
      "train loss:0.7686149860948052\n",
      "train loss:0.9236451547642999\n",
      "train loss:0.9088453112109656\n",
      "train loss:0.6943741588262741\n",
      "train loss:0.9132480661445035\n",
      "train loss:1.0010060966642602\n",
      "train loss:0.9503641242614657\n",
      "train loss:0.8821155019938633\n",
      "train loss:0.8426180053715797\n",
      "train loss:1.0033420336204046\n",
      "train loss:0.8827500682433835\n",
      "train loss:0.8583792037140665\n",
      "train loss:0.7605765173774399\n",
      "train loss:1.0099769253129216\n",
      "train loss:0.8256635470238289\n",
      "train loss:0.9982807940399792\n",
      "train loss:0.943942586259571\n",
      "train loss:0.9079431893388238\n",
      "train loss:0.8476289155988903\n",
      "train loss:0.8241449540636109\n",
      "train loss:0.8422236947968248\n",
      "train loss:0.9681334338119021\n",
      "train loss:0.9492095869140585\n",
      "train loss:0.9901568356993309\n",
      "train loss:0.8362605450120014\n",
      "train loss:0.8523001194093089\n",
      "train loss:0.8426499782132713\n",
      "train loss:0.9537862887981688\n",
      "train loss:0.988162816554909\n",
      "train loss:0.7916774697001204\n",
      "train loss:0.8849797495002848\n",
      "train loss:0.7363896150394553\n",
      "train loss:1.0133943895167241\n",
      "train loss:1.0168863475678145\n",
      "train loss:0.826649284908051\n",
      "train loss:0.8713175314070603\n",
      "train loss:0.7705078904651732\n",
      "train loss:0.9468707426231597\n",
      "train loss:0.9158846628666341\n",
      "train loss:0.9239643103642\n",
      "train loss:0.9308045060281447\n",
      "train loss:0.8573222143435903\n",
      "train loss:0.9906959611629902\n",
      "train loss:0.7621105917040712\n",
      "train loss:0.9488290949402425\n",
      "train loss:1.1130247141730805\n",
      "train loss:0.8962661963481282\n",
      "train loss:0.9176243822073223\n",
      "train loss:0.9900315398738836\n",
      "train loss:0.8628870204253041\n",
      "train loss:0.8006746438541463\n",
      "train loss:0.8503696700219926\n",
      "train loss:0.8255493605394061\n",
      "train loss:0.765093495106821\n",
      "train loss:0.8085911079384502\n",
      "train loss:0.8517934148585372\n",
      "train loss:0.9876231975801995\n",
      "train loss:0.9083111536018182\n",
      "train loss:0.9905336745842684\n",
      "train loss:1.0646206680195867\n",
      "train loss:0.8869113889996698\n",
      "train loss:0.9257344221659212\n",
      "train loss:0.8897382872519958\n",
      "train loss:0.8966127168126071\n",
      "train loss:0.8983512475508252\n",
      "train loss:0.8970257713048416\n",
      "train loss:0.7864141774250187\n",
      "train loss:1.0537569514723046\n",
      "train loss:0.7674385159305638\n",
      "train loss:1.0388796123016781\n",
      "train loss:0.9223848353725149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.909772326813098\n",
      "train loss:0.8354922251478023\n",
      "train loss:1.0904790883141526\n",
      "train loss:0.7824887751055691\n",
      "train loss:0.9518072794834108\n",
      "train loss:0.8882376741942163\n",
      "train loss:0.8630246336651227\n",
      "train loss:0.918436088747977\n",
      "train loss:0.7548888501534514\n",
      "train loss:0.9386341771107989\n",
      "train loss:0.9219887030285201\n",
      "train loss:0.7455118067348699\n",
      "train loss:0.9255031961009879\n",
      "train loss:0.9359466015439765\n",
      "train loss:0.9790572941445969\n",
      "train loss:0.844992451378471\n",
      "train loss:0.8720786676588209\n",
      "train loss:0.9547163313020787\n",
      "train loss:0.9765857230389282\n",
      "train loss:0.8269205488418927\n",
      "train loss:0.8677328234141103\n",
      "=== epoch:12, train acc:0.995, test acc:0.994 ===\n",
      "train loss:0.9194891073990858\n",
      "train loss:0.7555517232037707\n",
      "train loss:0.814272302145623\n",
      "train loss:1.1480096394382402\n",
      "train loss:0.8482736455967799\n",
      "train loss:0.7784951014401656\n",
      "train loss:0.7729325329079818\n",
      "train loss:0.8408740030284738\n",
      "train loss:0.8239153531637716\n",
      "train loss:0.7674732239050622\n",
      "train loss:0.8126601783151897\n",
      "train loss:0.8355729834841381\n",
      "train loss:0.9346534095235632\n",
      "train loss:0.9608655576997773\n",
      "train loss:0.7118485007491877\n",
      "train loss:0.9194700061621225\n",
      "train loss:0.7788810785409267\n",
      "train loss:1.1060255482888273\n",
      "train loss:0.9124917310253196\n",
      "train loss:0.7796455559672582\n",
      "train loss:0.8960540339198743\n",
      "train loss:1.067629556569573\n",
      "train loss:0.8720186498856565\n",
      "train loss:0.8811466030694147\n",
      "train loss:0.8169208486602728\n",
      "train loss:0.9640449337182067\n",
      "train loss:0.9252911637406637\n",
      "train loss:0.9329483848105319\n",
      "train loss:0.8214439429023254\n",
      "train loss:0.9717748736172535\n",
      "train loss:0.9297737107321469\n",
      "train loss:0.8386873332372808\n",
      "train loss:0.7031638426962803\n",
      "train loss:0.9126519554370026\n",
      "train loss:0.874014012078221\n",
      "train loss:0.9724603831838343\n",
      "train loss:0.9536154038433999\n",
      "train loss:0.9237479580555308\n",
      "train loss:0.9347518958295801\n",
      "train loss:0.8641875425042894\n",
      "train loss:0.8583851324420267\n",
      "train loss:0.9870701193998623\n",
      "train loss:0.9408274758367273\n",
      "train loss:0.7648102643244625\n",
      "train loss:1.0105427891869738\n",
      "train loss:0.9964312771733159\n",
      "train loss:0.8729747482629193\n",
      "train loss:0.732843515434524\n",
      "train loss:0.8941048867198\n",
      "train loss:0.7970934275590124\n",
      "train loss:0.8542719164529398\n",
      "train loss:0.9573475108114998\n",
      "train loss:0.7492485057320835\n",
      "train loss:0.9163904867169553\n",
      "train loss:0.9639894608193086\n",
      "train loss:0.883334150885165\n",
      "train loss:0.8534952051453324\n",
      "train loss:0.9235928012481551\n",
      "train loss:0.8150891129350374\n",
      "train loss:0.8690313405132177\n",
      "train loss:0.8004667024600726\n",
      "train loss:0.8941500597986142\n",
      "train loss:0.9534064447064751\n",
      "train loss:0.8881297928852628\n",
      "train loss:0.8619672974081858\n",
      "train loss:0.941240913236604\n",
      "train loss:0.8219194663006297\n",
      "train loss:0.7298972612084167\n",
      "train loss:0.9561693765290592\n",
      "train loss:0.9633779748229526\n",
      "train loss:0.7839221371026571\n",
      "train loss:0.7942206621562726\n",
      "train loss:0.8118048584861817\n",
      "train loss:0.8409001836354446\n",
      "train loss:0.9318227736803338\n",
      "train loss:0.8367889457902349\n",
      "train loss:0.9192974658778161\n",
      "train loss:0.930865535441125\n",
      "train loss:1.0684658441073702\n",
      "train loss:0.7836899920849066\n",
      "train loss:0.8335224093478167\n",
      "train loss:0.6570438991118595\n",
      "train loss:0.7466879265585366\n",
      "train loss:0.8067487595658218\n",
      "train loss:0.8249279617252276\n",
      "train loss:0.9313017981580193\n",
      "train loss:0.788616562099012\n",
      "train loss:0.7818247523072543\n",
      "train loss:0.8579511238133454\n",
      "train loss:0.9179039833579948\n",
      "train loss:0.7689072577750596\n",
      "train loss:0.864601003123305\n",
      "train loss:0.7166947161857102\n",
      "train loss:0.917915210595092\n",
      "train loss:0.7803645372786059\n",
      "train loss:0.9592645321693706\n",
      "train loss:0.8892893864757645\n",
      "train loss:0.8246676838919481\n",
      "train loss:0.8432122706351847\n",
      "train loss:0.8526025207470734\n",
      "train loss:0.9718712611291369\n",
      "train loss:0.988067519885352\n",
      "train loss:0.8350626136967878\n",
      "train loss:1.0142938410206377\n",
      "train loss:0.8632979041936241\n",
      "train loss:0.978064777378561\n",
      "train loss:0.8987369530108115\n",
      "train loss:1.0626712826946756\n",
      "train loss:0.8248380705326949\n",
      "train loss:0.8678622849719556\n",
      "train loss:0.8573743920658637\n",
      "train loss:0.81299318768484\n",
      "train loss:0.912472770307414\n",
      "train loss:0.8523814967087007\n",
      "train loss:0.8920893784776996\n",
      "train loss:0.9760777448363297\n",
      "train loss:0.8835724425718523\n",
      "train loss:0.9275915394397497\n",
      "train loss:0.7153124662481101\n",
      "train loss:0.8616525124968928\n",
      "train loss:1.0509088990374882\n",
      "train loss:0.9047827704014381\n",
      "train loss:0.7298890005402804\n",
      "train loss:0.8846547252284637\n",
      "train loss:0.8233456551481911\n",
      "train loss:0.738427599750949\n",
      "train loss:0.9134749673486522\n",
      "train loss:0.9119761821637414\n",
      "train loss:0.9097457550568453\n",
      "train loss:0.7717841144378721\n",
      "train loss:0.7970322895972621\n",
      "train loss:0.9604088907564108\n",
      "train loss:0.7954438942524136\n",
      "train loss:1.0422780165049013\n",
      "train loss:0.814846503548534\n",
      "train loss:0.784814746541628\n",
      "train loss:0.786384766755033\n",
      "train loss:0.8154608479552392\n",
      "train loss:0.8034695062294136\n",
      "train loss:0.7676026986973224\n",
      "train loss:0.9560625918026922\n",
      "train loss:0.7677585479586917\n",
      "train loss:0.941701353459671\n",
      "train loss:0.8506607527070272\n",
      "train loss:1.0229199509509628\n",
      "train loss:0.706543339015703\n",
      "train loss:0.9036681402408839\n",
      "train loss:1.0767256331774469\n",
      "train loss:0.8439277858420827\n",
      "train loss:0.7793926571026283\n",
      "train loss:0.8435355962039377\n",
      "train loss:0.8504766477705209\n",
      "train loss:0.8325518784752703\n",
      "train loss:0.9417738853726192\n",
      "train loss:0.86978885684774\n",
      "train loss:1.08231460198945\n",
      "train loss:0.8745006954302528\n",
      "train loss:0.8528582517938357\n",
      "train loss:0.9678979780064637\n",
      "train loss:0.8266554794233\n",
      "train loss:0.8101830555668958\n",
      "train loss:0.8082580876131894\n",
      "train loss:0.8205725502341052\n",
      "train loss:0.9509975532786252\n",
      "train loss:0.9319951631676717\n",
      "train loss:0.980095014053131\n",
      "train loss:0.8350450602161068\n",
      "train loss:0.9039673824126069\n",
      "train loss:0.9466620147923354\n",
      "train loss:0.8203267889375355\n",
      "train loss:0.7525059070586664\n",
      "train loss:0.8997604916879602\n",
      "train loss:0.7273471288412849\n",
      "train loss:0.766670711140128\n",
      "train loss:0.8707912501061216\n",
      "train loss:0.8823723742655258\n",
      "train loss:1.009020661228651\n",
      "train loss:0.8239994977512473\n",
      "train loss:0.927320389430033\n",
      "train loss:0.8906726428754196\n",
      "train loss:0.7964392669662899\n",
      "train loss:0.9431040002518971\n",
      "train loss:0.8484056786935514\n",
      "train loss:0.9489507385696114\n",
      "train loss:0.9054553304012205\n",
      "train loss:0.9602587787707466\n",
      "train loss:0.8008179190374943\n",
      "train loss:0.877361546994804\n",
      "train loss:0.9912511172390128\n",
      "train loss:0.7506467817504165\n",
      "train loss:0.8057823252441291\n",
      "train loss:0.9475763895768138\n",
      "train loss:0.8434662251724074\n",
      "train loss:1.040162645943354\n",
      "train loss:0.7999083513241849\n",
      "train loss:1.0792436419740037\n",
      "train loss:0.9778536030455126\n",
      "train loss:0.9209571269109587\n",
      "train loss:0.7122642418645526\n",
      "train loss:0.9265598619022052\n",
      "train loss:0.8976254007847393\n",
      "train loss:1.0111379217574061\n",
      "train loss:0.9720847677650242\n",
      "train loss:0.9870011277521921\n",
      "train loss:0.9341634051770551\n",
      "train loss:0.8481254201895555\n",
      "train loss:0.8420662867584052\n",
      "train loss:0.9354494083036405\n",
      "train loss:0.8081017749005344\n",
      "train loss:0.8070963064502626\n",
      "train loss:0.8881015005074658\n",
      "train loss:1.0592762767234385\n",
      "train loss:0.8062719735313931\n",
      "train loss:0.8977518786583135\n",
      "train loss:0.8964900975461803\n",
      "train loss:0.9308737509049152\n",
      "train loss:0.9459859178081825\n",
      "train loss:0.8041918702080575\n",
      "train loss:0.8652996683201485\n",
      "train loss:0.8249385857749049\n",
      "train loss:0.8872616219360797\n",
      "train loss:0.6799723825297589\n",
      "train loss:0.8569097034054545\n",
      "train loss:0.794232240286703\n",
      "train loss:0.8868545432351533\n",
      "train loss:0.9126588698367949\n",
      "train loss:0.9865476561594609\n",
      "train loss:0.891004921032344\n",
      "train loss:0.8988114596600804\n",
      "train loss:0.8654652428188798\n",
      "train loss:0.8884534207318547\n",
      "train loss:0.7926117274240535\n",
      "train loss:0.9137450033962401\n",
      "train loss:0.8529038039306882\n",
      "train loss:0.9142957223741693\n",
      "train loss:0.8489155346020486\n",
      "train loss:0.8156916642364804\n",
      "train loss:0.890527707698305\n",
      "train loss:1.1500631479046692\n",
      "train loss:0.8699381663719659\n",
      "train loss:0.9673909828001169\n",
      "train loss:0.8043751477863116\n",
      "train loss:0.9539849982216172\n",
      "train loss:0.9537580370597403\n",
      "train loss:0.9016922756325463\n",
      "train loss:0.9846737467920463\n",
      "train loss:0.9169299147044342\n",
      "train loss:0.8340199101424005\n",
      "train loss:0.7740669605045766\n",
      "train loss:0.9777612004695929\n",
      "train loss:0.9627309562475225\n",
      "train loss:1.061717598510555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.911252533893909\n",
      "train loss:0.8402830503595254\n",
      "train loss:0.8300810000186136\n",
      "train loss:0.7703665113602072\n",
      "train loss:0.8916533919558565\n",
      "train loss:0.9823398544147078\n",
      "train loss:0.824195143196484\n",
      "train loss:1.0242234230594927\n",
      "train loss:0.9396701686150919\n",
      "train loss:0.8939470196576282\n",
      "train loss:0.7419949824138574\n",
      "train loss:0.8271212426612706\n",
      "train loss:0.8365553361985744\n",
      "train loss:0.9040590925220564\n",
      "train loss:0.8513714304323574\n",
      "train loss:0.9184216898075254\n",
      "train loss:1.0177561551962933\n",
      "train loss:0.8362803717008517\n",
      "train loss:0.9486752876405317\n",
      "train loss:0.7461316345658753\n",
      "train loss:0.8144117214464707\n",
      "train loss:0.8868134517326876\n",
      "train loss:0.8727562829116503\n",
      "train loss:0.9836818583579191\n",
      "train loss:0.9589314830297853\n",
      "train loss:0.9637231530709169\n",
      "train loss:0.7818894121239893\n",
      "train loss:0.8550258672898629\n",
      "train loss:0.930533835332898\n",
      "train loss:0.9206567794598116\n",
      "train loss:0.804263728967355\n",
      "train loss:0.8922756967577773\n",
      "train loss:0.9925905316595138\n",
      "train loss:0.8421282067232224\n",
      "train loss:0.9286727401335577\n",
      "train loss:0.9406090095300111\n",
      "train loss:0.9084379413618814\n",
      "train loss:0.7980215379562011\n",
      "train loss:0.7347357500940077\n",
      "train loss:0.8775488757176366\n",
      "train loss:0.9913713671967984\n",
      "train loss:1.0279793710717182\n",
      "train loss:0.9183262178535336\n",
      "train loss:0.9490025784211495\n",
      "train loss:0.8480433209459008\n",
      "train loss:0.855071418986498\n",
      "train loss:0.7919959517014691\n",
      "train loss:0.8632816806627361\n",
      "train loss:0.9848428384681155\n",
      "train loss:1.05743542630484\n",
      "train loss:1.1298290237879254\n",
      "train loss:0.7224488647772308\n",
      "train loss:0.7636765174561269\n",
      "train loss:0.8713147633664675\n",
      "train loss:0.9889074676544892\n",
      "train loss:0.9197479773453605\n",
      "train loss:0.8671987459325072\n",
      "train loss:0.9047786624494062\n",
      "train loss:0.8327911683738063\n",
      "train loss:0.8080022156025586\n",
      "train loss:0.8666451212297837\n",
      "train loss:0.6816227186278858\n",
      "train loss:0.8607113540915399\n",
      "train loss:0.9168365365102473\n",
      "train loss:0.8304635366624404\n",
      "train loss:0.876344672018845\n",
      "train loss:0.902668292922704\n",
      "train loss:0.8840257267197726\n",
      "train loss:0.8084353527722042\n",
      "train loss:0.8628288632191832\n",
      "train loss:0.8869704381217326\n",
      "train loss:0.967186245355873\n",
      "train loss:1.0230646745018348\n",
      "train loss:0.8868162659078022\n",
      "train loss:0.9204232057887894\n",
      "train loss:0.8588638736954886\n",
      "train loss:0.9857230147925042\n",
      "train loss:0.9144046226764252\n",
      "train loss:0.76389862754062\n",
      "train loss:0.9424098936153884\n",
      "train loss:0.7401780439057575\n",
      "train loss:0.755134067685637\n",
      "train loss:1.0317382861477196\n",
      "train loss:0.8448235985906772\n",
      "train loss:0.8332421891307145\n",
      "train loss:0.8635493616182385\n",
      "train loss:0.751766991454587\n",
      "train loss:1.163457413594652\n",
      "train loss:0.8126451472241285\n",
      "train loss:0.8246180082009825\n",
      "train loss:0.9146979071484292\n",
      "train loss:0.8892795406088841\n",
      "train loss:0.9427491928601094\n",
      "train loss:0.9707401293203977\n",
      "train loss:0.844942864809825\n",
      "train loss:0.8200475318958257\n",
      "train loss:0.9109286350254118\n",
      "train loss:0.8779764551382356\n",
      "train loss:0.9602599540523741\n",
      "train loss:0.8745409889945007\n",
      "train loss:0.9080650749591144\n",
      "train loss:1.0642537566065553\n",
      "train loss:0.9721165768153243\n",
      "train loss:0.8851307683992361\n",
      "train loss:0.8233809854895949\n",
      "train loss:0.6588780617516499\n",
      "train loss:0.9873709664452539\n",
      "train loss:0.8668546807429257\n",
      "train loss:0.8269190063096294\n",
      "train loss:0.77959985139657\n",
      "train loss:0.692425624303583\n",
      "train loss:0.8734962998847624\n",
      "train loss:0.8587275586285543\n",
      "train loss:0.8966917433066739\n",
      "train loss:0.8555285585400598\n",
      "train loss:0.8147732716244482\n",
      "train loss:0.8575537674692205\n",
      "train loss:0.7317911305825082\n",
      "train loss:0.8209954715591858\n",
      "train loss:0.936919938310079\n",
      "train loss:0.8890003101490072\n",
      "train loss:0.8623271843276225\n",
      "train loss:0.8938545296932945\n",
      "train loss:0.9829928526495911\n",
      "train loss:0.879306214549716\n",
      "train loss:0.908080598647217\n",
      "train loss:0.8653017206788803\n",
      "train loss:1.0194799033022957\n",
      "train loss:0.9060200248623786\n",
      "train loss:1.013669399957706\n",
      "train loss:0.781878444022391\n",
      "train loss:0.9617313427234377\n",
      "train loss:0.9464317940529948\n",
      "train loss:0.9471176985574603\n",
      "train loss:1.0880049471312667\n",
      "train loss:0.9716961595139564\n",
      "train loss:0.9133983515154657\n",
      "train loss:0.8024969933029225\n",
      "train loss:1.066285563799122\n",
      "train loss:0.8947036197989024\n",
      "train loss:0.8495847221400339\n",
      "train loss:0.9683390595155725\n",
      "train loss:1.0322514158196205\n",
      "train loss:1.0432013306270553\n",
      "train loss:0.7652961498375658\n",
      "train loss:0.6832749258028548\n",
      "train loss:0.7748659499376555\n",
      "train loss:0.8317367786470645\n",
      "train loss:0.9276783680600817\n",
      "train loss:0.8612196833771797\n",
      "train loss:0.9032814909340274\n",
      "train loss:1.0913799974889649\n",
      "train loss:0.8204071717743706\n",
      "train loss:0.9589362071210936\n",
      "train loss:0.8600899225153866\n",
      "train loss:0.9123856121926814\n",
      "train loss:0.8980756274696153\n",
      "train loss:0.880054139561623\n",
      "train loss:0.948888219958168\n",
      "train loss:0.774286556393059\n",
      "train loss:0.9626014813798459\n",
      "train loss:0.8221405444985392\n",
      "train loss:0.732149533499122\n",
      "train loss:0.900108515554473\n",
      "train loss:0.7412255670367159\n",
      "train loss:0.8100267298708946\n",
      "train loss:0.936357545430912\n",
      "train loss:0.7934264752350869\n",
      "train loss:0.8775048076101245\n",
      "train loss:0.9661818666045082\n",
      "train loss:0.7505380128584446\n",
      "train loss:0.9250688670552242\n",
      "train loss:0.9424533610374763\n",
      "train loss:0.858006211044187\n",
      "train loss:0.898676357283887\n",
      "train loss:0.7895902300823616\n",
      "train loss:1.0962377191436095\n",
      "train loss:0.8971276880156132\n",
      "train loss:0.7980872586542926\n",
      "train loss:0.9662052063840771\n",
      "train loss:0.6787572680820589\n",
      "train loss:0.8628938832471238\n",
      "train loss:0.9385218527734274\n",
      "train loss:0.9010751875586704\n",
      "train loss:0.9616118116565326\n",
      "train loss:0.7565038528658667\n",
      "train loss:1.0387723799556268\n",
      "train loss:0.9322066248006037\n",
      "train loss:0.8548286182152072\n",
      "train loss:0.907973016863411\n",
      "train loss:1.0495665693229237\n",
      "train loss:0.9128725179199816\n",
      "train loss:0.85839508869783\n",
      "train loss:0.9795507942129342\n",
      "train loss:0.925588860457345\n",
      "train loss:1.0301435248075277\n",
      "train loss:0.9294661369884623\n",
      "train loss:0.8794764699225346\n",
      "train loss:0.7754370203296495\n",
      "train loss:0.7392191114115915\n",
      "train loss:0.7946347831792043\n",
      "train loss:0.8333893072363628\n",
      "train loss:0.7647532629502318\n",
      "train loss:0.9006273835604568\n",
      "train loss:0.8003010852949042\n",
      "train loss:0.7623070520833979\n",
      "train loss:0.9725871507513911\n",
      "train loss:0.9675781239038059\n",
      "train loss:0.9097640949556396\n",
      "train loss:0.8613878723536246\n",
      "train loss:1.1699786168333184\n",
      "train loss:1.0269437630941016\n",
      "train loss:0.9259891180738903\n",
      "train loss:0.7305800766571418\n",
      "train loss:0.9256474703851645\n",
      "train loss:0.8485075559115169\n",
      "train loss:0.7911001857857342\n",
      "train loss:0.915718779719498\n",
      "train loss:1.0027995746910527\n",
      "train loss:0.9189968592945458\n",
      "train loss:0.904691455551752\n",
      "train loss:0.8791706946420079\n",
      "train loss:0.9023541346175021\n",
      "train loss:0.7454479561349356\n",
      "train loss:0.7760327310204798\n",
      "train loss:0.8998094025349568\n",
      "train loss:0.9166954845677744\n",
      "train loss:0.9181712545859249\n",
      "train loss:0.738143734087239\n",
      "train loss:0.8923345895888251\n",
      "train loss:1.0421123540272295\n",
      "train loss:0.9203003998867036\n",
      "train loss:0.9607468598053963\n",
      "train loss:0.7287768433878401\n",
      "train loss:0.828863839524671\n",
      "train loss:0.9172083502386035\n",
      "train loss:0.8143520846290652\n",
      "train loss:0.9161900139848217\n",
      "train loss:0.95974924704486\n",
      "train loss:0.8062454650763538\n",
      "train loss:0.7367920393664399\n",
      "train loss:0.9801266943307918\n",
      "train loss:0.9522177822973329\n",
      "train loss:0.7798340904191907\n",
      "train loss:0.8981321226901834\n",
      "train loss:1.0507186402666855\n",
      "train loss:0.9585062467700491\n",
      "train loss:0.8547523211969232\n",
      "train loss:0.9481603442673319\n",
      "train loss:0.8332221279550851\n",
      "train loss:0.921942087770087\n",
      "train loss:0.9115501636619583\n",
      "train loss:0.6557181897350258\n",
      "train loss:0.9753889475083048\n",
      "train loss:0.9274300023180424\n",
      "train loss:0.883607276687234\n",
      "train loss:0.992352059198033\n",
      "train loss:0.9156660202908691\n",
      "train loss:0.6790087920053502\n",
      "train loss:0.9792479894195513\n",
      "train loss:0.6694057493727515\n",
      "train loss:0.8029503339329558\n",
      "train loss:1.0011730713621676\n",
      "train loss:0.8702434535444104\n",
      "train loss:0.9094734125461332\n",
      "train loss:0.9268442663452374\n",
      "train loss:0.9139669719414608\n",
      "train loss:0.9360934173967236\n",
      "train loss:0.8912500511578239\n",
      "train loss:0.8005461573558849\n",
      "train loss:1.0069429777320318\n",
      "train loss:0.9783550633754702\n",
      "train loss:0.9077821279726527\n",
      "train loss:0.9573781388146491\n",
      "train loss:0.7559011109301527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.9538346350765875\n",
      "train loss:1.0996905999517799\n",
      "train loss:0.7817880486427523\n",
      "train loss:0.7430696735528626\n",
      "train loss:0.8323282071282325\n",
      "train loss:0.8571743460180943\n",
      "train loss:0.984043599610978\n",
      "train loss:0.8272830352562246\n",
      "train loss:0.7650933361681765\n",
      "train loss:0.9491208731339235\n",
      "train loss:1.0902326805709093\n",
      "train loss:0.9243543686500268\n",
      "train loss:0.8750109051722167\n",
      "train loss:0.8704753243381775\n",
      "train loss:0.9117177825496432\n",
      "train loss:0.9557231811811684\n",
      "train loss:0.8548350012962868\n",
      "train loss:0.997267931617015\n",
      "train loss:0.96459300885854\n",
      "train loss:0.7814804152173295\n",
      "train loss:0.9439122491323754\n",
      "train loss:0.8021166410740709\n",
      "train loss:1.0153859509411765\n",
      "train loss:0.8837950699576038\n",
      "train loss:0.6591110676271724\n",
      "train loss:1.0257560926124678\n",
      "train loss:1.0883699597092802\n",
      "train loss:0.9192334266137121\n",
      "train loss:0.7758036743491903\n",
      "train loss:0.8095992679402041\n",
      "train loss:0.6598955365583654\n",
      "train loss:0.948835813721957\n",
      "train loss:0.8053010529932731\n",
      "train loss:0.7911399008501775\n",
      "train loss:0.725628921375974\n",
      "train loss:0.8993633147296187\n",
      "train loss:0.7723105513290619\n",
      "train loss:0.7761573089775555\n",
      "train loss:0.8280831518364551\n",
      "train loss:1.191138717486979\n",
      "train loss:0.9187971371815676\n",
      "train loss:0.8192316324305796\n",
      "train loss:0.7648432446999509\n",
      "train loss:0.7359778172594589\n",
      "train loss:0.9592716810677974\n",
      "train loss:0.815997330279327\n",
      "train loss:1.0221956965731736\n",
      "train loss:0.9836275362713\n",
      "train loss:0.7566379253739419\n",
      "train loss:0.934163125182519\n",
      "train loss:0.870026190005484\n",
      "train loss:0.9563114646457355\n",
      "train loss:0.9215336054631722\n",
      "train loss:0.9175303826913452\n",
      "train loss:0.8836874546691378\n",
      "train loss:0.7310643854234534\n",
      "train loss:0.8942015129444532\n",
      "train loss:0.8265494133912621\n",
      "train loss:0.9660361498729897\n",
      "train loss:0.8761118006817284\n",
      "train loss:0.8025627465565741\n",
      "train loss:0.8489698657425478\n",
      "train loss:1.0526150818690025\n",
      "train loss:0.7134640599804107\n",
      "train loss:0.9087451938382108\n",
      "train loss:0.8062767919234101\n",
      "train loss:0.7939876731060395\n",
      "train loss:0.8192141282497998\n",
      "train loss:0.725243331393594\n",
      "train loss:0.8921197278639285\n",
      "train loss:0.7049835723127457\n",
      "train loss:1.0878232151543898\n",
      "train loss:0.7205944336975024\n",
      "=== epoch:13, train acc:0.999, test acc:0.991 ===\n",
      "train loss:0.9838885943906882\n",
      "train loss:0.914936206968559\n",
      "train loss:0.6275547388269197\n",
      "train loss:0.9181234744465548\n",
      "train loss:0.8150468283478941\n",
      "train loss:0.8105301534034585\n",
      "train loss:0.898919212717484\n",
      "train loss:0.9269538913239995\n",
      "train loss:0.9017306548026639\n",
      "train loss:0.6255726640880649\n",
      "train loss:0.973214021958846\n",
      "train loss:0.7564537868839166\n",
      "train loss:0.8838870413133489\n",
      "train loss:0.8421263807710646\n",
      "train loss:0.6808982769261692\n",
      "train loss:0.8878268646642753\n",
      "train loss:0.8523183381511608\n",
      "train loss:0.8409654851519348\n",
      "train loss:0.8233157240979806\n",
      "train loss:1.0452443427682174\n",
      "train loss:0.8719988498851857\n",
      "train loss:0.8705619958303261\n",
      "train loss:0.9096785381938737\n",
      "train loss:0.9473348197471688\n",
      "train loss:1.040246449780854\n",
      "train loss:0.8725970823156268\n",
      "train loss:0.9020350855296632\n",
      "train loss:0.8050167578498458\n",
      "train loss:0.9799967901639778\n",
      "train loss:1.035563492527747\n",
      "train loss:0.9537839064041358\n",
      "train loss:0.9919601557655146\n",
      "train loss:0.6664646370267797\n",
      "train loss:1.0001965755342221\n",
      "train loss:0.8677210671347518\n",
      "train loss:0.8778225184783561\n",
      "train loss:0.7537965408183444\n",
      "train loss:0.8582879769667211\n",
      "train loss:0.8404463353407198\n",
      "train loss:0.8250436953601418\n",
      "train loss:0.9716869979151552\n",
      "train loss:0.8710541429959118\n",
      "train loss:0.7798352836066006\n",
      "train loss:0.8840465705082795\n",
      "train loss:1.0448807362389325\n",
      "train loss:0.7381096874455936\n",
      "train loss:0.9214449389351365\n",
      "train loss:0.8834048571465886\n",
      "train loss:0.8660533575988993\n",
      "train loss:0.8101100844659446\n",
      "train loss:0.8183316533227931\n",
      "train loss:0.7960933150305602\n",
      "train loss:0.8607532504569917\n",
      "train loss:0.8061516230993352\n",
      "train loss:0.7877923737282617\n",
      "train loss:0.9343620148562998\n",
      "train loss:0.9102799993519192\n",
      "train loss:0.8633814273137377\n",
      "train loss:0.9580723709063957\n",
      "train loss:0.8556098294845484\n",
      "train loss:0.8911507061079389\n",
      "train loss:0.815312988079809\n",
      "train loss:0.788956638524324\n",
      "train loss:0.9463264196573568\n",
      "train loss:0.8175950486589614\n",
      "train loss:0.873366646049642\n",
      "train loss:0.8896722018111083\n",
      "train loss:1.015454283648392\n",
      "train loss:0.8197614585932981\n",
      "train loss:0.9056940164897012\n",
      "train loss:0.9358514835101432\n",
      "train loss:1.068477154069946\n",
      "train loss:0.8851731807238984\n",
      "train loss:0.884435097689239\n",
      "train loss:0.8962707371475375\n",
      "train loss:0.8760196229517767\n",
      "train loss:1.051203784943098\n",
      "train loss:0.9200907321401105\n",
      "train loss:0.6897935018197872\n",
      "train loss:0.7987220484958784\n",
      "train loss:0.9406187929939628\n",
      "train loss:0.8667261471774078\n",
      "train loss:0.9905040204452114\n",
      "train loss:0.9830806913536202\n",
      "train loss:0.9211993972360113\n",
      "train loss:0.8223850017079083\n",
      "train loss:0.9165631158076718\n",
      "train loss:0.9053034834064594\n",
      "train loss:0.8421237517958056\n",
      "train loss:0.9872811333932741\n",
      "train loss:0.8757778532002749\n",
      "train loss:0.8158514658793402\n",
      "train loss:0.8836343427062405\n",
      "train loss:0.8711279279688315\n",
      "train loss:0.698939079850097\n",
      "train loss:0.9188879711577975\n",
      "train loss:0.9182955253205466\n",
      "train loss:0.9117136628139596\n",
      "train loss:0.7632714232812919\n",
      "train loss:0.7837836990645373\n",
      "train loss:0.8912448076253441\n",
      "train loss:0.7873670256120255\n",
      "train loss:0.8106029077674128\n",
      "train loss:0.8176931788573085\n",
      "train loss:0.9344076962559608\n",
      "train loss:0.8592625232177835\n",
      "train loss:0.964629690172279\n",
      "train loss:0.769057866510446\n",
      "train loss:0.8997236059639292\n",
      "train loss:0.9994939055101003\n",
      "train loss:0.9441811637610453\n",
      "train loss:0.8147800351639729\n",
      "train loss:0.8397087135178635\n",
      "train loss:0.8303958451661069\n",
      "train loss:0.8420777839066829\n",
      "train loss:0.8025032680225951\n",
      "train loss:0.8017557545287779\n",
      "train loss:1.0730821876487557\n",
      "train loss:0.7869655012329875\n",
      "train loss:0.8314101187221629\n",
      "train loss:1.0042199983282947\n",
      "train loss:0.8211401213601767\n",
      "train loss:0.8434899680326056\n",
      "train loss:0.7586718881603091\n",
      "train loss:0.7080765912588357\n",
      "train loss:0.8012537912260461\n",
      "train loss:0.7747866359782951\n",
      "train loss:0.7612024220912397\n",
      "train loss:0.9532358818031392\n",
      "train loss:0.7969506853049125\n",
      "train loss:0.8608593446580067\n",
      "train loss:0.8587724461373492\n",
      "train loss:0.7863052205462298\n",
      "train loss:0.7005925063381869\n",
      "train loss:0.909047951652519\n",
      "train loss:0.8760655239562619\n",
      "train loss:0.8366680695093206\n",
      "train loss:0.9458418500434975\n",
      "train loss:0.8165437815218966\n",
      "train loss:1.0253032021957709\n",
      "train loss:0.8522779755931116\n",
      "train loss:0.9563367654722296\n",
      "train loss:0.7548954922830936\n",
      "train loss:0.9324061697385129\n",
      "train loss:0.8906756319169264\n",
      "train loss:0.784808755222163\n",
      "train loss:0.9367174004501229\n",
      "train loss:0.7699196229527291\n",
      "train loss:0.8276711383025472\n",
      "train loss:0.8929841912235122\n",
      "train loss:0.9773755946713909\n",
      "train loss:0.8435099988186434\n",
      "train loss:0.793953569057592\n",
      "train loss:0.7456891501545349\n",
      "train loss:0.7508772766733215\n",
      "train loss:0.9810288261966481\n",
      "train loss:0.9164880738013875\n",
      "train loss:0.9254922889753499\n",
      "train loss:0.8455281110125058\n",
      "train loss:0.9590155729412967\n",
      "train loss:0.8710673352989269\n",
      "train loss:0.8155167817144818\n",
      "train loss:0.7602232615057055\n",
      "train loss:0.9966301136517584\n",
      "train loss:0.8947691093812877\n",
      "train loss:0.9165837210932923\n",
      "train loss:0.8060070638568447\n",
      "train loss:0.8745445849672973\n",
      "train loss:0.6759274962731261\n",
      "train loss:0.7455467267643645\n",
      "train loss:0.8724790122962784\n",
      "train loss:0.9240102935438503\n",
      "train loss:0.7705528527876033\n",
      "train loss:0.8116004140222769\n",
      "train loss:0.8933525924313033\n",
      "train loss:0.7588951955275227\n",
      "train loss:1.0377854651937126\n",
      "train loss:0.7775102489376404\n",
      "train loss:0.9803606994879259\n",
      "train loss:0.9129620596528716\n",
      "train loss:0.8909817571472695\n",
      "train loss:0.947317835512512\n",
      "train loss:0.9467566970502403\n",
      "train loss:1.0572243772912602\n",
      "train loss:0.7537767246092636\n",
      "train loss:0.8131338447634912\n",
      "train loss:0.8452916820066771\n",
      "train loss:0.9669957315106069\n",
      "train loss:1.0267977113155806\n",
      "train loss:0.827790670730289\n",
      "train loss:0.9469872006234944\n",
      "train loss:0.8573244506949103\n",
      "train loss:0.8060575962918577\n",
      "train loss:0.914313137055402\n",
      "train loss:1.0113689153495924\n",
      "train loss:0.9305071657413229\n",
      "train loss:0.9309828258952525\n",
      "train loss:1.0005567352680131\n",
      "train loss:0.741304127171027\n",
      "train loss:0.7592427436245515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.767589586657257\n",
      "train loss:0.8390904605288156\n",
      "train loss:1.010545864515466\n",
      "train loss:0.7825573402692859\n",
      "train loss:1.052339910335682\n",
      "train loss:0.8312589414349877\n",
      "train loss:1.0108402140976829\n",
      "train loss:0.8891184975400331\n",
      "train loss:0.8650399062279555\n",
      "train loss:0.8654933843562662\n",
      "train loss:0.8518978598401539\n",
      "train loss:0.7821102909347175\n",
      "train loss:0.9142543235526174\n",
      "train loss:0.8478333888062682\n",
      "train loss:1.0114998068575403\n",
      "train loss:0.9764884480412218\n",
      "train loss:0.7834229078184343\n",
      "train loss:0.8721392435560356\n",
      "train loss:0.7745664396478296\n",
      "train loss:0.8558647842622452\n",
      "train loss:0.9527249652256877\n",
      "train loss:0.8300495753245325\n",
      "train loss:0.9477291373616459\n",
      "train loss:0.8594748062852327\n",
      "train loss:0.897517666372443\n",
      "train loss:0.826126310452089\n",
      "train loss:0.8416451274446175\n",
      "train loss:0.942525234366054\n",
      "train loss:1.002421908434909\n",
      "train loss:0.885947240605588\n",
      "train loss:0.891865731588012\n",
      "train loss:0.8739078757148873\n",
      "train loss:0.9017496243824696\n",
      "train loss:0.8488339608864088\n",
      "train loss:0.8752899872909655\n",
      "train loss:0.7080855964203812\n",
      "train loss:0.8833379062211929\n",
      "train loss:0.8200846724586205\n",
      "train loss:0.883677624805239\n",
      "train loss:0.9947896172067621\n",
      "train loss:1.1581456714227636\n",
      "train loss:0.9150432837290481\n",
      "train loss:0.8716566733074447\n",
      "train loss:0.9796301109328526\n",
      "train loss:0.9334710013975895\n",
      "train loss:0.9786950008398798\n",
      "train loss:0.6481285735226504\n",
      "train loss:0.85978008329531\n",
      "train loss:0.8800918804935121\n",
      "train loss:1.034080909398131\n",
      "train loss:0.9185800249371292\n",
      "train loss:0.8162720255612967\n",
      "train loss:0.7493892560719003\n",
      "train loss:1.036212759550344\n",
      "train loss:0.7836868328953034\n",
      "train loss:0.7385670050816815\n",
      "train loss:0.7562024833107448\n",
      "train loss:0.916712107236759\n",
      "train loss:1.0238516747843407\n",
      "train loss:0.8310732015970339\n",
      "train loss:0.8072176370890921\n",
      "train loss:0.9284721176798473\n",
      "train loss:0.8106247645960083\n",
      "train loss:0.9103905026884136\n",
      "train loss:0.8767035243652979\n",
      "train loss:0.8533186551480145\n",
      "train loss:0.6546171169659456\n",
      "train loss:0.7874617100210033\n",
      "train loss:0.9017385373648078\n",
      "train loss:1.1056092650255902\n",
      "train loss:0.7805303691465402\n",
      "train loss:0.8881454884870769\n",
      "train loss:0.8115198674768578\n",
      "train loss:0.8617197280224922\n",
      "train loss:0.8505531750240992\n",
      "train loss:0.944231894504607\n",
      "train loss:0.9883257524627482\n",
      "train loss:0.8782615260725838\n",
      "train loss:0.7077305159972046\n",
      "train loss:0.9176331501097382\n",
      "train loss:0.6675819933270237\n",
      "train loss:0.9186474492990694\n",
      "train loss:0.768051135676537\n",
      "train loss:0.9375952656871108\n",
      "train loss:0.8081110034618888\n",
      "train loss:0.9896400633367913\n",
      "train loss:0.8190518675742713\n",
      "train loss:0.8101199639185223\n",
      "train loss:1.0115619532828937\n",
      "train loss:0.9519578560706928\n",
      "train loss:0.837614316566538\n",
      "train loss:0.8648867333850981\n",
      "train loss:0.995794645013677\n",
      "train loss:1.07420829619898\n",
      "train loss:1.0391461068210428\n",
      "train loss:0.9634981728432429\n",
      "train loss:0.8538917730168483\n",
      "train loss:0.9177791943442777\n",
      "train loss:0.9615949155094806\n",
      "train loss:0.7659356289823607\n",
      "train loss:0.9274160359783598\n",
      "train loss:1.008269851605163\n",
      "train loss:0.8652229144281943\n",
      "train loss:0.7153892630422165\n",
      "train loss:0.918342823513585\n",
      "train loss:0.8682763069298706\n",
      "train loss:0.8731900317288539\n",
      "train loss:0.7291338466969577\n",
      "train loss:1.0021985020012338\n",
      "train loss:0.8527403318267064\n",
      "train loss:1.009585066114027\n",
      "train loss:0.9020482361821509\n",
      "train loss:0.9562620629114723\n",
      "train loss:0.8043720955457901\n",
      "train loss:0.8541803541736732\n",
      "train loss:0.8575445697608515\n",
      "train loss:0.914192633604821\n",
      "train loss:0.7451432710900385\n",
      "train loss:0.9679194866440892\n",
      "train loss:0.7875989618522081\n",
      "train loss:0.9020507917630699\n",
      "train loss:0.6706951740654615\n",
      "train loss:0.7332418550764036\n",
      "train loss:1.0271479545095155\n",
      "train loss:0.9006116190615334\n",
      "train loss:0.7329155718679887\n",
      "train loss:1.139796900621548\n",
      "train loss:0.7884117823428451\n",
      "train loss:0.8738085491064005\n",
      "train loss:0.9185645663529614\n",
      "train loss:0.8243992417936756\n",
      "train loss:0.7495744471475362\n",
      "train loss:0.8960416123974886\n",
      "train loss:0.6458175524207702\n",
      "train loss:0.8802082357019653\n",
      "train loss:0.7017826286435143\n",
      "train loss:0.9358286839165316\n",
      "train loss:0.7547143042821556\n",
      "train loss:0.898096100999507\n",
      "train loss:0.9302703829365545\n",
      "train loss:0.7526631650407252\n",
      "train loss:0.8952091031565468\n",
      "train loss:0.9379354498147144\n",
      "train loss:0.8747770766003963\n",
      "train loss:0.9482774314254523\n",
      "train loss:0.8072250363622486\n",
      "train loss:1.0144859045890942\n",
      "train loss:0.7807936529729315\n",
      "train loss:0.8574779541234586\n",
      "train loss:1.0000824049465356\n",
      "train loss:0.8783680437789936\n",
      "train loss:0.8743266297096975\n",
      "train loss:0.9765323503553259\n",
      "train loss:0.7781649588310654\n",
      "train loss:0.8487842412972546\n",
      "train loss:0.8991459870868768\n",
      "train loss:0.8979559682002205\n",
      "train loss:0.8645866449507197\n",
      "train loss:0.8376937171851497\n",
      "train loss:0.74220017744418\n",
      "train loss:0.7900887144108162\n",
      "train loss:0.898676061540797\n",
      "train loss:0.7597152256598156\n",
      "train loss:1.0239818003924344\n",
      "train loss:0.8326988561061206\n",
      "train loss:0.9321651193683662\n",
      "train loss:0.955224821875145\n",
      "train loss:0.8981833347620416\n",
      "train loss:0.8717342477098365\n",
      "train loss:0.9682157487478488\n",
      "train loss:0.9114516331749297\n",
      "train loss:0.9619213077332642\n",
      "train loss:0.926220424898889\n",
      "train loss:0.9971620800414085\n",
      "train loss:0.8979524039250749\n",
      "train loss:0.8065441189317457\n",
      "train loss:0.8333469589062454\n",
      "train loss:0.9700833418314696\n",
      "train loss:0.5830616860604995\n",
      "train loss:0.8367420206509633\n",
      "train loss:0.7702390664171405\n",
      "train loss:0.8052796551298713\n",
      "train loss:0.8733078032632099\n",
      "train loss:0.9172314984189895\n",
      "train loss:0.746630681971043\n",
      "train loss:0.9076451799515844\n",
      "train loss:0.7925002397583593\n",
      "train loss:0.9065535211138085\n",
      "train loss:0.9217501615190173\n",
      "train loss:0.6499974077134155\n",
      "train loss:0.9576055012049253\n",
      "train loss:0.7950908569789652\n",
      "train loss:0.9545032613987721\n",
      "train loss:0.8521375302691774\n",
      "train loss:0.8977485677213689\n",
      "train loss:0.9393292820906883\n",
      "train loss:0.8566483521646268\n",
      "train loss:0.787513962529777\n",
      "train loss:1.0495793525742265\n",
      "train loss:1.0995173806794067\n",
      "train loss:0.9026430062333435\n",
      "train loss:0.9004813252701043\n",
      "train loss:0.8582109537798869\n",
      "train loss:0.8214681200418167\n",
      "train loss:0.9369166095028827\n",
      "train loss:0.8569545908289549\n",
      "train loss:0.9518664941679905\n",
      "train loss:0.7538533019567062\n",
      "train loss:0.8034947984719146\n",
      "train loss:0.8660137943071724\n",
      "train loss:0.88626933449304\n",
      "train loss:0.9493964475274947\n",
      "train loss:1.0661446517345445\n",
      "train loss:0.8877141591232782\n",
      "train loss:0.9157950668087556\n",
      "train loss:0.9731167704667919\n",
      "train loss:0.7858015037435293\n",
      "train loss:0.7813031051971958\n",
      "train loss:0.894929708795442\n",
      "train loss:0.743869795245001\n",
      "train loss:0.875099355741296\n",
      "train loss:0.8675385706730063\n",
      "train loss:1.0536214943575013\n",
      "train loss:0.8721522987475636\n",
      "train loss:0.9652913308365244\n",
      "train loss:0.8419946212503655\n",
      "train loss:0.9855045322431946\n",
      "train loss:0.8492631523530008\n",
      "train loss:0.8265174510168453\n",
      "train loss:0.9002118916410395\n",
      "train loss:0.9968152815610946\n",
      "train loss:0.6670551652225906\n",
      "train loss:0.9199050734265881\n",
      "train loss:0.9448638007355811\n",
      "train loss:0.9213109298489323\n",
      "train loss:0.9491594996707075\n",
      "train loss:0.8709785820668917\n",
      "train loss:0.8727082698436\n",
      "train loss:0.9233493105904462\n",
      "train loss:0.7257362815066125\n",
      "train loss:0.893281088239927\n",
      "train loss:0.8184646905299171\n",
      "train loss:0.841714632759654\n",
      "train loss:0.9239336674298961\n",
      "train loss:0.8649421550524024\n",
      "train loss:0.8880904982287958\n",
      "train loss:0.72044152188221\n",
      "train loss:1.0810147310584959\n",
      "train loss:0.8277605140236282\n",
      "train loss:0.8576060819365408\n",
      "train loss:0.7946332186398781\n",
      "train loss:0.7839648549135104\n",
      "train loss:0.9552080231722186\n",
      "train loss:0.7794744573062334\n",
      "train loss:1.0196937087445248\n",
      "train loss:0.7739637280606888\n",
      "train loss:0.844473725731262\n",
      "train loss:0.8510277824423749\n",
      "train loss:1.0816429665485328\n",
      "train loss:0.8792766784079058\n",
      "train loss:0.8301284398415898\n",
      "train loss:0.794096710745232\n",
      "train loss:0.8374213341249012\n",
      "train loss:0.8135139202780265\n",
      "train loss:0.922888661017906\n",
      "train loss:0.7407177769637575\n",
      "train loss:0.9188904051781068\n",
      "train loss:0.8054413975072339\n",
      "train loss:0.9000537197548641\n",
      "train loss:0.9741711434131303\n",
      "train loss:0.9839719797558879\n",
      "train loss:0.6842014194836952\n",
      "train loss:0.9852621618263467\n",
      "train loss:0.8705746316748687\n",
      "train loss:0.8260131220205743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.88487371098155\n",
      "train loss:0.7793746221502965\n",
      "train loss:1.0006287321611609\n",
      "train loss:0.87840589834805\n",
      "train loss:0.9479684159186198\n",
      "train loss:0.7526754495621865\n",
      "train loss:0.965840837846173\n",
      "train loss:0.9694713450121338\n",
      "train loss:0.9575532257766998\n",
      "train loss:0.7708767912378804\n",
      "train loss:0.9348541334170715\n",
      "train loss:0.9203865830258671\n",
      "train loss:0.7871085457441858\n",
      "train loss:0.9205170063384789\n",
      "train loss:0.8235870220802721\n",
      "train loss:0.7299909522917828\n",
      "train loss:0.7280990064922426\n",
      "train loss:0.742451903389438\n",
      "train loss:0.9104290494394336\n",
      "train loss:0.7978861668621736\n",
      "train loss:0.9343731405532238\n",
      "train loss:0.8758821394021749\n",
      "train loss:1.0063042962347388\n",
      "train loss:0.7296571498448177\n",
      "train loss:0.9438578318686947\n",
      "train loss:0.8872224372558007\n",
      "train loss:0.7324908420694304\n",
      "train loss:0.8761797727161407\n",
      "train loss:0.8653990688088617\n",
      "train loss:0.9770774310391621\n",
      "train loss:1.0097426249268695\n",
      "train loss:0.7827460041275351\n",
      "train loss:0.8673915545705414\n",
      "train loss:0.820680384301612\n",
      "train loss:0.7965199469851655\n",
      "train loss:0.657387716649287\n",
      "train loss:1.021524821616788\n",
      "train loss:0.8228579887307264\n",
      "train loss:0.8628761478021921\n",
      "train loss:0.9105624991541859\n",
      "train loss:0.9010648238847624\n",
      "train loss:0.7635691011290087\n",
      "train loss:0.9573253123717165\n",
      "train loss:0.8166144477445165\n",
      "train loss:0.9331164482305526\n",
      "train loss:0.7882928835786064\n",
      "train loss:0.7685223365529242\n",
      "train loss:0.6677965236499661\n",
      "train loss:0.8251393230691232\n",
      "train loss:0.7269943219071159\n",
      "train loss:0.7143931619709991\n",
      "train loss:0.8319053238850668\n",
      "train loss:1.029047299775462\n",
      "train loss:1.0560976750667563\n",
      "train loss:0.8590061401747272\n",
      "train loss:0.8412678341593127\n",
      "train loss:0.9725594964646953\n",
      "train loss:0.8929078244383316\n",
      "train loss:0.7220112164278693\n",
      "train loss:0.9228618530204357\n",
      "train loss:0.880350594245948\n",
      "train loss:0.9437367493963399\n",
      "train loss:0.8783039523385999\n",
      "train loss:0.7825964261585087\n",
      "train loss:0.8211831309368972\n",
      "train loss:0.7344048745266634\n",
      "train loss:0.8142186228829975\n",
      "train loss:0.8621725338387547\n",
      "train loss:0.8228430661488778\n",
      "train loss:0.8632328259925458\n",
      "train loss:0.9305767126429845\n",
      "train loss:0.7799384366545619\n",
      "train loss:0.9760286925537268\n",
      "train loss:0.9287309954724499\n",
      "train loss:0.7996960898120264\n",
      "train loss:0.7374266508247974\n",
      "train loss:0.8823326046930834\n",
      "train loss:0.8724648187120786\n",
      "train loss:0.9666463636902517\n",
      "train loss:0.784820498997039\n",
      "train loss:1.0281308885985057\n",
      "train loss:0.9139699798714764\n",
      "train loss:0.9576690189040197\n",
      "train loss:0.9211541207967443\n",
      "train loss:1.1260515045010235\n",
      "train loss:0.7840907180098791\n",
      "train loss:0.7303485476004192\n",
      "train loss:0.8808005709537136\n",
      "train loss:0.7379128730894281\n",
      "train loss:0.8590852777248013\n",
      "train loss:0.8323289659742832\n",
      "train loss:0.837545484524332\n",
      "train loss:0.8076609550266102\n",
      "train loss:0.8931797824367484\n",
      "train loss:0.9724768290223182\n",
      "train loss:0.935268777530859\n",
      "train loss:0.8389223394977353\n",
      "train loss:0.7400109155337584\n",
      "train loss:0.8449839766678262\n",
      "train loss:0.9011347575636077\n",
      "train loss:0.6624069715506969\n",
      "train loss:0.9405740457098762\n",
      "train loss:1.005188871482472\n",
      "train loss:0.8224042150934554\n",
      "train loss:0.8241824399704372\n",
      "train loss:0.85635019090723\n",
      "train loss:0.9314521726644087\n",
      "train loss:0.8765237494608428\n",
      "train loss:0.8596476740753283\n",
      "train loss:0.8601041083035611\n",
      "train loss:0.9330025992614039\n",
      "train loss:0.8609316601395324\n",
      "train loss:0.8159731441616068\n",
      "train loss:0.818117899963694\n",
      "train loss:0.8191150556990076\n",
      "train loss:0.8996035180881833\n",
      "train loss:0.6102704923673353\n",
      "train loss:0.937053460744037\n",
      "train loss:0.8196010027560015\n",
      "train loss:0.8888909407653253\n",
      "train loss:0.9101592459149697\n",
      "train loss:0.8527688978817345\n",
      "train loss:0.8184667875287654\n",
      "train loss:0.6682770261903253\n",
      "train loss:0.8195104053418117\n",
      "=== epoch:14, train acc:0.994, test acc:0.99 ===\n",
      "train loss:0.8801274255028209\n",
      "train loss:0.9666223056646434\n",
      "train loss:0.9085068085068337\n",
      "train loss:0.965104634726943\n",
      "train loss:0.9177419165679133\n",
      "train loss:0.9555721279929942\n",
      "train loss:0.8176489453333745\n",
      "train loss:0.8168146152704948\n",
      "train loss:0.8530595992734439\n",
      "train loss:0.9173874648459289\n",
      "train loss:0.9497797959850156\n",
      "train loss:0.8728082905498661\n",
      "train loss:0.8498947477376129\n",
      "train loss:0.8425207139747894\n",
      "train loss:0.9373067654768442\n",
      "train loss:0.9705176479666849\n",
      "train loss:0.8652677961862179\n",
      "train loss:0.9685741771786841\n",
      "train loss:0.8712726173234223\n",
      "train loss:0.8371606516258852\n",
      "train loss:0.9819906857770837\n",
      "train loss:0.7028518947521653\n",
      "train loss:0.8993971781379231\n",
      "train loss:0.7947211997408344\n",
      "train loss:0.8699692702030752\n",
      "train loss:0.8825172327498908\n",
      "train loss:0.7516732054228925\n",
      "train loss:0.9358670970687311\n",
      "train loss:0.8005443781301177\n",
      "train loss:0.9511810727219436\n",
      "train loss:0.8338327530567906\n",
      "train loss:0.7307050693731313\n",
      "train loss:0.8522502400434281\n",
      "train loss:0.9110528746301023\n",
      "train loss:0.885432848269363\n",
      "train loss:0.8190013777102876\n",
      "train loss:0.9604582685780194\n",
      "train loss:0.8833903084084909\n",
      "train loss:0.9054210663424123\n",
      "train loss:0.7932483412517837\n",
      "train loss:0.8068687184053427\n",
      "train loss:0.8761718867060506\n",
      "train loss:0.7268174707678078\n",
      "train loss:0.8352681469594618\n",
      "train loss:0.8930488294762137\n",
      "train loss:0.8718125163241851\n",
      "train loss:0.9139323326391507\n",
      "train loss:0.9285397954670345\n",
      "train loss:0.746189294797271\n",
      "train loss:0.9043057695522344\n",
      "train loss:0.9471380069971719\n",
      "train loss:0.8520477538559719\n",
      "train loss:0.9777099209414559\n",
      "train loss:0.851354855809917\n",
      "train loss:0.9477715936028274\n",
      "train loss:0.8455969265118906\n",
      "train loss:0.8615152105144597\n",
      "train loss:0.8843825784891349\n",
      "train loss:0.8549868375755739\n",
      "train loss:0.8976656572912448\n",
      "train loss:0.8584351982679466\n",
      "train loss:0.8403970180483518\n",
      "train loss:0.8439978584578794\n",
      "train loss:0.8586043725994311\n",
      "train loss:0.9672774671970888\n",
      "train loss:0.9837705555176355\n",
      "train loss:0.812939749381161\n",
      "train loss:0.7724469763037989\n",
      "train loss:0.9204324848454943\n",
      "train loss:1.0028748790388498\n",
      "train loss:0.8747696495222814\n",
      "train loss:0.8240934290687023\n",
      "train loss:0.8076228135933882\n",
      "train loss:0.8458050956919948\n",
      "train loss:0.8639247176500683\n",
      "train loss:0.8252737246754437\n",
      "train loss:0.9157045916844906\n",
      "train loss:0.7077311240493401\n",
      "train loss:0.8345566594823032\n",
      "train loss:0.9158340381044328\n",
      "train loss:0.8921556153684272\n",
      "train loss:0.8791264449533857\n",
      "train loss:0.8409829523268522\n",
      "train loss:0.7748799387343136\n",
      "train loss:0.7257897713306789\n",
      "train loss:0.8279409067060486\n",
      "train loss:0.8576104996575215\n",
      "train loss:0.9068267524504101\n",
      "train loss:1.024941440810116\n",
      "train loss:0.7507991811795697\n",
      "train loss:0.9101672109831145\n",
      "train loss:0.91815970517493\n",
      "train loss:0.8733761541402025\n",
      "train loss:0.9723821473566682\n",
      "train loss:0.8611688386082976\n",
      "train loss:0.7697551420527629\n",
      "train loss:0.9389103763126954\n",
      "train loss:0.7848485921653697\n",
      "train loss:0.8932959165279637\n",
      "train loss:0.9082990275895861\n",
      "train loss:0.8313982638554951\n",
      "train loss:1.0151262604541333\n",
      "train loss:0.8953164623083727\n",
      "train loss:0.9647536941917574\n",
      "train loss:0.8258915602597748\n",
      "train loss:0.7482707801706345\n",
      "train loss:1.0181999310395848\n",
      "train loss:0.8447886300020383\n",
      "train loss:1.0237664420476635\n",
      "train loss:0.9280760977853622\n",
      "train loss:0.7989359281407813\n",
      "train loss:0.8536253791109969\n",
      "train loss:0.7579067865009974\n",
      "train loss:0.8578739896005149\n",
      "train loss:0.9219537122730318\n",
      "train loss:0.885198423421035\n",
      "train loss:0.748454578400493\n",
      "train loss:1.0009566335534874\n",
      "train loss:0.9597947289283781\n",
      "train loss:0.7100405939464239\n",
      "train loss:0.8689617157265451\n",
      "train loss:0.7677375520175052\n",
      "train loss:0.9488614947227567\n",
      "train loss:0.8779333737419321\n",
      "train loss:0.7753722521110668\n",
      "train loss:0.8265278045938176\n",
      "train loss:0.7849109056864934\n",
      "train loss:0.7829249169543167\n",
      "train loss:0.9195509698874146\n",
      "train loss:0.7412163134553288\n",
      "train loss:0.7625089236337143\n",
      "train loss:0.770185254280194\n",
      "train loss:0.8481368754201491\n",
      "train loss:0.8232510969077641\n",
      "train loss:0.8276426921746928\n",
      "train loss:0.8782427901988435\n",
      "train loss:0.9790048997695826\n",
      "train loss:0.7831796764000566\n",
      "train loss:0.8355423631414253\n",
      "train loss:0.895242314517603\n",
      "train loss:0.9853521483680655\n",
      "train loss:0.9480460573897982\n",
      "train loss:0.8483531894176082\n",
      "train loss:0.836897964863449\n",
      "train loss:1.0132774741118566\n",
      "train loss:0.7169464625452048\n",
      "train loss:0.8726009081675492\n",
      "train loss:0.7790861170405411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.6557349716070462\n",
      "train loss:0.9276450691256062\n",
      "train loss:0.9688708984658245\n",
      "train loss:0.9177005483430656\n",
      "train loss:0.8282231347226019\n",
      "train loss:0.8697193394015582\n",
      "train loss:0.7576152543596614\n",
      "train loss:0.8576875994457726\n",
      "train loss:0.8711536203216483\n",
      "train loss:0.7971017898428131\n",
      "train loss:1.0081794724479791\n",
      "train loss:0.8956197210365893\n",
      "train loss:0.7843922066438043\n",
      "train loss:0.8012038731523622\n",
      "train loss:0.7791245701873266\n",
      "train loss:0.9048419996220487\n",
      "train loss:0.8370058707561663\n",
      "train loss:0.8128440352850725\n",
      "train loss:0.8425986002829507\n",
      "train loss:0.9297197632341205\n",
      "train loss:0.8325966041710567\n",
      "train loss:0.8852497502080916\n",
      "train loss:0.8385029758997562\n",
      "train loss:0.8735203464818898\n",
      "train loss:0.9206478506546766\n",
      "train loss:0.8358709590631013\n",
      "train loss:0.8715461760237311\n",
      "train loss:0.9695298151871623\n",
      "train loss:0.9413581142748672\n",
      "train loss:1.0309321040594792\n",
      "train loss:0.7519020543191786\n",
      "train loss:0.841442744657126\n",
      "train loss:0.9302071492990275\n",
      "train loss:0.9173514707402741\n",
      "train loss:0.9679959361161439\n",
      "train loss:0.8839388874497897\n",
      "train loss:0.7659408259497655\n",
      "train loss:0.8087093684694849\n",
      "train loss:0.9951560566112212\n",
      "train loss:0.8500741723504452\n",
      "train loss:0.8161364707307315\n",
      "train loss:0.952022764094261\n",
      "train loss:0.9013687165206217\n",
      "train loss:0.8928700536738932\n",
      "train loss:0.7199125110756402\n",
      "train loss:1.0110819447156454\n",
      "train loss:0.8407904763101787\n",
      "train loss:1.0750431437218078\n",
      "train loss:0.910518122022436\n",
      "train loss:0.9660832911901539\n",
      "train loss:0.9214841087105495\n",
      "train loss:0.9316539132413445\n",
      "train loss:0.8550492790734058\n",
      "train loss:0.7915161751886743\n",
      "train loss:0.8891440033369613\n",
      "train loss:0.7991554875977951\n",
      "train loss:0.878616221974362\n",
      "train loss:0.8739774948879581\n",
      "train loss:0.8001503040122613\n",
      "train loss:0.8654624822244831\n",
      "train loss:0.8083496623941032\n",
      "train loss:0.8520278601997986\n",
      "train loss:0.8523010620882616\n",
      "train loss:0.8577036848036572\n",
      "train loss:1.0615064847606392\n",
      "train loss:0.8993568081188357\n",
      "train loss:0.7432539256101076\n",
      "train loss:0.8003291594921624\n",
      "train loss:0.8885667507693387\n",
      "train loss:0.846924886813563\n",
      "train loss:0.8797334683943498\n",
      "train loss:0.890775131564377\n",
      "train loss:0.7839023867888179\n",
      "train loss:0.8268541828249143\n",
      "train loss:0.8330223419120989\n",
      "train loss:0.9759621153424181\n",
      "train loss:0.8529855464516355\n",
      "train loss:0.9722498598521209\n",
      "train loss:1.0271032946419572\n",
      "train loss:0.8976029677523373\n",
      "train loss:0.9870204643793759\n",
      "train loss:1.0709095099853547\n",
      "train loss:0.7181397418927018\n",
      "train loss:0.721221847258819\n",
      "train loss:1.1443062758249833\n",
      "train loss:0.9857941492006553\n",
      "train loss:0.739974259953263\n",
      "train loss:0.7886372799649723\n",
      "train loss:0.8526117418467031\n",
      "train loss:0.9503618498612778\n",
      "train loss:0.903539285743056\n",
      "train loss:0.8359938643029052\n",
      "train loss:0.8815935444410825\n",
      "train loss:0.8783229803616628\n",
      "train loss:0.8295484856864416\n",
      "train loss:1.0232099600144609\n",
      "train loss:0.8534677811453849\n",
      "train loss:0.9460312378178984\n",
      "train loss:0.8856068616377528\n",
      "train loss:0.8877206817010458\n",
      "train loss:0.8385195621732494\n",
      "train loss:0.8080167588356953\n",
      "train loss:0.8587343478405614\n",
      "train loss:0.841307689363562\n",
      "train loss:0.9379611780676587\n",
      "train loss:0.8623642345313381\n",
      "train loss:0.6980911611052819\n",
      "train loss:0.6615322667342256\n",
      "train loss:0.971999750536882\n",
      "train loss:0.9345065671486613\n",
      "train loss:0.7088918155984026\n",
      "train loss:0.8770377309673141\n",
      "train loss:0.8877237215159749\n",
      "train loss:0.9043300412477923\n",
      "train loss:0.9296991755086701\n",
      "train loss:0.9853718643796375\n",
      "train loss:1.0428471978673624\n",
      "train loss:0.9378620176742268\n",
      "train loss:0.8649050749036498\n",
      "train loss:0.7857848464303459\n",
      "train loss:0.9526442726156962\n",
      "train loss:0.8954769744253387\n",
      "train loss:0.81898942305796\n",
      "train loss:0.8763732251059453\n",
      "train loss:0.9311934806899614\n",
      "train loss:0.7169159208486734\n",
      "train loss:0.7631081003140653\n",
      "train loss:0.975925728068473\n",
      "train loss:0.9982210099039586\n",
      "train loss:0.8416963128768313\n",
      "train loss:0.8114306672598639\n",
      "train loss:0.9049989198746425\n",
      "train loss:0.8775343353890467\n",
      "train loss:0.8901731813362467\n",
      "train loss:1.0464937050475975\n",
      "train loss:0.7667111068432543\n",
      "train loss:0.7427198507912185\n",
      "train loss:0.8234544834145499\n",
      "train loss:0.8152319180181616\n",
      "train loss:0.843211068907709\n",
      "train loss:0.8473254512602431\n",
      "train loss:0.8571898813893142\n",
      "train loss:0.9951019196350773\n",
      "train loss:0.9619198988597385\n",
      "train loss:0.892670768163696\n",
      "train loss:0.8125111365174433\n",
      "train loss:0.9602675880343549\n",
      "train loss:1.0124940844337535\n",
      "train loss:0.7862348615768062\n",
      "train loss:0.8695655022547694\n",
      "train loss:0.8522612452529833\n",
      "train loss:0.9802028228744262\n",
      "train loss:0.7711790103818167\n",
      "train loss:1.044660957044684\n",
      "train loss:0.7842934731294575\n",
      "train loss:0.8795887696588478\n",
      "train loss:0.8080565358571187\n",
      "train loss:0.8826909483061535\n",
      "train loss:1.0169241563039797\n",
      "train loss:0.7590029699738237\n",
      "train loss:0.7345290907601576\n",
      "train loss:0.8985518574947151\n",
      "train loss:0.9244460100548529\n",
      "train loss:0.8282932020671367\n",
      "train loss:0.8298272549571322\n",
      "train loss:0.8292542541553947\n",
      "train loss:0.8739576980125845\n",
      "train loss:0.9467551855253586\n",
      "train loss:0.7739081951513398\n",
      "train loss:0.8720513349941994\n",
      "train loss:0.8431318529715156\n",
      "train loss:0.9353227130789833\n",
      "train loss:0.7537849929165484\n",
      "train loss:0.9465195003291007\n",
      "train loss:0.8305979860308673\n",
      "train loss:0.9497371492428401\n",
      "train loss:1.0620976100951824\n",
      "train loss:1.0209750429163433\n",
      "train loss:0.8884538897989591\n",
      "train loss:0.8680922734287642\n",
      "train loss:1.0064915491948525\n",
      "train loss:0.9032915761832634\n",
      "train loss:0.8432963983549269\n",
      "train loss:0.8124928162145977\n",
      "train loss:0.9087162538916105\n",
      "train loss:0.9994382595672896\n",
      "train loss:0.9511411376785808\n",
      "train loss:0.7753648506799903\n",
      "train loss:0.7830067130528521\n",
      "train loss:0.8996827074114054\n",
      "train loss:0.7315107584895888\n",
      "train loss:1.0253336908044908\n",
      "train loss:0.867219788753634\n",
      "train loss:0.9747719829482037\n",
      "train loss:0.8105574980369255\n",
      "train loss:0.8873514328635477\n",
      "train loss:0.9747795906685179\n",
      "train loss:0.8838257474963014\n",
      "train loss:0.8846680321252017\n",
      "train loss:0.9653178289491069\n",
      "train loss:0.8910736224694437\n",
      "train loss:0.897125167811842\n",
      "train loss:0.7250137263836827\n",
      "train loss:0.8477070527562386\n",
      "train loss:0.9686840090126172\n",
      "train loss:0.8928137917510793\n",
      "train loss:0.8054733464930683\n",
      "train loss:0.9488081766851613\n",
      "train loss:1.012278112443081\n",
      "train loss:1.043341395425547\n",
      "train loss:0.8664765340645317\n",
      "train loss:0.9246376356727256\n",
      "train loss:0.8940465015595597\n",
      "train loss:0.8668557525750855\n",
      "train loss:0.8152204434523561\n",
      "train loss:0.8507944304851233\n",
      "train loss:0.8818325528431464\n",
      "train loss:0.9065854362805775\n",
      "train loss:0.9614267697199004\n",
      "train loss:0.749814101535586\n",
      "train loss:0.7955643563456454\n",
      "train loss:0.8787266430665859\n",
      "train loss:0.8383528304155429\n",
      "train loss:1.055437688778102\n",
      "train loss:1.002616355492029\n",
      "train loss:0.967063850280039\n",
      "train loss:0.9495676692507408\n",
      "train loss:0.7093694677315745\n",
      "train loss:0.8694015638908438\n",
      "train loss:1.1153243885426922\n",
      "train loss:0.8580027785939648\n",
      "train loss:0.8074841060523771\n",
      "train loss:0.9516075236506946\n",
      "train loss:0.8965679592113688\n",
      "train loss:1.0777095611586176\n",
      "train loss:0.8924739815639089\n",
      "train loss:0.8732011122251425\n",
      "train loss:0.9276013045164249\n",
      "train loss:0.8340682672495758\n",
      "train loss:0.8515604037138124\n",
      "train loss:0.8804032675302398\n",
      "train loss:0.6894151208517886\n",
      "train loss:0.8099755715579365\n",
      "train loss:0.9557350129155967\n",
      "train loss:0.8745239533023523\n",
      "train loss:1.0535655910015584\n",
      "train loss:0.8360209552768285\n",
      "train loss:0.8940407171321029\n",
      "train loss:0.8658756416344136\n",
      "train loss:0.9423931136997926\n",
      "train loss:0.9294175720867264\n",
      "train loss:0.7472849629581185\n",
      "train loss:0.96633598196285\n",
      "train loss:1.043870542203197\n",
      "train loss:0.9311407350982656\n",
      "train loss:0.9740869320863246\n",
      "train loss:0.9528239321107325\n",
      "train loss:0.9301275833048553\n",
      "train loss:0.7904898362814117\n",
      "train loss:0.8912338624931258\n",
      "train loss:0.7038349304604695\n",
      "train loss:0.9539228028336038\n",
      "train loss:0.9311271579457697\n",
      "train loss:0.7915502863343761\n",
      "train loss:0.7943097472024564\n",
      "train loss:0.9088119157340009\n",
      "train loss:1.005921367597871\n",
      "train loss:0.6722002782008272\n",
      "train loss:0.8721232223472903\n",
      "train loss:0.914976379829461\n",
      "train loss:1.0059582713175113\n",
      "train loss:1.0148341460214747\n",
      "train loss:0.9408575718507844\n",
      "train loss:0.9806214734284492\n",
      "train loss:0.8176447445661682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.7737277313244687\n",
      "train loss:0.707690813572318\n",
      "train loss:1.0462996494594101\n",
      "train loss:1.0101474079330974\n",
      "train loss:0.8163672935963595\n",
      "train loss:0.9057140290585418\n",
      "train loss:0.9008264320044321\n",
      "train loss:0.9272770304995704\n",
      "train loss:0.7491491690716843\n",
      "train loss:0.8846291663060599\n",
      "train loss:0.8256256997900728\n",
      "train loss:0.8863790017576583\n",
      "train loss:0.7998046461720202\n",
      "train loss:1.0234758744778862\n",
      "train loss:0.8701272866569435\n",
      "train loss:0.7806486306466734\n",
      "train loss:0.7899879361920799\n",
      "train loss:0.9854129267610847\n",
      "train loss:0.8671551446692208\n",
      "train loss:0.9099049382360195\n",
      "train loss:0.7697830012763248\n",
      "train loss:0.8229850088955779\n",
      "train loss:0.9231889702269298\n",
      "train loss:0.8642028829098483\n",
      "train loss:1.0013338319183809\n",
      "train loss:1.0215123780215905\n",
      "train loss:0.8311122447973355\n",
      "train loss:0.7649820492145486\n",
      "train loss:0.8992490070606014\n",
      "train loss:0.7627025278379607\n",
      "train loss:0.8590368016773889\n",
      "train loss:0.9380208249915285\n",
      "train loss:0.9071270151491652\n",
      "train loss:0.8155933845169249\n",
      "train loss:0.88585775593489\n",
      "train loss:0.7479148129229621\n",
      "train loss:0.7255755449086422\n",
      "train loss:0.7839229416251681\n",
      "train loss:1.0197563275647206\n",
      "train loss:0.8633603683759766\n",
      "train loss:0.9431795702033843\n",
      "train loss:0.9338912265617278\n",
      "train loss:0.8677745668031207\n",
      "train loss:0.8042346408013388\n",
      "train loss:0.7086233834718486\n",
      "train loss:0.861606164833869\n",
      "train loss:0.9275013369645874\n",
      "train loss:0.9854179788888514\n",
      "train loss:0.9292205593497311\n",
      "train loss:0.7757591720864261\n",
      "train loss:0.8813580564964861\n",
      "train loss:0.7739888572350018\n",
      "train loss:0.9032384181982158\n",
      "train loss:0.9230389573875164\n",
      "train loss:0.8518570350258637\n",
      "train loss:0.7502048707314825\n",
      "train loss:0.9151766365084321\n",
      "train loss:0.888418026657685\n",
      "train loss:0.9342764749759327\n",
      "train loss:0.99486689293711\n",
      "train loss:0.8050085620164719\n",
      "train loss:0.9007100790186784\n",
      "train loss:0.7601840798129997\n",
      "train loss:0.9206341992436561\n",
      "train loss:0.7650933118935928\n",
      "train loss:0.8136422175432688\n",
      "train loss:0.93513005444709\n",
      "train loss:0.7923095281945243\n",
      "train loss:0.8900765003755418\n",
      "train loss:0.8695002795162107\n",
      "train loss:1.0401221633738706\n",
      "train loss:0.7025283838153429\n",
      "train loss:1.0309452987756555\n",
      "train loss:0.927088441427572\n",
      "train loss:0.9986758468120123\n",
      "train loss:0.855620923396617\n",
      "train loss:0.8249814220862187\n",
      "train loss:0.8430676035101137\n",
      "train loss:0.7780681794952479\n",
      "train loss:0.8803099493122063\n",
      "train loss:0.8597088349192272\n",
      "train loss:0.8048936969425164\n",
      "train loss:0.7525443968835224\n",
      "train loss:0.8439109416492541\n",
      "train loss:0.8394602904707428\n",
      "train loss:0.7087297304173978\n",
      "train loss:0.8891935249360907\n",
      "train loss:0.9345241403490037\n",
      "train loss:0.9417964816027253\n",
      "train loss:0.8459095251153961\n",
      "train loss:0.7164590091327382\n",
      "train loss:0.7899891977908822\n",
      "train loss:0.8707707069277825\n",
      "train loss:1.0008602436718852\n",
      "train loss:0.8806151846201231\n",
      "train loss:0.5963203893487885\n",
      "train loss:0.7928635897070017\n",
      "train loss:0.8292148190143616\n",
      "train loss:0.9092318883202505\n",
      "train loss:1.0084134094431136\n",
      "train loss:0.8774740244854029\n",
      "train loss:0.9002826599105456\n",
      "train loss:0.7873657225771052\n",
      "train loss:0.9640793845442299\n",
      "train loss:1.0017996629517025\n",
      "train loss:0.7418609193974626\n",
      "train loss:0.9062624551623957\n",
      "train loss:0.8564166803419063\n",
      "train loss:0.9726105217470369\n",
      "train loss:0.7818338164949173\n",
      "train loss:0.8899069544823268\n",
      "train loss:0.732815988428059\n",
      "train loss:1.0069987441873645\n",
      "train loss:0.8896700403532646\n",
      "train loss:1.0105283593067755\n",
      "train loss:0.9497519572097213\n",
      "train loss:0.8087201532885975\n",
      "train loss:0.9544306522125058\n",
      "train loss:0.7576406104945592\n",
      "train loss:1.019453777931459\n",
      "train loss:0.9739775148536824\n",
      "train loss:0.865761670993786\n",
      "train loss:0.7126814730937268\n",
      "train loss:0.8790126859167963\n",
      "train loss:0.9092632602582552\n",
      "train loss:0.8792582923777608\n",
      "train loss:1.010419616441922\n",
      "train loss:0.9504930110120346\n",
      "train loss:0.8509280697270346\n",
      "train loss:0.7848869735767624\n",
      "train loss:0.8780967294044194\n",
      "train loss:0.8230107359969443\n",
      "train loss:0.8864650039987425\n",
      "train loss:0.8853024665427085\n",
      "train loss:0.8425803677718129\n",
      "train loss:0.8661749646652938\n",
      "train loss:0.8162644712351157\n",
      "train loss:0.8548632684329476\n",
      "train loss:0.7567887728095718\n",
      "train loss:0.8881154324323667\n",
      "train loss:0.7356680004391368\n",
      "train loss:0.7724717003985136\n",
      "train loss:1.0232394284672937\n",
      "train loss:0.8475134448614433\n",
      "train loss:0.8555635583494429\n",
      "train loss:0.902526471826098\n",
      "train loss:0.8072894159192139\n",
      "train loss:0.9858035311957467\n",
      "train loss:1.0002380235825186\n",
      "train loss:0.7379968129897537\n",
      "train loss:0.72042883509996\n",
      "train loss:0.7992996735684919\n",
      "train loss:0.9560551930464665\n",
      "train loss:0.9540732972731348\n",
      "train loss:0.836555447723271\n",
      "train loss:0.9205982296807513\n",
      "train loss:0.7618942345326493\n",
      "train loss:0.7709711673842733\n",
      "train loss:0.8361940608912565\n",
      "train loss:0.8276523946062152\n",
      "train loss:1.092139288095509\n",
      "train loss:0.7989215265427579\n",
      "train loss:1.063210062021839\n",
      "train loss:0.8634532967317177\n",
      "train loss:0.7938422144654765\n",
      "train loss:0.8916556126290971\n",
      "train loss:0.7977082623666883\n",
      "train loss:0.7237279034689835\n",
      "train loss:0.838270474983855\n",
      "train loss:0.8934479659675965\n",
      "train loss:0.6886031083636369\n",
      "train loss:0.8636344323285557\n",
      "train loss:0.8262639383381907\n",
      "train loss:0.7000528886577172\n",
      "train loss:1.0314039645378432\n",
      "train loss:0.64530105936443\n",
      "train loss:0.8437870275050127\n",
      "=== epoch:15, train acc:0.997, test acc:0.995 ===\n",
      "train loss:0.924437137636701\n",
      "train loss:0.8791111667782127\n",
      "train loss:0.8493564759483193\n",
      "train loss:0.8550284824280078\n",
      "train loss:0.845264978176147\n",
      "train loss:0.9581878813556939\n",
      "train loss:0.9588207360569535\n",
      "train loss:0.8274334806868181\n",
      "train loss:0.9161154143877842\n",
      "train loss:0.8801617878407596\n",
      "train loss:0.9090253109793346\n",
      "train loss:0.9655142993354161\n",
      "train loss:0.9148252329511908\n",
      "train loss:0.8884472468377603\n",
      "train loss:0.7571186931241373\n",
      "train loss:0.7347253928344033\n",
      "train loss:0.9287941460970839\n",
      "train loss:0.8982143461215232\n",
      "train loss:0.8677156111260836\n",
      "train loss:0.8138306541294683\n",
      "train loss:0.8239131083536974\n",
      "train loss:1.088458282545878\n",
      "train loss:1.0002783954465515\n",
      "train loss:0.9187202050819465\n",
      "train loss:1.096550065820327\n",
      "train loss:0.7186764889801063\n",
      "train loss:0.8516625258437323\n",
      "train loss:0.8811178758160928\n",
      "train loss:0.8109642163137671\n",
      "train loss:0.8730124661548418\n",
      "train loss:0.9108982928984486\n",
      "train loss:1.026703197446807\n",
      "train loss:0.9097526279041891\n",
      "train loss:0.9306002595881124\n",
      "train loss:0.8158647500589455\n",
      "train loss:0.8975796421306559\n",
      "train loss:0.7738348201328858\n",
      "train loss:0.9198098867643641\n",
      "train loss:0.886075306845995\n",
      "train loss:0.8074734545820044\n",
      "train loss:0.9075781296214587\n",
      "train loss:0.7466548964883987\n",
      "train loss:0.9019405879821677\n",
      "train loss:0.7822840180438704\n",
      "train loss:0.8714019474183898\n",
      "train loss:1.0092668035249373\n",
      "train loss:0.8660761880778515\n",
      "train loss:0.6981444273337196\n",
      "train loss:0.900703341314144\n",
      "train loss:0.8709920406976949\n",
      "train loss:0.8267750719908358\n",
      "train loss:0.8340152040722849\n",
      "train loss:1.0201528694258355\n",
      "train loss:0.9462984826559476\n",
      "train loss:1.0471817310478633\n",
      "train loss:0.892013448032791\n",
      "train loss:0.7553321888080425\n",
      "train loss:0.7873406027557809\n",
      "train loss:0.8864561690037087\n",
      "train loss:0.9182480740351798\n",
      "train loss:0.9150223120200475\n",
      "train loss:0.9810749859703302\n",
      "train loss:0.9301316030625845\n",
      "train loss:0.8165521007547264\n",
      "train loss:0.8016088560279302\n",
      "train loss:0.944544422857161\n",
      "train loss:0.902883713232825\n",
      "train loss:0.8325652429440814\n",
      "train loss:0.9259728328596463\n",
      "train loss:0.9010528612376647\n",
      "train loss:0.9371184233321546\n",
      "train loss:0.8278916753613678\n",
      "train loss:0.8361116558190234\n",
      "train loss:0.8184819698296748\n",
      "train loss:0.8807679122174306\n",
      "train loss:0.9011780133107046\n",
      "train loss:0.9407026823075997\n",
      "train loss:0.8430791616102628\n",
      "train loss:0.9351138468011173\n",
      "train loss:0.8909382542247306\n",
      "train loss:0.7026241421193883\n",
      "train loss:0.9799356383808707\n",
      "train loss:0.8736463300813274\n",
      "train loss:0.6821988781620922\n",
      "train loss:0.9156253575780378\n",
      "train loss:0.7740602127797311\n",
      "train loss:0.6787268701829351\n",
      "train loss:0.9201066655361171\n",
      "train loss:0.9263431377358907\n",
      "train loss:0.8937949731033474\n",
      "train loss:0.7705535777062151\n",
      "train loss:0.7956566019771725\n",
      "train loss:0.9384491429570245\n",
      "train loss:1.028201740461963\n",
      "train loss:0.875619610373889\n",
      "train loss:0.8791848111796996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.9298731903542573\n",
      "train loss:0.8329309237525722\n",
      "train loss:0.8583818603649624\n",
      "train loss:0.8110569578839372\n",
      "train loss:0.8549691464910748\n",
      "train loss:0.7648969807934684\n",
      "train loss:0.9137054915203349\n",
      "train loss:0.9577179179019041\n",
      "train loss:0.8855008885100979\n",
      "train loss:0.9680926643068579\n",
      "train loss:0.8135001925020465\n",
      "train loss:0.8435207432213676\n",
      "train loss:0.9307615870360831\n",
      "train loss:0.7794477795293099\n",
      "train loss:0.8973806282592869\n",
      "train loss:0.7679613560519266\n",
      "train loss:0.7653837613676511\n",
      "train loss:0.7719016587732885\n",
      "train loss:0.7470249301346747\n",
      "train loss:0.7221048912478829\n",
      "train loss:0.8968429901643585\n",
      "train loss:0.8801081579170357\n",
      "train loss:1.01124214520192\n",
      "train loss:0.7689900245003533\n",
      "train loss:0.7167019628873419\n",
      "train loss:0.7158536814385544\n",
      "train loss:0.7962816927456765\n",
      "train loss:0.7555982653789141\n",
      "train loss:1.0290710582724687\n",
      "train loss:0.7066129377726277\n",
      "train loss:0.938167148263883\n",
      "train loss:0.8806154351928573\n",
      "train loss:0.7308039167901441\n",
      "train loss:0.7607936688169354\n",
      "train loss:0.9429194846328788\n",
      "train loss:0.9710792632059991\n",
      "train loss:0.9121772440993614\n",
      "train loss:0.8043081430168764\n",
      "train loss:0.9066414579347682\n",
      "train loss:0.8831327317987987\n",
      "train loss:0.9382168574274771\n",
      "train loss:0.6794812602697502\n",
      "train loss:0.7842972492930694\n",
      "train loss:0.8491864940003582\n",
      "train loss:0.8032800452545102\n",
      "train loss:0.8544631504945945\n",
      "train loss:0.9316362846642128\n",
      "train loss:0.9331220256784581\n",
      "train loss:0.8392820177863541\n",
      "train loss:0.8597580538811624\n",
      "train loss:0.9595681720322814\n",
      "train loss:0.8774850268571219\n",
      "train loss:0.8188164352885857\n",
      "train loss:0.7848163035723609\n",
      "train loss:0.9967589617901628\n",
      "train loss:0.8792004060105906\n",
      "train loss:0.7420177663578782\n",
      "train loss:0.9085818492426944\n",
      "train loss:0.9388563731291196\n",
      "train loss:0.8299362678394148\n",
      "train loss:0.8448623311394408\n",
      "train loss:0.8215597313060568\n",
      "train loss:0.9320031507334605\n",
      "train loss:0.9467758206171919\n",
      "train loss:0.9489858097189061\n",
      "train loss:0.9503044905967465\n",
      "train loss:0.8477766072800754\n",
      "train loss:0.9860905810914682\n",
      "train loss:1.018712461725661\n",
      "train loss:0.9606020833204925\n",
      "train loss:0.9262395091919342\n",
      "train loss:0.8466360430433132\n",
      "train loss:1.020625657690118\n",
      "train loss:0.8043434384308319\n",
      "train loss:0.9700187203045771\n",
      "train loss:0.7108148254351261\n",
      "train loss:0.8252176794373048\n",
      "train loss:0.7639973279279131\n",
      "train loss:0.7942219844928223\n",
      "train loss:0.734061484699041\n",
      "train loss:0.8210226537249252\n",
      "train loss:0.8526679041008006\n",
      "train loss:0.8113701976570468\n",
      "train loss:0.8134752848076148\n",
      "train loss:0.9170133065084163\n",
      "train loss:1.0078663895668836\n",
      "train loss:0.7130922399087113\n",
      "train loss:0.8734479086721767\n",
      "train loss:0.9804748386464851\n",
      "train loss:0.8580010012909731\n",
      "train loss:0.943639888717756\n",
      "train loss:1.0773321227075079\n",
      "train loss:0.9570224506299209\n",
      "train loss:0.8033515154363312\n",
      "train loss:0.8609919111227528\n",
      "train loss:0.9040233399676986\n",
      "train loss:0.8858867430290429\n",
      "train loss:0.8610997547846229\n",
      "train loss:0.978382704764062\n",
      "train loss:0.9683798147045991\n",
      "train loss:0.9901479325104827\n",
      "train loss:0.7245327623737106\n",
      "train loss:0.8638041675267851\n",
      "train loss:0.8823952564486243\n",
      "train loss:0.9456150821491304\n",
      "train loss:0.830041092646409\n",
      "train loss:1.115645162618826\n",
      "train loss:0.8378655944420659\n",
      "train loss:0.7743185638672087\n",
      "train loss:0.8037274629125484\n",
      "train loss:0.882016110400121\n",
      "train loss:0.8194644056690666\n",
      "train loss:0.9244747640688361\n",
      "train loss:0.9005721395698353\n",
      "train loss:0.9004966931226739\n",
      "train loss:0.7943152873743038\n",
      "train loss:0.7970486407269933\n",
      "train loss:1.0086752208590333\n",
      "train loss:1.0861938962887236\n",
      "train loss:1.009038319572046\n",
      "train loss:0.8008608750647652\n",
      "train loss:0.9132008318844318\n",
      "train loss:0.9880414809799524\n",
      "train loss:0.6830376651672819\n",
      "train loss:0.7612929448839286\n",
      "train loss:0.7471267745913686\n",
      "train loss:0.8053964040130549\n",
      "train loss:0.8009023391469549\n",
      "train loss:0.9186160977686206\n",
      "train loss:0.9336634771143618\n",
      "train loss:1.0535485726805711\n",
      "train loss:0.8531582103774993\n",
      "train loss:0.827737617565092\n",
      "train loss:0.727011830776332\n",
      "train loss:0.8212439587512137\n",
      "train loss:0.8405410312074405\n",
      "train loss:1.0379512553597037\n",
      "train loss:0.8052521787720655\n",
      "train loss:0.694400655674336\n",
      "train loss:0.6778560967324618\n",
      "train loss:1.1014366756781113\n",
      "train loss:0.8473761790623984\n",
      "train loss:0.9419193443685784\n",
      "train loss:0.9308405592698896\n",
      "train loss:0.8294277350652614\n",
      "train loss:0.6389854304325201\n",
      "train loss:0.9071717258235173\n",
      "train loss:0.803386332698857\n",
      "train loss:0.9561357839596673\n",
      "train loss:0.9370941479383694\n",
      "train loss:0.8430127449196256\n",
      "train loss:0.990011445699288\n",
      "train loss:0.8158731195595249\n",
      "train loss:0.9833465926254072\n",
      "train loss:0.8925984570122264\n",
      "train loss:0.9592096445215765\n",
      "train loss:0.7856455197364863\n",
      "train loss:0.946017763742182\n",
      "train loss:0.7203388559703896\n",
      "train loss:0.8602378823861067\n",
      "train loss:1.0010636403126518\n",
      "train loss:0.816790544863892\n",
      "train loss:1.0349618935271527\n",
      "train loss:0.7827713761332525\n",
      "train loss:0.8044758632438759\n",
      "train loss:0.8585543168967387\n",
      "train loss:0.8052976292413548\n",
      "train loss:0.8264066176491436\n",
      "train loss:1.0629222420240942\n",
      "train loss:0.961583731516724\n",
      "train loss:0.7768834639075601\n",
      "train loss:0.9519749601297239\n",
      "train loss:0.9399339331318342\n",
      "train loss:1.0356177805812894\n",
      "train loss:0.8377500540023008\n",
      "train loss:0.7790887367386915\n",
      "train loss:0.9475859797127916\n",
      "train loss:0.9412062747264223\n",
      "train loss:0.7147040280168473\n",
      "train loss:0.7661228587889584\n",
      "train loss:0.8399145643862689\n",
      "train loss:0.8124046183012618\n",
      "train loss:0.8617003017584643\n",
      "train loss:0.902332137456023\n",
      "train loss:0.9547548876010411\n",
      "train loss:0.6804778755200032\n",
      "train loss:0.8550833421057549\n",
      "train loss:0.7922814026887828\n",
      "train loss:1.0087940618504283\n",
      "train loss:0.815954579192644\n",
      "train loss:0.8260575780684983\n",
      "train loss:0.891630307699439\n",
      "train loss:0.8206890444643433\n",
      "train loss:0.9126276141259747\n",
      "train loss:0.7151894111749787\n",
      "train loss:0.8757761813231357\n",
      "train loss:0.7958850030419421\n",
      "train loss:0.8965464438563694\n",
      "train loss:0.8748192977949875\n",
      "train loss:0.9172586020334594\n",
      "train loss:0.9667680080741218\n",
      "train loss:0.7181295086393251\n",
      "train loss:0.9890505776660582\n",
      "train loss:0.9383157113609978\n",
      "train loss:0.7982467245672678\n",
      "train loss:0.8307167705581836\n",
      "train loss:0.81066645777101\n",
      "train loss:0.8138792975034356\n",
      "train loss:0.9018593404509325\n",
      "train loss:0.873273162025391\n",
      "train loss:1.0195029170980952\n",
      "train loss:0.8381566730560586\n",
      "train loss:0.9343287695199548\n",
      "train loss:0.9487154644181734\n",
      "train loss:0.9606120504524347\n",
      "train loss:0.9446316791696294\n",
      "train loss:0.9646038168795215\n",
      "train loss:1.0102973725543896\n",
      "train loss:0.9364718686705947\n",
      "train loss:0.8565099199130812\n",
      "train loss:0.9039964028992675\n",
      "train loss:0.8690881927450413\n",
      "train loss:0.9336698088362172\n",
      "train loss:0.837725782709529\n",
      "train loss:0.8491076729344408\n",
      "train loss:0.8675811625330693\n",
      "train loss:0.8337908927495725\n",
      "train loss:0.8208767685061592\n",
      "train loss:0.8962116816821966\n",
      "train loss:0.9013742219149364\n",
      "train loss:0.8733619662841666\n",
      "train loss:0.8213194569256472\n",
      "train loss:0.8276396692977539\n",
      "train loss:0.8229118612718542\n",
      "train loss:0.9359872719063499\n",
      "train loss:0.870014274342957\n",
      "train loss:0.825244146062791\n",
      "train loss:1.0194803695713033\n",
      "train loss:0.7688563219641061\n",
      "train loss:0.7536490752053894\n",
      "train loss:0.9083833456017709\n",
      "train loss:0.9297376817962121\n",
      "train loss:0.7761514357576231\n",
      "train loss:1.053861396968513\n",
      "train loss:0.8729104790275565\n",
      "train loss:0.7152931918129585\n",
      "train loss:0.9684042039340848\n",
      "train loss:0.811410371307705\n",
      "train loss:0.8510562294026593\n",
      "train loss:0.7726545892199843\n",
      "train loss:0.8654143599424401\n",
      "train loss:0.7519866210780519\n",
      "train loss:0.7810202857360224\n",
      "train loss:0.8382076592194082\n",
      "train loss:1.051812427096965\n",
      "train loss:0.9463852349413119\n",
      "train loss:1.082450001221615\n",
      "train loss:0.8231679713401285\n",
      "train loss:0.8845532847848496\n",
      "train loss:0.9503336509622652\n",
      "train loss:0.8568458737019303\n",
      "train loss:0.7442565696279176\n",
      "train loss:0.7223849514665496\n",
      "train loss:0.875907379387041\n",
      "train loss:0.868787965399773\n",
      "train loss:0.89321135707188\n",
      "train loss:0.8832572222722861\n",
      "train loss:0.8438531376644886\n",
      "train loss:0.6925956419068754\n",
      "train loss:0.9649489403837643\n",
      "train loss:1.076124840888788\n",
      "train loss:0.9280745474766018\n",
      "train loss:0.783604100526019\n",
      "train loss:0.8494881793271037\n",
      "train loss:0.780929597439182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.9564988733590987\n",
      "train loss:0.7598356279679157\n",
      "train loss:0.9386033823180812\n",
      "train loss:0.780112663752265\n",
      "train loss:0.9700572764797388\n",
      "train loss:0.9014900909314099\n",
      "train loss:0.9399128389136964\n",
      "train loss:0.9106101670906309\n",
      "train loss:0.8252242269430594\n",
      "train loss:0.8172866245586691\n",
      "train loss:0.9443451449844337\n",
      "train loss:0.7300877090082463\n",
      "train loss:1.0493672438547343\n",
      "train loss:0.8528595996319727\n",
      "train loss:1.0087218914888862\n",
      "train loss:0.9449326726299624\n",
      "train loss:1.1573872566497478\n",
      "train loss:0.7658014214802522\n",
      "train loss:0.842236334062107\n",
      "train loss:0.8815854764391166\n",
      "train loss:0.9318280308092968\n",
      "train loss:0.8174177052971687\n",
      "train loss:0.8251933782977452\n",
      "train loss:0.8792659714445556\n",
      "train loss:0.8937393212135635\n",
      "train loss:0.7412316639499297\n",
      "train loss:0.9832648877486408\n",
      "train loss:0.8020347427490451\n",
      "train loss:0.8814672312267322\n",
      "train loss:0.8769349720912697\n",
      "train loss:0.944677370118148\n",
      "train loss:0.829823859411041\n",
      "train loss:0.7911865016008511\n",
      "train loss:0.8251536554520513\n",
      "train loss:0.8111526146690502\n",
      "train loss:0.8915495947457819\n",
      "train loss:0.7603584251299691\n",
      "train loss:0.7804215422375756\n",
      "train loss:1.0401705526930822\n",
      "train loss:0.843490847205812\n",
      "train loss:0.6974113408269418\n",
      "train loss:0.8502432269273883\n",
      "train loss:0.8756098990000888\n",
      "train loss:0.9417441466594412\n",
      "train loss:0.8782304801997198\n",
      "train loss:0.8526979790126974\n",
      "train loss:0.8546499442635347\n",
      "train loss:0.8035487399396589\n",
      "train loss:0.8986869878586545\n",
      "train loss:1.0180328696459247\n",
      "train loss:0.8329848701996891\n",
      "train loss:0.7871623592111999\n",
      "train loss:0.7700042931666232\n",
      "train loss:0.924314096136779\n",
      "train loss:1.0281149701420191\n",
      "train loss:0.7185963529727387\n",
      "train loss:0.7483891551987285\n",
      "train loss:0.8107941869052178\n",
      "train loss:0.8033748552191384\n",
      "train loss:0.9068710627379245\n",
      "train loss:0.8956366228417163\n",
      "train loss:0.8870150938164771\n",
      "train loss:0.7011221029545357\n",
      "train loss:0.8394535504046265\n",
      "train loss:0.733697524940533\n",
      "train loss:0.8879408887323744\n",
      "train loss:0.6771164789746947\n",
      "train loss:0.9585188224874966\n",
      "train loss:0.9787666876187309\n",
      "train loss:0.703435096028801\n",
      "train loss:0.7338107681590712\n",
      "train loss:0.9941356809487436\n",
      "train loss:0.7016128716594798\n",
      "train loss:0.8534664774932024\n",
      "train loss:0.8162831390735551\n",
      "train loss:0.8951134216506772\n",
      "train loss:0.9222611693003506\n",
      "train loss:0.7657982114344266\n",
      "train loss:0.8744500165939791\n",
      "train loss:0.8852224993071681\n",
      "train loss:0.8463504448689636\n",
      "train loss:0.948056571598508\n",
      "train loss:0.9679292938243073\n",
      "train loss:0.8832565027354609\n",
      "train loss:0.8743839276461718\n",
      "train loss:0.7778544734181954\n",
      "train loss:0.8180556549675825\n",
      "train loss:0.9717240524716128\n",
      "train loss:0.9232919228735842\n",
      "train loss:0.8256960341575572\n",
      "train loss:0.8373539146070044\n",
      "train loss:0.8587448444028689\n",
      "train loss:0.9096602073395015\n",
      "train loss:0.9216795700400562\n",
      "train loss:1.0352624653621454\n",
      "train loss:0.9284063210736393\n",
      "train loss:0.8232672191089123\n",
      "train loss:0.7934297828853316\n",
      "train loss:0.6709576713601146\n",
      "train loss:0.7771012867352873\n",
      "train loss:0.9910071482133198\n",
      "train loss:0.9655106666116607\n",
      "train loss:0.8788544562083177\n",
      "train loss:0.8816191160158386\n",
      "train loss:0.8208802261249233\n",
      "train loss:0.9251047664517938\n",
      "train loss:0.9810725945540665\n",
      "train loss:0.8475886351948404\n",
      "train loss:0.9261676913693637\n",
      "train loss:0.7854221425438002\n",
      "train loss:0.8954306070031726\n",
      "train loss:0.8951967571286764\n",
      "train loss:0.9334215430769772\n",
      "train loss:1.003519369287029\n",
      "train loss:0.8309594088358447\n",
      "train loss:0.9106083495205647\n",
      "train loss:0.879411294602628\n",
      "train loss:0.8520871237232847\n",
      "train loss:0.837700420678396\n",
      "train loss:0.8646723162517786\n",
      "train loss:0.8788352209280145\n",
      "train loss:0.7096055707094645\n",
      "train loss:0.7402682720252887\n",
      "train loss:0.7502336479898429\n",
      "train loss:0.8197712822399962\n",
      "train loss:0.8678080058142599\n",
      "train loss:0.9876413020234844\n",
      "train loss:0.7911924241491154\n",
      "train loss:0.8427679314010367\n",
      "train loss:0.8545012747561995\n",
      "train loss:0.9350162698349348\n",
      "train loss:0.9280583623975273\n",
      "train loss:0.8812262699699751\n",
      "train loss:0.8869462766137772\n",
      "train loss:0.8437113678102038\n",
      "train loss:0.7940590687005143\n",
      "train loss:0.8385369310897329\n",
      "train loss:0.8090684521836544\n",
      "train loss:0.8967882606398656\n",
      "train loss:0.9756425288257955\n",
      "train loss:1.057593412374088\n",
      "train loss:0.9358327602895768\n",
      "train loss:0.8892639073074621\n",
      "train loss:0.8473399652256696\n",
      "train loss:0.9068796547856067\n",
      "train loss:0.7566186140370776\n",
      "train loss:0.7004555230898406\n",
      "train loss:0.9576051864288724\n",
      "train loss:0.9652707141082888\n",
      "train loss:0.8567505013543167\n",
      "train loss:0.9952402078228844\n",
      "train loss:0.9741919503193343\n",
      "train loss:0.7109261992041407\n",
      "train loss:0.8565164760925222\n",
      "train loss:0.9975158737677827\n",
      "train loss:0.8176469248323284\n",
      "train loss:0.9050244823014515\n",
      "train loss:0.8122996465742185\n",
      "train loss:0.8809281195932386\n",
      "train loss:0.9168492250022261\n",
      "train loss:0.7095154334539191\n",
      "train loss:0.8355886119435532\n",
      "train loss:0.811018735464734\n",
      "train loss:0.9593983679359241\n",
      "train loss:0.9328777510155074\n",
      "train loss:0.8331062831052646\n",
      "train loss:0.9350241168396218\n",
      "train loss:0.7850460331374939\n",
      "train loss:1.003782397358632\n",
      "train loss:0.806368550580051\n",
      "train loss:0.9238122460673562\n",
      "train loss:0.872186473158449\n",
      "train loss:0.8461383468327199\n",
      "train loss:0.8930944406829507\n",
      "train loss:0.7491481208239372\n",
      "train loss:0.8444112017420835\n",
      "train loss:0.7828296899265561\n",
      "train loss:0.8458681841733862\n",
      "train loss:0.9791657350212759\n",
      "train loss:0.9085258688452942\n",
      "train loss:0.7927802119567047\n",
      "train loss:0.8363002686441959\n",
      "train loss:0.861400713587597\n",
      "train loss:0.7515554634684828\n",
      "train loss:0.843036981897049\n",
      "train loss:0.9075997316799898\n",
      "train loss:0.8681937681370738\n",
      "train loss:0.8033098093534042\n",
      "train loss:0.780025455751173\n",
      "train loss:0.9244925628636652\n",
      "train loss:0.8834514873382584\n",
      "train loss:0.8939770372629335\n",
      "train loss:0.8814174554546207\n",
      "train loss:0.9628090672958054\n",
      "train loss:0.8379074296663392\n",
      "train loss:0.9206005726068126\n",
      "train loss:0.8783714688136519\n",
      "train loss:0.949435530329939\n",
      "train loss:0.7975314155629163\n",
      "train loss:0.8912853058270119\n",
      "train loss:0.7761744906936747\n",
      "train loss:0.7877060198751614\n",
      "train loss:0.8547832521682522\n",
      "train loss:1.0609025683903464\n",
      "train loss:0.8197325844903713\n",
      "train loss:0.8637657671017397\n",
      "train loss:0.9306873922430096\n",
      "train loss:0.8408420344879073\n",
      "train loss:0.733461770523869\n",
      "train loss:0.7608609749431701\n",
      "train loss:0.7823921818718829\n",
      "train loss:0.8856087700571444\n",
      "train loss:0.9469715308184413\n",
      "train loss:0.8765139707129738\n",
      "train loss:0.8808195807479542\n",
      "train loss:0.8256629948863341\n",
      "train loss:0.7791282152087627\n",
      "train loss:0.8863332584610422\n",
      "train loss:0.86249775081039\n",
      "train loss:0.8872157015447223\n",
      "train loss:0.8589109885335037\n",
      "train loss:0.7627291138864228\n",
      "train loss:0.8523012221336694\n",
      "train loss:0.8264663048202715\n",
      "train loss:0.8774395580067821\n",
      "train loss:0.7991622730505195\n",
      "train loss:0.8031772070805596\n",
      "train loss:0.8423259922720386\n",
      "train loss:0.7810226530772815\n",
      "=== epoch:16, train acc:0.998, test acc:0.996 ===\n",
      "train loss:0.8557081221041433\n",
      "train loss:0.8084614498904861\n",
      "train loss:0.8202220777827294\n",
      "train loss:0.8200995756925835\n",
      "train loss:1.0346035399569775\n",
      "train loss:0.8138004581104009\n",
      "train loss:0.8650692321428898\n",
      "train loss:0.935252366730445\n",
      "train loss:0.9769396793953123\n",
      "train loss:0.7219714233670256\n",
      "train loss:0.8520988863363729\n",
      "train loss:0.6729705706180948\n",
      "train loss:0.7111896807674676\n",
      "train loss:0.7377626090799088\n",
      "train loss:0.9721476459142044\n",
      "train loss:0.9287046912120774\n",
      "train loss:0.7829764890979088\n",
      "train loss:0.9541080269947261\n",
      "train loss:0.7787375020181053\n",
      "train loss:0.8638276159734722\n",
      "train loss:0.8214538211202864\n",
      "train loss:0.8485794304107572\n",
      "train loss:0.885261991794456\n",
      "train loss:0.8556411290124593\n",
      "train loss:0.9797963751810522\n",
      "train loss:0.889473080838964\n",
      "train loss:0.8082084786057122\n",
      "train loss:0.7160957134456349\n",
      "train loss:0.7601454654764109\n",
      "train loss:0.9087324292938677\n",
      "train loss:0.8913534349928507\n",
      "train loss:0.6270369401792079\n",
      "train loss:0.8925440807342878\n",
      "train loss:0.7029058651348531\n",
      "train loss:0.9542480168539424\n",
      "train loss:1.0561650218265612\n",
      "train loss:1.0141514956382154\n",
      "train loss:0.7324016178302013\n",
      "train loss:0.855863234262851\n",
      "train loss:0.7811105596131945\n",
      "train loss:0.8169313769991762\n",
      "train loss:0.9371107400323866\n",
      "train loss:0.9858778295466348\n",
      "train loss:0.9229957650424285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.814960676582291\n",
      "train loss:0.7907087080983151\n",
      "train loss:0.9449792770465134\n",
      "train loss:0.9945728798169572\n",
      "train loss:0.8749570849683947\n",
      "train loss:0.8971656914898442\n",
      "train loss:0.803487625954532\n",
      "train loss:0.8900778875826831\n",
      "train loss:0.8903017743237212\n",
      "train loss:0.9339826669485408\n",
      "train loss:1.0997013311032753\n",
      "train loss:0.9301383490024956\n",
      "train loss:0.7833952176519163\n",
      "train loss:0.8768263714333838\n",
      "train loss:0.7929849339623406\n",
      "train loss:0.8113171467250718\n",
      "train loss:1.0261714489904596\n",
      "train loss:0.9726198982253261\n",
      "train loss:0.8597922142867934\n",
      "train loss:0.7116532493994024\n",
      "train loss:0.9180323996528675\n",
      "train loss:0.8810998443124762\n",
      "train loss:0.9929662301578892\n",
      "train loss:0.8359275351835165\n",
      "train loss:1.0466507321494598\n",
      "train loss:0.7677978145786435\n",
      "train loss:1.008045870105823\n",
      "train loss:0.8850783910944213\n",
      "train loss:0.90494407253271\n",
      "train loss:0.8575251181883017\n",
      "train loss:0.7302510103142555\n",
      "train loss:0.8496301049243964\n",
      "train loss:0.9226010017619097\n",
      "train loss:0.7531688217023162\n",
      "train loss:0.7533392678085061\n",
      "train loss:0.7625693675879676\n",
      "train loss:1.0056752236297422\n",
      "train loss:1.0301588073440455\n",
      "train loss:0.8435294973082255\n",
      "train loss:0.9155614317273438\n",
      "train loss:0.8574770680952729\n",
      "train loss:0.7501012726377725\n",
      "train loss:0.927798192680186\n",
      "train loss:0.8295767078875292\n",
      "train loss:0.8060410891057141\n",
      "train loss:0.9550909521827236\n",
      "train loss:0.9653854640239814\n",
      "train loss:0.878720911287539\n",
      "train loss:0.7788043177151076\n",
      "train loss:0.961114707299838\n",
      "train loss:1.0129815645950933\n",
      "train loss:0.7013655785931527\n",
      "train loss:0.8802718967549873\n",
      "train loss:0.8682758780845581\n",
      "train loss:0.8346224720246286\n",
      "train loss:0.8326148652799406\n",
      "train loss:0.8441300895097605\n",
      "train loss:0.902383974126777\n",
      "train loss:1.0411967371340483\n",
      "train loss:0.7553377666717428\n",
      "train loss:0.7174259340566165\n",
      "train loss:0.8425279251605582\n",
      "train loss:1.0908012983263735\n",
      "train loss:0.7952134847360456\n",
      "train loss:0.8737205300962287\n",
      "train loss:0.8648364337155088\n",
      "train loss:0.6964668083738546\n",
      "train loss:0.8386127417726205\n",
      "train loss:0.8979877968946225\n",
      "train loss:0.8938224074336132\n",
      "train loss:0.9691579588501249\n",
      "train loss:0.8619529265742715\n",
      "train loss:0.8709364503897915\n",
      "train loss:0.9865407354327754\n",
      "train loss:0.7348426659740065\n",
      "train loss:0.7217515830007736\n",
      "train loss:0.8944048945044843\n",
      "train loss:0.9235252776860138\n",
      "train loss:0.8549975883629156\n",
      "train loss:0.9263571207437126\n",
      "train loss:0.8767701326920959\n",
      "train loss:1.0704149048249851\n",
      "train loss:0.7359758284096355\n",
      "train loss:0.7569948087378887\n",
      "train loss:0.6129447738823959\n",
      "train loss:0.8622808298187506\n",
      "train loss:0.9717821455879425\n",
      "train loss:1.0047572255371444\n",
      "train loss:0.9448092031720968\n",
      "train loss:0.9103729978369135\n",
      "train loss:0.8922288604468447\n",
      "train loss:0.7541413565417308\n",
      "train loss:0.7922137304338623\n",
      "train loss:0.9453395210877861\n",
      "train loss:0.8441361849028087\n",
      "train loss:0.8006875658166551\n",
      "train loss:0.9599269307744607\n",
      "train loss:0.9363231221321872\n",
      "train loss:0.8609421615555197\n",
      "train loss:0.8399710195139228\n",
      "train loss:0.8315475401647643\n",
      "train loss:0.7577371296217704\n",
      "train loss:1.057513197884132\n",
      "train loss:0.9216321456247896\n",
      "train loss:0.831339757855525\n",
      "train loss:0.8689856375512699\n",
      "train loss:0.9486560772577103\n",
      "train loss:0.8545201726895622\n",
      "train loss:0.8699675932669645\n",
      "train loss:0.8020982655137388\n",
      "train loss:0.8944000413418642\n",
      "train loss:0.9091950911475669\n",
      "train loss:0.9240739545914738\n",
      "train loss:0.8203636976764597\n",
      "train loss:0.8882789052662998\n",
      "train loss:0.875500700215315\n",
      "train loss:0.9728795237912415\n",
      "train loss:0.9725172636957754\n",
      "train loss:0.8391669106376399\n",
      "train loss:0.9364593520996238\n",
      "train loss:0.8350346116069653\n",
      "train loss:0.9675267350348524\n",
      "train loss:0.9491474029491396\n",
      "train loss:1.0091885342800462\n",
      "train loss:0.996642364053492\n",
      "train loss:0.8866170694179973\n",
      "train loss:0.7719584404277097\n",
      "train loss:1.1067029868401144\n",
      "train loss:0.877205576249742\n",
      "train loss:0.7488136259765611\n",
      "train loss:0.830916371707151\n",
      "train loss:0.6822688245658131\n",
      "train loss:0.9828229620546649\n",
      "train loss:0.7121047638226687\n",
      "train loss:0.8820033081350713\n",
      "train loss:0.8183139978172832\n",
      "train loss:0.8999089533109341\n",
      "train loss:0.8934575462060411\n",
      "train loss:0.662864564334481\n",
      "train loss:0.7259980115386151\n",
      "train loss:0.976117674267492\n",
      "train loss:0.8595732309632915\n",
      "train loss:0.8132890524328847\n",
      "train loss:0.9180419478599492\n",
      "train loss:0.8887978261779419\n",
      "train loss:0.6794485164444285\n",
      "train loss:0.7671959260848403\n",
      "train loss:0.7215978325459171\n",
      "train loss:0.8325014486249589\n",
      "train loss:0.8925101305436967\n",
      "train loss:0.8182540767138584\n",
      "train loss:0.783510608066442\n",
      "train loss:0.8781550362098001\n",
      "train loss:0.8919211577532373\n",
      "train loss:0.9218606384777642\n",
      "train loss:0.9569271048924263\n",
      "train loss:0.9686390843705235\n",
      "train loss:0.7373702742473562\n",
      "train loss:0.8279291359934591\n",
      "train loss:0.9248500044573449\n",
      "train loss:0.656607704286971\n",
      "train loss:0.8965190955346237\n",
      "train loss:0.8007348220442563\n",
      "train loss:1.050993796073068\n",
      "train loss:0.9324235876651471\n",
      "train loss:0.8162807608175663\n",
      "train loss:0.9293552910574386\n",
      "train loss:0.8289673787853037\n",
      "train loss:0.8194846692898241\n",
      "train loss:0.8838263960118701\n",
      "train loss:0.7749548355991875\n",
      "train loss:0.9259412035669661\n",
      "train loss:0.8685763633123499\n",
      "train loss:0.8122409169185506\n",
      "train loss:0.7810002434894041\n",
      "train loss:0.8796563783324827\n",
      "train loss:0.6640347142370898\n",
      "train loss:1.101340927853854\n",
      "train loss:0.8345217467939311\n",
      "train loss:0.949774648560678\n",
      "train loss:0.953938166509432\n",
      "train loss:0.9187102226915265\n",
      "train loss:0.9007898463243811\n",
      "train loss:0.8197441901082378\n",
      "train loss:0.7556180753196068\n",
      "train loss:0.7886391663639185\n",
      "train loss:0.8892341424526258\n",
      "train loss:1.0033701001464213\n",
      "train loss:0.8679912888364933\n",
      "train loss:0.7833347459234027\n",
      "train loss:0.8139562987497304\n",
      "train loss:0.7935516103763437\n",
      "train loss:0.8012217301638551\n",
      "train loss:0.8789159930745332\n",
      "train loss:0.9696150388155003\n",
      "train loss:0.9260140406747243\n",
      "train loss:0.7387535919789159\n",
      "train loss:0.8098843580984222\n",
      "train loss:0.8992093461295363\n",
      "train loss:0.8172909415252341\n",
      "train loss:0.7326752671870078\n",
      "train loss:0.8654298014955794\n",
      "train loss:0.8900229572435837\n",
      "train loss:0.8202359693582718\n",
      "train loss:0.9593076636011847\n",
      "train loss:0.85388514285979\n",
      "train loss:0.7705868962796436\n",
      "train loss:0.8566309429065219\n",
      "train loss:0.8062916792468198\n",
      "train loss:0.9748417187076562\n",
      "train loss:0.8554462700486372\n",
      "train loss:0.7728662303308709\n",
      "train loss:0.7455340202085435\n",
      "train loss:0.8551793710218272\n",
      "train loss:0.8784508363744485\n",
      "train loss:0.9593712802654122\n",
      "train loss:0.849333974377072\n",
      "train loss:0.8561628337233023\n",
      "train loss:0.657013247740399\n",
      "train loss:0.880955262232295\n",
      "train loss:0.8487559070018618\n",
      "train loss:0.8688168642979255\n",
      "train loss:0.8496561675672402\n",
      "train loss:0.7891374654561227\n",
      "train loss:1.0119516425623711\n",
      "train loss:1.0111395504400145\n",
      "train loss:0.8008663079392546\n",
      "train loss:0.7245533504190544\n",
      "train loss:0.8808474641759574\n",
      "train loss:0.9045843525271853\n",
      "train loss:0.8310792413014164\n",
      "train loss:0.8497312132978934\n",
      "train loss:0.8983869246475336\n",
      "train loss:0.8605286856503\n",
      "train loss:1.0376939188329923\n",
      "train loss:0.8405954486205115\n",
      "train loss:1.0288561448531857\n",
      "train loss:0.9777231863396019\n",
      "train loss:1.0092205590316483\n",
      "train loss:0.9081408540491468\n",
      "train loss:0.8148029706106893\n",
      "train loss:0.8285376763826027\n",
      "train loss:0.9233475977299522\n",
      "train loss:0.7375560230207028\n",
      "train loss:0.9356595989003794\n",
      "train loss:0.887220041727782\n",
      "train loss:0.9044751893028163\n",
      "train loss:0.8895667702920933\n",
      "train loss:0.7911046933065969\n",
      "train loss:0.9228218521654624\n",
      "train loss:0.8558412526641418\n",
      "train loss:0.8795060620076771\n",
      "train loss:0.8213005656998346\n",
      "train loss:0.725196023474753\n",
      "train loss:0.8508609253687021\n",
      "train loss:0.834323679932683\n",
      "train loss:0.847554806659829\n",
      "train loss:0.7111674301945458\n",
      "train loss:0.857833574476911\n",
      "train loss:0.7486413072710661\n",
      "train loss:0.8882435174124222\n",
      "train loss:0.8647540921093148\n",
      "train loss:0.7203637836464155\n",
      "train loss:0.8553193066081878\n",
      "train loss:0.7666442597665744\n",
      "train loss:0.816135140306253\n",
      "train loss:0.9566916596938562\n",
      "train loss:0.9560206939355829\n",
      "train loss:0.79021249438041\n",
      "train loss:0.9824919558635201\n",
      "train loss:0.7329833136943641\n",
      "train loss:0.8205382147833212\n",
      "train loss:0.9907386253559889\n",
      "train loss:0.7914195449708596\n",
      "train loss:0.7536883563236114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.8268629533469306\n",
      "train loss:0.9449565269549359\n",
      "train loss:0.9916104120199507\n",
      "train loss:0.8818802361543338\n",
      "train loss:0.9044768789325501\n",
      "train loss:0.9124768208867813\n",
      "train loss:0.8766461130027635\n",
      "train loss:0.7319817708596931\n",
      "train loss:0.9227994802260966\n",
      "train loss:1.0153970480014285\n",
      "train loss:0.654643834664935\n",
      "train loss:0.9286436347681769\n",
      "train loss:0.8600671946511247\n",
      "train loss:0.8561124273737366\n",
      "train loss:0.912811896702121\n",
      "train loss:0.6390707632203134\n",
      "train loss:0.9733914063653085\n",
      "train loss:0.8627691570644069\n",
      "train loss:0.746021975455975\n",
      "train loss:0.9323304836312457\n",
      "train loss:0.9576471619464697\n",
      "train loss:0.7165124789593114\n",
      "train loss:0.8286820135752164\n",
      "train loss:0.7740973016694892\n",
      "train loss:0.7954334662672486\n",
      "train loss:0.6178895759226104\n",
      "train loss:0.778735276341116\n",
      "train loss:0.8479725563034893\n",
      "train loss:0.7632460440679603\n",
      "train loss:0.9733778101034015\n",
      "train loss:0.8231612141040645\n",
      "train loss:0.88338391086848\n",
      "train loss:0.930756259851765\n",
      "train loss:0.7810827953543757\n",
      "train loss:1.035298826700979\n",
      "train loss:1.0933245757356203\n",
      "train loss:0.7385463806019154\n",
      "train loss:0.8389469056725499\n",
      "train loss:0.8846013386276158\n",
      "train loss:0.8399495667272068\n",
      "train loss:0.8801435219780133\n",
      "train loss:0.9208657043340166\n",
      "train loss:0.7311333984256815\n",
      "train loss:0.9399169237591662\n",
      "train loss:0.9369564821197548\n",
      "train loss:0.7962675510282243\n",
      "train loss:0.7749281503046697\n",
      "train loss:0.7876819130801479\n",
      "train loss:1.029637054657303\n",
      "train loss:0.9115745049739239\n",
      "train loss:0.8576303718121426\n",
      "train loss:0.8419652794704892\n",
      "train loss:0.8105382590558975\n",
      "train loss:0.8081119111006891\n",
      "train loss:0.8367049459811932\n",
      "train loss:0.8572154409083877\n",
      "train loss:0.9508099683998094\n",
      "train loss:0.7857539765779893\n",
      "train loss:0.7747005656146863\n",
      "train loss:0.7950929050748191\n",
      "train loss:0.9767881091853664\n",
      "train loss:0.7987215377433645\n",
      "train loss:0.8466508774838666\n",
      "train loss:0.7894479276814417\n",
      "train loss:0.7693095313074564\n",
      "train loss:0.7279997524763009\n",
      "train loss:0.7785055618812816\n",
      "train loss:0.8237684872042983\n",
      "train loss:0.7151906621472137\n",
      "train loss:0.9291594985075012\n",
      "train loss:0.6648852226397832\n",
      "train loss:0.9177327031201421\n",
      "train loss:0.8804380799217537\n",
      "train loss:0.7914814373174613\n",
      "train loss:0.858429258543186\n",
      "train loss:0.9354769660919924\n",
      "train loss:0.9441701016227084\n",
      "train loss:0.8940572773726646\n",
      "train loss:0.8022030512883529\n",
      "train loss:0.7924705489936974\n",
      "train loss:0.7846048306470327\n",
      "train loss:0.9237871950792383\n",
      "train loss:0.8293122239568187\n",
      "train loss:0.9145638613031071\n",
      "train loss:0.8689331997530841\n",
      "train loss:0.906992920122523\n",
      "train loss:0.7108295993934471\n",
      "train loss:0.9006906021873508\n",
      "train loss:0.7144022342984626\n",
      "train loss:0.9078533564364705\n",
      "train loss:0.7793262064491951\n",
      "train loss:0.9249030685188475\n",
      "train loss:0.9435139756193988\n",
      "train loss:0.8195581774276826\n",
      "train loss:0.8298777078986541\n",
      "train loss:0.7637638511177884\n",
      "train loss:1.0595925740555132\n",
      "train loss:1.0118414934542859\n",
      "train loss:0.70028897314523\n",
      "train loss:0.8895728820076805\n",
      "train loss:0.7925014656377887\n",
      "train loss:0.9787072041395986\n",
      "train loss:1.066176292597546\n",
      "train loss:0.9066229334055826\n",
      "train loss:0.7700974990668\n",
      "train loss:0.914030398678086\n",
      "train loss:0.8101798387219319\n",
      "train loss:0.7280490745487229\n",
      "train loss:0.8670317303794062\n",
      "train loss:0.9233112835043674\n",
      "train loss:0.9362052652991588\n",
      "train loss:0.910770534880433\n",
      "train loss:0.7835716494365117\n",
      "train loss:0.8795601992036475\n",
      "train loss:0.7780866472811978\n",
      "train loss:0.836994029819163\n",
      "train loss:0.9665759113028376\n",
      "train loss:0.8837191067600111\n",
      "train loss:0.8472688801685782\n",
      "train loss:0.905961130650885\n",
      "train loss:0.8075632969356473\n",
      "train loss:0.9398954313896346\n",
      "train loss:0.8692977596809061\n",
      "train loss:0.887206762972323\n",
      "train loss:0.9118595942781533\n",
      "train loss:0.9641921256005461\n",
      "train loss:0.8396608038565222\n",
      "train loss:0.8069705535346013\n",
      "train loss:0.8552326404980475\n",
      "train loss:0.8354625936962442\n",
      "train loss:0.8162861340113368\n",
      "train loss:0.7702104743816109\n",
      "train loss:1.012088415613852\n",
      "train loss:0.8561434671782747\n",
      "train loss:0.8545401700078338\n",
      "train loss:0.9093183173824264\n",
      "train loss:0.8468896643374424\n",
      "train loss:0.8298034708286894\n",
      "train loss:0.7158339929879793\n",
      "train loss:0.9128898931730203\n",
      "train loss:0.9853247707073577\n",
      "train loss:1.00958087903458\n",
      "train loss:0.7832556197815986\n",
      "train loss:0.8918308232277767\n",
      "train loss:0.9947472263341968\n",
      "train loss:0.8651643280621653\n",
      "train loss:0.9501116602088726\n",
      "train loss:0.787762242508141\n",
      "train loss:1.0763943279304273\n",
      "train loss:0.6344412871887757\n",
      "train loss:0.6600309677082847\n",
      "train loss:0.9771771458927376\n",
      "train loss:0.8323293311951072\n",
      "train loss:0.6170582693180122\n",
      "train loss:0.811899343773411\n",
      "train loss:0.9422497404346095\n",
      "train loss:1.0396059812031917\n",
      "train loss:1.098772754614832\n",
      "train loss:0.6998378238933561\n",
      "train loss:0.7867684618543498\n",
      "train loss:0.927140177310364\n",
      "train loss:0.8150295934702498\n",
      "train loss:0.946138597750307\n",
      "train loss:0.8538653327304695\n",
      "train loss:0.7948138282479182\n",
      "train loss:0.9004455132123594\n",
      "train loss:0.8673835394906425\n",
      "train loss:0.8179274827657934\n",
      "train loss:0.8825652074241698\n",
      "train loss:0.8595784439254329\n",
      "train loss:0.8553684879735622\n",
      "train loss:0.801202735591846\n",
      "train loss:0.6609552211109747\n",
      "train loss:0.7226399665246059\n",
      "train loss:0.8195620515994103\n",
      "train loss:0.8612748632497573\n",
      "train loss:0.8317461363621373\n",
      "train loss:0.668368144377276\n",
      "train loss:1.103525887357526\n",
      "train loss:0.6680416648370039\n",
      "train loss:0.9092491825603691\n",
      "train loss:0.74294745312738\n",
      "train loss:1.002626146653878\n",
      "train loss:0.7066199309477041\n",
      "train loss:0.9452307171136992\n",
      "train loss:0.7558990694507424\n",
      "train loss:0.8209955122585262\n",
      "train loss:0.9480850652860349\n",
      "train loss:0.7227960994768633\n",
      "train loss:0.9662204645206475\n",
      "train loss:0.8112626618259609\n",
      "train loss:0.9148735463538621\n",
      "train loss:0.9181465180041449\n",
      "train loss:0.6394342019448478\n",
      "train loss:0.8355157272117055\n",
      "train loss:1.0323235588697934\n",
      "train loss:0.9385813755053909\n",
      "train loss:0.8848808144929382\n",
      "train loss:0.9018805321207621\n",
      "train loss:0.7293467752146406\n",
      "train loss:0.788327652490438\n",
      "train loss:0.8464734703238602\n",
      "train loss:0.7541659676790773\n",
      "train loss:0.7891345491881968\n",
      "train loss:0.7564512975069906\n",
      "train loss:0.8964808291407703\n",
      "train loss:0.8379108014860404\n",
      "train loss:0.905365425164825\n",
      "train loss:0.821949948075875\n",
      "train loss:0.826217424217692\n",
      "train loss:0.6155016926854593\n",
      "train loss:1.0044030517607467\n",
      "train loss:0.7225359943335776\n",
      "train loss:0.8198153586739119\n",
      "train loss:0.9355493970413307\n",
      "train loss:0.9739563854716939\n",
      "train loss:0.8527574304596091\n",
      "train loss:0.9135354534256593\n",
      "train loss:0.8629181260135175\n",
      "train loss:1.0606039518607013\n",
      "train loss:0.8862413187475421\n",
      "train loss:0.9511684759628342\n",
      "train loss:0.8377445424900651\n",
      "train loss:0.8300138334920822\n",
      "train loss:0.8224069813080094\n",
      "train loss:0.914789734054833\n",
      "train loss:0.918272092084282\n",
      "train loss:0.761817751055039\n",
      "train loss:0.8993154406247154\n",
      "train loss:0.7793071184199114\n",
      "train loss:0.9196058222826045\n",
      "train loss:0.9491981642041099\n",
      "train loss:0.93351039799962\n",
      "train loss:0.9620718852286515\n",
      "train loss:0.7327902228902984\n",
      "train loss:0.8374157114893148\n",
      "train loss:1.0109340950545123\n",
      "train loss:0.9318265336969014\n",
      "train loss:0.792373947281177\n",
      "train loss:0.8085496612303545\n",
      "train loss:0.9430612370985252\n",
      "train loss:0.7119150544876929\n",
      "train loss:0.9546295697598045\n",
      "train loss:0.8605842845811769\n",
      "train loss:1.028409104030673\n",
      "train loss:0.8566880133985332\n",
      "train loss:0.8542411141728317\n",
      "train loss:0.9256755943969562\n",
      "train loss:0.7912293259539946\n",
      "train loss:0.6714684060037913\n",
      "train loss:0.7976326423710689\n",
      "train loss:0.8702118481491408\n",
      "train loss:0.9133919886601766\n",
      "train loss:0.9191423324066406\n",
      "train loss:0.7876037217027678\n",
      "train loss:0.8907240755613728\n",
      "train loss:0.8430852064875495\n",
      "train loss:0.7995312913008319\n",
      "train loss:0.9391619266107893\n",
      "train loss:0.8113154856795242\n",
      "train loss:0.7600903569681976\n",
      "train loss:0.90614420216969\n",
      "train loss:0.7294687123744928\n",
      "train loss:0.7066720378354548\n",
      "train loss:0.8442325252169822\n",
      "train loss:0.9169912715609045\n",
      "train loss:0.8404076650373037\n",
      "train loss:0.9239911192023351\n",
      "train loss:0.9773420509016936\n",
      "train loss:0.7706960689453536\n",
      "train loss:0.9858058301566011\n",
      "train loss:1.0363273466033542\n",
      "train loss:0.8530629037002878\n",
      "train loss:0.7043411270636277\n",
      "train loss:1.0180953363547072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.8947707543868634\n",
      "train loss:0.9124404174375954\n",
      "train loss:0.9107568514960321\n",
      "train loss:0.9883275829972594\n",
      "train loss:0.908258332434307\n",
      "train loss:0.8832507353334228\n",
      "=== epoch:17, train acc:0.998, test acc:0.993 ===\n",
      "train loss:0.9878207016613411\n",
      "train loss:0.9471653297516714\n",
      "train loss:1.0479217624314359\n",
      "train loss:0.9125796443610236\n",
      "train loss:0.7194188495639738\n",
      "train loss:0.9435063057619463\n",
      "train loss:0.9111834736674047\n",
      "train loss:0.7405931973894576\n",
      "train loss:0.9307378655112983\n",
      "train loss:0.9547074565840721\n",
      "train loss:0.7689276129994249\n",
      "train loss:0.8687172223263003\n",
      "train loss:0.8707088815824366\n",
      "train loss:0.9616951074451096\n",
      "train loss:0.8377848433485512\n",
      "train loss:0.8977428814410944\n",
      "train loss:0.9923031703629266\n",
      "train loss:0.9278837402800039\n",
      "train loss:0.7423701351355834\n",
      "train loss:0.7760383400189756\n",
      "train loss:0.9127737366218862\n",
      "train loss:0.8201255008239844\n",
      "train loss:0.8389771184295763\n",
      "train loss:0.8334443587823179\n",
      "train loss:0.9361546166734389\n",
      "train loss:0.8480754413521494\n",
      "train loss:1.0474185653457133\n",
      "train loss:0.8716703118974108\n",
      "train loss:0.8608256814359524\n",
      "train loss:0.9525154814997066\n",
      "train loss:0.9852120425904691\n",
      "train loss:0.9815875034795276\n",
      "train loss:0.9005813967305376\n",
      "train loss:0.8067843799265979\n",
      "train loss:0.8977307935900527\n",
      "train loss:0.8475506488771741\n",
      "train loss:0.7590050648723989\n",
      "train loss:0.6785241182904552\n",
      "train loss:0.8781060193998494\n",
      "train loss:0.8366562984257524\n",
      "train loss:0.7337364811787992\n",
      "train loss:0.7417804437894432\n",
      "train loss:1.003536291759825\n",
      "train loss:0.7911053398694884\n",
      "train loss:0.700738126157281\n",
      "train loss:0.7588290227154745\n",
      "train loss:0.9044059105051369\n",
      "train loss:0.7978922227163722\n",
      "train loss:0.9430288125408535\n",
      "train loss:0.8581266401944561\n",
      "train loss:0.8253313256094194\n",
      "train loss:0.8071462179120674\n",
      "train loss:0.8575989878644009\n",
      "train loss:0.8763918546011442\n",
      "train loss:0.7431589180240743\n",
      "train loss:0.8384142725208578\n",
      "train loss:0.833264928592313\n",
      "train loss:1.030040990596985\n",
      "train loss:0.7597192948113611\n",
      "train loss:0.8073376304865009\n",
      "train loss:0.852099932811212\n",
      "train loss:0.8096511394331108\n",
      "train loss:0.7347364367700483\n",
      "train loss:0.850183819297988\n",
      "train loss:0.8388120095697417\n",
      "train loss:1.0682503479277834\n",
      "train loss:0.8466157479494201\n",
      "train loss:0.7616809197588514\n",
      "train loss:0.7091538098330594\n",
      "train loss:0.8402194524692949\n",
      "train loss:0.9073172765233309\n",
      "train loss:0.9326219623114882\n",
      "train loss:0.906310762107516\n",
      "train loss:0.765251151085818\n",
      "train loss:0.9275200675090757\n",
      "train loss:0.9790340186063865\n",
      "train loss:0.896789228086096\n",
      "train loss:0.8789661024027441\n",
      "train loss:0.7462015331690798\n",
      "train loss:1.0713970686700434\n",
      "train loss:0.8567868246724977\n",
      "train loss:0.8718915286754964\n",
      "train loss:0.789536154970831\n",
      "train loss:0.9663385149181427\n",
      "train loss:0.8098811945662827\n",
      "train loss:0.7923666148302277\n",
      "train loss:0.9158960386832896\n",
      "train loss:0.9967897653566078\n",
      "train loss:0.8282033551546385\n",
      "train loss:0.8818180491740197\n",
      "train loss:0.913370896383462\n",
      "train loss:0.8340089991086032\n",
      "train loss:0.9904750270658096\n",
      "train loss:0.6504851178103934\n",
      "train loss:0.9109509657064733\n",
      "train loss:0.9418860569309118\n",
      "train loss:0.8035927309829153\n",
      "train loss:0.8650092520174703\n",
      "train loss:0.9125150331100723\n",
      "train loss:0.8031500842750381\n",
      "train loss:0.8772661601024159\n",
      "train loss:0.7904809991782352\n",
      "train loss:0.873397695389902\n",
      "train loss:0.7312826305953615\n",
      "train loss:0.8992285166512863\n",
      "train loss:0.8245028047837032\n",
      "train loss:1.0597250128385594\n",
      "train loss:0.8857340273986729\n",
      "train loss:0.9331438164973762\n",
      "train loss:0.7624712813087249\n",
      "train loss:1.0892076397538077\n",
      "train loss:0.9515852529277591\n",
      "train loss:0.8995483383409016\n",
      "train loss:1.0135807851824155\n",
      "train loss:0.8632516723069691\n",
      "train loss:0.9414313721086929\n",
      "train loss:0.7752244115066111\n",
      "train loss:0.8186535869443232\n",
      "train loss:1.1599504741059397\n",
      "train loss:1.1216452321839487\n",
      "train loss:0.8730435114965104\n",
      "train loss:0.6601427621073962\n",
      "train loss:0.7279647785937854\n",
      "train loss:0.8109762196261816\n",
      "train loss:0.8173938502017664\n",
      "train loss:0.8011366614215935\n",
      "train loss:0.8625270114568673\n",
      "train loss:0.7135135318252037\n",
      "train loss:1.023809070041726\n",
      "train loss:0.9830620200083888\n",
      "train loss:0.8669216217880799\n",
      "train loss:0.780606200441651\n",
      "train loss:0.9213852919054877\n",
      "train loss:0.793113239237217\n",
      "train loss:0.9135515129559466\n",
      "train loss:0.8847238237302458\n",
      "train loss:0.8703231642729824\n",
      "train loss:1.0294894364149796\n",
      "train loss:0.9174282607669475\n",
      "train loss:0.6844790898949349\n",
      "train loss:1.0581862809421605\n",
      "train loss:0.8850290776638957\n",
      "train loss:0.8313537503375553\n",
      "train loss:0.8961585295823457\n",
      "train loss:0.804339934549917\n",
      "train loss:0.8232865680181848\n",
      "train loss:0.8045921687884744\n",
      "train loss:0.8003716875834823\n",
      "train loss:0.7983040292157323\n",
      "train loss:0.8581316200013984\n",
      "train loss:0.8155816454307238\n",
      "train loss:0.7213757976394232\n",
      "train loss:0.8647060405277013\n",
      "train loss:0.846950759949478\n",
      "train loss:0.889015895844786\n",
      "train loss:0.8090249164020724\n",
      "train loss:0.9218220604945441\n",
      "train loss:1.039240607429881\n",
      "train loss:0.8100529974217932\n",
      "train loss:0.7833231749691992\n",
      "train loss:0.8038229836358289\n",
      "train loss:0.8239089778120973\n",
      "train loss:0.7638832100873961\n",
      "train loss:0.8590259645930091\n",
      "train loss:0.9403511412574946\n",
      "train loss:0.89642653622228\n",
      "train loss:0.9167652526417441\n",
      "train loss:0.7808668441410102\n",
      "train loss:0.948512488808621\n",
      "train loss:0.9309805308864935\n",
      "train loss:1.1472752078798987\n",
      "train loss:0.8094559538919861\n",
      "train loss:0.8246986433206316\n",
      "train loss:0.9717402596323081\n",
      "train loss:0.8307501566041444\n",
      "train loss:0.7199118002181263\n",
      "train loss:0.8764292180059262\n",
      "train loss:0.9004174033069718\n",
      "train loss:0.8179304994069625\n",
      "train loss:0.9163580932591878\n",
      "train loss:0.8474091644070165\n",
      "train loss:0.9319000691496475\n",
      "train loss:0.9168633833193733\n",
      "train loss:0.9754502152736461\n",
      "train loss:0.9282311675878661\n",
      "train loss:0.7878470626323099\n",
      "train loss:1.0071790810554533\n",
      "train loss:0.9034535854530392\n",
      "train loss:0.9679716194913388\n",
      "train loss:0.9348205685641754\n",
      "train loss:0.8310192195697158\n",
      "train loss:0.6814292924994322\n",
      "train loss:0.8914500855533646\n",
      "train loss:0.8500101517636477\n",
      "train loss:0.76831849504636\n",
      "train loss:0.9280704959935053\n",
      "train loss:0.7250161372726042\n",
      "train loss:0.8846868068157385\n",
      "train loss:1.0103821240089326\n",
      "train loss:0.8585082904057605\n",
      "train loss:0.826148020677295\n",
      "train loss:0.8416804185243868\n",
      "train loss:0.7382675037720209\n",
      "train loss:0.8139155162309244\n",
      "train loss:0.9448212886645573\n",
      "train loss:0.6565752144359377\n",
      "train loss:0.9069078633195802\n",
      "train loss:0.9053843091986659\n",
      "train loss:0.807168974988529\n",
      "train loss:0.845093236267982\n",
      "train loss:0.8776935952036352\n",
      "train loss:0.8136797998172711\n",
      "train loss:0.8154216195416368\n",
      "train loss:0.8201750166042109\n",
      "train loss:0.9310387952046252\n",
      "train loss:0.8198701296941864\n",
      "train loss:0.8354491289272221\n",
      "train loss:0.7975640735733793\n",
      "train loss:0.8403130023202765\n",
      "train loss:0.8923779936970331\n",
      "train loss:1.0185073969243374\n",
      "train loss:0.881052248659731\n",
      "train loss:0.8203037663519097\n",
      "train loss:0.9388670187329905\n",
      "train loss:0.7722075300290353\n",
      "train loss:0.8808877352743526\n",
      "train loss:0.9755067154634165\n",
      "train loss:0.9125207334965385\n",
      "train loss:0.8053786964179918\n",
      "train loss:0.8745188394581613\n",
      "train loss:0.8554213758883538\n",
      "train loss:0.9475955687435348\n",
      "train loss:0.8230571306616556\n",
      "train loss:0.8515926463389932\n",
      "train loss:0.6166986112152992\n",
      "train loss:0.6328028184120293\n",
      "train loss:0.9021127492390167\n",
      "train loss:1.0026239322102433\n",
      "train loss:0.8303121649794686\n",
      "train loss:0.8495233870110589\n",
      "train loss:1.093703449299713\n",
      "train loss:0.8490830819322093\n",
      "train loss:0.925025747508288\n",
      "train loss:0.8881722712024134\n",
      "train loss:0.9318877289656169\n",
      "train loss:0.9836834501741909\n",
      "train loss:0.6768600712345559\n",
      "train loss:1.1051970963315143\n",
      "train loss:0.7895527049465072\n",
      "train loss:0.7932503105723522\n",
      "train loss:0.8029673635632929\n",
      "train loss:0.8098356049648379\n",
      "train loss:0.814360788548307\n",
      "train loss:0.7452223023610551\n",
      "train loss:0.845591457903782\n",
      "train loss:0.9487286961697768\n",
      "train loss:0.9642623074465412\n",
      "train loss:0.9347165141945082\n",
      "train loss:0.6710923799026061\n",
      "train loss:0.8825148831347444\n",
      "train loss:0.9133053239110679\n",
      "train loss:0.9823442600191082\n",
      "train loss:0.7424891389815167\n",
      "train loss:0.8528993150432104\n",
      "train loss:0.8274383813790263\n",
      "train loss:0.7854753469990581\n",
      "train loss:0.8648335064358355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.8409480035771816\n",
      "train loss:0.8415696214637184\n",
      "train loss:0.8291752923570822\n",
      "train loss:0.8012523801542187\n",
      "train loss:0.7763457414457682\n",
      "train loss:0.6955773918802874\n",
      "train loss:0.9790840238178792\n",
      "train loss:0.8193500795118179\n",
      "train loss:0.8940282569606954\n",
      "train loss:0.8289825458530412\n",
      "train loss:0.8496130392344005\n",
      "train loss:0.9334784410498947\n",
      "train loss:0.675439462738424\n",
      "train loss:0.8877041604018611\n",
      "train loss:0.9235294062562809\n",
      "train loss:0.8541084112527194\n",
      "train loss:0.72682690790004\n",
      "train loss:0.8103880526632927\n",
      "train loss:0.8868599853091566\n",
      "train loss:0.774448143044816\n",
      "train loss:0.7692892598323655\n",
      "train loss:0.8073551607541841\n",
      "train loss:0.8112379624165571\n",
      "train loss:0.8124485162410409\n",
      "train loss:0.911355049223992\n",
      "train loss:0.9965473358193282\n",
      "train loss:0.7907290178148325\n",
      "train loss:0.9290943234552302\n",
      "train loss:0.8461269884866609\n",
      "train loss:0.8595628442529913\n",
      "train loss:0.724081336458849\n",
      "train loss:0.7380453903466883\n",
      "train loss:0.8906789918248543\n",
      "train loss:0.893201881924902\n",
      "train loss:0.6944731984954796\n",
      "train loss:0.8860539704375282\n",
      "train loss:0.9655251800713536\n",
      "train loss:0.8632266796417367\n",
      "train loss:1.0496413319493312\n",
      "train loss:0.8755944420858558\n",
      "train loss:0.742682237256294\n",
      "train loss:0.9412318354369689\n",
      "train loss:0.7596596774012854\n",
      "train loss:0.7480933240909315\n",
      "train loss:0.9401345500674967\n",
      "train loss:0.8659522696853873\n",
      "train loss:0.8205480150472566\n",
      "train loss:0.8209601277158156\n",
      "train loss:0.911771358134744\n",
      "train loss:1.0055865918381481\n",
      "train loss:0.7593917699307963\n",
      "train loss:0.8047525148782784\n",
      "train loss:0.9379204549142672\n",
      "train loss:0.8728351719271591\n",
      "train loss:0.8531240945686557\n",
      "train loss:0.8565852222547895\n",
      "train loss:0.9676810180916651\n",
      "train loss:0.9214074451291068\n",
      "train loss:0.690231780714718\n",
      "train loss:0.9933261532722767\n",
      "train loss:0.9134283174140018\n",
      "train loss:0.8032645768846847\n",
      "train loss:0.7942299249042623\n",
      "train loss:0.8808695170571127\n",
      "train loss:0.8442282524383112\n",
      "train loss:0.8934241823673296\n",
      "train loss:0.9826379671529142\n",
      "train loss:0.818310012636177\n",
      "train loss:0.8562127021402602\n",
      "train loss:0.9292986270713627\n",
      "train loss:0.8878293763104693\n",
      "train loss:0.9880462014586455\n",
      "train loss:0.7306600601831548\n",
      "train loss:0.8876050930874656\n",
      "train loss:0.8485716406843679\n",
      "train loss:0.8114944402392095\n",
      "train loss:0.8408327814961392\n",
      "train loss:0.8286707558228941\n",
      "train loss:0.7995939905673741\n",
      "train loss:0.9851466154255492\n",
      "train loss:0.8060952004259917\n",
      "train loss:0.9454453172008164\n",
      "train loss:0.7763757299298801\n",
      "train loss:0.8629031726396208\n",
      "train loss:0.9167581492661994\n",
      "train loss:0.7971342997289611\n",
      "train loss:0.7812871515037411\n",
      "train loss:0.9182752787907813\n",
      "train loss:0.8641080435046128\n",
      "train loss:0.8578593538214347\n",
      "train loss:0.8633250768858483\n",
      "train loss:0.7723438773563954\n",
      "train loss:0.9118764575394512\n",
      "train loss:1.0270119882797644\n",
      "train loss:0.9026435496662384\n",
      "train loss:0.9385252521216277\n",
      "train loss:0.8425669080200183\n",
      "train loss:0.9134548222130385\n",
      "train loss:1.0519318171688208\n",
      "train loss:0.8943806164662673\n",
      "train loss:0.9206782000328145\n",
      "train loss:0.9119006808836477\n",
      "train loss:0.9081588378340153\n",
      "train loss:0.8789100557041805\n",
      "train loss:0.8546077129688031\n",
      "train loss:0.7263262315785162\n",
      "train loss:0.7513662230470932\n",
      "train loss:0.8192565496723584\n",
      "train loss:0.7241796555785812\n",
      "train loss:0.8190045364237983\n",
      "train loss:0.7470763032106972\n",
      "train loss:0.831884907463406\n",
      "train loss:0.9380208893416078\n",
      "train loss:0.8575191581603323\n",
      "train loss:0.926180335534817\n",
      "train loss:0.9356205346248809\n",
      "train loss:0.8756733942169689\n",
      "train loss:0.8474470388421443\n",
      "train loss:0.7893606156167771\n",
      "train loss:0.7141930715361526\n",
      "train loss:0.9891653526299272\n",
      "train loss:0.8030730023207142\n",
      "train loss:0.9422121162578422\n",
      "train loss:1.0393060392370794\n",
      "train loss:0.850881508687198\n",
      "train loss:0.7819709109357924\n",
      "train loss:0.8317852668874369\n",
      "train loss:0.8146683394423277\n",
      "train loss:0.9352679807548521\n",
      "train loss:0.9650096498999797\n",
      "train loss:0.8757655687522938\n",
      "train loss:0.8115435088647895\n",
      "train loss:0.9231263194515017\n",
      "train loss:0.8985856015120379\n",
      "train loss:0.7838218210101009\n",
      "train loss:0.9558621511194338\n",
      "train loss:0.8133294657407595\n",
      "train loss:0.9059439301606281\n",
      "train loss:0.9790406500804939\n",
      "train loss:0.9039143651443295\n",
      "train loss:0.8705101614114201\n",
      "train loss:1.0152605652042548\n",
      "train loss:0.7967304405975514\n",
      "train loss:1.079404977930323\n",
      "train loss:0.7440520457719237\n",
      "train loss:0.9013785310035546\n",
      "train loss:0.8836139840572528\n",
      "train loss:0.9453940195787982\n",
      "train loss:0.9489909634363041\n",
      "train loss:0.8758884102064252\n",
      "train loss:0.8573959876835054\n",
      "train loss:0.7041765133059298\n",
      "train loss:0.9149137652376126\n",
      "train loss:0.87861667451502\n",
      "train loss:0.9149752171107073\n",
      "train loss:0.7462167489562352\n",
      "train loss:0.705827206744005\n",
      "train loss:0.8692682764744777\n",
      "train loss:0.8953640379828629\n",
      "train loss:0.7954165792281128\n",
      "train loss:0.7605763810945093\n",
      "train loss:0.8532425888927455\n",
      "train loss:0.9150118988361399\n",
      "train loss:0.8675405180319851\n",
      "train loss:0.9710581821493193\n",
      "train loss:0.8544268338355225\n",
      "train loss:0.876190500050422\n",
      "train loss:0.9201954764166415\n",
      "train loss:0.9483267422709623\n",
      "train loss:0.8545561531000935\n",
      "train loss:0.7200853253483156\n",
      "train loss:0.8594629528154245\n",
      "train loss:0.9020995596013457\n",
      "train loss:0.7823560464174357\n",
      "train loss:0.7962743571767047\n",
      "train loss:0.859069031751888\n",
      "train loss:0.7509444330613967\n",
      "train loss:0.8732538923202832\n",
      "train loss:0.9030284417911011\n",
      "train loss:0.8513239997668218\n",
      "train loss:0.8303430500042333\n",
      "train loss:0.9911291524077513\n",
      "train loss:0.8760930740563385\n",
      "train loss:0.8969615687353227\n",
      "train loss:0.8655805892228666\n",
      "train loss:0.8719210235276362\n",
      "train loss:0.8632105884045884\n",
      "train loss:0.8828090644054001\n",
      "train loss:0.7604317895171835\n",
      "train loss:0.8605926812458271\n",
      "train loss:0.8792082291081194\n",
      "train loss:0.9504184300220041\n",
      "train loss:0.8694747307031446\n",
      "train loss:0.807247714707707\n",
      "train loss:0.870925074844441\n",
      "train loss:0.9299055233010176\n",
      "train loss:0.9535597367090469\n",
      "train loss:0.9215210258422332\n",
      "train loss:0.7932854230594667\n",
      "train loss:0.7934074171217262\n",
      "train loss:0.9010667068008088\n",
      "train loss:0.7363380651954303\n",
      "train loss:1.035001729209851\n",
      "train loss:0.7673304724582275\n",
      "train loss:0.7680757672228675\n",
      "train loss:0.8481746229758649\n",
      "train loss:0.9540454172332686\n",
      "train loss:0.9964877186053163\n",
      "train loss:0.8588686587220254\n",
      "train loss:0.8756196651663073\n",
      "train loss:0.9165279885984091\n",
      "train loss:0.8428281753051516\n",
      "train loss:1.0478134632251757\n",
      "train loss:0.9487534847884668\n",
      "train loss:0.789938410636362\n",
      "train loss:0.9014243488424947\n",
      "train loss:0.8915792947506469\n",
      "train loss:0.8685073960934621\n",
      "train loss:0.7214673101172246\n",
      "train loss:0.8068687810286697\n",
      "train loss:0.8590868137705084\n",
      "train loss:0.747753808837309\n",
      "train loss:0.830390570270809\n",
      "train loss:0.8938570485176947\n",
      "train loss:0.8267106861902039\n",
      "train loss:0.815746502167103\n",
      "train loss:0.8286273429314943\n",
      "train loss:0.7816836312923929\n",
      "train loss:0.8394015450788305\n",
      "train loss:0.9157304271087072\n",
      "train loss:1.061128716136154\n",
      "train loss:0.823828487780809\n",
      "train loss:0.9795194950683027\n",
      "train loss:0.8833000795558279\n",
      "train loss:0.830946918634033\n",
      "train loss:0.8169952389704797\n",
      "train loss:0.8674159172527106\n",
      "train loss:0.8696192594408743\n",
      "train loss:0.852046977808916\n",
      "train loss:0.8067289461716001\n",
      "train loss:0.7737923693221515\n",
      "train loss:0.6972398640908282\n",
      "train loss:0.8522722259592393\n",
      "train loss:0.9396051943373613\n",
      "train loss:0.8234127545757514\n",
      "train loss:0.8816958787753721\n",
      "train loss:0.8123891091586308\n",
      "train loss:0.8992356871373791\n",
      "train loss:0.7727568393254981\n",
      "train loss:0.888362715600472\n",
      "train loss:0.8825674962856409\n",
      "train loss:0.9322529602566635\n",
      "train loss:0.9686653911310037\n",
      "train loss:0.8260394546202288\n",
      "train loss:0.9466522794368295\n",
      "train loss:0.9391315710089194\n",
      "train loss:0.7001652455100218\n",
      "train loss:0.8637695657158069\n",
      "train loss:0.8626132051678392\n",
      "train loss:0.872331260243879\n",
      "train loss:0.9015103687588855\n",
      "train loss:0.7865152995654602\n",
      "train loss:0.8163238824605678\n",
      "train loss:0.8276223280811322\n",
      "train loss:0.8751692701241819\n",
      "train loss:0.7521319203874807\n",
      "train loss:0.8999512405011285\n",
      "train loss:0.8397391984334172\n",
      "train loss:0.8731816411954675\n",
      "train loss:0.7109250342035248\n",
      "train loss:0.8699566547739478\n",
      "train loss:0.7760464498269016\n",
      "train loss:0.8781340701198179\n",
      "train loss:0.8037288890665796\n",
      "train loss:0.8640480769971387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.7490383734534839\n",
      "train loss:0.8444088090749682\n",
      "train loss:0.8544620334920495\n",
      "train loss:0.903564231746865\n",
      "train loss:0.8735728946414102\n",
      "train loss:0.7916690064038793\n",
      "train loss:0.8000579733344749\n",
      "train loss:0.8933388913750403\n",
      "train loss:1.0385753992667008\n",
      "train loss:1.1013118387663925\n",
      "train loss:0.8002682265842145\n",
      "train loss:0.8258526240455352\n",
      "train loss:0.9532403169212824\n",
      "train loss:0.813951726526403\n",
      "train loss:0.9131798122214808\n",
      "train loss:0.7850958239848626\n",
      "train loss:0.8864716644995707\n",
      "train loss:0.8742871565536635\n",
      "train loss:0.9354568830639248\n",
      "train loss:0.9349753113500776\n",
      "train loss:1.0987328607204128\n",
      "train loss:0.9044892297748939\n",
      "train loss:0.8210419494502665\n",
      "train loss:0.8985583826652156\n",
      "train loss:0.8571249567457933\n",
      "train loss:0.9085287138361735\n",
      "train loss:0.7998085468159172\n",
      "train loss:0.8876317227494279\n",
      "train loss:1.0819821647553234\n",
      "train loss:1.0125410856518524\n",
      "train loss:0.9134001113051644\n",
      "train loss:0.8009920551492705\n",
      "train loss:0.7621684934405077\n",
      "train loss:0.9334437315302104\n",
      "train loss:0.8276581511881094\n",
      "train loss:0.912722274523172\n",
      "train loss:0.8687917687652741\n",
      "train loss:0.875879966097923\n",
      "train loss:0.7694274644338069\n",
      "train loss:0.7587896751345065\n",
      "train loss:0.9419582196782796\n",
      "train loss:0.9126338438318148\n",
      "train loss:0.7284943221537346\n",
      "train loss:0.7685091598243656\n",
      "train loss:0.8317886541668131\n",
      "train loss:0.6960807985964985\n",
      "train loss:0.9376634984149468\n",
      "train loss:0.9106434769361783\n",
      "train loss:0.92595257757452\n",
      "train loss:0.8807079158568384\n",
      "train loss:0.9618072053140171\n",
      "train loss:0.9670147311103087\n",
      "train loss:0.8628424436290723\n",
      "train loss:0.7765190945501328\n",
      "train loss:0.9097330755294984\n",
      "train loss:0.7953247101907214\n",
      "train loss:0.7808946247678017\n",
      "train loss:0.9642620874674896\n",
      "=== epoch:18, train acc:0.998, test acc:0.992 ===\n",
      "train loss:0.783833706254405\n",
      "train loss:0.8866720461949404\n",
      "train loss:0.8313059278598287\n",
      "train loss:0.8678318072065193\n",
      "train loss:0.8361656061093676\n",
      "train loss:1.0141119930796798\n",
      "train loss:0.9667237746549512\n",
      "train loss:0.8549634484559653\n",
      "train loss:0.7805018633994742\n",
      "train loss:0.9370376220212596\n",
      "train loss:0.8444671806253493\n",
      "train loss:0.6902565651804352\n",
      "train loss:0.8071576996340479\n",
      "train loss:0.777466606907426\n",
      "train loss:0.7243652369751807\n",
      "train loss:0.9367800832777807\n",
      "train loss:0.7424516891755353\n",
      "train loss:0.922323503450836\n",
      "train loss:0.7791213494516224\n",
      "train loss:0.9876655282076974\n",
      "train loss:0.8015979386499784\n",
      "train loss:0.9007467868410086\n",
      "train loss:0.8020710764214827\n",
      "train loss:0.7546420995492976\n",
      "train loss:0.7767250380371503\n",
      "train loss:0.685434999444642\n",
      "train loss:0.7285024941835171\n",
      "train loss:0.8984206000170518\n",
      "train loss:0.8249544943978077\n",
      "train loss:0.7578204757381735\n",
      "train loss:0.8133141068611818\n",
      "train loss:0.9947728047367461\n",
      "train loss:0.7752985263328064\n",
      "train loss:0.7980637442968986\n",
      "train loss:0.8162490923559776\n",
      "train loss:0.9316549951887416\n",
      "train loss:0.9693281876210338\n",
      "train loss:0.7909702264371206\n",
      "train loss:0.8337123121681306\n",
      "train loss:0.8696675313016435\n",
      "train loss:0.8660616098589309\n",
      "train loss:0.8740167800094533\n",
      "train loss:0.7602520544459569\n",
      "train loss:0.6544352100732204\n",
      "train loss:0.9924255547061104\n",
      "train loss:0.7827648855588043\n",
      "train loss:0.9434508975153486\n",
      "train loss:0.9226042147650146\n",
      "train loss:0.7750367065544934\n",
      "train loss:0.8447517141063429\n",
      "train loss:0.8780232033158047\n",
      "train loss:0.8647291134985824\n",
      "train loss:0.9129725646785687\n",
      "train loss:0.8457073942779954\n",
      "train loss:0.758231404155075\n",
      "train loss:0.8471068332465395\n",
      "train loss:0.9793334136904173\n",
      "train loss:0.8386672057995416\n",
      "train loss:0.9417698808495086\n",
      "train loss:0.9216990626251702\n",
      "train loss:0.7408600913881861\n",
      "train loss:0.9502261229830156\n",
      "train loss:1.00203485725291\n",
      "train loss:0.7993950532422215\n",
      "train loss:0.8714185248699579\n",
      "train loss:0.8912873609476355\n",
      "train loss:0.8585851331608135\n",
      "train loss:0.9182972611273255\n",
      "train loss:0.8808282111002715\n",
      "train loss:0.7512827973787176\n",
      "train loss:0.8951112898230146\n",
      "train loss:0.9723034269492387\n",
      "train loss:0.9438633493334153\n",
      "train loss:0.820844563780821\n",
      "train loss:0.7701559936950233\n",
      "train loss:0.7264370600942804\n",
      "train loss:0.8865438167808262\n",
      "train loss:0.7842501340172916\n",
      "train loss:0.8514944342286703\n",
      "train loss:0.8746381849454522\n",
      "train loss:1.0073451210185165\n",
      "train loss:0.86116341159415\n",
      "train loss:1.0484986626201915\n",
      "train loss:0.8473673539271185\n",
      "train loss:0.8430563549343942\n",
      "train loss:0.8402602527196125\n",
      "train loss:0.8238118714300089\n",
      "train loss:0.9294377262257\n",
      "train loss:0.9454368087255517\n",
      "train loss:0.8279104355992274\n",
      "train loss:0.8714521558954857\n",
      "train loss:0.9757914537173992\n",
      "train loss:1.0357752932152573\n",
      "train loss:0.9848828250468108\n",
      "train loss:0.9655643533027469\n",
      "train loss:0.8171566590152873\n",
      "train loss:0.8870286043872728\n",
      "train loss:0.9909629249298577\n",
      "train loss:0.9112486361066946\n",
      "train loss:0.7604068289776227\n",
      "train loss:0.9307390912692252\n",
      "train loss:0.8541648444837077\n",
      "train loss:0.7636903180585172\n",
      "train loss:0.8130172503399703\n",
      "train loss:0.7977908044565274\n",
      "train loss:0.694940698721899\n",
      "train loss:0.9690248448763925\n",
      "train loss:0.8748844394681692\n",
      "train loss:0.8331345072184212\n",
      "train loss:0.7776350129340786\n",
      "train loss:0.9040699830429497\n",
      "train loss:0.8573802429987002\n",
      "train loss:0.8213981042634723\n",
      "train loss:0.7564905354182477\n",
      "train loss:0.882175497215173\n",
      "train loss:0.8662301392021202\n",
      "train loss:0.8161630202269702\n",
      "train loss:0.958876451085127\n",
      "train loss:0.880767269631081\n",
      "train loss:1.0037976556399344\n",
      "train loss:0.8517479869118071\n",
      "train loss:0.6615563547192379\n",
      "train loss:0.9396253012382367\n",
      "train loss:0.928784580751868\n",
      "train loss:0.7720389619757814\n",
      "train loss:0.9504662695821534\n",
      "train loss:0.8553504013485577\n",
      "train loss:0.9250160672780002\n",
      "train loss:0.9135074610676817\n",
      "train loss:0.7272396679968068\n",
      "train loss:0.8688284491750171\n",
      "train loss:0.6497852152405565\n",
      "train loss:0.9429688025739871\n",
      "train loss:0.7288076896396903\n",
      "train loss:0.8443653621708731\n",
      "train loss:0.7439434800276763\n",
      "train loss:0.760863005812373\n",
      "train loss:0.7636681086563325\n",
      "train loss:0.85116514686215\n",
      "train loss:0.844005492939912\n",
      "train loss:0.9795529179120217\n",
      "train loss:0.6640354003143653\n",
      "train loss:0.6117918597593606\n",
      "train loss:0.8331762999585642\n",
      "train loss:0.8197390479386816\n",
      "train loss:0.8731543505229727\n",
      "train loss:0.7551820547808364\n",
      "train loss:0.7156153288150991\n",
      "train loss:0.9551810330752559\n",
      "train loss:0.8431250312426636\n",
      "train loss:0.6909764049240326\n",
      "train loss:0.7433795949656234\n",
      "train loss:0.8223176817951168\n",
      "train loss:0.784441126963505\n",
      "train loss:0.8083756005417762\n",
      "train loss:0.7533301719572258\n",
      "train loss:0.8695530866795695\n",
      "train loss:0.8755364723955826\n",
      "train loss:0.8702447229316259\n",
      "train loss:0.961496214707021\n",
      "train loss:0.7566330825762342\n",
      "train loss:0.9431258173882161\n",
      "train loss:0.775321921464935\n",
      "train loss:0.771470258092484\n",
      "train loss:0.8749607010251249\n",
      "train loss:0.7833902187577181\n",
      "train loss:0.8477974928872498\n",
      "train loss:0.9738553022090193\n",
      "train loss:0.844946197357106\n",
      "train loss:0.6656132318258493\n",
      "train loss:1.0522585825534032\n",
      "train loss:0.8677339777346673\n",
      "train loss:0.861665902525747\n",
      "train loss:0.7990217518254973\n",
      "train loss:1.0058435169962114\n",
      "train loss:0.7572650188201392\n",
      "train loss:0.7347082261195855\n",
      "train loss:0.8230052959320495\n",
      "train loss:0.7410450292196099\n",
      "train loss:0.9252911252296486\n",
      "train loss:0.8523367603080935\n",
      "train loss:0.9197854220469962\n",
      "train loss:0.8910325154289858\n",
      "train loss:0.9283134085775554\n",
      "train loss:0.9972899055449622\n",
      "train loss:0.7915091567565392\n",
      "train loss:1.0223291838993063\n",
      "train loss:0.7718963052695887\n",
      "train loss:0.8948331827800149\n",
      "train loss:0.9120832397101514\n",
      "train loss:0.68843635016688\n",
      "train loss:0.8178117211925602\n",
      "train loss:0.8272584005500137\n",
      "train loss:0.9617565968020391\n",
      "train loss:0.850777336290231\n",
      "train loss:0.831530081936391\n",
      "train loss:1.0144657407621012\n",
      "train loss:0.779221874722586\n",
      "train loss:0.955567912125063\n",
      "train loss:0.9039453467151568\n",
      "train loss:0.8134485361249305\n",
      "train loss:0.9066561817347554\n",
      "train loss:0.815889734047823\n",
      "train loss:0.7629797610025196\n",
      "train loss:0.9720154913651252\n",
      "train loss:0.8677952666650119\n",
      "train loss:0.8702183933821545\n",
      "train loss:0.7378964924665032\n",
      "train loss:0.9442107659183059\n",
      "train loss:0.6708885414714938\n",
      "train loss:0.7685290119976982\n",
      "train loss:0.9222647998450654\n",
      "train loss:0.9624333931495502\n",
      "train loss:0.8241143155521615\n",
      "train loss:0.8142494386338669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.9375151128106318\n",
      "train loss:0.9129220659980102\n",
      "train loss:0.8561119497176936\n",
      "train loss:0.7352636064175481\n",
      "train loss:0.8610074608296968\n",
      "train loss:0.9036863905744991\n",
      "train loss:0.8718161514802554\n",
      "train loss:0.7752242686927789\n",
      "train loss:0.6703705523097306\n",
      "train loss:0.9311041158112877\n",
      "train loss:1.0366290397791555\n",
      "train loss:1.0405935743265373\n",
      "train loss:0.8068281977849141\n",
      "train loss:1.0198776526660958\n",
      "train loss:0.7903980427319035\n",
      "train loss:0.8108849358063884\n",
      "train loss:0.782936913691791\n",
      "train loss:0.9847117615950648\n",
      "train loss:0.9384654353976459\n",
      "train loss:0.9010509662856069\n",
      "train loss:0.8456122227398168\n",
      "train loss:0.7143104967661057\n",
      "train loss:0.8661379563039209\n",
      "train loss:0.9971481287604184\n",
      "train loss:0.7434390671000439\n",
      "train loss:0.947706301308638\n",
      "train loss:0.7602027358418307\n",
      "train loss:1.0515093288974569\n",
      "train loss:0.7564346510648922\n",
      "train loss:0.7583744772699643\n",
      "train loss:1.0219619911879498\n",
      "train loss:0.7647459314883581\n",
      "train loss:0.8223585672167826\n",
      "train loss:0.9212963050383369\n",
      "train loss:1.0336975409999674\n",
      "train loss:0.9380429208774421\n",
      "train loss:0.9375034083857579\n",
      "train loss:0.7042135397402791\n",
      "train loss:0.873138452948349\n",
      "train loss:0.9673287980392835\n",
      "train loss:0.9340602050218664\n",
      "train loss:0.8247936000488933\n",
      "train loss:0.8488915796237676\n",
      "train loss:0.7109292694524093\n",
      "train loss:0.7471086422825354\n",
      "train loss:0.801264286237526\n",
      "train loss:0.850075690440145\n",
      "train loss:0.8607563766165641\n",
      "train loss:0.8320798617292883\n",
      "train loss:0.8135037315738911\n",
      "train loss:0.7650253862814431\n",
      "train loss:0.7804594518959947\n",
      "train loss:0.8433987361369514\n",
      "train loss:0.8772719987122194\n",
      "train loss:0.8617839169860747\n",
      "train loss:0.8861336711397005\n",
      "train loss:0.8324440240544222\n",
      "train loss:1.0015767837221818\n",
      "train loss:0.9636325139742443\n",
      "train loss:0.8257015608372305\n",
      "train loss:0.8198165322388702\n",
      "train loss:1.0938956931686246\n",
      "train loss:0.80663204209361\n",
      "train loss:0.8750967274474403\n",
      "train loss:0.9022352856875273\n",
      "train loss:0.7674361054819266\n",
      "train loss:0.8080321107124473\n",
      "train loss:0.8515053934677042\n",
      "train loss:0.8769718730662026\n",
      "train loss:0.7511486424458937\n",
      "train loss:0.9089521168781117\n",
      "train loss:0.7769578315259144\n",
      "train loss:0.8944727165473185\n",
      "train loss:0.9326951872252854\n",
      "train loss:0.7931789758293335\n",
      "train loss:1.0216978268433743\n",
      "train loss:0.8458968497279232\n",
      "train loss:0.8517210319082393\n",
      "train loss:0.7771553278445098\n",
      "train loss:0.9062181590737883\n",
      "train loss:0.8979343280289439\n",
      "train loss:0.8201383831545587\n",
      "train loss:0.8547858209614695\n",
      "train loss:0.8680267546193302\n",
      "train loss:0.8542524229519093\n",
      "train loss:0.9546416875616752\n",
      "train loss:0.901923258742938\n",
      "train loss:0.9324702615918674\n",
      "train loss:0.8295601720474015\n",
      "train loss:0.8984193481491733\n",
      "train loss:0.8057645316174152\n",
      "train loss:0.9062780113217008\n",
      "train loss:0.8903488739966814\n",
      "train loss:0.8575229233472527\n",
      "train loss:1.0958482200287218\n",
      "train loss:0.9466248946826505\n",
      "train loss:0.820764476213062\n",
      "train loss:0.9131270272289597\n",
      "train loss:0.79184426754925\n",
      "train loss:0.8415849626006701\n",
      "train loss:0.8882121734791788\n",
      "train loss:1.0026918986429443\n",
      "train loss:0.9465526245043229\n",
      "train loss:0.8821169545830945\n",
      "train loss:0.7903012566849749\n",
      "train loss:1.0240437879397266\n",
      "train loss:0.872746446713241\n",
      "train loss:0.9309367717157963\n",
      "train loss:0.8508735049748327\n",
      "train loss:0.8227011767774078\n",
      "train loss:0.9071885101367846\n",
      "train loss:0.9564154385884792\n",
      "train loss:0.8427278097859436\n",
      "train loss:0.9417145533158612\n",
      "train loss:1.016228674176569\n",
      "train loss:0.9127607763211871\n",
      "train loss:0.952450700585374\n",
      "train loss:0.9546446027247528\n",
      "train loss:0.8417174857751626\n",
      "train loss:0.9607729677377707\n",
      "train loss:0.8305039714162322\n",
      "train loss:0.8171943688664122\n",
      "train loss:0.8076374309115723\n",
      "train loss:0.9082638329545848\n",
      "train loss:0.9024585155272057\n",
      "train loss:0.8471998488662925\n",
      "train loss:0.8757706445737782\n",
      "train loss:0.8438768388341121\n",
      "train loss:1.014274277090879\n",
      "train loss:0.8084305350241288\n",
      "train loss:0.8161769011042391\n",
      "train loss:0.8363977286613594\n",
      "train loss:0.9194152865038113\n",
      "train loss:0.8679379330438027\n",
      "train loss:0.8003451857538212\n",
      "train loss:0.9271543518621747\n",
      "train loss:0.8890193740859891\n",
      "train loss:0.8522452900682072\n",
      "train loss:0.870360106040989\n",
      "train loss:0.7809371664957407\n",
      "train loss:0.7996789929119608\n",
      "train loss:0.8149636810468253\n",
      "train loss:0.8160097422184275\n",
      "train loss:0.8156270000154119\n",
      "train loss:1.003406672187882\n",
      "train loss:0.9798498373876668\n",
      "train loss:0.8671621783473078\n",
      "train loss:0.8315700731447749\n",
      "train loss:0.7872110077149742\n",
      "train loss:0.8533101986418338\n",
      "train loss:0.9227576915598091\n",
      "train loss:0.8040485130016225\n",
      "train loss:0.6870851917240082\n",
      "train loss:0.9354064795818822\n",
      "train loss:0.8953191408881537\n",
      "train loss:0.9949244931913561\n",
      "train loss:0.8871734264111842\n",
      "train loss:0.8300411269191259\n",
      "train loss:1.0662274701015018\n",
      "train loss:1.0165778028215144\n",
      "train loss:1.0430827706504227\n",
      "train loss:0.9111463028724859\n",
      "train loss:0.7487205509317092\n",
      "train loss:0.8229871675508056\n",
      "train loss:0.8082100833339472\n",
      "train loss:0.8654439554072456\n",
      "train loss:1.0363581383305136\n",
      "train loss:0.8291194785682785\n",
      "train loss:0.7997012871383987\n",
      "train loss:0.7054842432702699\n",
      "train loss:0.8062183749570468\n",
      "train loss:0.8073203651398154\n",
      "train loss:0.8280533106316732\n",
      "train loss:0.7633088023357917\n",
      "train loss:0.8914964237665076\n",
      "train loss:0.8420559313041209\n",
      "train loss:0.9008217768943924\n",
      "train loss:0.7969504410180699\n",
      "train loss:0.8514943108774531\n",
      "train loss:0.9256361369667009\n",
      "train loss:0.8174492567193381\n",
      "train loss:0.7645239233109712\n",
      "train loss:0.806847654167474\n",
      "train loss:0.9009149346996586\n",
      "train loss:0.992804554088479\n",
      "train loss:0.7960860800695368\n",
      "train loss:0.9290799990513163\n",
      "train loss:0.9600650405654627\n",
      "train loss:0.7617562332161745\n",
      "train loss:0.9309004292449266\n",
      "train loss:0.8549485689637748\n",
      "train loss:0.7862024902504848\n",
      "train loss:0.7456138354069834\n",
      "train loss:0.917634440345385\n",
      "train loss:0.9000652750679948\n",
      "train loss:0.9742614527839037\n",
      "train loss:0.8434040415496332\n",
      "train loss:0.8020525084545334\n",
      "train loss:0.7907253839451953\n",
      "train loss:0.9075004493376188\n",
      "train loss:0.8055258447975799\n",
      "train loss:0.7586799746464101\n",
      "train loss:0.8321738893864294\n",
      "train loss:0.8625412180146514\n",
      "train loss:0.7835168141672568\n",
      "train loss:0.840300220506854\n",
      "train loss:0.9454812837672919\n",
      "train loss:0.717576921947679\n",
      "train loss:0.8569754650894543\n",
      "train loss:0.913692645738547\n",
      "train loss:0.8198170442555061\n",
      "train loss:0.9994060308345313\n",
      "train loss:0.9425583941844718\n",
      "train loss:0.9626134988918639\n",
      "train loss:0.776612884254205\n",
      "train loss:0.930674278742756\n",
      "train loss:0.9615594602812007\n",
      "train loss:0.8395595775928457\n",
      "train loss:0.8769352629573646\n",
      "train loss:0.893247543529433\n",
      "train loss:0.935546650441887\n",
      "train loss:0.7683832471912507\n",
      "train loss:0.7549284058726914\n",
      "train loss:0.9097736930569654\n",
      "train loss:0.9685835015941037\n",
      "train loss:0.823721966921141\n",
      "train loss:0.7921095004287619\n",
      "train loss:1.0511119301082628\n",
      "train loss:0.8004038402558129\n",
      "train loss:0.907293192536853\n",
      "train loss:0.7091582535088267\n",
      "train loss:0.9918232772307699\n",
      "train loss:0.7225800600929009\n",
      "train loss:0.7610099634511583\n",
      "train loss:0.9720844835156044\n",
      "train loss:0.7909113429336568\n",
      "train loss:1.0251377585533872\n",
      "train loss:0.8050481271177672\n",
      "train loss:0.8011881749198076\n",
      "train loss:0.8690476968735362\n",
      "train loss:0.7535405625076567\n",
      "train loss:0.8981436150543074\n",
      "train loss:0.9017243375982966\n",
      "train loss:0.7736809308268139\n",
      "train loss:0.9631355584379615\n",
      "train loss:0.9424442989857705\n",
      "train loss:0.7808149805503843\n",
      "train loss:0.7817091045363297\n",
      "train loss:0.8868883395583318\n",
      "train loss:0.8880033793364235\n",
      "train loss:0.8647212612551577\n",
      "train loss:0.8475807876132094\n",
      "train loss:1.0313572812817866\n",
      "train loss:0.7843890121298648\n",
      "train loss:0.8326808705386526\n",
      "train loss:0.8370362346243995\n",
      "train loss:0.8294511647006508\n",
      "train loss:0.7944227111419349\n",
      "train loss:0.8507363962107702\n",
      "train loss:0.8788133905863657\n",
      "train loss:0.8140624671918916\n",
      "train loss:0.873282245256071\n",
      "train loss:0.7946631634306108\n",
      "train loss:0.7468336696181597\n",
      "train loss:0.8818372313476918\n",
      "train loss:0.8705284641291021\n",
      "train loss:0.8846483151474055\n",
      "train loss:0.8428302431240797\n",
      "train loss:0.7886234091084371\n",
      "train loss:0.9482741301777948\n",
      "train loss:0.8398942990712344\n",
      "train loss:0.9294234279593253\n",
      "train loss:0.9806812513217489\n",
      "train loss:0.9503542578962688\n",
      "train loss:0.79524748413945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.8477080055451247\n",
      "train loss:1.0390739244487703\n",
      "train loss:0.8951190185749529\n",
      "train loss:0.8140808456720837\n",
      "train loss:0.7694774876359409\n",
      "train loss:0.925427957470489\n",
      "train loss:0.8283154175722943\n",
      "train loss:0.8137547781852892\n",
      "train loss:0.728848611849298\n",
      "train loss:0.7841033314225693\n",
      "train loss:0.7664709499117468\n",
      "train loss:0.8908946552472777\n",
      "train loss:0.7899349700668603\n",
      "train loss:1.0636852504361622\n",
      "train loss:0.8358042430621502\n",
      "train loss:0.8083042289214862\n",
      "train loss:0.9802758528991857\n",
      "train loss:0.8576970604494875\n",
      "train loss:0.6678611774855296\n",
      "train loss:0.9207754857286353\n",
      "train loss:0.8706673654031378\n",
      "train loss:0.8115286344696168\n",
      "train loss:0.8803208124816354\n",
      "train loss:0.781865778264673\n",
      "train loss:0.7849169293400625\n",
      "train loss:1.0272881991294165\n",
      "train loss:0.8577028550437957\n",
      "train loss:0.9211000572729912\n",
      "train loss:0.7633269756650063\n",
      "train loss:0.9112763597781149\n",
      "train loss:0.9196108275168996\n",
      "train loss:0.8363785773699043\n",
      "train loss:0.8688432675113389\n",
      "train loss:0.7373490730923433\n",
      "train loss:0.7168148075526274\n",
      "train loss:0.8765677103735701\n",
      "train loss:0.8221462132211624\n",
      "train loss:0.9494472484062573\n",
      "train loss:0.8352569857072196\n",
      "train loss:0.8630275281933248\n",
      "train loss:0.9056807175628232\n",
      "train loss:0.7463042425623094\n",
      "train loss:0.8369351101865405\n",
      "train loss:0.9238785919190947\n",
      "train loss:1.0219316057516332\n",
      "train loss:0.9908522554029986\n",
      "train loss:0.7784731092230858\n",
      "train loss:0.9066934723303626\n",
      "train loss:0.6962035076337741\n",
      "train loss:0.8468659035016043\n",
      "train loss:0.9670521111418993\n",
      "train loss:0.8844945663473943\n",
      "train loss:0.78640796717497\n",
      "train loss:0.8209725221370431\n",
      "train loss:0.892358719222329\n",
      "train loss:0.953179383263722\n",
      "train loss:0.8552582975395986\n",
      "train loss:0.77919254271548\n",
      "train loss:0.8529423230833934\n",
      "train loss:0.861368374704918\n",
      "train loss:0.7680063455307515\n",
      "train loss:0.8723878011518655\n",
      "train loss:0.8076954202203538\n",
      "train loss:0.9038932086245512\n",
      "train loss:0.8890904549710561\n",
      "train loss:0.8091810037109947\n",
      "train loss:0.872977509460556\n",
      "train loss:0.9518965954127155\n",
      "train loss:0.9327934428246875\n",
      "train loss:0.9303912475479993\n",
      "train loss:0.8874375619368231\n",
      "train loss:0.8806723645953205\n",
      "train loss:0.9708883386982776\n",
      "train loss:1.0439050685401425\n",
      "train loss:0.8733776760825084\n",
      "train loss:0.9646113086839138\n",
      "train loss:0.9829608204392619\n",
      "train loss:0.7800949884729929\n",
      "train loss:0.9542694299443524\n",
      "train loss:1.0504631929725021\n",
      "train loss:0.97021129494637\n",
      "train loss:0.8959859519951704\n",
      "train loss:0.904616712233485\n",
      "train loss:0.8207391206624102\n",
      "train loss:0.9526506977707047\n",
      "train loss:0.8619154558212628\n",
      "train loss:0.8903833171409736\n",
      "train loss:0.9596543142855558\n",
      "train loss:0.8261315025427011\n",
      "train loss:0.8845048346477097\n",
      "train loss:0.7592998079900282\n",
      "train loss:0.9630751881508336\n",
      "train loss:0.8319134435612473\n",
      "train loss:0.7479831243061358\n",
      "train loss:0.8150757732805133\n",
      "train loss:0.7421018578183233\n",
      "train loss:0.9349849322100227\n",
      "train loss:0.9845774444383782\n",
      "train loss:0.864716973146042\n",
      "train loss:0.8764636845267689\n",
      "train loss:0.8699992898263664\n",
      "train loss:0.6908363806014884\n",
      "train loss:1.1329355119476823\n",
      "train loss:0.8251850834440259\n",
      "train loss:0.8040023378066842\n",
      "train loss:0.7364077193409588\n",
      "train loss:0.6829272164569947\n",
      "train loss:0.8767847971997921\n",
      "train loss:0.8899406377216827\n",
      "train loss:0.8983991590112561\n",
      "=== epoch:19, train acc:0.996, test acc:0.994 ===\n",
      "train loss:0.8200939294296533\n",
      "train loss:0.9721986464416336\n",
      "train loss:0.9165225826927056\n",
      "train loss:0.8846539743153903\n",
      "train loss:0.9146267546707658\n",
      "train loss:0.8581776778470106\n",
      "train loss:0.7149997385832085\n",
      "train loss:0.9312413172970284\n",
      "train loss:0.90520126607535\n",
      "train loss:0.8387685725501357\n",
      "train loss:0.7206737446520074\n",
      "train loss:0.888435174511654\n",
      "train loss:0.9233237861481202\n",
      "train loss:0.72562898972232\n",
      "train loss:0.8569964832322208\n",
      "train loss:0.8869683432637874\n",
      "train loss:0.8037093021420882\n",
      "train loss:0.9396333353732862\n",
      "train loss:0.9406029061156467\n",
      "train loss:0.9699495955371722\n",
      "train loss:0.8945844482858647\n",
      "train loss:0.788544286653995\n",
      "train loss:0.8287282721874667\n",
      "train loss:0.8381439447010084\n",
      "train loss:0.699541030669121\n",
      "train loss:0.875146873988178\n",
      "train loss:0.7224322144908752\n",
      "train loss:0.8810644423763171\n",
      "train loss:0.8500036683943207\n",
      "train loss:0.8908486215846187\n",
      "train loss:0.9454376431757011\n",
      "train loss:0.9470204737567686\n",
      "train loss:0.914436187241072\n",
      "train loss:0.9261141244322674\n",
      "train loss:0.9422602214433211\n",
      "train loss:0.9546001046490812\n",
      "train loss:0.8708203078896314\n",
      "train loss:0.8916757706834655\n",
      "train loss:0.8464415355605788\n",
      "train loss:0.7962376607513053\n",
      "train loss:0.9388776317162786\n",
      "train loss:0.9022152798973818\n",
      "train loss:0.8228567165052558\n",
      "train loss:0.9426874155983385\n",
      "train loss:1.0455876505933706\n",
      "train loss:0.9641905709733478\n",
      "train loss:0.9508527608001965\n",
      "train loss:0.9617273413207179\n",
      "train loss:0.7718493060835917\n",
      "train loss:0.8296140024717249\n",
      "train loss:1.0200081986439893\n",
      "train loss:0.8812048034576127\n",
      "train loss:0.6754685103601824\n",
      "train loss:0.8456991773496728\n",
      "train loss:0.7750122198179813\n",
      "train loss:0.833265709126145\n",
      "train loss:0.7949245506900491\n",
      "train loss:0.8591234059602917\n",
      "train loss:0.8239725514764322\n",
      "train loss:1.0341776535891887\n",
      "train loss:0.7840206253390484\n",
      "train loss:0.9200183998831988\n",
      "train loss:0.9094679326583492\n",
      "train loss:0.7038704889095587\n",
      "train loss:0.7244121773183291\n",
      "train loss:0.9740718269690177\n",
      "train loss:0.8692335705002181\n",
      "train loss:1.023678480996839\n",
      "train loss:0.8346982174228121\n",
      "train loss:0.8509354468267132\n",
      "train loss:0.9076542377012039\n",
      "train loss:0.9686182819298069\n",
      "train loss:0.8962369918709686\n",
      "train loss:0.8960726352791777\n",
      "train loss:0.9040522261713193\n",
      "train loss:0.8782482326172476\n",
      "train loss:0.7988912810206225\n",
      "train loss:0.8866092741319461\n",
      "train loss:1.1271166814916584\n",
      "train loss:0.8986598622309454\n",
      "train loss:0.8964865854377417\n",
      "train loss:0.7177894920345139\n",
      "train loss:0.8311894349439923\n",
      "train loss:0.8363274825233478\n",
      "train loss:0.7037046089142645\n",
      "train loss:0.8148054682192807\n",
      "train loss:0.8829164996460338\n",
      "train loss:1.0146511060940542\n",
      "train loss:0.8521766407941236\n",
      "train loss:0.80552255608086\n",
      "train loss:0.8691790845219234\n",
      "train loss:0.9250207403580987\n",
      "train loss:0.9209093938295259\n",
      "train loss:0.8793276184992815\n",
      "train loss:0.9674107989771389\n",
      "train loss:0.8344157719093885\n",
      "train loss:0.8270801463103448\n",
      "train loss:0.8898585268506816\n",
      "train loss:0.8861321364255\n",
      "train loss:0.631953610946072\n",
      "train loss:0.831050479502099\n",
      "train loss:0.9284755179997936\n",
      "train loss:0.8137288523734143\n",
      "train loss:0.8631173401134075\n",
      "train loss:0.7957288732212597\n",
      "train loss:0.9023919271275865\n",
      "train loss:0.9376408776190125\n",
      "train loss:0.9419272367882688\n",
      "train loss:0.7964547204497338\n",
      "train loss:0.8980827001069077\n",
      "train loss:0.8225574267466013\n",
      "train loss:1.0047955471937544\n",
      "train loss:0.6969482292523216\n",
      "train loss:0.9685762807701466\n",
      "train loss:0.9074897303080326\n",
      "train loss:0.8907910266558183\n",
      "train loss:0.7389151302938631\n",
      "train loss:0.8429829634027871\n",
      "train loss:0.8016194254461645\n",
      "train loss:0.9474626268434312\n",
      "train loss:0.9274310991576498\n",
      "train loss:0.8136689320308438\n",
      "train loss:0.8707517128826328\n",
      "train loss:0.7316566876994446\n",
      "train loss:0.7812232117754949\n",
      "train loss:0.6994891267430835\n",
      "train loss:0.9521286620502758\n",
      "train loss:0.9425404492791941\n",
      "train loss:0.958769353851633\n",
      "train loss:0.8384810596091415\n",
      "train loss:0.985890038148646\n",
      "train loss:0.9973298491270204\n",
      "train loss:0.9688358086852848\n",
      "train loss:0.8930422452291994\n",
      "train loss:0.9768849256277751\n",
      "train loss:0.7802632289346075\n",
      "train loss:0.9299073820552535\n",
      "train loss:0.9631159567021221\n",
      "train loss:0.7090748797299298\n",
      "train loss:0.8335767758287163\n",
      "train loss:0.7137967289278089\n",
      "train loss:0.8787541421402204\n",
      "train loss:0.8270447567203469\n",
      "train loss:0.8788103315190852\n",
      "train loss:0.8283162101524064\n",
      "train loss:0.994343487332467\n",
      "train loss:0.844111258727849\n",
      "train loss:0.871919569038974\n",
      "train loss:0.7878746216677832\n",
      "train loss:0.8665826221088646\n",
      "train loss:0.9081763044321286\n",
      "train loss:0.8497151166424931\n",
      "train loss:0.9451861366961358\n",
      "train loss:0.8219804708762686\n",
      "train loss:0.9026283614843129\n",
      "train loss:0.8207154871064762\n",
      "train loss:0.9052020618337981\n",
      "train loss:0.8464095888248653\n",
      "train loss:0.7518996811427773\n",
      "train loss:0.807905726481296\n",
      "train loss:0.9841896231217635\n",
      "train loss:0.8309853156340964\n",
      "train loss:0.8264635957311387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.8122572155121722\n",
      "train loss:0.9021894046808991\n",
      "train loss:0.8657105320344834\n",
      "train loss:1.021921259844644\n",
      "train loss:0.9461147928276307\n",
      "train loss:0.7610436157225674\n",
      "train loss:0.8184526819998021\n",
      "train loss:1.0289759767843387\n",
      "train loss:0.7840347635854884\n",
      "train loss:0.9090981640508348\n",
      "train loss:0.8366651771872938\n",
      "train loss:0.9216553670303584\n",
      "train loss:0.81212804028906\n",
      "train loss:0.9407766189776627\n",
      "train loss:0.93380106164514\n",
      "train loss:0.8428284150895834\n",
      "train loss:0.8104464993570281\n",
      "train loss:0.9308368221967217\n",
      "train loss:0.9468866466640903\n",
      "train loss:0.7407622827214333\n",
      "train loss:0.8374793110896901\n",
      "train loss:0.9107085853202525\n",
      "train loss:0.8631623062967815\n",
      "train loss:0.8045983494000246\n",
      "train loss:0.919537753444809\n",
      "train loss:0.9413335938985218\n",
      "train loss:0.860467677166767\n",
      "train loss:0.8601404258332055\n",
      "train loss:0.9186713727728307\n",
      "train loss:0.9688763173183316\n",
      "train loss:0.9400955046635889\n",
      "train loss:0.8563027257941539\n",
      "train loss:0.7466744124647753\n",
      "train loss:0.8620077060954173\n",
      "train loss:0.9592961246928924\n",
      "train loss:0.8030994919142728\n",
      "train loss:0.8331335222278063\n",
      "train loss:0.8606247985179055\n",
      "train loss:0.8623719861601791\n",
      "train loss:0.8586468376800659\n",
      "train loss:0.7714513050637946\n",
      "train loss:0.8682957614259352\n",
      "train loss:1.045234942799794\n",
      "train loss:0.8969346296313208\n",
      "train loss:0.878663748942505\n",
      "train loss:0.7666026171154963\n",
      "train loss:0.928705221772878\n",
      "train loss:0.8928592929465747\n",
      "train loss:0.98647665951427\n",
      "train loss:1.0280851770506958\n",
      "train loss:0.9352469682383567\n",
      "train loss:0.8830435747552632\n",
      "train loss:0.7337649104301412\n",
      "train loss:0.7894552843849112\n",
      "train loss:0.8795284898572525\n",
      "train loss:0.8387863109946389\n",
      "train loss:0.8440376277463539\n",
      "train loss:0.7666262764067099\n",
      "train loss:0.8935435955230152\n",
      "train loss:0.833700382519119\n",
      "train loss:1.0286761428022344\n",
      "train loss:1.04158430109017\n",
      "train loss:0.7530400456322102\n",
      "train loss:0.7873979862547275\n",
      "train loss:0.785742699358902\n",
      "train loss:0.8632476694738951\n",
      "train loss:0.6645886395449022\n",
      "train loss:0.7080003051859818\n",
      "train loss:0.9281501008077369\n",
      "train loss:1.0751485796375662\n",
      "train loss:0.8754544191354716\n",
      "train loss:0.9082033480036824\n",
      "train loss:0.8466082482839603\n",
      "train loss:0.8473844583208466\n",
      "train loss:0.6698496096809399\n",
      "train loss:0.8616139895971149\n",
      "train loss:0.770683475709495\n",
      "train loss:0.8181833951176466\n",
      "train loss:0.7980950301054395\n",
      "train loss:0.7727043580190734\n",
      "train loss:1.0315467435815355\n",
      "train loss:0.766318301661102\n",
      "train loss:0.7397371541159192\n",
      "train loss:0.7750494174451567\n",
      "train loss:0.8395274326967358\n",
      "train loss:0.7648228370318484\n",
      "train loss:0.7910088298084001\n",
      "train loss:0.8907571945619377\n",
      "train loss:0.7257434999339446\n",
      "train loss:0.8229124609303792\n",
      "train loss:0.8786247880454343\n",
      "train loss:0.8090221151785336\n",
      "train loss:0.8436832183338676\n",
      "train loss:0.9542074178481903\n",
      "train loss:0.9466963559352365\n",
      "train loss:0.9989251739728892\n",
      "train loss:0.9122824918307735\n",
      "train loss:0.8259549808698271\n",
      "train loss:0.7839214913676709\n",
      "train loss:0.9996482286084659\n",
      "train loss:0.9174770920297849\n",
      "train loss:0.933433181694608\n",
      "train loss:0.8815623411171423\n",
      "train loss:0.8345015786035155\n",
      "train loss:0.8176508332212474\n",
      "train loss:0.8073101195044284\n",
      "train loss:0.8964325257650533\n",
      "train loss:0.753863764710325\n",
      "train loss:0.9163342619086758\n",
      "train loss:0.8162262840137295\n",
      "train loss:0.9149345763865141\n",
      "train loss:0.9720681103313793\n",
      "train loss:0.7297889244769062\n",
      "train loss:0.7870965409041587\n",
      "train loss:0.7744110631233134\n",
      "train loss:1.0046606071795847\n",
      "train loss:0.9646053676264589\n",
      "train loss:0.9427662488823306\n",
      "train loss:0.8719230058355891\n",
      "train loss:0.9486260613651071\n",
      "train loss:0.7425011588342951\n",
      "train loss:0.8664941260154013\n",
      "train loss:0.9318975948558064\n",
      "train loss:0.8368525423638075\n",
      "train loss:0.7083080980964482\n",
      "train loss:0.9646114195641117\n",
      "train loss:0.7824344107535347\n",
      "train loss:0.975162713520983\n",
      "train loss:0.8296220989987033\n",
      "train loss:0.7924822927226852\n",
      "train loss:0.8086929388277662\n",
      "train loss:0.9292571323500239\n",
      "train loss:0.8079705777474347\n",
      "train loss:0.852574906479936\n",
      "train loss:1.019141036769594\n",
      "train loss:0.7778445475715432\n",
      "train loss:0.7814849000472017\n",
      "train loss:0.9663442004594319\n",
      "train loss:1.0418354893923485\n",
      "train loss:1.0559678267933261\n",
      "train loss:0.8386760876359074\n",
      "train loss:0.9289321078212383\n",
      "train loss:0.8594116071250223\n",
      "train loss:0.9603934007896142\n",
      "train loss:0.8543779141573006\n",
      "train loss:0.960987483862547\n",
      "train loss:0.6550511274557386\n",
      "train loss:0.8971609848826337\n",
      "train loss:0.7840816861915808\n",
      "train loss:0.9413423423668137\n",
      "train loss:0.9121763674751872\n",
      "train loss:0.9490279986136235\n",
      "train loss:0.8697281679474108\n",
      "train loss:1.0387567150086323\n",
      "train loss:0.8358412786952896\n",
      "train loss:0.80468997471756\n",
      "train loss:0.9960111947708558\n",
      "train loss:0.9711245489188584\n",
      "train loss:0.9125229384940269\n",
      "train loss:0.88119201388972\n",
      "train loss:0.8758516997954092\n",
      "train loss:0.7557320609106943\n",
      "train loss:0.9420857648449993\n",
      "train loss:0.9219219100305891\n",
      "train loss:0.8125422410177986\n",
      "train loss:0.8397026495377551\n",
      "train loss:0.7456199248974784\n",
      "train loss:0.839093126649783\n",
      "train loss:0.9811467832847589\n",
      "train loss:0.9045922829307762\n",
      "train loss:0.8853250748495642\n",
      "train loss:0.7516857870065732\n",
      "train loss:0.8854834957109068\n",
      "train loss:0.8528040996167908\n",
      "train loss:0.8441557815583656\n",
      "train loss:0.8064761589100552\n",
      "train loss:0.7461472888037811\n",
      "train loss:0.8472150511499228\n",
      "train loss:0.7342733892682731\n",
      "train loss:0.8746528385857967\n",
      "train loss:0.6631936594124797\n",
      "train loss:1.0761701136087922\n",
      "train loss:0.9862768308466521\n",
      "train loss:0.859639021180937\n",
      "train loss:0.7087562625066006\n",
      "train loss:0.7461376611047826\n",
      "train loss:0.9619055741183942\n",
      "train loss:0.8571697964258341\n",
      "train loss:0.8738439190525372\n",
      "train loss:1.0571285219748934\n",
      "train loss:0.7236337607059453\n",
      "train loss:0.7698445860433739\n",
      "train loss:1.0546136085963684\n",
      "train loss:0.7442523801323563\n",
      "train loss:0.9489274967348826\n",
      "train loss:0.8761104218258066\n",
      "train loss:0.863852699392594\n",
      "train loss:0.6294630708369772\n",
      "train loss:0.8242904812319951\n",
      "train loss:0.8317817131026548\n",
      "train loss:0.7761477625735312\n",
      "train loss:0.8901880691858381\n",
      "train loss:0.9136055661152982\n",
      "train loss:0.6901311771249554\n",
      "train loss:0.7468180166986452\n",
      "train loss:0.9893131814944806\n",
      "train loss:0.8801585578343498\n",
      "train loss:0.9309509495588095\n",
      "train loss:0.9984107261659743\n",
      "train loss:0.7975525720725727\n",
      "train loss:0.9541567370485745\n",
      "train loss:0.8042228813694728\n",
      "train loss:0.8676633933527804\n",
      "train loss:0.7170388118895155\n",
      "train loss:0.9939004588397613\n",
      "train loss:0.682009164698279\n",
      "train loss:0.9167941547303716\n",
      "train loss:0.8430099577233281\n",
      "train loss:0.7328372093840602\n",
      "train loss:0.8806516511995361\n",
      "train loss:1.0300512298697464\n",
      "train loss:0.8480588155691953\n",
      "train loss:0.9746368977474239\n",
      "train loss:0.7198271372169124\n",
      "train loss:1.0029374535598654\n",
      "train loss:0.7804947992988873\n",
      "train loss:0.9331478314868549\n",
      "train loss:0.7749581400639448\n",
      "train loss:0.7560012761092446\n",
      "train loss:0.8401569352828159\n",
      "train loss:0.8721253708240684\n",
      "train loss:0.7872995523924451\n",
      "train loss:0.7982359504774925\n",
      "train loss:0.8860860112114929\n",
      "train loss:0.926755187811859\n",
      "train loss:0.7626107985450481\n",
      "train loss:0.8212205853440077\n",
      "train loss:0.805394162740492\n",
      "train loss:0.8152563435483343\n",
      "train loss:1.0610508744869405\n",
      "train loss:0.9098422160622733\n",
      "train loss:0.7668654839321403\n",
      "train loss:0.8625986833211602\n",
      "train loss:0.9336264968741513\n",
      "train loss:0.8127053741776212\n",
      "train loss:0.8841212040695946\n",
      "train loss:0.799070468805261\n",
      "train loss:0.8915314141622943\n",
      "train loss:0.7789720362216882\n",
      "train loss:0.88245029372479\n",
      "train loss:0.9117586152390144\n",
      "train loss:0.7120992283871082\n",
      "train loss:0.8570322505590964\n",
      "train loss:1.00529354596165\n",
      "train loss:0.7543113165305008\n",
      "train loss:0.7881217221733327\n",
      "train loss:0.6975545601353044\n",
      "train loss:0.726353944212822\n",
      "train loss:0.7273858890044019\n",
      "train loss:0.7928545869125948\n",
      "train loss:0.8574434254145991\n",
      "train loss:0.8194894986680946\n",
      "train loss:0.9266113869543573\n",
      "train loss:0.7949522389092765\n",
      "train loss:0.9308106797662425\n",
      "train loss:0.8560080767575506\n",
      "train loss:0.9300578128205221\n",
      "train loss:0.828087082634857\n",
      "train loss:0.9508953742318238\n",
      "train loss:0.8356672244920043\n",
      "train loss:0.8053705561214032\n",
      "train loss:0.9631482463076861\n",
      "train loss:0.8476159842679805\n",
      "train loss:0.8555333584901019\n",
      "train loss:0.7680893528711448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.8251667118832969\n",
      "train loss:0.7402543104612096\n",
      "train loss:0.8739550641265627\n",
      "train loss:0.9498341421959172\n",
      "train loss:0.9291937301261873\n",
      "train loss:0.7943242341491252\n",
      "train loss:0.9933285629262216\n",
      "train loss:0.8696103386389359\n",
      "train loss:0.7657353560762611\n",
      "train loss:0.7686902143521975\n",
      "train loss:0.8551593125105558\n",
      "train loss:0.8920104888857344\n",
      "train loss:0.8454257783200648\n",
      "train loss:0.8953320537323318\n",
      "train loss:0.9745598884539308\n",
      "train loss:0.834469725243868\n",
      "train loss:0.7907221975023255\n",
      "train loss:0.7175036362715627\n",
      "train loss:0.855084411491292\n",
      "train loss:0.8587041408797479\n",
      "train loss:0.8732369059013871\n",
      "train loss:0.929137264019203\n",
      "train loss:0.8222490198058877\n",
      "train loss:0.9662297900761274\n",
      "train loss:0.8173108105612107\n",
      "train loss:0.8405028240547447\n",
      "train loss:0.8442693904830324\n",
      "train loss:0.770780820301995\n",
      "train loss:0.699554510018578\n",
      "train loss:0.9792199494207598\n",
      "train loss:0.7122703500661527\n",
      "train loss:0.7356810028718829\n",
      "train loss:1.0646369457321552\n",
      "train loss:0.8310032010195657\n",
      "train loss:1.0715037161293055\n",
      "train loss:0.9736925308581426\n",
      "train loss:0.9076728916768895\n",
      "train loss:0.8748325728792516\n",
      "train loss:0.9650622662042685\n",
      "train loss:0.9903670712031121\n",
      "train loss:0.6819045437815234\n",
      "train loss:0.9002389759490375\n",
      "train loss:0.9122468634971592\n",
      "train loss:0.8075673294158229\n",
      "train loss:0.7389041012030352\n",
      "train loss:0.9611584929335751\n",
      "train loss:0.641835878731729\n",
      "train loss:0.9911448649717056\n",
      "train loss:0.8680047095817433\n",
      "train loss:0.8966116598537265\n",
      "train loss:0.7320163203467209\n",
      "train loss:0.9208243070405925\n",
      "train loss:0.9180732089596301\n",
      "train loss:0.8157424233594868\n",
      "train loss:0.9030022877731935\n",
      "train loss:0.752575280010031\n",
      "train loss:0.7324766052699264\n",
      "train loss:0.8998149471332113\n",
      "train loss:0.7215619785177703\n",
      "train loss:1.108693684011914\n",
      "train loss:0.6952591770585798\n",
      "train loss:0.8999592040143983\n",
      "train loss:0.9479276699085236\n",
      "train loss:0.8206153572548555\n",
      "train loss:0.7968240712608906\n",
      "train loss:0.920709789256354\n",
      "train loss:0.6496275949584945\n",
      "train loss:0.7466167336731303\n",
      "train loss:0.8195296076744902\n",
      "train loss:0.8680518416560387\n",
      "train loss:0.8208958410133743\n",
      "train loss:0.9332899303215427\n",
      "train loss:0.909205852428168\n",
      "train loss:0.9440376806069782\n",
      "train loss:0.8817423217106645\n",
      "train loss:0.917131588521682\n",
      "train loss:0.8663759397652058\n",
      "train loss:1.096871693611121\n",
      "train loss:0.9770790392802816\n",
      "train loss:0.8829021452310513\n",
      "train loss:0.8195856310384565\n",
      "train loss:0.9021433635572634\n",
      "train loss:0.9154231466770915\n",
      "train loss:0.8680351300263085\n",
      "train loss:0.782330318000282\n",
      "train loss:0.8587161444045942\n",
      "train loss:0.9627400658059014\n",
      "train loss:0.9557477378569718\n",
      "train loss:1.028966085992468\n",
      "train loss:0.6695003912943084\n",
      "train loss:0.7982135925493994\n",
      "train loss:0.8667176787657402\n",
      "train loss:0.8953002895307511\n",
      "train loss:1.0210166726793428\n",
      "train loss:0.9375341661639226\n",
      "train loss:0.882503121557725\n",
      "train loss:0.9125297255649456\n",
      "train loss:0.9122783782244887\n",
      "train loss:0.8398395813680359\n",
      "train loss:0.7196073080893531\n",
      "train loss:0.8611862360331505\n",
      "train loss:0.9130617409531036\n",
      "train loss:0.7691003867160403\n",
      "train loss:0.7563153622235526\n",
      "train loss:0.7061755795614033\n",
      "train loss:1.0042315181628243\n",
      "train loss:0.9257545455182793\n",
      "train loss:0.9242436169133211\n",
      "train loss:0.7738688479547793\n",
      "train loss:0.8602012329363115\n",
      "train loss:0.8378456136584256\n",
      "train loss:0.8423913141072678\n",
      "train loss:0.9631176819740632\n",
      "train loss:0.8530319827465423\n",
      "train loss:0.9102774755785658\n",
      "train loss:0.8931793688581277\n",
      "train loss:0.7297903745811154\n",
      "train loss:0.8524948943169955\n",
      "train loss:0.910608661783899\n",
      "train loss:0.7083243479702123\n",
      "train loss:0.7265534401412337\n",
      "train loss:0.897368343223127\n",
      "train loss:1.0072595873090415\n",
      "train loss:0.8801988133146179\n",
      "train loss:0.7553928952592933\n",
      "train loss:0.8607729046557813\n",
      "train loss:0.8944703643143009\n",
      "train loss:0.8229025156602376\n",
      "train loss:1.0036230902625751\n",
      "train loss:0.9582439977548131\n",
      "train loss:1.0681981456453633\n",
      "train loss:0.9721974109675415\n",
      "train loss:0.9243940143775411\n",
      "train loss:0.8494904098908076\n",
      "train loss:0.7905184666231116\n",
      "train loss:0.850523316970758\n",
      "train loss:0.8442472613289281\n",
      "train loss:0.9839206733206478\n",
      "train loss:1.038726333814764\n",
      "train loss:0.8026323056483142\n",
      "train loss:0.8919769242943286\n",
      "train loss:0.89981547106392\n",
      "train loss:0.8586717792092089\n",
      "train loss:0.8879505909480411\n",
      "train loss:0.8312570159240312\n",
      "train loss:0.7407656637951324\n",
      "train loss:0.8374626102628637\n",
      "train loss:0.7554944322781235\n",
      "train loss:0.7803314665863172\n",
      "train loss:0.7562569972032647\n",
      "train loss:0.9134021721898532\n",
      "train loss:0.978168847958572\n",
      "train loss:0.9419423226113852\n",
      "train loss:0.9916821009014634\n",
      "train loss:0.8719557927154988\n",
      "train loss:0.8512809673257746\n",
      "train loss:0.8881125184526616\n",
      "train loss:0.8187171889654912\n",
      "train loss:0.7191640701697699\n",
      "train loss:1.0418302639669492\n",
      "train loss:0.9731652917930694\n",
      "train loss:0.9233298868135964\n",
      "=== epoch:20, train acc:0.998, test acc:0.994 ===\n",
      "train loss:0.7761945407898335\n",
      "train loss:0.935797213270541\n",
      "train loss:0.6459609452576348\n",
      "train loss:1.0517030111105226\n",
      "train loss:0.9736775844274939\n",
      "train loss:0.7343922464173961\n",
      "train loss:0.788207998272084\n",
      "train loss:0.8854405661267188\n",
      "train loss:0.7159108202330315\n",
      "train loss:0.7588070930343506\n",
      "train loss:0.8104665099506518\n",
      "train loss:0.8057538081714225\n",
      "train loss:0.8579931517558133\n",
      "train loss:0.7106171911207962\n",
      "train loss:0.8119038711413354\n",
      "train loss:0.770402979646204\n",
      "train loss:0.9216988791045603\n",
      "train loss:0.9707374455623574\n",
      "train loss:0.9986891567436237\n",
      "train loss:0.7618127035577871\n",
      "train loss:0.8578881683125882\n",
      "train loss:0.9440309573393831\n",
      "train loss:0.8764824976349879\n",
      "train loss:0.8107038736727875\n",
      "train loss:0.8124220630237831\n",
      "train loss:0.9022699896160435\n",
      "train loss:0.8255498215414809\n",
      "train loss:0.9072636058904981\n",
      "train loss:0.8643862151351374\n",
      "train loss:1.1098115467554128\n",
      "train loss:0.8031722035307759\n",
      "train loss:0.8952880014869378\n",
      "train loss:0.7402847359659295\n",
      "train loss:0.9713198015935223\n",
      "train loss:0.7323230830873633\n",
      "train loss:1.1192469588494478\n",
      "train loss:1.0772486849302694\n",
      "train loss:0.7424530681444882\n",
      "train loss:0.80394145432434\n",
      "train loss:1.0036278555699232\n",
      "train loss:0.8560140469131856\n",
      "train loss:0.8293456290025131\n",
      "train loss:0.9856896859806641\n",
      "train loss:0.8256507309869359\n",
      "train loss:0.8962756750656965\n",
      "train loss:0.8705557594011261\n",
      "train loss:0.9053489294140941\n",
      "train loss:0.8626560164662438\n",
      "train loss:0.6760032733878476\n",
      "train loss:0.9139991911966505\n",
      "train loss:1.1278020068192902\n",
      "train loss:0.8269640446281619\n",
      "train loss:0.7559927505557676\n",
      "train loss:0.947971266507739\n",
      "train loss:0.8106805266642592\n",
      "train loss:0.7728726186129208\n",
      "train loss:1.0015360581802666\n",
      "train loss:0.8616565625255209\n",
      "train loss:0.7931008807021004\n",
      "train loss:0.8763591164739458\n",
      "train loss:0.907308756873719\n",
      "train loss:0.8507635161062934\n",
      "train loss:0.9107643227321693\n",
      "train loss:0.8567723064536352\n",
      "train loss:0.7631707314850796\n",
      "train loss:0.9368738477266685\n",
      "train loss:0.9785215644995975\n",
      "train loss:0.8470256012799772\n",
      "train loss:0.8035224370284252\n",
      "train loss:0.6619466862261079\n",
      "train loss:0.7748585838907937\n",
      "train loss:0.8086767816290272\n",
      "train loss:0.9988509047095989\n",
      "train loss:0.6869478361756802\n",
      "train loss:0.7974775968212404\n",
      "train loss:0.9892445748827993\n",
      "train loss:0.7226514422537038\n",
      "train loss:1.109300515162042\n",
      "train loss:0.8626165976757744\n",
      "train loss:0.7982249448275108\n",
      "train loss:0.806231209178744\n",
      "train loss:0.8435952141549456\n",
      "train loss:1.0008271516951996\n",
      "train loss:0.770268505624294\n",
      "train loss:0.9012519643299285\n",
      "train loss:1.0823011822727115\n",
      "train loss:0.6539328351338056\n",
      "train loss:0.8434307715302438\n",
      "train loss:0.8110374067250682\n",
      "train loss:0.7954598178848187\n",
      "train loss:0.8001112250790299\n",
      "train loss:0.7242368384726042\n",
      "train loss:0.8371382701837744\n",
      "train loss:0.855767475492913\n",
      "train loss:0.7380104778233272\n",
      "train loss:1.0066269285512408\n",
      "train loss:0.9069967857118669\n",
      "train loss:0.9823893624597955\n",
      "train loss:0.7129323976941125\n",
      "train loss:1.165250300575426\n",
      "train loss:0.7943797151046368\n",
      "train loss:0.9149152705551432\n",
      "train loss:0.9908700964668563\n",
      "train loss:0.7070575081456449\n",
      "train loss:0.851498354653815\n",
      "train loss:1.0193467059376993\n",
      "train loss:0.7560092715912776\n",
      "train loss:0.8861294956000216\n",
      "train loss:0.9332617910760331\n",
      "train loss:0.8000200865475466\n",
      "train loss:0.8580588892082877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.9053228245644869\n",
      "train loss:0.8084750966650072\n",
      "train loss:0.8820294713899783\n",
      "train loss:0.7853575000406141\n",
      "train loss:0.8364928342499839\n",
      "train loss:0.8680200396269124\n",
      "train loss:0.7028073722399054\n",
      "train loss:0.8657846902505189\n",
      "train loss:1.003577963816715\n",
      "train loss:0.8289176007062204\n",
      "train loss:0.8403073007745949\n",
      "train loss:0.882432246850952\n",
      "train loss:0.7521035865613682\n",
      "train loss:0.731283366279347\n",
      "train loss:0.9584364760627967\n",
      "train loss:0.7303474942269449\n",
      "train loss:0.6718688957492139\n",
      "train loss:0.8668305375661478\n",
      "train loss:0.7429089688727382\n",
      "train loss:0.9686544404323842\n",
      "train loss:0.6978019517500519\n",
      "train loss:0.886496756159287\n",
      "train loss:0.8333609685631217\n",
      "train loss:0.7257797016350237\n",
      "train loss:0.8096363290732242\n",
      "train loss:0.8442829284379195\n",
      "train loss:0.9076492149124844\n",
      "train loss:0.9720267427366573\n",
      "train loss:0.9961987046887537\n",
      "train loss:0.7961287514369307\n",
      "train loss:1.0654597066994476\n",
      "train loss:0.7298437411197247\n",
      "train loss:0.7576073142381357\n",
      "train loss:0.9150674363760565\n",
      "train loss:0.9066225441624123\n",
      "train loss:0.8106095875485838\n",
      "train loss:0.8580891865911012\n",
      "train loss:0.8648618951213263\n",
      "train loss:0.8368651798699007\n",
      "train loss:0.783161631548155\n",
      "train loss:0.9002977788338348\n",
      "train loss:1.019007210216934\n",
      "train loss:0.8443027588010544\n",
      "train loss:0.7682360165581784\n",
      "train loss:0.9779911683034435\n",
      "train loss:0.8468710107116755\n",
      "train loss:0.8445617122176919\n",
      "train loss:0.8502894481121818\n",
      "train loss:0.941726059888829\n",
      "train loss:0.8085544413189223\n",
      "train loss:0.8176699410236664\n",
      "train loss:0.8580823342233423\n",
      "train loss:0.8607247656783573\n",
      "train loss:0.9754057370882983\n",
      "train loss:0.927999837399704\n",
      "train loss:0.7853079533774838\n",
      "train loss:0.841011398886092\n",
      "train loss:0.7764470038700757\n",
      "train loss:0.9119617752646748\n",
      "train loss:0.7061097518676646\n",
      "train loss:0.8737492430164577\n",
      "train loss:0.9748888655444407\n",
      "train loss:0.9106489009101689\n",
      "train loss:0.8684524281404953\n",
      "train loss:0.7665004840230338\n",
      "train loss:0.7933662169449833\n",
      "train loss:0.8423893769379834\n",
      "train loss:0.9095320811813532\n",
      "train loss:0.778777565009536\n",
      "train loss:0.7673915806290023\n",
      "train loss:0.9579221507975083\n",
      "train loss:0.8397642395015932\n",
      "train loss:0.7725276209657979\n",
      "train loss:0.8723814074145829\n",
      "train loss:0.9909282397105514\n",
      "train loss:0.9944903286477128\n",
      "train loss:0.7613999484519346\n",
      "train loss:0.9173876833291805\n",
      "train loss:0.8908401069339521\n",
      "train loss:0.9702880374532222\n",
      "train loss:0.9194219295659759\n",
      "train loss:0.791995170092704\n",
      "train loss:0.8384281104376241\n",
      "train loss:0.6969008787175076\n",
      "train loss:0.9695980987844015\n",
      "train loss:0.8491419292833301\n",
      "train loss:0.8689282348777541\n",
      "train loss:0.729854182433772\n",
      "train loss:0.8186952612595424\n",
      "train loss:0.8826134113787154\n",
      "train loss:0.8060108543374647\n",
      "train loss:0.9071388000110303\n",
      "train loss:0.8153260215065471\n",
      "train loss:0.6987586736595622\n",
      "train loss:0.9089925563047211\n",
      "train loss:0.804302098602761\n",
      "train loss:0.9588434309802149\n",
      "train loss:0.886884221823291\n",
      "train loss:0.9916236097489444\n",
      "train loss:0.9711223877415914\n",
      "train loss:0.7312104456310629\n",
      "train loss:0.766762034205619\n",
      "train loss:0.6843954971524444\n",
      "train loss:0.819894145255113\n",
      "train loss:0.8997644947638088\n",
      "train loss:0.9159596876228601\n",
      "train loss:0.9642051065240647\n",
      "train loss:0.9822043887023457\n",
      "train loss:0.9377797259620203\n",
      "train loss:1.0637715966660168\n",
      "train loss:0.9095773935594618\n",
      "train loss:0.973315883363706\n",
      "train loss:0.8305464514114785\n",
      "train loss:0.9562671722737146\n",
      "train loss:0.7897707685960605\n",
      "train loss:0.9543923841473226\n",
      "train loss:0.8756929049952556\n",
      "train loss:0.9449304276585146\n",
      "train loss:0.8066636045006041\n",
      "train loss:0.8214666806612385\n",
      "train loss:0.9113767708198358\n",
      "train loss:0.8364638482811446\n",
      "train loss:0.8472260307494109\n",
      "train loss:0.8550550474798031\n",
      "train loss:0.8766892818971189\n",
      "train loss:0.8075552537232809\n",
      "train loss:0.8552387188750304\n",
      "train loss:1.0451261725084107\n",
      "train loss:1.145686107963821\n",
      "train loss:1.000552970962829\n",
      "train loss:0.8419010668883367\n",
      "train loss:0.851629892358016\n",
      "train loss:0.8121481093461924\n",
      "train loss:0.9737186718283377\n",
      "train loss:0.7697972739389225\n",
      "train loss:0.8765528242397427\n",
      "train loss:0.7929966774810466\n",
      "train loss:0.8468636470648512\n",
      "train loss:0.8400810376476173\n",
      "train loss:0.7288801367031498\n",
      "train loss:0.5748788290915628\n",
      "train loss:0.8804115845946316\n",
      "train loss:1.029850848232416\n",
      "train loss:0.8301607363931829\n",
      "train loss:0.9609429179327184\n",
      "train loss:0.9225844864055931\n",
      "train loss:0.6823773171407255\n",
      "train loss:0.9201280979654632\n",
      "train loss:0.9847853231396969\n",
      "train loss:0.8938216150397931\n",
      "train loss:0.8636669715196014\n",
      "train loss:0.892909636396161\n",
      "train loss:0.7220510896658655\n",
      "train loss:0.9155488783581306\n",
      "train loss:0.7918063517253502\n",
      "train loss:0.72374658335354\n",
      "train loss:0.7979763829022524\n",
      "train loss:0.9300796793055889\n",
      "train loss:0.9935126191200291\n",
      "train loss:0.6701697694270383\n",
      "train loss:0.9854944466627954\n",
      "train loss:0.8887498224426635\n",
      "train loss:1.0299263897036541\n",
      "train loss:1.0305295900635845\n",
      "train loss:0.806233167289015\n",
      "train loss:0.9079700807522826\n",
      "train loss:0.9322471214286966\n",
      "train loss:0.8254288711902733\n",
      "train loss:0.766979880924477\n",
      "train loss:0.7219813186617388\n",
      "train loss:0.7416002250191761\n",
      "train loss:0.6989531545524416\n",
      "train loss:1.0064040837359713\n",
      "train loss:0.7396239254295321\n",
      "train loss:0.861584843490284\n",
      "train loss:0.8098824712641247\n",
      "train loss:0.8138128285529601\n",
      "train loss:0.8433225195477958\n",
      "train loss:0.8013961578528734\n",
      "train loss:0.9258547491841675\n",
      "train loss:0.7588036969144888\n",
      "train loss:0.8421355142986522\n",
      "train loss:0.8302084842814494\n",
      "train loss:0.7563980250332922\n",
      "train loss:0.937036335759909\n",
      "train loss:0.9065619515527374\n",
      "train loss:0.8503925819420728\n",
      "train loss:0.9574004192928999\n",
      "train loss:0.7803567410909088\n",
      "train loss:0.8566488176731933\n",
      "train loss:0.871271406925164\n",
      "train loss:0.86350761948749\n",
      "train loss:0.8097892266730394\n",
      "train loss:0.9580369430366836\n",
      "train loss:0.9229037231577769\n",
      "train loss:0.9940719422244847\n",
      "train loss:0.8187234368794701\n",
      "train loss:0.8683232176415774\n",
      "train loss:0.9732518410362538\n",
      "train loss:0.8041090273348575\n",
      "train loss:0.8520088430157863\n",
      "train loss:0.8929772171003527\n",
      "train loss:0.8905901968411953\n",
      "train loss:0.7792204042700297\n",
      "train loss:0.90097760864087\n",
      "train loss:0.9229820892016416\n",
      "train loss:0.8859927794785086\n",
      "train loss:0.965056245012706\n",
      "train loss:0.8252762989590084\n",
      "train loss:0.9539856852528737\n",
      "train loss:0.9467879022883801\n",
      "train loss:0.7815957190647999\n",
      "train loss:0.913434411418815\n",
      "train loss:0.805592307642258\n",
      "train loss:0.8457584893647979\n",
      "train loss:0.8016039490316429\n",
      "train loss:0.862601854463376\n",
      "train loss:1.2525815131573177\n",
      "train loss:0.7395148591683074\n",
      "train loss:0.8946002919457584\n",
      "train loss:0.7283494986234583\n",
      "train loss:0.8357207020218099\n",
      "train loss:0.8267117723495511\n",
      "train loss:0.923567027917549\n",
      "train loss:1.1188192415722926\n",
      "train loss:0.8455437407953952\n",
      "train loss:0.8164350750728251\n",
      "train loss:0.897114715040083\n",
      "train loss:0.9377170768564279\n",
      "train loss:0.8383866642176376\n",
      "train loss:0.8890004089260408\n",
      "train loss:0.8254074043514344\n",
      "train loss:0.8141862091189764\n",
      "train loss:0.8056841224570054\n",
      "train loss:0.8430220815328812\n",
      "train loss:0.88284825214911\n",
      "train loss:0.7610098949340103\n",
      "train loss:0.755629706334236\n",
      "train loss:0.8573817011673041\n",
      "train loss:0.8262168791631598\n",
      "train loss:0.8858033239877307\n",
      "train loss:0.9773624196773103\n",
      "train loss:0.8414388512772242\n",
      "train loss:0.8943849119022821\n",
      "train loss:0.8625809064456443\n",
      "train loss:0.8022716041927535\n",
      "train loss:0.8399332276030473\n",
      "train loss:0.9544471038226362\n",
      "train loss:0.8553402890433891\n",
      "train loss:0.8008035798620875\n",
      "train loss:0.9130826905041093\n",
      "train loss:1.0270256156426951\n",
      "train loss:0.8969730279220061\n",
      "train loss:0.8116682719470448\n",
      "train loss:0.7252647451699842\n",
      "train loss:0.7069307028703191\n",
      "train loss:0.7251968402843098\n",
      "train loss:1.0550483524909549\n",
      "train loss:0.8924894508861734\n",
      "train loss:0.6856496317032397\n",
      "train loss:0.7832136120117879\n",
      "train loss:0.7524850609354391\n",
      "train loss:0.8246676761604942\n",
      "train loss:0.874694846973138\n",
      "train loss:0.8738287828794671\n",
      "train loss:0.8553498066307152\n",
      "train loss:0.6556181665640167\n",
      "train loss:0.8099425125396524\n",
      "train loss:0.7694849610589823\n",
      "train loss:0.8633279840030648\n",
      "train loss:0.9592602793577484\n",
      "train loss:0.8448669429641917\n",
      "train loss:1.023044577884502\n",
      "train loss:0.8356470402043458\n",
      "train loss:0.7907682385115102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.7654497035207822\n",
      "train loss:0.7827395332305261\n",
      "train loss:0.8427305401071864\n",
      "train loss:0.8623738293871328\n",
      "train loss:0.8622736139124263\n",
      "train loss:0.762617349720117\n",
      "train loss:0.816216178008736\n",
      "train loss:0.7425902651467573\n",
      "train loss:0.8822086057893174\n",
      "train loss:0.8850655564393949\n",
      "train loss:0.7091236255358747\n",
      "train loss:0.9949148231188428\n",
      "train loss:0.9069387909028558\n",
      "train loss:1.0725364148764236\n",
      "train loss:0.8143489780614344\n",
      "train loss:0.8334560033467645\n",
      "train loss:0.9034604408652257\n",
      "train loss:0.894062148705832\n",
      "train loss:0.8079258441363861\n",
      "train loss:0.8151195661624646\n",
      "train loss:0.8477131791994305\n",
      "train loss:0.8987882453019546\n",
      "train loss:0.9838272260262856\n",
      "train loss:0.862764262990261\n",
      "train loss:0.7942785262163318\n",
      "train loss:0.7860910131307017\n",
      "train loss:0.9022605893102515\n",
      "train loss:0.8702677628573261\n",
      "train loss:0.7078949061195259\n",
      "train loss:1.027083342594264\n",
      "train loss:0.8735343525912875\n",
      "train loss:0.7834405708442831\n",
      "train loss:0.9123014320468709\n",
      "train loss:0.8634554552147584\n",
      "train loss:0.854188536398707\n",
      "train loss:0.8565765307403578\n",
      "train loss:0.9012286249044341\n",
      "train loss:0.8838754414154285\n",
      "train loss:0.8093662224942265\n",
      "train loss:0.8978409574446189\n",
      "train loss:0.903919583048837\n",
      "train loss:0.7858364946280445\n",
      "train loss:0.9623699490374126\n",
      "train loss:0.6955236039507359\n",
      "train loss:0.8314505954886614\n",
      "train loss:0.7979094429724256\n",
      "train loss:0.6694479522463294\n",
      "train loss:0.811999766254526\n",
      "train loss:0.9614857972995362\n",
      "train loss:0.9892590768979899\n",
      "train loss:1.0116431056763127\n",
      "train loss:0.6089198570536366\n",
      "train loss:0.8657032900989833\n",
      "train loss:0.7904171745238843\n",
      "train loss:0.918635949200468\n",
      "train loss:0.9565979995134947\n",
      "train loss:0.8945817054964409\n",
      "train loss:0.9858477637343624\n",
      "train loss:0.9948140221661113\n",
      "train loss:0.9610884958671281\n",
      "train loss:0.786263915379354\n",
      "train loss:0.9817570601631904\n",
      "train loss:0.9519176601444456\n",
      "train loss:0.9669804758573336\n",
      "train loss:0.7625327275969082\n",
      "train loss:1.0486924121725842\n",
      "train loss:0.9268935081994488\n",
      "train loss:0.8544632017289183\n",
      "train loss:0.8157943662317383\n",
      "train loss:1.0095218708797915\n",
      "train loss:0.8292392193690006\n",
      "train loss:0.6887374128737188\n",
      "train loss:0.7588207461245912\n",
      "train loss:0.9565889799078786\n",
      "train loss:0.7305597523665592\n",
      "train loss:0.7367225622639553\n",
      "train loss:0.8561064480804265\n",
      "train loss:0.9389091936462425\n",
      "train loss:0.8304093718482386\n",
      "train loss:0.9930077124007289\n",
      "train loss:0.7917416259822274\n",
      "train loss:0.9572471195770089\n",
      "train loss:0.9566606453208883\n",
      "train loss:0.8659671751075296\n",
      "train loss:0.8838176027697443\n",
      "train loss:0.9031002470592745\n",
      "train loss:0.9064913452548551\n",
      "train loss:0.9046470077756124\n",
      "train loss:0.8214056935540134\n",
      "train loss:0.9255968179812162\n",
      "train loss:0.7829432713561755\n",
      "train loss:0.8031171311848638\n",
      "train loss:0.7584171781025519\n",
      "train loss:0.8700400564609937\n",
      "train loss:0.8038030400601981\n",
      "train loss:0.7740707688201369\n",
      "train loss:0.9194018013426378\n",
      "train loss:0.7385238833012724\n",
      "train loss:0.6797244909097668\n",
      "train loss:1.0269687620007888\n",
      "train loss:0.9646112386997849\n",
      "train loss:0.96562508395398\n",
      "train loss:0.8736930299729038\n",
      "train loss:0.9527086113726575\n",
      "train loss:0.7885384072629839\n",
      "train loss:0.8246638326819431\n",
      "train loss:0.9351412686838146\n",
      "train loss:0.9218835946483072\n",
      "train loss:0.8220259182000577\n",
      "train loss:0.7384314719013826\n",
      "train loss:0.89640548458947\n",
      "train loss:0.9072548144614008\n",
      "train loss:0.8798836908654094\n",
      "train loss:0.9562994246133552\n",
      "train loss:0.8877460877842762\n",
      "train loss:0.8688283030307369\n",
      "train loss:0.8556147816349682\n",
      "train loss:0.839174497346355\n",
      "train loss:0.738489512585618\n",
      "train loss:0.8928199684095497\n",
      "train loss:0.8844450811194433\n",
      "train loss:0.8300513052433992\n",
      "train loss:0.8197772032180864\n",
      "train loss:0.9617991317060439\n",
      "train loss:1.01245260334838\n",
      "train loss:0.84593945342513\n",
      "train loss:0.8279828936323791\n",
      "train loss:0.8928489273295651\n",
      "train loss:0.8439125848438394\n",
      "train loss:0.7755467693370036\n",
      "train loss:1.0125983160909136\n",
      "train loss:0.8763428694464451\n",
      "train loss:0.859596035695121\n",
      "train loss:0.799307432458163\n",
      "train loss:0.7882301352315647\n",
      "train loss:0.9293028527664898\n",
      "train loss:0.8887693547675717\n",
      "train loss:0.982872894327121\n",
      "train loss:0.7849846985271753\n",
      "train loss:0.825853458991577\n",
      "train loss:0.9444243391175129\n",
      "train loss:0.7701342261495552\n",
      "train loss:0.8151409978593044\n",
      "train loss:0.7532550285438134\n",
      "train loss:0.8487857763533776\n",
      "train loss:0.9491441110671136\n",
      "train loss:0.8502977219685448\n",
      "train loss:0.865014526894291\n",
      "train loss:0.9845515969813095\n",
      "train loss:0.8658247990903982\n",
      "train loss:0.892768029932932\n",
      "train loss:0.901834363638974\n",
      "train loss:0.9337984955857077\n",
      "train loss:0.7960538235428484\n",
      "train loss:0.9265586924784072\n",
      "train loss:0.8253880966838811\n",
      "train loss:0.8176380361256961\n",
      "train loss:1.0368024915431162\n",
      "train loss:0.9556425847434887\n",
      "train loss:0.8274996894954176\n",
      "train loss:0.9205422875750466\n",
      "train loss:0.8141849034109245\n",
      "train loss:0.8207231063827196\n",
      "train loss:0.9436246707290062\n",
      "train loss:0.6963681625169749\n",
      "train loss:0.8745219600998203\n",
      "train loss:0.7750834965807227\n",
      "train loss:0.9985293695068477\n",
      "train loss:0.8511641237646685\n",
      "train loss:0.8393419745731563\n",
      "train loss:0.9481520673198852\n",
      "train loss:0.807267829510381\n",
      "train loss:0.7435860032005741\n",
      "train loss:0.9245741355441365\n",
      "train loss:0.5914061600947094\n",
      "train loss:1.0333100328584115\n",
      "train loss:0.8114496241071996\n",
      "train loss:0.7763509270922054\n",
      "train loss:0.836811685900228\n",
      "train loss:0.8428334195991605\n",
      "train loss:0.9718317198753131\n",
      "train loss:1.0392585101788778\n",
      "train loss:1.1227700448910192\n",
      "train loss:0.8491425503203842\n",
      "train loss:0.8763884724258527\n",
      "train loss:0.8348860431984148\n",
      "train loss:0.8660464742235567\n",
      "train loss:0.7539290209518837\n",
      "train loss:0.9132179127052286\n",
      "train loss:0.7243931830320858\n",
      "train loss:0.782037103845874\n",
      "train loss:0.912841671059249\n",
      "train loss:0.9654424471208561\n",
      "train loss:0.8119458182374618\n",
      "train loss:0.7759429259593825\n",
      "train loss:0.9434458509090762\n",
      "train loss:0.7704355371400926\n",
      "train loss:0.8948868481204052\n",
      "train loss:0.9322006183168877\n",
      "train loss:0.8347731818958785\n",
      "train loss:0.8468628757530494\n",
      "train loss:0.8889655021135162\n",
      "train loss:1.0902611888537477\n",
      "train loss:1.0079841293052534\n",
      "train loss:0.8136689637914206\n",
      "train loss:0.9513538088312979\n",
      "train loss:0.8726772641139209\n",
      "train loss:0.8330059449426713\n",
      "train loss:0.7577499762281218\n",
      "train loss:0.954030600388267\n",
      "train loss:0.9196181377410108\n",
      "train loss:0.8745049899034717\n",
      "train loss:1.0035238711978292\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.994\n",
      "Saved Network Parameters!\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 为了导入父目录而进行的设定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "#from deep_convnet import DeepConvNet\n",
    "from common.trainer import Trainer\n",
    "\n",
    "# 开始时间 2020/30/12 16:00\n",
    "#\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "network = DeepConvNet()  \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=20, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr':0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# 保存参数\n",
    "network.save_params(\"deep_convnet_params.pkl\")\n",
    "print(\"Saved Network Parameters!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.2 深度学习的历史\n",
    "# ImageNet\n",
    "\n",
    "# VGG\n",
    "# GoogLeNet\n",
    "# ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.3 深度学习的高速化\n",
    "\n",
    "# GPU\n",
    "# 分布式运算\n",
    "\n",
    "# 位数缩减"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.4 深度学习应用案例\n",
    "\n",
    "# NIC\n",
    "# 多模态处理\n",
    "# 风格迁移\n",
    "# GAN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
